[MISSING_PAGE_EMPTY:1603]

_Springer Texts in Statistics (STS)_ includes advanced textbooks from 3rd- to 4th-year undergraduate courses to 1st- to 2nd-year graduate courses. Exercise sets should be included. The series editors are currently Genevera I. Allen, Richard D. De Veaux, and Rebecca Nugent. Stephen Fienberg, George Casella, and Ingram Olkin were editors of the series for many years.

More information about this series at [https://link.springer.com/bookseries/417](https://link.springer.com/bookseries/417)

[MISSING_PAGE_EMPTY:1606]

[MISSING_PAGE_FAIL:5]

Dedicated to the Memory of

Erich L. Lehmann (1917-2009)

## Preface to the Fourth Edition

The fourth edition of _Testing Statistical Hypotheses_ provides a significant update to the third edition, which appeared in 2005. The storied history of the first two editions is recounted in DeGroot (1986) and Lehmann (1997), and an account of the third edition is given in Romano (2012).

In order to accommodate new topics, one principal change from the third edition is the expansion of the book into two volumes. Volume I (Chapters 1-10) treats finite-sample theory, while Volume II (Chapters 11-18) treats asymptotic theory. A major addition to the treatment of finite-sample theory is a more expansive chapter (Chapter 9) on multiple hypothesis testing, including such topics as: the closure method, the false discovery rate, and other generalized error rates. A new section on the principle of monotonicity is included in Chapter 8. The asymptotic theory presented in Volume II has been reorganized. Chapter 12 covers asymptotic methods for sums of dependent variables and their application to inference. In particular, Chapter 12 includes: limit theorems for random sampling without replacement; some theory of U-statistics; central limit theorems for stationary, mixing processes; and Stein's method. Chapter 13 includes an introduction to high-dimensional testing; see Section 13.5. The asymptotic theory of permutation and randomization tests is now expanded to its own chapter, Chapter 17, largely driven by the resurgence of such methodology. There are over 100 new problems in the volumes, bringing the total to around 900.

Except for some of the basic results from Volume I, a detailed knowledge of finite-sample theory is not required for Volume II. In particular, the necessary background should include Chapter 3, Sections 3.1-3.5, 3.8-3.9; Chapter 4, Sections 4.1-4.4; Chapter 5, Sections 5.1-5.3; Chapter 6, Sections 6.1-6.2; Chapter 7, Sections 7.1-7.2; Chapter 8, Sections 8.1-8.2, 8.4-8.5; and Chapter 9, Sections 9.1-9.3. These sections could form the basis of a one-semester course in hypothesis testing.

A second course could begin with fundamental concepts of asymptotic theory presented in Chapter 11. For the reader with a background including a rigorous course in probability theory, Chapter 11 serves as a review of some important theorems and tools. From there, one could focus on the large-sample properties of some fundamental methods, such as the one- and two-sample Wilcoxon tests in Chapter 12,or the robustness of some classical parametric tests in Chapter 13. The sections in Chapters 12 and 13 are largely independent of each other. Chapter 14 requires careful study so that the concepts and theorems developed there can be applied to the theory of large-sample optimality presented in Chapter 15. Chapters 17 and 18 provide the foundations for computer-intensive methods that remain vital in contemporary statistical research.

With great admiration, I dedicate the present volumes to the memory of Erich Lehmann. After working with Erich during the several years leading to the third edition, my memories of Erich can be summed up in some of his qualities that I most respect: humility, generosity, professionalism, and clarity of thought. Despite Erich's passing in 2009, the fourth edition is highly influenced by Erich's philosophy and renowned scholarship. Every decision on the fourth edition, including content as well as the decision to divide the work in two volumes, was based on what I believed Erich would have wanted. Those interested in Erich's life should read the fine remembrances in Rojo (2011) and van Zwet (2011), as well as Erich's own account in Lehmann (2008). Needless to say, Erich will always be remembered as a founding father of mathematical statistics.

Special thanks go to those who provided helpful support by proofreading drafts and offering constructive suggestions. They include: Thomas DiCiccio, Wenge Guo, Bala Rajaratnam, David Ritzwoller (especially for help with the figures), Azeem Shaikh, Marius Tirlea, Michael Wolf, and the many students at Stanford University who proofread new sections and worked through many of the problems. Finally, heartfelt thanks go to friends and family who provided continual encouragement, especially my partner Frank Adair, Ann Marie and Mark Hodges, Anna and Kirk Warshaw, Theresa Montagna, David Fogle, Scott Madover, Tom Neville, and last but not least, my mother.

Joseph P. Romano August 2020

Stanford University

Stanford, CA

USA

###### Contents

* 1 The General Decision Problem
	* 1.1 Statistical Inference and Statistical Decisions
	* 1.2 Specification of a Decision Problem
	* 1.3 Randomization; Choice of Experiment
	* 1.4 Optimum Procedures
	* 1.5 Invariance and Unbiasedness
	* 1.6 Bayes and Minimax Procedures
	* 1.7 Maximum Likelihood
	* 1.8 Complete Classes
	* 1.9 Sufficient Statistics
* 1.10 Problems
* 1.11 Notes
* 2 The Probability Background
	* 2.1 Probability and Measure
	* 2.2 Integration
	* 2.3 Statistics and Subfields
	* 2.4 Conditional Expectation and Probability
	* 2.5 Conditional Probability Distributions
	* 2.6 Characterization of Sufficiency
	* 2.7 Exponential Families
	* 2.8 Problems
	* 2.9 Notes
* 3 Uniformly Most Powerful Tests
	* 3.1 Stating the Problem
	* 3.2 The Neyman-Pearson Fundamental Lemma
	* 3.3 \(p\)-values
	* 3.4 Distributions with Monotone Likelihood Ratio
	* 3.5 Confidence Bounds

	* 3.6 A Generalization of the Fundamental Lemma
	* 3.7 Two-Sided Hypotheses
	* 3.8 Least Favorable Distributions
	* 3.9 Applications to Normal Distributions
		* 3.9.1 Univariate Normal Models
		* 3.9.2 Multivariate Normal Models
* 3.10 Problems
* 3.11 Notes
* 4Unbiasedness: Theory and First Applications
	* 4.1 Unbiasedness for Hypothesis Testing
	* 4.2 One-Parameter Exponential Families
	* 4.3 Similarity and Completeness
	* 4.4 UMP Unbiased Tests for Multiparameter Exponential Families
	* 4.5 Comparing Two Poisson or Binomial Populations
	* 4.6 Testing for Independence in a 2 x 2 Table
	* 4.7 Alternative Models for 2 x 2 Tables
	* 4.8 Some Three-Factor Contingency Tables
	* 4.9 The Sign Test
* 4.10 Problems
* 4.11 Notes
* 5Unbiasedness: Applications to Normal Distributions; Confidence Intervals
	* 5.1 Statistics Independent of a Sufficient Statistic
	* 5.2 Testing the Parameters of a Normal Distribution
	* 5.3 Comparing the Means and Variances of Two Normal Distributions
	* 5.4 Confidence Intervals and Families of Tests
	* 5.5 Unbiased Confidence Sets
	* 5.6 Regression
	* 5.7 Bayesian Confidence Sets
	* 5.8 Permutation Tests
	* 5.9 Most Powerful Permutation Tests
* 5.10 Randomization as a Basis For Inference
* 5.11 Permutation Tests and Randomization
* 5.12 Randomization Model and Confidence Intervals
* 5.13 Testing for Independence in a Bivariate Normal Distribution
* 5.14 Problems
* 5.15 Notes

[MISSING_PAGE_EMPTY:11]

	* 9.2 The Closure Method
		* 9.2.1 The Basic Method and Some Examples
		* 9.2.2 Simes' Identity and Hommel's Method
		* 9.2.3 The Higher Criticism and Other Joint Tests
		* 9.2.4 Coherence and Cosonance
	* 9.3 False Discovery Rate and Other Generalized Error Rates
		* 9.3.1 Introduction to Various Error Rates
		* 9.3.2 False Discovery Rate
		* 9.3.3 Control of the \(k\)-FWER
		* 9.3.4 Control of the False Discovery Proportion
	* 9.4 Maximin Procedures
	* 9.5 The Hypothesis of Homogeneity
	* 9.6 Scheffe's \(S\)-Method: A Special Case
	* 9.7 Scheffe's \(S\)-Method for General Linear Models
	* 9.8 Problems
	* 9.9 Notes
* 10 Conditional Inference
	* 10.1 Mixtures of Experiments
	* 10.2 Ancillary Statistics
	* 10.3 Optimal Conditional Tests
	* 10.4 Relevant Subsets
	* 10.5 Problems
	* 10.6 Notes

**Volume II Asymptotic Theory**

**11 Basic Large-Sample Theory** * 11.1 Introduction * 11.2 Weak Convergence and Central Limit Theorems * 11.3 Convergence in Probability and Applications * 11.4 Almost Sure Convergence * 11.5 Problems * 11.6 Notes
* 12 Extensions of the CLT to Sums of Dependent Random Variables
	* 12.1 Introduction
	* 12.2 Random Sampling Without Replacement from a Finite Population
	* 12.3 U-Statistics
	* 12.4 Stationary Mixing Processes
	* 12.5 Stein's Method
	* 12.6 Problems
	* 12.7 Notes
* 13 Applications to Inference
	* 13.1 Introduction
	* 13.2 Robustness of Some Classical Tests
		* 13.2.1 Effect of Distribution
		* 13.2.2 Effect of Dependence
		* 13.2.3 Robustness in Linear Models
	* 13.3 Edgeworth Expansions
	* 13.4 Nonparametric Inference for the Mean
		* 13.4.1 Uniform Behavior of \(t\)-test
		* 13.4.2 A Result of Bahadur and Savage
		* 13.4.3 Alternative Tests
	* 13.5 Testing Many Means: The Gaussian Sequence Model
		* 13.5.1 Chi-Squared Test
		* 13.5.2 Maximin Test for Sparse Alternatives
		* 13.5.3 Test Based on Maximum and Bonferroni
		* 13.5.4 Some Comparisons and the Higher Criticism
	* 13.6 Problems
	* 13.7 Notes
* 14 Quadratic Mean Differentiable Families
	* 14.1 Introduction
	* 14.2 Quadratic Mean Differentiability (q.m.d.)
	* 14.3 Contiguity
	* 14.4 Likelihood Methods in Parametric Models
		* 14.4.1 Efficient Likelihood Estimation
		* 14.4.2 Wald Tests and Confidence Regions
		* 14.4.3 Rao Score Tests
		* 14.4.4 Likelihood Ratio Tests
	* 14.5 Problems
	* 14.6 Notes
* 15 Large-Sample Optimality
	* 15.1 Testing Sequences, Metrics, and Inequalities
	* 15.2 Asymptotic Relative Efficiency
	* 15.3 AUMP Tests in Univariate Models
	* 15.4 Asymptotically Normal Experiments
	* 15.5 Applications to Parametric Models
		* 15.5.1 One-Sided Hypotheses
		* 15.5.2 Equivalence Hypotheses
		* 15.5.3 Multisided Hypotheses
	* 15.6 Applications to Nonparametric Models
		* 15.6.1 Nonparametric Mean
		* 15.6.2 Nonparametric Testing of Functionals
	* 15.7 Problems
	* 15.8 Notes

[MISSING_PAGE_FAIL:14]

[MISSING_PAGE_EMPTY:1617]

[MISSING_PAGE_EMPTY:1618]

## Chapter 1 The General Decision Problem

### 1.1 Statistical Inference and Statistical Decisions

The raw material of a statistical investigation is a set of observations; these are the values taken on by random variables \(X\) whose distribution \(P_{\theta}\) is at least partly unknown. Of the parameter \(\theta\), which labels the distribution, it is assumed known only that it lies in a certain set \(\Omega\), the _parameter space_. _Statistical inference_ is concerned with methods of using this observational material to obtain information concerning the distribution of \(X\) or the parameter \(\theta\) with which it is labeled. To arrive at a more precise formulation of the problem we shall consider the purpose of the inference.

The need for statistical analysis stems from the fact that the distribution of \(X\), and hence some aspect of the situation underlying the mathematical model, is not known. The consequence of such a lack of knowledge is uncertainty as to the best mode of behavior. To formalize this, suppose that a choice has to be made between a number of alternative actions. The observations, by providing information about the distribution from which they came, also provide guidance as to the best decision. The problem is to determine a rule which, for each set of values of the observations, specifies what decision should be taken. Mathematically such a rule is a function \(\delta\), which to each possible value \(x\) of the random variables assigns a decision \(d=\delta(x)\), that is, a function whose domain is the set of values of \(X\) and whose range is the set of possible decisions.

In order to see how \(\delta\) should be chosen, one must compare the consequences of using different rules. To this end suppose that the consequence of taking decision \(d\) when the distribution of \(X\) is \(P_{\theta}\) is a _loss_, which can be expressed as a nonnegative real number \(L(\theta,d)\). Then the long-term average loss that would result from the use of \(\delta\) in a number of repetitions of the experiment is the expectation \(E[L(\theta,\delta(X))]\) evaluated under the assumption that \(P_{\theta}\) is the true distribution of \(X\). This expectation, which depends on the decision rule \(\delta\) and the distribution \(P_{\theta}\), is called the _risk function_ of \(\delta\) and will be denoted by \(R(\theta,\delta)\). By basing the decision on the observations, the original problem of choosing a decision \(d\) with loss function \(L(\theta,d)\) is thus replaced by that of choosing \(\delta\), where the loss is now \(R(\theta,\delta)\).

The above discussion suggests that the aim of statistics is the selection of a decision function which minimizes the resulting risk. As will be seen later, this statement of aims is not sufficiently precise to be meaningful; its proper interpretation is in fact one of the basic problems of the theory.

### Specification of a Decision Problem

The methods required for the solution of a specific statistical problem depend quite strongly on the three elements that define it: the class \(\mathcal{P}=\{P_{\theta},\theta\in\Omega\}\) to which the distribution of \(X\) is assumed to belong; the structure of the space \(D\) of possible decisions \(d\); and the form of the loss function \(L\). In order to obtain concrete results it is therefore necessary to make specific assumptions about these elements. On the other hand, if the theory is to be more than a collection of isolated results, the assumptions must be broad enough either to be of wide applicability or to define classes of problems for which a unified treatment is possible.

Consider first the specification of the class \(\mathcal{P}\). Precise numerical assumptions concerning probabilities or probability distributions are usually not warranted. However, it is frequently possible to assume that certain events have equal probabilities and that certain other are statistically independent. Another type of assumption concerns the relative order of certain infinitesimal probabilities, for example, the probability of occurrences in an interval of time or space as the length of the internal tends to zero. The following classes of distributions are derived on the basis of only such assumptions, and are therefore applicable in a great variety of situations.

The _binomial_ distribution \(b(p,n)\) with

\[P(X=x)=\binom{n}{x}p^{x}(1-p)^{n-x},\ \ \ \ \ x=0,\ldots,n.\ \ \ 0\leq p\leq 1. \tag{1.1}\]

This is the distribution of the total number of successes in \(n\) independent trials when the probability of success for each trial is \(p\).

The _Poisson_ distribution \(P(\tau)\) with

\[P(X=x)=\frac{\tau^{x}}{x!}e^{-\tau},\ \ \ \ \ \ x=0,1,\ldots,\ \ \ 0<\tau. \tag{1.2}\]

This is the distribution of the number of events occurring in a fixed interval of time or space if the probability of more than one occurrence in a very short interval is of smaller order of magnitude than that of a single occurrence, and if the numbers of events in nonoverlapping intervals are statistically independent. Under these assumptions, the process generating the events is called a _Poisson process_. Such processes are discussed, for example, in the books by Feller (1968), Ross (1996), and Taylor and Karlin (1998).

The _normal_ distribution \(N(\xi,\sigma^{2})\) with probability density

\[p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{1}{2\sigma^{2}}(x-\xi)^{2} \right],\qquad-\infty<x,\,\xi<\infty,\quad 0<\sigma. \tag{1.3}\]

Under very general conditions, which are made precise by the central limit theorem, this is the approximate distribution of the sum of a large number of independent random variables when the relative contribution of each term to the sum is small.

We consider next the structure of the decision space \(D\). The great variety of possibilities is indicated by the following examples.

**Example 1.2.1**: Let \(X_{1},\ldots,X_{n}\) be a _sample_ from one of the distributions (1.1)-(1.3), that is let the \(X\)'s be distributed independently and identically according to one of these distributions. Let \(\theta\) be \(p\), \(\tau\), or the pair \((\xi,\sigma)\), respectively, and let \(\gamma=\gamma(\theta)\) be a real-valued function of \(\theta\).

(i) If one wishes to decide whether or not \(\gamma\) exceeds some specified value \(\gamma_{0}\), the choice lies between the two decisions \(d_{0}:\gamma>\gamma_{0}\) and \(d_{1}:\gamma\leq\gamma_{0}\). In specific applications these decisions might correspond to the acceptance or rejection of a lot of manufactured goods, of an experimental airplane as ready for flight testing, of a new treatment as an improvement over a standard one, and so on. The loss function of course depends on the application to be made. Typically, the loss is \(0\) if the correct decision is chosen, while for an incorrect decision the losses \(L(\gamma,d_{0})\) and \(L(\gamma,d_{1})\) are increasing functions of \(|\gamma-\gamma_{0}|\).

(ii) At the other end of the scale is the much more detailed problem of obtaining a numerical estimate of \(\gamma\). Here a decision \(d\) of the statistician is a real number, the estimate of \(\gamma\), and the losses might be \(L(\gamma,d)=v(\gamma)w(|d-\gamma|)\), where \(w\) is a strictly increasing function of the error \(|d-\gamma|\).

(iii) An intermediate case is the choice between the three alternatives \(d_{0}:\gamma<\gamma_{0}\), \(d_{1}:\gamma>\gamma_{1}\), \(d_{2}:\gamma_{0}\leq\gamma\leq\gamma_{1}\), for example accepting a new treatment, rejecting it, or recommending it for further study. \(\blacksquare\)

The distinction illustrated by this example is the basis for one of the principal classifications of statistical methods. Two-decision problems such as (i) are usually formulated in terms of _testing a hypothesis_ which is to be accepted or rejected (see Chapter 3). It is the theory of this class of problems with which we shall be mainly concerned here. The other principal branch of statistics is the theory of _point estimation_ dealing with problems such as (ii). This is the subject of Lehmann and Casella (1998). The intermediate problem (iii) is a special case of a _multiple decision procedure_. Some problems of this kind are treated in Ferguson (Ferguson 1967, Chapter 6); a discussion of some others is given in Chapter 9.

**Example 1.2.2**: Suppose that the data consist of samples \(X_{ij}\), \(j=1,\ldots,n_{i}\), from normal populations \(N(\xi_{i},\sigma^{2})\), \(i=1,\ldots,s\).

(i) Consider first the case \(s=2\) and the question of whether or not there is a material difference between the two populations. This has the same structure as problem (iii) of the previous example. Here the choice lies between the three decisions\(d_{0}:|\xi_{2}-\xi_{1}|\leq\Delta\), \(d_{1}:\xi_{2}>\xi_{1}+\Delta\), \(d_{2}:\xi_{2}<\xi_{1}-\Delta\), where \(\Delta\) is preassigned. An analogous problem, involving \(k+1\) possible decisions, occurs in the general case of \(k\) populations. In this case one must choose between the decision that the \(k\) distributions do not differ materially, \(d_{0}:\max|\xi_{j}-\xi_{i}|\leq\Delta\), and the decisions \(d_{k}:\max|\xi_{j}-\xi_{i}|>\Delta\) and \(\xi_{k}\) is the largest of the means.

(ii) A related problem is that of ranking the distributions in increasing order of their mean \(\xi\).

(iii) Alternatively, a standard \(\xi_{0}\) may be given and the problem is to decide which, if any, of the population means exceed the standard. \(\blacksquare\)

**Example 1.2.3**: Consider two distributions--to be specific, two Poisson distributions \(P(\tau_{1})\), \(P(\tau_{2})\)--and suppose that \(\tau_{1}\) is known to be less than \(\tau_{2}\) but that otherwise the \(\tau\)'s are unknown. Let \(Z_{1},\ldots,Z_{n}\) be independently distributed, each according to either \(P(\tau_{1})\) or \(P(\tau_{2})\). Then each \(Z\) is to be classified as to which of the two distributions it comes from. Here the loss might be the number of \(Z\)'s that are incorrectly classified, multiplied by a suitable function of \(\tau_{1}\) and \(\tau_{2}\). An example of the complexity that such problems can attain and the conceptual as well as mathematical difficulties that they may involve is provided by the efforts of anthropologists to classify the human population into a number of homogeneous races by studying the frequencies of the various blood groups and of other genetic characters. \(\blacksquare\)

All the problems considered so far could be termed _action problems_. It was assumed in all of them that if \(\theta\) were known a unique correct decision would be available, that is, given any \(\theta\), there exists a unique \(d\) for which \(L(\theta,d)=0\). However, not all statistical problems are so clear-cut. Frequently it is a question of providing a convenient summary of the data or indicating what information is available concerning the unknown parameter or distribution. This information will be used for guidance in various considerations but will not provide the sole basis for any specific decisions. In such cases the emphasis is on the inference rather than on the decision aspect of the problem. Although formally it can still be considered a decision problem if the inferential statement itself is interpreted as the decision to be taken, the distinction is of conceptual and practical significance despite the fact that frequently it is ignored.1 An important class of such problems, estimation by interval, is illustrated by the following example. (For the more usual formulation in terms of confidence intervals, see Sections 3.5, 5.4 and 5.5.)

Footnote 1: For a more detailed discussion of this distinction see, for example, Cox (1958), Blyth (1970), Barnett (1999).

**Example 1.2.4**: Let \(X=(X_{1},\ldots,X_{n})\) be a sample from \(N(\xi,\sigma^{2})\) and let a decision consist in selecting an interval \([\underline{L},L]\) and stating that it contains \(\xi\). Suppose that decision procedures are restricted to intervals \([\underline{L}(X),\tilde{L}(X)]\) whose expected length for all \(\xi\) and \(\sigma\) does not exceed \(k\sigma\) where \(k\) is some preassigned constant. An appropriate loss function would be \(0\) if the decision is correct and would otherwise depend on the relative position of the interval to the true value of \(\xi\). In this case there are many correct decisions corresponding to a given distribution \(N(\xi,\sigma^{2})\). \(\blacksquare\)It remains to discuss the choice of loss function, and of the three elements defining the problem this is perhaps the most difficult to specify. Even in the simplest case, where all losses eventually reduce to financial ones, it can hardly be expected that one will be able to evaluate all the short- and long-term consequences of an action. Frequently it is possible to simplify the formulation by taking into account only certain aspects of the loss function. As an illustration consider Example 1.2.1(i) and let \(L(\theta,d_{0})=a\) for \(\gamma(\theta)\leq\gamma_{0}\) and \(L(\theta,d_{1})=b\) for \(\gamma(\theta)>\gamma_{0}\). The risk function becomes

\[R(\theta,\delta)=\left\{\begin{array}{ll}a\,P_{\theta}\{\delta(X)=d_{0}\} \text{ if }&\gamma\leq\gamma_{0},\\ b\,P_{\theta}\{\delta(X)=d_{1}\}\text{ if }&\gamma>\gamma_{0},\end{array}\right. \tag{1.4}\]

and is seen to involve only the two probabilities of error, with weights which can be adjusted according to the relative importance of these errors. Similarly, in Example 1.2.3, one may wish to restrict attention to the number of misclassifications.

Unfortunately, such a natural simplification is not always available, and in the absence of specific knowledge it becomes necessary to select the loss function in some conventional way, with mathematical simplicity usually an important consideration. In point estimation problems such as that considered in Example 1.2.1(ii), if one is interested in estimating a real-valued function \(\gamma=\gamma(\theta)\), it is customary to take the square of the error, or somewhat more generally to put

\[L(\theta,d)=v(\theta)(d-\gamma)^{2}. \tag{1.5}\]

Besides being particularly simple mathematically, this can be considered as an approximation to the true loss function \(L\) provided that for each fixed \(\theta\), \(L(\theta,d)\) is twice differentiable in \(d\), that \(L(\theta,\gamma(\theta))=0\) for all \(\theta\), and that the error is not large.

It is frequently found that, within one problem, quite different types of losses may occur, which are difficult to measure on a common scale. Consider once more Example 1.2.1(i) and suppose that \(\gamma_{0}\) is the value of \(\gamma\) when a standard treatment is applied to a situation in medicine, agriculture, or industry. The problem is that of comparing some new process with unknown \(\gamma\) to the standard one. Turning down the new method when it is actually superior, or adopting it when it is not, clearly entails quite different consequences. In such cases it is sometimes convenient to treat the various loss components, say \(L_{1}\), \(L_{2}\),..., \(L_{r}\), separately. Suppose in particular that \(r=2\) and the \(L_{1}\) represents the more serious possibility. One can then assign a bound to this risk component, that is, impose the condition

\[EL_{1}(\theta,\delta(X))\leq\alpha, \tag{1.6}\]

and subject to this condition minimize the other component of the risk. Example 1.2.4 provides an illustration of this procedure. The length of the interval \([\underline{L},\,\tilde{L}]\) (measured in \(\sigma\)-units) is one component of the loss function, the other being the loss that results if the interval does not cover the true \(\xi\).

### 1.3 Randomization; Choice of Experiment

The description of the general decision problem given so far is still too narrow in certain respects. It has been assumed that for each possible value of the random variables a definite decision must be chosen. Instead, it is convenient to permit the selection of one out of a number of decisions according to stated probabilities, or more generally the selection of a decision according to a probability distribution defined over the decision space; which distribution depends of course on what \(X\) is observed. One way to describe such a randomized procedure is in terms of a nonrandomized procedure depending on \(X\) and a random variable \(Y\) whose values lie in the decision space and whose conditional distribution given \(X\) is independent of \(\theta\).

Although it may run counter to one's intuition that such extra randomization should have any value, there is no harm in permitting this greater freedom of choice. If the intuitive misgivings are correct, it will turn out that the optimum procedures always are of the simple nonrandomized kind. Actually, the introduction of randomized procedures leads to an important mathematical simplification by enlarging the class of risk functions so that it becomes convex. In addition, there are problems in which some features of the risk function such as its maximum can be improved by using a randomized procedure.

Another assumption that tacitly has been made so far is that a definite experiment has already been decided upon so that it is known what observations will be taken. However, the statistical considerations involved in designing an experiment are no less important than those concerning its analysis. One question in particular that must be decided before an investigation is undertaken is how many observations should be taken so that the risk resulting from wrong decisions will not be excessive. Frequently it turns out that the required sample size depends on the unknown distribution and therefore cannot be determined in advance as a fixed number. Instead it is then specified as a function of the observations and the decision whether or not to continue experimentation is made _sequentially_ at each stage of the experiment on the basis of the observations taken up to that point.

**Example 1.3.1**: On the basis of a sample \(X_{1},\ldots,X_{n}\) from a normal distribution \(N(\xi,\sigma^{2})\) one wishes to estimate \(\xi\). Here the risk function of an estimate, for example its expected squared error, depends on \(\sigma\). For large \(\sigma\) the sample contains only little information in the sense that two distributions \(N(\xi_{1},\sigma^{2})\) and \(N(\xi_{2},\sigma^{2})\) with fixed difference \(\xi_{2}-\xi_{1}\) become indistinguishable as \(\sigma\to\infty\), with the result that the risk tends to infinity. Conversely, the risk approaches zero as \(\sigma\to 0\), since then effectively the mean becomes known. Thus the number of observations needed to control the risk at a given level is unknown. However, as soon as some observations have been taken, it is possible to estimate \(\sigma^{2}\) and hence to determine the additional number of observations required. \(\blacksquare\)

**Example 1.3.2**: In a sequence of trials with constant probability \(p\) of success, one wishes to decide whether \(p\leq\frac{1}{2}\) or \(p>\frac{1}{2}\). It will usually be possible to reach a decision at an early stage if \(p\) is close to 0 or 1 so that practically all observationsare of one kind, while a larger sample will be needed for intermediate values of \(p\). This difference may be partially balanced by the fact that for intermediate values a loss resulting from a wrong decision is presumably less serious than for the more extreme values. \(\blacksquare\)

**Example 1.3.3**: The possibility of determining the sample size sequentially is important not only because the distributions \(P_{\theta}\) can be more or less informative but also because the same is true of the observations themselves. Consider, for example, observations from the uniform distribution over the interval \((\theta-\frac{1}{2},\theta+\frac{1}{2})\) and the problem of estimating \(\theta\). Here there is no difference in the amount of information provided by the different distributions \(P_{\theta}\). However, a sample \(X_{1}\), \(X_{2}\),..., \(X_{n}\) can practically pinpoint \(\theta\) if \(\max|X_{j}-X_{i}|\) is sufficiently close to 1, or it can give essentially no more information than a single observation if \(\max|X_{j}-X_{i}|\) is close to 0. Again the required sample size should be determined sequentially. \(\blacksquare\)

Except in the simplest situations, the determination of the appropriate sample size is only one aspect of the design problem. In general, one must decide not only how many but also what kind of observations to take. In clinical trials, for example, when a new treatment is being compared with a standard procedure, a protocol is required which specifies to which of the two treatments each of the successive incoming patients is to be assigned. Formally, such questions can be subsumed under the general decision problem described at the beginning of the chapter, by interpreting \(X\) as the set of all available variables, by introducing the decisions whether or not to stop experimentation at the various stages, by specifying in case of continuance which type of variable to observe next, and by including the cost of observation in the loss function.

The determination of optimum sequential stopping rules and experimental designs is outside the scope of this book. An introduction to this subject is provided, for example, by Siegmund (1985).

### 1.4 Optimum Procedures

At the end of Section 1.1 the aim of statistical theory was stated to be the determination of a decision function \(\delta\) which minimizes the risk function

\[R(\theta,\,\delta)=E_{\theta}[L\,(\theta,\,\delta(X))]. \tag{1.7}\]

Unfortunately, in general the minimizing \(\delta\) depends on \(\theta\), which is unknown. Consider, for example, some particular decision \(d_{0}\), and the decision procedure \(\delta(x)\equiv d_{0}\) according to which decision \(d_{0}\) is taken regardless of the outcome of the experiment. Suppose that \(d_{0}\) is the correct decision for some \(\theta_{0}\), so that \(L(\theta_{0},\,d_{0})=0\). Then \(\delta\) minimizes the risk at \(\theta_{0}\) since \(R(\theta_{0},\,\delta)=0\), but presumably at the cost of a high risk for other values of \(\theta\).

In the absence of a decision function that minimizes the risk for all \(\theta\), the mathematical problem is still not defined, since it is not clear what is meant by a best procedure. Although it does not seem possible to give a definition of optimality that will be appropriate in all situations, the following two methods of approach frequently are satisfactory.

The nonexistence of an optimum decision rule is a consequence of the possibility that a procedure devotes too much of its attention to a single parameter value at the cost of neglecting the various other values that might arise. This suggests the restriction to decision procedures which possess a certain degree of impartiality, and the possibility that within such a restricted class there may exist a procedure with uniformly smallest risk. Two conditions of this kind, invariance and unbiasedness, will be discussed in the next section.

Instead of restricting the class of procedures, one can approach the problem somewhat differently. Consider the risk functions corresponding to two different decision rules \(\delta_{1}\) and \(\delta_{2}\). If \(R(\theta,\,\delta_{1})<R(\theta,\,\delta_{2})\) for all \(\theta\), then \(\delta_{1}\) is clearly preferable to \(\delta_{2}\), since its use will lead to a smaller risk no matter what the true value of \(\theta\) is. However, the situation is not clear when the two risk functions intersect as in Figure 1. What is needed is a principle which in such cases establishes a preference of one of the two risk functions over the other, that is, which introduces an ordering into the set of all risk functions. A procedure will then be optimum if its risk function is best according to this ordering. Some criteria that have been suggested for ordering risk functions will be discussed in Section 6.

A weakness of the theory of optimum procedures sketched above is its dependence on an extraneous restricting or ordering principle, and on knowledge concerning the loss function and the distributions of the observable random variables which in applications is frequently unavailable or unreliable. These difficulties, which may raise doubt concerning the value of an optimum theory resting on such shaky foundations, are in principle no different from those arising in any application of mathematics to reality. Mathematical formulations always involve simplification and approximation, so that solutions obtained through their use cannot be relied upon without additional checking. In the present case a check consists in an overall evaluation of the performance of the procedure that the theory produces, and an investigation of its sensitivity to departure from the assumptions under which it was derived.

The optimum theory discussed in this book should therefore not be understood to be prescriptive. The fact that a procedure \(\delta\) is optimal according to some optimality criterion does not necessarily mean that it is the right procedure to use, or even a

Figure 1: Two risk functions

satisfactory procedure. It does show how well one can do in this particular direction and how much is lost when other aspects have to be taken into account.

The aspect of the formulation that typically has the greatest influence on the solution of the optimality problem is the family \(\mathcal{P}\) to which the distribution of the observations is assumed to belong. The investigation of the _robustness_ of a proposed procedure to departures from the specified model is an indispensable feature of a suitable statistical procedure, and although optimality (exact or asymptotic) may provide a good starting point, modifications are often necessary before an acceptable solution is found. It is possible to extend the decision-theoretic framework to include robustness as well as optimality. Suppose robustness is desired against some class \(\mathcal{P}^{\prime}\) of distributions which is larger (possibly much larger) than the given \(\mathcal{P}\). Then one may assign a bound \(M\) to the risk to be tolerated over \(\mathcal{P}^{\prime}\). Within the class of procedures satisfying this restriction, one can then optimize the risk over \(\mathcal{P}\) as before. Such an approach has been proposed and applied to a number of specific problems by Berk and Bickel (1968) and Kempthorne (1988).

Another possible extension concerns the actual choice of the family \(\mathcal{P}\), the model used to represent the actual physical situation. The problem of choosing a model which provides an adequate description of the situation without being unnecessarily complex can be treated within the decision-theoretic formulation of Section 1.1 by adding to the loss function a component representing the complexity of the proposed model. Such approaches to _model selection_ are discussed in Stone (1981), de Leeuw (1992) and Rao and Wu (2001).

### 1.5 Invariance and Unbiasedness

A natural definition of impartiality suggests itself in situations which are symmetric with respect to the various parameter values of interest2: _The procedure is then required to act symmetrically with respect to these values._

Footnote 2: The concepts discussed here for general decision theory will be developed in more specialized form in later chapters. The present section may therefore be omitted at first reading.

**Example 1.5.1**: Suppose two treatments are to be compared and that each is applied \(n\) times. The resulting observations \(X_{11},\,\ldots,\,X_{1n}\) and \(X_{21},\,\ldots,\,X_{2n}\) are samples from \(N(\xi_{1},\sigma^{2})\) and \(N(\xi_{2},\sigma^{2})\), respectively. The three available decisions are \(d_{0}:|\xi_{2}-\xi_{1}|\leq\Delta,d_{1}:\xi_{2}>\xi_{1}+\Delta,d_{2}:\xi_{2}< \xi_{1}-\Delta\), and the loss is \(w_{ij}\) if decision \(d_{j}\) is taken when \(d_{i}\) would have been correct. If the treatments are to be compared solely in terms of the \(\xi\)'s and no outside considerations are involved, the losses are symmetric with respect to the two treatments so that \(w_{01}=w_{02}\), \(w_{10}=w_{20}\), \(w_{12}=w_{21}\). Suppose now that the labeling of the two treatments as 1 and 2 is reversed, and correspondingly also the labeling of the \(X\)'s, the \(\xi\)'s, and the decisions \(d_{1}\) and \(d_{2}\). This changes the meaning of the symbols, but the formal decision problem, because of its symmetry, remains unaltered. It is then natural to require the corresponding symmetry from the procedure \(\delta\) and ask that \(\delta(x_{11},\,\ldots,\,x_{1n},\,x_{21},\,\ldots,\,x_{2n})=d_{0}\), \(d_{1}\)or \(d_{2}\) as \(\delta(x_{21},\ldots,x_{2n},x_{11},\ldots,x_{1n})=d_{0}\), \(d_{2}\), or \(d_{1}\) respectively. If this condition was not satisfied, the decision as to which population has the greater mean would depend on the presumably quite accidental and irrelevant labeling of the samples. Similar remarks apply to a number of further symmetries that are present in this problem.

**Example 1.5.2**: Consider a sample \(X_{1},\ldots,X_{n}\) from a distribution with density \(\sigma^{-1}f[(x-\xi)/\sigma]\) and the problem of estimating the location parameter \(\xi\), say the mean of the \(X\)'s, when the loss is \((d-\xi)^{2}/\sigma^{2}\), the square of the error expressed in \(\sigma\)-units. Suppose that the observations are originally expressed in feet, and let \(X^{\prime}_{i}=aX\) with \(a=12\) be the corresponding observations in inches. In the transformed problem the density is \(\sigma^{\prime-1}f[(x^{\prime}-\xi^{\prime})/\sigma^{\prime}]\) with \(\xi^{\prime}=a\xi\), \(\sigma^{\prime}=a\sigma\). Since \((d^{\prime}-\xi^{\prime})^{2}/\sigma^{\prime 2}=(d-\xi)^{2}/\sigma^{2}\), the problem is formally unchanged. The same estimation procedure that is used for the original observations is therefore appropriate after the transformation and leads to \(\delta(aX_{1},\ldots,aX_{n})\) as an estimate of \(\xi^{\prime}=a\xi\), the parameter \(\xi\) expressed in inches. On reconverting the estimate into feet one finds that if the result is to be independent of the scale of measurements, \(\delta\) must satisfy the condition of scale invariance

\[\frac{\delta(aX_{1},\ldots,aX_{n})}{a}=\delta(X_{1},\ldots,X_{n})\.\ \blacksquare\]

The general mathematical expression of symmetry is invariance under a suitable group of transformations. A group \(G\) of transformations \(g\) of the sample space is said to leave a statistical decision problem invariant if it satisfies the following conditions:

1. It leaves invariant the family of distributions \(\mathcal{P}=\{P_{\theta},\,\theta\in\Omega\}\), that is, for any possible distribution \(P_{\theta}\) of \(X\) the distribution of \(gX\), say \(P_{\theta^{\prime}}\), is also in \(\mathcal{P}\). The resulting mapping \(\theta^{\prime}=\bar{g}\theta\) of \(\Omega\) is assumed to be onto3\(\Omega\) and 1:1. Footnote 3: The term _onto_ is used to indicate that \(\bar{g}\Omega\) is not only contained in but actually equals \(\Omega\), that is, given any \(\theta^{\prime}\) in \(\Omega\), there exists \(\theta\) in \(\Omega\) such that \(\bar{g}\theta=\theta^{\prime}\).
2. To each \(g\in G\), there corresponds a transformation \(g^{*}=h(g)\) of the decision space \(D\) onto itself such that \(h\) is a homomorphism, that is, satisfies the relation \(h(g_{1}g_{2})=h(g_{1})h(g_{2})\), and the loss function \(L\) is unchanged under the transformation, so that \[L(\bar{g}\theta,g^{*}d)=L(\theta,d).\]

Under these assumptions the transformed problem, in terms of \(X^{\prime}=gX\), \(\theta^{\prime}=\bar{g}\theta\), and \(d^{\prime}=g^{*}d\), is formally identical with the original problem in terms of \(X\), \(\theta\), and \(d\). Given a decision procedure \(\delta\) for the latter, this is therefore still appropriate after the transformation. Interpreting the transformation as a change of coordinate system and hence of the names of the elements, one would, on observing \(x^{\prime}\), select the decision which in the new system has the name \(\delta(x^{\prime})\), so that its old name is \(g^{*-1}\delta(x^{\prime})\). If the decision taken is to be independent of the particular coordinate system adopted, this should coincide with the original decision \(\delta(x)\), that is, the procedure must satisfy the _invariance_ condition \[\delta(gx)=g^{*}\delta(x)\quad\quad\text{for all}\quad x\in X,\quad g\in G. \tag{8}\]

The model described in Example 1.5.1 is invariant also under the transformations \(X^{\prime}_{ij}=X_{ij}+c\), \(\xi^{\prime}_{i}=\xi_{i}+c\). Since the decisions \(d_{0}\), \(d_{1}\), and \(d_{2}\) concern only the differences \(\xi_{2}-\xi_{1}\), they should remain unchanged under these transformations, so that one would expect to have \(g^{*}d_{i}=d_{i}\) for \(i=0,1,2\). It is in fact easily seen that the loss function does satisfy \(L(\bar{g}\theta,d)=L(\theta,d)\), and hence that \(g^{*}d=d\). A decision procedure therefore remains invariant in the present case if it satisfies \(\delta(gx)=\delta(x)\) for all \(g\in G\), \(x\in X\).

It is helpful to make a terminological distinction between situations like that of Example 1.5.3 in which \(g^{*}d=d\) for all \(d\), and those like Examples 1.5.1 and 1.5.2 where invariance considerations require \(\delta(gx)\) to vary with \(g\). In the former case the decision procedure remains unchanged under the transformations \(X^{\prime}=gX\) and is thus truly invariant; in the latter, the procedure varies with \(g\) and may then more appropriately be called _equivariant_ rather than invariant. Typically, hypothesis testing leads to procedures that are invariant in this sense; estimation problems (whether by point or interval estimation), to equivariant ones. Invariant tests and equivariant confidence sets will be discussed in Chapter 6. For a brief discussion of equivariant point estimation, see Bondessen (1983); a fuller treatment is given in Lehmann and Casella (1998), Chapter 3.

Invariance considerations are applicable only when a problem exhibits certain symmetries. An alternative impartiality restriction which is applicable to other types of problems is the following condition of unbiasedness. Suppose the problem is such that for each \(\theta\) there exists a unique correct decision and that each decision is correct for some \(\theta\). Assume further that \(L(\theta_{1},d)=L(\theta_{2},d)\) for all \(d\) whenever the same decision is correct for both \(\theta_{1}\) and \(\theta_{2}\). Then the loss \(L(\theta,d^{\prime})\) depends only on the actual decision taken, say \(d^{\prime}\), and the correct decision \(d\). The loss can thus be denoted by \(L(d,d^{\prime})\) and this function measures how far apart \(d\) and \(d^{\prime}\) are. Under these assumptions a decision function \(\delta\) is said to be unbiased with respect to the loss function \(L\), or \(L\)-unbiased, if for all \(\theta\) and \(d^{\prime}\)

\[E_{\theta}L(d^{\prime},\,\delta(X))\geq E_{\theta}L(d,\,\delta(X))\]

where the subscript \(\theta\) indicates the distribution with respect to which the expectation is taken and where \(d\) is the decision that is correct for \(\theta\). Thus \(\delta\) is unbiased if on the average \(\delta(X)\) comes closer to the correct decision than to any wrong one. Extending this definition, \(\delta\) is said to be \(L\)_-unbiased_ for an arbitrary decision problem if for all \(\theta\) and \(\theta^{\prime}\)

\[E_{\theta}L(\theta^{\prime},\,\delta(X))\geq E_{\theta}L(\theta,\delta(X)). \tag{9}\]

Suppose that in the problem of estimating a real-valued parameter \(\theta\) by confidence intervals, as in Example 1.2.4, the loss is 0 or 1 as the interval \([\underline{L},\,\tilde{L}]\) does or does not cover the true \(\theta\). Then the set of intervals \([\underline{L}(X),\,\tilde{L}(X)]\) is unbiased if the probability of covering the true value is greater than or equal to the probability of covering any false value.

**Example 1.5.5**: In a two-decision problem such as that of Example 1.2.1(i), let \(\omega_{0}\) and \(\omega_{1}\) be the sets of \(\theta\)-values for which \(d_{0}\) and \(d_{1}\) are the correct decisions. Assume that the loss is 0 when the correct decision is taken, and otherwise is given by \(L(\theta,\,d_{0})=a\) for \(\theta\in\omega_{1}\), and \(L(\theta,\,d_{1})=b\) for \(\theta\in\omega_{0}\). Then

\[E_{\theta}L(\theta^{\prime},\,\delta(X))=\left\{\begin{array}{ll}aP_{\theta} \{\delta(X)=d_{0}\}&\mbox{if}\ \ \theta^{\prime}\in\omega_{1},\\ bP_{\theta}\{\delta(X)=d_{1}\}&\mbox{if}\ \ \theta^{\prime}\in\omega_{0},\end{array}\right.\]

so that (1.9) reduces to

\[aP_{\theta}\{\delta(X)=d_{0}\}\geq b\,P_{\theta}\{\delta(X)=d_{1}\}\ \ \ \ \mbox{for}\ \ \theta^{\prime}\in\omega_{0},\]

with the reverse inequality holding for \(\theta\in\omega_{1}\). Since \(P_{\theta}\{\delta(X)=d_{0}\}+P_{\theta}\{\delta(X)=d_{1}\}=1\), the unbiasedness condition (1.9) becomes

\[\begin{array}{ll}P_{\theta}\{\delta(X)=d_{1}\}\leq\frac{a}{a+b}\ \mbox{for}\ \ \theta\in\omega_{0},\\ P_{\theta}\{\delta(X)=d_{1}\}\geq\frac{a}{a+b}\ \mbox{for}\ \ \theta\in\omega_{1}\.\ \blacksquare\end{array} \tag{1.10}\]

**Example 1.5.6**: In the problem of estimating a real-valued function \(\gamma(\theta)\) with the square of the error as loss, the condition of unbiasedness becomes

\[E_{\theta}[\delta(X)-\gamma(\theta^{\prime})]^{2}\geq E_{\theta}[\delta(X)- \gamma(\theta)]^{2}\ \ \ \ \ \mbox{for all}\ \theta,\,\theta^{\prime}.\]

On adding and subtracting \(h(\theta)=E_{\theta}\delta(X)\) inside the brackets on both sides, this reduces to

\[[h(\theta)-\gamma(\theta^{\prime})]^{2}\geq[h(\theta)-\gamma(\theta)]^{2}\ \ \ \ \ \mbox{for all}\ \theta,\,\theta^{\prime}.\]

If \(h(\theta)\) is one of the possible values of the function \(\gamma\), this condition holds if and only if

\[E_{\theta}\delta(X)=\gamma(\theta)\.\ \blacksquare \tag{1.11}\]

In the theory of point estimation, (1.11) is customarily taken as the definition of unbiasedness. Except under rather pathological conditions, it is both a necessary and sufficient condition for \(\delta\) to satisfy (1.9). (See Problem 1.2.)

### 1.6 Bayes and Minimax Procedures

We now turn to a discussion of some preference orderings of decision procedures and their risk functions. One such ordering is obtained by assuming that in repeated experiments the parameter itself is a random variable \(\Theta\), the distribution of which is known. If for the sake of simplicity one supposes that this distribution has a probability density \(\rho(\theta)\), the overall average loss resulting from the use of a decision procedure \(\delta\) is\[r(\rho,\delta)=\int E_{\theta}L(\theta,\delta(X))\rho(\theta)\,d\theta=\int R(\theta,\delta)\rho(\theta)\,d\theta \tag{12}\]

and the smaller \(r(\rho,\delta)\), the better is \(\delta\). An optimum procedure is one that minimizes \(r(\rho,\delta)\), and is called a _Bayes solution_ of the given decision problem corresponding to a priori density \(\rho\). The resulting minimum of \(r(\rho,\delta)\) is called the _Bayes risk_ of \(\delta\).

Unfortunately, in order to apply this principle it is necessary to assume not only that \(\theta\) is a random variable but also that its distribution is known. This assumption is usually not warranted in applications. Alternatively, the right-hand side of (12) can be considered as a weighted average of the risks; for \(\rho(\theta)\equiv 1\), in particular, it is then the area under the risk curve. With this interpretation the choice of a weight function \(\rho\) expresses the importance the experimenter attaches to the various values of \(\theta\). A systematic Bayes theory has been developed which interprets \(\rho\) as describing the state of mind of the investigator toward \(\theta\). For an account of this approach see, for example, Berger (1985a) and Robert (1994).

If no prior information regarding \(\theta\) is available, one might consider the maximum of the risk function its most important feature. Of two risk functions the one with the smaller maximum is then preferable, and the optimum procedures are those with the _minimax_ property of minimizing the maximum risk. Since this maximum represents the worst (average) loss that can result from the use of a given procedure, a minimax solution is one that gives the greatest possible protection against large losses. That such a principle may sometimes be quite unreasonable is indicated in Figure 2, where under most circumstances one would prefer \(\delta_{1}\) to \(\delta_{2}\) although its risk function has the larger maximum.

Perhaps the most common situation is one intermediate to the two just described. On the one hand, past experience with the same or similar kind of experiment is available and provides an indication of what values of \(\theta\) to expect; on the other, this information is neither sufficiently precise nor sufficiently reliable to warrant the assumptions that the Bayes approach requires. In such circumstances it seems desirable to make use of the available information without trusting it to such an extent that catastrophically high risks might result if it is inaccurate or misleading. To achieve this one can place a bound on the risk and restrict consideration to decision procedures \(\delta\) for which

\[R(\theta,\delta)\leq C\quad\mbox{ for all }\ \theta. \tag{13}\]

Figure 2: Risk function of minimax solution and a competitor

[Here the constant \(C\) will have to be larger than the maximum risk \(C_{0}\) of the minimax procedure, since otherwise there will exist no procedures satisfying (1.13).] Having thus assured that the risk can under no circumstances get out of hand, the experimenter can now safely exploit his knowledge of the situation, which may be based on theoretical considerations as well as on past experience; he can follow his hunches and guess at a distribution \(\rho\) for \(\theta\). This leads to the selection of a procedure \(\delta\) (a _restricted Bayes solution_), which minimizes the average risk (1.12) for this a priori distribution subject to (1.13). The more certain one is of \(\rho\), the larger one will select \(C\), thereby running a greater risk in case of a poor guess but improving the risk if the guess is good.

Instead of specifying an ordering directly, one can postulate conditions that the ordering should satisfy. Various systems of such conditions have been investigated and have generally led to the conclusion that the only orderings satisfying these systems are those which order the procedures according to their Bayes risk with respect to some prior distribution of \(\theta\). For details, see for example Blackwell and Girshick (1954), Ferguson (1967), Savage (1962), Berger (1951a), Bernardo and Smith (1994).

### Maximum Likelihood

Another approach, which is based on considerations somewhat different from those of the preceding sections, is the method of maximum likelihood. It has led to reasonable procedures in a great variety of problems, and is still playing a dominant role in the development of new tests and estimates. Suppose for a moment that \(X\) can take on only a countable set of values \(x_{1},x_{2},\ldots,\) with \(P_{\theta}(x)=P_{\theta}\{X=x\}\), and that one wishes to determine the correct value of \(\theta\), that is, the value that produced the observed \(x\). This suggests considering for each possible \(\theta\) how probable the observed \(x\) would be if \(\theta\) were the true value. The higher this probability, the more one is attracted to the explanation that the \(\theta\) in question produced \(x\), and the more likely the value of \(\theta\) appears. Therefore, the expression \(P_{\theta}(x)\) considered for fixed \(x\) as a function of \(\theta\) has been called the _likelihood_ of \(\theta\). To indicate the change in point of view, let it be denoted by \(L_{x}(\theta)\). Suppose now that one is concerned with an action problem involving a countable number of decisions, and that it is formulated in terms of a gain function (instead of the usual loss function), which is \(0\) if the decision taken is incorrect and is \(a(\theta)>0\) if the decision taken is correct and \(\theta\) is the true value. Then it seems natural to weight the likelihood \(L_{x}(\theta)\) by the amount that can be gained if \(\theta\) is true, to determine the value of \(\theta\) that maximizes \(a(\theta)L_{x}(\theta)\) and to select the decision that would be correct if this were the true value of \(\theta\). Essentially the same remarks apply in the case in which \(P_{\theta}(x)\) is a probability density rather than a discrete probability.

In problems of point estimation, one usually assumes that \(a(\theta)\) is independent of \(\theta\). This leads to estimating \(\theta\) by the value that maximizes the likelihood \(L_{x}(\theta)\), the _maximum likelihood estimate_ of \(\theta\). Another case of interest is the class of two decision problems illustrated by Example 1.1(i). Let \(\omega_{0}\) and \(\omega_{1}\) denote the sets of \(\theta\)-values for which \(d_{0}\) and \(d_{1}\) are the correct decisions, and assume that \(a(\theta)=a_{0}\) or \(a_{1}\) as \(\theta\) belongs to \(\omega_{0}\) or \(\omega_{1}\), respectively. Then decision \(d_{0}\) or \(d_{1}\) is taken as \(a_{1}\sup_{\theta\in\omega_{1}}L_{x}(\theta)<\text{or}>a_{0}\sup_{\theta\in \omega_{0}}L_{x}(\theta)\), that is, as

\[\frac{\sup_{\theta\in\omega_{0}}L_{x}(\theta)}{\sup_{\theta\in \omega_{1}}L_{x}(\theta)}>\quad\text{or}\quad<\frac{a_{1}}{a_{0}}. \tag{1.14}\]

This is known as a _likelihood ratio procedure_.4

Footnote 4: This definition differs slightly from the usual one where in the denominator on the left-hand side of (1.14) the supremum is taken over the set \(\omega_{0}\cup\omega_{1}\). The two definitions agree whenever the left-hand side of (1.14) is \(\leq 1\), and the procedures therefore agree is \(a_{1}<a_{0}\).

Although the maximum likelihood principle is not based on any clearly defined optimum considerations, it has been very successful in leading to satisfactory procedures in many specific problems. For wide classes of problems, maximum likelihood procedures will be shown in Chapter 15 to possess various asymptotic optimum properties as the sample size tends to infinity; also see Lehmann and Casella (1998), Chapter 6. On the other hand, there exist examples for which the maximum likelihood procedure is worse than useless, where it is, in fact, so bad that one can do better without making any use of the observations (see Problem 6.30).

### 1.8 Complete Classes

None of the approaches described so far is reliable in the sense that the resulting procedure is necessarily satisfactory. There are problems in which a decision procedure \(\delta_{0}\) exists with uniformly minimum risk among all unbiased or invariant procedures, but where there exists a procedure \(\delta_{1}\) not possessing this particular impartiality property and preferable to \(\delta_{0}\). (See Problems 1.14 and 1.16.) As was seen earlier, minimax procedures can also be quite undesirable, while the success of Bayes and restricted Bayes solutions depends on a priori information which is usually not very reliable if it is available at all. In fact, it seems that in the absence of reliable a priori information no principle leading to a unique solution can be entirely satisfactory.

This suggests the possibility, at least as a first step, of not insisting on a unique solution but asking only how far a decision problem can be reduced without loss of relevant information. It has already been seen that a decision procedure \(\delta\) can sometimes be eliminated from consideration because there exists a procedure \(\delta^{\prime}\)_dominating_ it in the sense that

\[\begin{array}{rcl}R(\theta,\,\delta^{\prime})&\leq&R(\theta,\,\delta)\text{ for all }\theta\\ R(\theta,\,\delta^{\prime})&<&R(\theta,\,\delta)\text{ for some }\theta.\end{array} \tag{1.15}\]In this case \(\delta\) is said to be _inadmissible_; \(\delta\) is called _admissible_ if no such dominating \(\delta^{\prime}\) exists. A class \(\mathcal{C}\) of decision procedures is said to be _complete_ if for any \(\delta\) not in \(\mathcal{C}\) there exists \(\delta^{\prime}\) in \(\mathcal{C}\) dominating it. A complete class is _minimal_ if it does not contain a complete subclass. If a minimal complete class exists, as is typically the case, it consists exactly of the totality of admissible procedures.

It is convenient to define also the following variant of the complete class notion. A class \(\mathcal{C}\) is said to be _essentially complete_ if for any procedure \(\delta\) there exists \(\delta^{\prime}\) in \(\mathcal{C}\) such that \(R(\theta,\,\delta^{\prime})\leq R(\theta,\,\delta)\) for all \(\theta\). Clearly, any complete class is also essentially complete. In fact, the two definitions differ only in their treatment of equivalent decision rules, that is, decision rules with identical risk function. If \(\delta\) belongs to the minimal complete class \(\mathcal{C}\), any equivalent decision rule must also belong to \(\mathcal{C}\). On the other hand, a minimal essentially complete class need contain only one member from such a set of equivalent procedures.

In a certain sense a minimal essentially complete class provides the maximum possible reduction of a decision problem. On the one hand, there is no reason to consider any of the procedures that have been weeded out. For each of them, there is included one in \(\mathcal{C}\) that is as good or better. On the other hand, it is not possible to reduce the class further. Given any two procedures in \(\mathcal{C}\), each of them is better in places than the other, so that without additional information it is not known which of the two is preferable.

The primary concern in statistics has been with the explicit determination of procedures, or classes of procedures, for various specific decision problems. Those studied most extensively have been estimation problems, and problems involving a choice between only two decisions (hypothesis testing), the theory of which constitutes the subject of the present volume. However, certain conclusions are possible without such specialization. In particular, two results concerning the structure of complete classes and minimax procedures have been proved to hold under very general assumptions.5

Footnote 5: Precise statements and proofs of these results are given in the book by Wald (1939). See also Ferguson (1967) and Berger (1985a). Additional results and references are given in Brown and Marden (1989) and Kowalski (1995).

1. The totality of Bayes solutions and limits of Bayes solutions constitute a complete class.
2. Minimax procedures are Bayes solutions with respect to a _least favorable_ a priori distribution, that is, an a priori distribution that maximizes the associated Bayes risk, and the minimax risk equals this maximum Bayes risk. Somewhat more generally, if there exists no least favorable a priori distribution but only a sequence for which the Bayes risk tends to the maximum, the minimax procedures are limits of the associated sequence of Bayes solutions.

### Sufficient Statistics

A minimal complete class was seen in the preceding section to provide the maximum possible reduction of a decision problem without loss of information. Frequently it is possible to obtain a less extensive reduction of the data, which applies simultaneously to all problems relating to a given class \(\mathcal{P}=\{P_{\theta},\ \theta\in\Omega\}\) of distributions of the given random variable \(X\). It consists essentially in discarding that part of the data which contains no information regarding the unknown distribution \(P_{\theta}\), and which is therefore of no value for any decision problem concerning \(\theta\).

**Example 1.9.1**: Trials are performed with constant unknown probability \(p\) of success. If \(X_{i}\) is 1 or 0 as the \(i\)th trial is a success or failure, the sample (\(X_{1}\),..., \(X_{n}\)) shows how many successes there were and in which trials they occurred. The second of these pieces of information contains no evidence as to the value of \(p\). Once the total number of successes \(\sum X_{i}\) is known to be equal to \(t\), each of the \({n\choose t}\) possible positions of these successes is equally likely regardless of \(p\). It follows that knowing \(\sum X_{i}\) but neither the individual \(X_{i}\) nor \(p\), one can, from a table of random numbers, construct a set of random variables \(X^{\prime}_{1}\),..., \(X^{\prime}_{n}\) whose joint distribution is the same as that of \(X_{1}\),..., \(X_{n}\). Therefore, the information contained in the \(X_{i}\) is the same as that contained in \(\sum X_{i}\) and a table of random numbers. \(\blacksquare\)

**Example 1.9.2**: If \(X_{1}\),..., \(X_{n}\) are independently normally distributed with zero mean and variance \(\sigma^{2}\), the conditional distribution of the sample point over each of the spheres, \(\sum X_{i}^{2}=\) constant, is uniform irrespective of \(\sigma^{2}\). One can therefore construct an equivalent sample \(X^{\prime}_{1}\),..., \(X^{\prime}_{n}\) from a knowledge of \(\sum X_{i}^{2}\) and a mechanism that can produce a point randomly distributed over a sphere. \(\blacksquare\)

More generally, a statistic \(T\) is said to be _sufficient_ for the family \(\mathcal{P}=\{P_{\theta},\ \theta\in\Omega\}\) (or sufficient for \(\theta\), if it is clear from the context what set \(\Omega\) is being considered) if the conditional distribution of \(X\) given \(T=t\) is independent of \(\theta\). As in the two examples it then follows under mild assumptions6 that it is not necessary to utilize the original observations \(X\). If one is permitted to observe only \(T\) instead of \(X\), this does not restrict the class of available decision procedures. For any value \(t\) of \(T\) let \(X_{t}\) be a random variable possessing the conditional distribution of \(X\) given \(t\). Such a variable can, at least theoretically, be constructed by means of a suitable random mechanism. If one then observes \(T\) to be \(t\) and \(X_{t}\) to be \(x^{\prime}\), the random variable \(X^{\prime}\) defined through this two-stage process has the same distribution as \(X\). Thus, given any procedure based on \(X\), it is possible to construct an equivalent one based on \(X^{\prime}\) which can be viewed as a randomized procedure based solely on \(T\). Hence if randomization is permitted (and we shall assume throughout that this is the case), there is no loss of generality in restricting consideration to a sufficient statistic.

Footnote 6: These are connected with difficulties concerning the behavior of conditional probabilities. For a discussion of these difficulties see Sections 2.3–2.5.

It is inconvenient to have to compute the conditional distribution of \(X\) given \(t\) in order to determine whether or not \(T\) is sufficient. A simple check is provided by the following _factorization criterion_.

[MISSING_PAGE_EMPTY:1636]

provided the denominator is different from zero. Regularity conditions for the validity of (1.18) are given by Tukey (1958b).

Since in the conditional distribution given \(t\) only the \(Y\)'s vary, \(T\) is sufficient for \(\theta\) if the conditional distribution of \(Y\) given \(t\) is independent of \(\theta\). Suppose that \(T\) satisfies (1.19). Then analogously to the discrete case, a necessary and sufficient condition for \(T\) to be sufficient is a factorization of the density of the form

\[p_{\theta}^{X}(x)=g_{\theta}[T(x)]h(x). \tag{1.20}\]

(See Problem 1.19.) The following two examples illustrate the application of the criterion in this case. In both examples the existence of functions \(Y\) satisfying (1.17)-(1.19) will be assumed but not proved. As will be shown later (Section 2.6), this assumption is actually not needed for the validity of the factorization criterion.

**Example 1.9.4**: Let \(X_{1}\),..., \(X_{n}\) be independently distributed with normal probability density

\[p_{\xi,\sigma}(x)=(2\pi\sigma^{2})^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\sum x _{i}^{2}+\frac{\xi}{\sigma^{2}}\sum x_{i}-\frac{n}{2\sigma^{2}}\xi^{2}\right).\]

Then the factorization criterion shows (\(\sum X_{i}\), \(\sum X_{i}^{2}\)) to be sufficient for (\(\xi\), \(\sigma\)). \(\blacksquare\)

**Example 1.9.5**: Let \(X_{1},\ldots,X_{n}\) be independently distributed according to the uniform distribution \(U(0,\theta)\) over the interval \((0,\theta)\). Then \(p_{\theta}(x)=\theta^{-n}(\max x_{i},\theta)\), where \(u(a,b)\) is \(1\) or \(0\) as \(a\leq b\) or \(a>b\), and hence \(\max X_{i}\) is sufficient for \(\theta\). \(\blacksquare\)

An alternative criterion of _Bayes sufficiency_, due to Kolmogorov (1942), provides a direct connection between this concept and some of the basic notions of decision theory. As in the theory of Bayes solutions, consider the unknown parameter \(\theta\) as a random variable \(\Theta\) with an a priori distribution, and assume for simplicity that it has a density \(\rho(\theta)\). Then if \(T\) is sufficient, the conditional distribution of \(\Theta\) given \(X=x\) depends only on \(T(x)\). Conversely, if \(\rho(\theta)\neq 0\) for all \(\theta\) and if the conditional distribution of \(\Theta\) given \(x\) depends only on \(T(x)\), then \(T\) is sufficient for \(\theta\).

In fact, under the assumptions made, the joint density of \(X\) and \(\Theta\) is \(p_{\theta}(x)\rho(\theta)\). If \(T\) is sufficient, it follows from (1.20) that the conditional density of \(\Theta\) given \(x\) depends only on \(T(x)\). Suppose, on the other hand, that for some a priori distribution for which \(\rho(\theta)\neq 0\) for all \(\theta\) the conditional distribution of \(\Theta\) given \(x\) depends only on \(T(x)\). Then

\[\frac{p_{\theta}(x)\rho(\theta)}{\int p_{\theta^{\prime}}(x)\rho(\theta^{\prime })\,d\theta^{\prime}}=f_{\theta}[T(x)]\]

and by solving for \(p_{\theta}(x)\) it is seen that \(T\) is sufficient.

Any Bayes solution depends only on the conditional distribution of \(\Theta\) given \(x\) (see Problem 1.8) and hence on \(T(x)\). Since typically Bayes solutions together with their limits form an essentially complete class, it follows that this is also true of the decision procedures based on \(T\). The same conclusion had already been reached more directly at the beginning of the section.

For a discussion of the relation of these different aspects of sufficiency in more general circumstances and references to the literature, see Le Cam (1956), Roy and Ramamoorthi (1979) and Yamada and Morimoto (1992). An example of a statistic which is Bayes sufficient in the Kolmogorov sense but not according to the definition given at the beginning of this section is provided by Blackwell and Blackwell and Ramamoorthi (1982).

By restricting attention to a sufficient statistic, one obtains a reduction of the data, and it is then desirable to carry this reduction as far as possible. To illustrate the different possibilities, consider once more the binomial Example 1.9.1. If \(m\) is any integer less than \(n\) and \(T_{1}=\sum_{i=1}^{m}X_{i}\), \(T_{2}=\sum_{i=m+1}^{n}X_{i}\), then \((T_{1},T_{2})\) constitutes a sufficient statistic, since the conditional distribution of \(X_{1},\ldots,X_{n}\) given \(T_{1}=t_{1}\), \(T_{2}=t_{2}\) is independent of \(p\). For the same reason, the full sample \((X_{1},\ldots,X_{n})\) itself is also a sufficient statistic. However, \(T=\sum_{i=1}^{n}X_{i}\) provides a more thorough reduction than either of these and than various others that can be constructed. A sufficient statistic \(T\) is said to be _minimal sufficient_ if the data cannot be reduced beyond \(T\) without losing sufficiency. For the binomial example in particular, \(\sum_{i=1}^{n}X_{i}\) can be shown to be minimal (Problem 1.17). This illustrates the fact that in specific examples the sufficient statistic determined by inspection through the factorization criterion usually turns out to be minimal. Explicit procedures for constructing minimal sufficient statistics are discussed in Section 1.5 of Lehmann and Casella (1998).

### Problems

#### Section 1.2

**Problem 1.1**: The following distributions arise on the basis of assumptions similar to those leading to (1.1)-(1.3).

(i) Independent trials with constant probability \(p\) of success are carried out until a preassigned number \(m\) of successes has been obtained. If the number of trials required is \(X+m\), then \(X\) has the _negative binomial_ distribution \(Nb(p,m)\):

\[P\{X=x\}=\binom{m+x-1}{x}p^{m}(1-p)^{x},\ \ \ \ \ x=0,1,2\ldots\ \.\]

(ii) In a sequence of random events, the number of events occurring in any time interval of length \(\tau\) has the Poisson distribution \(P(\lambda\tau)\), and the numbers of events in nonoverlapping time intervals are independent. Then the "waiting time" \(T\), which elapses from the starting point, say \(t=0\), until the first event occurs, has the _exponential_ probability densit\[p(t)=\lambda e^{-\lambda\tau},\ \ \ \ \ t\geq 0.\]

Let \(T_{i}\), \(i\geq 2\), be the time elapsing from the occurrence of the \((i-1)\)st event to that of the \(i\)th event. Then it is also true, although more difficult to prove, that \(T_{1}\), \(T_{2}\), \(\ldots\) are identically and independently distributed. A proof is given, for example, in Karlin and Taylor (1975).

(iii) A point \(X\) is selected "at random" in the interval \((a,b)\), that is, the probability of \(X\) falling in any subinterval of \((a,b)\) depends only on the length of the subinterval, not on its position. Then \(X\) has the _uniform_ distribution \(U(a,b)\) with probability density

\[p(x)=1/(b-a),\ \ \ \ \ a<x<b.\]

### _Section 1.5_

**Problem 1.2**: _Unbiasedness in point estimation._ Suppose that \(\gamma\) is a continuous real-valued function defined over \(\Omega\) which is not constant in any open subset of \(\Omega\), and that the expectation \(h(\theta)=E_{\theta}\delta(X)\) is a continuous function of \(\theta\) for every estimate \(\delta(X)\) of \(\gamma(\theta)\). Then (1.11) is a necessary and sufficient condition for \(\delta(X)\) to be unbiased when the loss function is the square of the error.

[Unbiasedness implies that \(\gamma^{2}(\theta^{\prime})-\gamma^{2}(\theta)\geq 2h(\theta)[\gamma( \theta^{\prime})-\gamma(\theta)]\) for all \(\theta\), \(\theta^{\prime}\). If \(\theta\) is neither a relative minimum nor maximum of \(\gamma\), it follows that there exist points \(\theta^{\prime}\) arbitrarily close to \(\theta\) both such that \(\gamma(\theta)+\gamma(\theta^{\prime})\geq\) and \(\leq 2h(\theta)\), and hence that \(\gamma(\theta)=h(\theta)\). That this equality also holds for an extremum of \(\gamma\) follows by continuity, since \(\gamma\) is not constant in any open set.]

**Problem 1.3**: _Median unbiasedness._

(i) A real number \(m\) is a median for the random variable \(Y\) if \(P\{Y\geq m\}\geq\frac{1}{2}\), \(P\{Y\leq m\}\geq\frac{1}{2}\). Then all real \(a_{1}\), \(a_{2}\) such that \(m\leq a_{1}\leq a_{2}\) or \(m\geq a_{1}\geq a_{2}\) satisfy \(E|Y-a_{1}|\leq E|Y-a_{2}|\).

(ii) For any estimate \(\delta(X)\) of \(\gamma(\theta)\), let \(m^{-}(\theta)\) and \(m^{+}(\theta)\) denote the infimum and supremum of the medians of \(\delta(X)\), and suppose that they are continuous functions of \(\theta\). Let \(\gamma(\theta)\) be continuous and not constant in any open subset of \(\Omega\). Then the estimate \(\delta(X)\) of \(\gamma(\theta)\) is unbiased with respect to the loss function \(L(\theta,d)=|\gamma(\theta)-d|\) if and only if \(\gamma(\theta)\) is a median of \(\delta(X)\) for each \(\theta\). An estimate with this property is said to be _median unbiased_.

**Problem 1.4**: _Nonexistence of unbiased procedures._ Let \(X_{1},\ldots,X_{n}\) be independently distributed with density \((1/a)\,f((x-\xi)/a)\), and let \(\theta=(\xi,a)\). Then no estimator of \(\xi\) exists which is unbiased with respect to the loss function \((d-\xi)^{k}/a^{k}\). _Note_. For more general results concerning the nonexistence of unbiased procedures see Rojo (1983).

**Problem 1.5**: Let \(\mathcal{C}\) be any class of procedures that is closed under the transformations of a group \(G\) in the sense that \(\delta\in\mathcal{C}\) implies \(g^{*}\delta g^{-1}\in\mathcal{C}\) for all \(g\in G\). If there exists a unique procedure \(\delta_{0}\) that uniformly minimizes the risk within the class \(\mathcal{C}\), then \(\delta_{0}\) is invariant.7 If \(\delta_{0}\) is unique only up to sets of measure zero, then it is _almost invariant_, that is, for each \(g\) it satisfies the equation \(\delta(gx)=g^{*}\delta(x)\) except on a set \(N_{g}\) of measure 0.

Footnote 7: Here and in Problems 1.6, 1.7, 1.11, 1.15, and 1.16 the term “invariant” is used in the general sense (1.8) of “invariant or equivalent”.

**Problem 1.6**: _Relation of unbiasedness and invariance._

(i) If \(\delta_{0}\) is the unique (up to sets of measure 0) unbiased procedure with uniformly minimum risk, it is almost invariant.

(ii) If \(\bar{G}\) is transitive and \(G^{*}\) commutative, and if among all invariant (almost invariant) procedures there exists a procedure \(\delta_{0}\) with uniformly minimum risk, then it is unbiased.

(iii) That conclusion (ii) need not hold without the assumptions concerning \(G^{*}\) and \(\bar{G}\) is shown by the problem of estimating the mean \(\xi\) of a normal distribution \(N(\xi,\sigma^{2})\) with loss function \((\xi-d)^{2}/\sigma^{2}\). This remains invariant under the groups \(G_{1}:gx=x+b,-\infty<b<\infty\) and \(G_{2}:gx=ax+b,0<a<\infty,-\infty<b<\infty\). The best invariant estimate relative to both groups is \(X\), but there does not exist an estimate which is unbiased with respect to the given loss function.

[(i): This follows from the preceding problem and the fact that when \(\delta\) is unbiased so is \(g^{*}\delta g^{-1}\).

(ii): It is the defining property of transitivity that given \(\theta,\theta^{\prime}\) there exists \(\bar{g}\) such that \(\theta^{\prime}=\bar{g}\theta\). Hence for any \(\theta,\theta^{\prime}\)

\[E_{\theta}L(\theta^{\prime},\delta_{0}(X))=E_{\theta}L(\bar{g}\theta,\delta_{0 }(X))=E_{\theta}L(\theta,g^{*-1}\delta_{0}(X)).\]

Since \(G^{*}\) is commutative, \(g^{*-1}\delta_{0}\) is invariant, so that

\[R(\theta,g^{*-1}\delta_{0})\geq R(\theta,\delta_{0})=E_{\theta}L(\theta,\delta_ {0}(X)).]\]

### _Section 1.6_

**Problem 1.7**: _Unbiasedness in interval estimation._ Confidence intervals \(I=(\underline{L},\,\bar{L})\) are unbiased for estimating \(\theta\) with loss function \(L(\theta,\,I)=(\theta-\underline{L})^{2}+(\bar{L}-\theta)^{2}\) provided \(E[\frac{1}{2}(\underline{L}+\bar{L})]=\theta\) for all \(\theta\), that is, provided the midpoint of \(I\) is an unbiased estimate of \(\theta\) in the sense of (1.11).

**Problem 1.8**: _Structure of Bayes solutions._

(i) Let \(\Theta\) be an unobservable random quantity with probability density \(\rho(\theta)\), and let the probability density of \(X\) be \(p_{\theta}(x)\) when \(\Theta=\theta\). Then \(\delta\) is a Bayes solution of a given decision problem if for each \(x\) the decision \(\delta(x)\) is chosen so as to minimize \(\int L(\theta,\,\delta(x))\pi(\theta\mid x)\,d\theta\), where \(\pi(\theta\mid x)=\rho(\theta)p_{\theta}(x)/\int\rho(\theta^{\prime})p_{ \theta^{\prime}}(x)\,d\theta^{\prime}\) is the conditional (a posteriori) probability density of \(\Theta\) given \(x\).

(ii) Let the problem be a two-decision problem with the losses as given in Example 1.5.5. Then the Bayes solution consists in choosing decision \(d_{0}\) if

\[aP\{\Theta\in\omega_{1}\mid x\}<bP\{\Theta\in\omega_{0}\mid x\}\]

and decision \(d_{1}\) if the reverse inequality holds. The choice of decision is immaterial in case of equality.

(iii) In the case of point estimation of a real-valued function \(g(\theta)\) with loss function \(L(\theta,d)=(g(\theta)-d)^{2}\), the Bayes solution becomes \(\delta(x)=E[g(\Theta)\mid x]\). When instead the loss function is \(L(\theta,d)=|g(\theta)-d|\), the Bayes estimate \(\delta(x)\) is any median of the conditional distribution of \(g(\Theta)\) given \(x\).

[(i): The Bayes risk \(r(\rho,\delta)\) can be written as \(\int[\int L(\theta,\delta(x))\pi(\theta\mid x)\,d\theta]\times p(x)\,dx\), where \(p(x)=\int\rho(\theta^{\prime})p_{\theta^{\prime}}(x)\,d\theta^{\prime}\).

(ii): The conditional expectation \(\int L(\theta,d_{0})\pi(\theta\mid x)\,d\theta\) reduces to \(aP\{\Theta\in\omega_{1}\mid x\}\), and similarly for \(d_{1}\).]

**Problem 1.9**: (i) As an example in which randomization reduces the maximum risk, suppose that a coin is known to be either standard (HT) or to have heads on both sides (HH). The nature of the coin is to be decided on the basis of a single toss, the loss being 1 for an incorrect decision and 0 for a correct one. Let the decision be HT when \(T\) is observed, whereas in the contrary case the decision is made at random, with probability \(\rho\) for HT and \(1-\rho\) for HH. Then the maximum risk is minimized for \(\rho=\frac{1}{3}\).

(ii) A genetic setting in which such a problem might arise is that of a couple, of which the husband is either dominant homozygous (AA) or heterozygous (Aa) with respect to a certain characteristic, and the wife is homozygous recessive (aa). Their child is heterozygous, and it is of importance to determine to which genetic type the husband belongs. However, in such cases an a priori probability is usually available for the two possibilities. One is then dealing with a Bayes problem, and randomization is no longer required. In fact, if the a priori probability is \(p\) that the husband is dominant, then the Bayes procedure classifies him as such if \(p>\frac{1}{3}\) and takes the contrary decision if \(p<\frac{1}{3}\).

**Problem 1.10**: _Unbiasedness and minimax._ Let \(\Omega=\Omega_{0}\cup\Omega_{1}\) where \(\Omega_{0},\,\Omega_{1}\) are mutually exclusive, and consider a two-decision problem with loss function \(L(\theta,d_{i})=a_{i}\) for \(\theta\in\Omega_{j}(j\neq i)\) and \(L(\theta,d_{i})=0\) for \(\theta\in\Omega_{i}(i=0,\,1)\).

(i) Any minimax procedure is unbiased. (ii) The converse of (i) holds provided \(P_{\theta}(A)\) is a continuous function of \(\theta\) for all \(A\), and if the sets \(\Omega_{0}\) and \(\Omega_{1}\) have at least one common boundary point.

[(i): The condition of unbiasedness in this case is equivalent to \(\sup R_{\delta}(\theta)\leq a_{0}a_{1}/(a_{0}+a_{1})\). That this is satisfied by any minimax procedure is seen by comparison with the procedure \(\delta(x)=d_{0}\) or \(=d_{1}\) with probabilities \(a_{1}/(a_{0}+a_{1})\) and \(a_{0}/(a_{0}+a_{1})\), respectively.

(ii): If \(\theta_{0}\) is a common boundary point, continuity of the risk function implies that any unbiased procedure satisfies \(R_{\delta}(\theta_{0})=a_{0}a_{1}/(a_{0}+a_{1})\) and hence \(\sup R_{\delta}(\theta_{0})=a_{0}a_{1}/(a_{0}+a_{1})\).]

**Problem 1.11**: _Invariance and minimax._ Let a problem remain invariant relative to the groups \(G\), \(\bar{G}\), and \(G^{*}\) over the spaces \(\mathcal{X}\), \(\Omega\), and \(D\), respectively. Then a randomized procedure \(Y_{x}\) is defined to be invariant if for all \(x\) and \(g\) the conditional distribution of \(Y_{x}\) given \(x\) is the same as that of \(g^{*-1}Y_{gx}\).

(i) Consider a decision procedure which remains invariant under a finite group \(G=\{g_{1},\ldots,g_{N}\}\). If a minimax procedure exists, then there exists one that is invariant.

(ii) This conclusion does not necessarily hold for infinite groups, as is shown by the following example. Let the parameter space \(\Omega\) consist of all elements \(\theta\) of the free group with two generators, that is, the totality of formal products \(\pi_{1}\ldots\pi_{n}\) (\(n=0,1,2,\ldots\)) where each \(\pi_{i}\) is one of the elements \(a,a^{-1},b,b^{-1}\) and in which all products \(aa^{-1}\), \(a^{-1}a\), \(bb^{-1}\), and \(b^{-1}b\) have been canceled. The empty product (\(n=0\)) is denoted by \(e\). The sample point \(X\) is obtained by multiplying \(\theta\) on the right by one of the four elements \(a\), \(a^{-1}\), \(b\), \(b^{-1}\) with probability \(\frac{1}{4}\) each, and canceling if necessary, that is, if the random factor equals \(\pi_{n}^{-1}\). The problem of estimating \(\theta\) with \(L(\theta,d)\) equal to \(0\) if \(d=\theta\) and equal to \(1\) otherwise remains invariant under multiplication of \(X\), \(\theta\), and \(d\) on the left by an arbitrary sequence \(\pi_{-m}\ldots\pi_{-2}\pi_{-1}\)(\(m=0\), \(1,\ldots\)). The invariant procedure that minimizes the maximum risk has risk function \(R(\theta,\delta)\equiv\frac{3}{4}\). However, there exists a noninvariant procedure with maximum risk \(\frac{1}{4}\).

[(i): If \(Y_{x}\) is a (possibly randomized) minimax procedure, an invariant minimax procedure \(Y_{x}^{\prime}\) is defined by \(P(Y_{x}^{\prime}=d)=\sum_{i=1}^{N}P(Y_{g_{i}x}=g_{i}^{*}d)/N\).

(ii): The better procedure consists in estimating \(\theta\) to be \(\pi_{1}\ldots\pi_{k-1}\) when \(\pi_{1}\ldots\pi_{k}\) is observed (\(k\geq 1\)), and estimating \(\theta\) to be \(a\), \(a^{-1}\), \(b\), \(b^{-1}\) with probability \(\frac{1}{4}\) each in case the identity is observed. The estimate will be correct unless the last element of \(X\) was canceled, and hence will be correct with probability \(\geq\frac{3}{4}\).]

**Section 1.7**:
**Problem 1.12**: (i) Let \(X\) have probability density \(p_{\theta}(x)\) with \(\theta\) one of the values \(\theta_{1}\), \(\ldots\), \(\theta_{n}\), and consider the problem of determining the correct value of \(\theta\), so that the choice lies between the \(n\) decisions \(d_{1}=\theta_{1}\), \(\ldots\), \(d_{n}=\theta_{n}\) with gain \(a(\theta_{i})\) if \(d_{i}=\theta_{i}\) and \(0\) otherwise. Then the Bayes solution (which maximizes the average gain) when \(\theta\) is a random variable taking on each of the \(n\) values with probability \(1/n\) coincides with the maximum likelihood procedure.

(ii)Let \(X\) have probability density \(p_{\theta}(x)\) with \(0\leq\theta\leq 1\). Then the maximum likelihood estimate is the mode (maximum value) of the a posteriori density of \(\Theta\) given \(x\) when \(\Theta\) is uniformly distributed over (\(0\), \(1\)).

**Problem 1.13**: (i) Let \(X_{1},\ldots,X_{n}\) be a sample from \(N(\xi,\sigma^{2})\), and consider the problem of deciding between \(\omega_{0}:\xi<0\) and \(\omega_{1}:\xi\geq 0\). If \(\tilde{x}=\sum x_{i}/n\) and \(C=(a_{1}/a_{0})^{2/n}\), the likelihood ratio procedure takes decision \(d_{0}\) or \(d\), as

\[\frac{\sqrt{n}\tilde{x}}{\sqrt{\sum(x_{i}-\tilde{x})^{2}}}<k\ \ \ \mbox{or}\ \ \ \ >k,\]where \(k=\sqrt{C-1}\) if \(C>1\) and \(k=\sqrt{(1-C)/C}\) if \(C<1\).

(ii) For the problem of deciding between \(\omega_{0}:\sigma<\sigma_{0}\) and \(\omega_{1}:\sigma\geq\sigma_{0}\) the likelihood ratio procedure takes decision \(d_{0}\) or \(d\) as

\[\frac{\sum(x_{i}-\bar{x})^{2}}{n\sigma_{0}^{2}}<\quad\text{or}\quad>k,\]

where \(k\) is the smaller root of the equation \(Cx=e^{x-1}\) if \(C>1\), and the larger root of \(x=Ce^{x-1}\) if \(C<1\), where \(C\) is defined as in (i).

### _Section 1.8_

**Problem 1.14**: _Admissibility of unbiased procedures._ (i) Under the assumptions of Problem 1.10, if among the unbiased procedures there exists one with uniformly minimum risk, it is admissible. (ii) That in general an unbiased procedure with uniformly minimum risk need not be admissible is seen by the following example. Let \(X\) have a Poisson distribution truncated at \(0\), so that \(P_{\theta}\{X=x\}=\theta^{x}e^{-\theta}/[x!(1-e^{-\theta})]\) for \(x=1,2,\,\ldots\). For estimating \(\gamma(\theta)=e^{-\theta}\) with loss function \(L(\theta,d)=(d-e^{-\theta})^{2}\), there exists a unique unbiased estimate, and it is not admissible.

[(ii): The unique unbiased estimate \(\delta_{0}(x)=(-1)^{x+1}\) is dominated by \(\delta_{1}(x)=0\) or \(1\) as \(x\) is even or odd.]

**Problem 1.15**: _Admissibility of invariant procedures._ If a decision problem remains invariant under a finite group, and if there exists a procedure \(\delta_{0}\) that uniformly minimizes the risk among all invariant procedures, then \(\delta_{0}\) is admissible.

[This follows from the identity \(R(\theta,\,\delta)=R(\bar{g}\theta,\,g^{*}\delta g^{-1})\) and the hint given in Problem 1.11(i).]

**Problem 1.16**: (i) Let \(X\) take on the values \(\theta-1\) and \(\theta+1\) with probability \(\frac{1}{2}\) each. The problem of estimating \(\theta\) with loss function \(L(\theta,d)=\min(|\theta-d|,\,1)\) remains invariant under the transformation \(g\,X=X+c\), \(\bar{g}\theta=\theta+c\), \(g^{*}d=d+c\). Among invariant estimates, those taking on the values \(X-1\) and \(X+1\) with probabilities \(p\) and \(q\) (independent of \(X\)) uniformly minimize the risk. (ii) That the conclusion of Problem 1.15 need not hold when \(G\) is infinite follows by comparing the best invariant estimates of (i) with the estimate \(\delta_{1}(x)\) which is \(X+1\) when \(X<0\) and \(X-1\) when \(X\geq 0\).

### _Section 1.9_

**Problem 1.17**: In \(n\) independent trials with constant probability \(p\) of success, let \(X_{i}=1\) or \(0\) as the \(i\)th trial is a success or not. Then \(\sum_{i=1}^{n}X_{i}\) is minimal sufficient.

[Let \(T=\sum X_{i}\) and suppose that \(U=f(T)\) is sufficient and that \(f(k_{1})=\cdots=f(k_{r})=u\). Then \(P\{T=t\mid U=u\}\) depends on \(p\).]

**Problem 1.18**: (i) Let \(X_{1}\),..., \(X_{n}\) be a sample from the uniform distribution \(U(0,\,\theta)\), \(0<\theta<\infty\), and let \(T=\max(X_{1},\,\ldots,\,X_{n})\). Show that \(T\) is sufficient, once by using the definition of sufficiency and once by using the factorization criterion and assuming the existence of statistics \(Y_{i}\) satisfying (1.17)-(1.19).

(ii) Let \(X_{1}\),..., \(X_{n}\) be a sample from the exponential distribution \(E(a,\,b)\) with density \((1/b)e^{-(x-a)/b}\) when \(x\geq a\) (\(-\infty<a<\infty,0<b\)). Use the factorization criterion to prove that (\(\min(X_{1},\,\ldots,\,X_{n})\), \(\sum_{i=1}^{n}X_{i}\)) is sufficient for \(a\), \(b\), assuming the existence of statistics \(Y_{i}\) satisfying (1.17)-(1.19).

**Problem 1.19**: A statistic \(T\) satisfying (1.17)-(1.19) is sufficient if and only if it satisfies (1.20).

### Notes

Some of the basic concepts of statistical theory were initiated during the first quarter of the nineteenth century by Laplace in his fundamental _Theorie Analytique des Probabilites_ (1812), and by Gauss in his papers on the method of least squares. Loss and risk functions are mentioned in their discussions of the problem of point estimation, for which Gauss also introduced the condition of unbiasedness.

A period of intensive development of statistical methods began toward the end of the century with the work of Karl Pearson. In particular, two areas were explored in the research of R. A. Fisher, J. Neyman, and many others: estimation and the testing of hypotheses. The work of Fisher can be found in his books (1925, 1935, 1956) and in the five volumes of his collected papers (1971-1973). An interesting review of Fisher's contributions is provided by Savage (1976), and his life and work are recounted in the biography by his daughter Joan Fisher Box (1978). Many of Neyman's principal ideas are summarized in his Lectures and Conferences (1938b). Collections of his early papers and of his joint papers with E. S. Pearson have been published [Neyman (1967) and Neyman and Pearson (1967)], and Constance Reid (1982) has written his biography. An influential synthesis of the work of this period by Cramer appeared in 1946. Further concepts were introduced in Lehmann (1950, 1951a, 1951b). More recent surveys of the modern theories of estimation and testing are contained, for example, in the books by Strasser (1985), Stuart and Ord (1991), Stuart et al. (1999), Schervish (1995), Shao (1999) and Bickel and Doksum (2001).

A formal unification of the theories of estimation and hypothesis testing, which also contains the possibility of many other specializations, was achieved by Wald in his general theory of decision procedures. An account of this theory, which is closely related to von Neumann's theory of games, is found in Wald's book (1950) and in those of Blackwell and Girshick (1954), Ferguson (1967), and Berger (1985b).

## Chapter 2 The Probability Background

### 2.1 Probability and Measure

The mathematical framework for statistical decision theory is provided by the theory of probability, which in turn has its foundations in the theory of measure and integration. The present chapter serves to define some of the basic concepts of these theories, to establish some notation, and to state without proof some of the principal results which will be used throughout Chapters 3-9. In the remainder of this chapter, certain special topics are treated in more detail. Basic notions of convergence in probability theory which will be needed for large-sample statistical theory are deferred to Section 11.2.

Probability theory is concerned with situations which may result in different outcomes. The totality of these possible outcomes is represented abstractly by the totality of points in a space \(\mathcal{Z}\). Since the events to be studied are aggregates of such outcomes, they are represented by subsets of \(\mathcal{Z}\). The union of two sets \(C_{1}\), \(C_{2}\) will be denoted by \(C_{1}\cup C_{2}\), their intersection by \(C_{1}\cap C_{2}\), the complement of \(C\) by \(C^{c}=\mathcal{Z}-C\), and the empty set by \(0\). The probability \(P(C)\) of an event \(C\) is a real number between \(0\) and \(1\), in particular,

\[P(0)=0\ \ \ \text{and}\ \ \ P(\mathcal{Z})=1. \tag{2.1}\]

Probabilities have the property of _countable additivity_,

\[P\left(\bigcup C_{i}\right)=\sum P(C_{i})\ \ \ \ \ \text{if}\ \ \ C_{i}\cap C_{j}=0\ \ \ \text{for all}\ \ \ i\neq j. \tag{2.2}\]

Unfortunately it turns out that the set functions with which we shall be concerned usually cannot be defined in a reasonable manner for all subsets of \(\mathcal{Z}\) if they are to satisfy (2.2). It is, for example, not possible to give a reasonable definition of "area" for all subsets of a unit square in the plane.

The sets for which the probability function \(P\) will be defined are said to be "measurable". The domain of definition of \(P\) should include with any set \(C\) its complement \(C^{c}\), and with any countable number of events their union. By (2.1), it should alsoinclude \(\mathcal{Z}\). A class of sets that contains \(\mathcal{Z}\) and is closed under complementation and countable unions is a \(\sigma\)-field. Such a class is automatically also closed under countable intersections.

The starting point of any probabilistic considerations is therefore a space \(\mathcal{Z}\), representing the possible outcomes, and a \(\sigma\)-field \(\mathcal{C}\) of subsets of \(\mathcal{Z}\), representing the events whose probability is to be defined. Such a couple \((\mathcal{Z},\mathcal{C})\) is called a _measurable space_, and the elements of \(\mathcal{C}\) constitute the _measurable sets_. A countably additive nonnegative (not necessarily finite) set function \(\mu\) defined over \(\mathcal{C}\) and such that \(\mu(0)=0\) is called a _measure_. If it assigns the value 1 to \(\mathcal{Z}\), it is a _probability measure_. More generally, \(\mu\) is _finite_ if \(\mu(\mathcal{Z})<\infty\) and \(\sigma\)-finite if there exist \(C_{1},C_{2},\ldots\) in \(\mathcal{C}\) (which may always be taken to be mutually exclusive) such that \(\cup C_{i}=\mathcal{Z}\) and \(\mu(C_{i})<\infty\) for \(i=1,2\),.... Important special cases are provided by the following examples.

**Example 2.1.1**: **(Lebesgue measure)** Let \(\mathcal{Z}\) be the \(n\)-dimensional Euclidean space \(E_{n}\), and \(\mathcal{C}\) the smallest \(\sigma\)-field containing all rectangles1

Footnote 1: If \(\pi(z)\) is a statement concerning certain objects \(z\), then \(\{z:\pi(z)\}\) denotes the set of all those \(z\) for which \(\pi(z)\) is true.

\[R=\{(z_{1},\ldots,z_{n}):a_{i}<z_{i}\leq b_{i},i=1,\ldots,n\}.\]

The elements of \(\mathcal{C}\) are called the _Borel sets_ of \(E_{n}\). Over \(\mathcal{C}\) a unique measure \(\mu\) can be defined, which to any rectangle \(R\) assigns as its measure the volume of \(R\),

\[\mu(R)=\prod_{i=1}^{n}(b_{i}-a_{i}).\]

The measure \(\mu\) can be _completed_ by adjoining to \(\mathcal{C}\) all subsets of sets of measure zero. The domain of \(\mu\) is thereby enlarged to a \(\sigma\)-field \(\mathcal{C}^{\prime}\), the class of _Lebesgue-measurable_ sets. The term _Lebesgue measure_ is used for \(\mu\) both when it is defined over the Borel sets and when it is defined over the Lebesgue-measurable sets.

This example can be generalized to any nonnegative set function \(\nu\), which is defined and countably additive over the class of rectangles \(R\). There exists then, as before, a unique measure \(\mu\) over \((\mathcal{Z},\mathcal{C})\) that agrees with \(\nu\) for all \(R\). This measure can again be completed; however, the resulting \(\sigma\)-field depends on \(\mu\) and need not agree with the \(\sigma\)-field \(\mathcal{C}^{\prime}\) obtained above.

**Example 2.1.2**: **(Counting measure)** Suppose the \(\mathcal{Z}\) is countable, and let \(\mathcal{C}\) be the class of all subsets of \(\mathcal{Z}\). For any set \(C\), define \(\mu(C)\) as the number of elements of \(C\) if that number is finite, and otherwise as \(+\infty\). This measure is sometimes called _counting measure_. \(\blacksquare\)In applications, the probabilities over \((\mathcal{Z},\mathcal{C})\) refer to random experiments or observations, the possible outcomes of which are the points \(z\in\mathcal{Z}\). When recording the results of an experiment, one is usually interested only in certain of its aspects, typically some counts or measurements. These may be represented by a function \(T\) taking values in some space \(\mathcal{T}\).

Such a function generates in \(\mathcal{T}\) the \(\sigma\)-field \(\mathcal{B}^{\prime}\) of sets \(B\) whose inverse image

\[C=T^{-1}(B)=\{z:z\in\mathcal{Z},\,T(z)\in B\}\]

is in \(\mathcal{C}\), and for any given probability measure \(P\) over \((\mathcal{Z},\mathcal{C})\) a probability measure \(Q\) over \((\mathcal{T},\mathcal{B}^{\prime})\) defined by

\[Q(B)=P(T^{-1}(B)). \tag{2.3}\]

Frequently, there is given a \(\sigma\)-field \(\mathcal{B}\) of sets in \(\mathcal{T}\) such that the probability of \(B\) should be defined if and only if \(B\in\mathcal{B}\). This requires that \(T^{-1}(B)\in\mathcal{C}\) for all \(B\in\mathcal{B}\), and the function (or transformation) \(T\) from \((\mathcal{Z},\mathcal{C})\) into2\((\mathcal{T},\mathcal{B})\) is then said to be \(\mathcal{C}\)-measurable. Another implication is the sometimes convenient restriction of probability statements to the sets \(B\in\mathcal{B}\) even though there may exist sets \(B\notin\mathcal{B}\) for which \(T^{-1}(B)\in\mathcal{C}\) and whose probability therefore could be defined.

Footnote 2: The term _into_ indicates that the range of \(T\) is in \(\mathcal{T}\); if \(T(\mathcal{Z})=\mathcal{T}\), the transformation is said to be from \(\mathcal{Z}\)_onto_\(\mathcal{T}\).

Of particular interest is the case of a single measurement in which the function of \(T\) is real-valued. Let us denote it by \(X\), and let \(\mathcal{A}\) be the class of Borel sets on the real line \(\mathcal{X}\). Such a measurable real-valued \(X\) is called a _random variable_, and the probability measure it generates over \((\mathcal{X},\mathcal{A})\) will be denoted by \(P^{X}\) and called the probability distribution of \(X\). The value this measure assigns to a set \(A\in\mathcal{A}\) will be denoted interchangeably by \(P^{X}(A)\) and \(P(X\in A)\). Since the intervals \(\{x:x\leq a\}\) are in \(\mathcal{A}\), the probabilities \(F(a)=P(X\leq a)\) are defined for all \(a\). The function \(F\), the _cumulative distribution function_ (cdf) of \(X\), is nondecreasing and continuous on the right, and \(F(-\infty)=0\), \(F(+\infty)=1\). Conversely, if \(F\) is any function with these properties, a measure can be defined over the intervals by \(P\{a<X\leq b\}=F(b)-F(a)\). It follows from Example 2.1 that this measure uniquely determines a probability distribution over the Borel sets. Thus the probability distribution \(P^{X}\) and the cumulative distribution function \(F\) uniquely determine each other. These remarks extend to probability distributions over \(n\)-dimensional Euclidean space, where the cumulative distribution function is defined by

\[F(a_{1},\ldots,a_{n})=P\{X_{1}\leq a_{1},\ldots,X_{n}\leq a_{n}\}.\]

In concrete problems, the space \((\mathcal{Z},\mathcal{C})\), corresponding to the totality of possible outcomes, is usually not specified and remains in the background. The real starting point is the set \(X\) of observations (typically vector-valued) that are being recorded and which constitute the _data_, and the associated measurable space \((\mathcal{X},\mathcal{A})\), the _sample space_. Random variables or vectors that are measurable transformationsfrom \((\mathcal{X},\mathcal{A})\) into some \((\mathcal{T},\mathcal{B})\) are called _statistics_. The distribution of \(T\) is then given by (2.3) applied to all \(B\in\mathcal{B}\). With this definition, a statistic is specified by the function \(T\) and the \(\sigma\)-field \(\mathcal{B}\). We shall, however, adopt the convention that when a function \(T\) takes on its values in a Euclidean space, unless otherwise stated the \(\sigma\)-field \(\mathcal{B}\) of measurable sets will be taken to be the class of Borel sets. It then becomes unnecessary to mention it explicitly or to indicate it in the notation.

The distinction between statistics and random variables as defined here is slight. The term statistic is used to indicate that the quantity is a function of more basic observations; all statistics in a given problem are functions defined over the same sample space \((\mathcal{X},\mathcal{A})\). On the other hand, any real-valued statistic \(T\) is a random variable, since it has a distribution over \((\mathcal{T},\mathcal{B})\), and it will be referred to as a random variable when its origin is irrelevant. Which term is used therefore depends on the point of view and to some extent is arbitrary.

### Integration

According to the convention of the preceding section, a real-valued function \(f\) defined over \((\mathcal{X},\mathcal{A})\) is measurable if \(f^{-1}(B)\in\mathcal{A}\) for every Borel set \(B\) on the real line. Such a function \(f\) is said to be _simple_ if it takes on only a finite number of values. Let \(\mu\) be a measure defined over \((\mathcal{X},\mathcal{A})\), and let \(f\) be a simple function taking on the distinct values \(a_{1},\,\ldots,\,a_{m}\) on the sets \(A_{1},\,\ldots,\,A_{m}\), which are in \(\mathcal{A}\), since \(f\) is measurable. If \(\mu(A_{i})<\infty\) when \(a_{i}\neq 0\), the integral of \(f\) with respect to \(\mu\) is defined by

\[\int f\,d\mu=\sum a_{i}\mu(A_{i}). \tag{2.4}\]

Given any nonnegative measurable function \(f\), there exists a nondecreasing sequence of simple functions \(f_{n}\) converging to \(f\). Then the integral of \(f\) is defined as

\[\int f\,d\mu=\lim_{n\to\infty}\int f_{n}\,d\mu, \tag{2.5}\]

which can be shown to be independent of the particular sequence of \(f_{n}\)'s chosen. For any measurable function \(f\) its positive and negative parts

\[f^{+}(x)=\max[f(x),0]\ \ \ \mbox{and}\ \ \ f^{-}(x)=\max[-f(x),0] \tag{2.6}\]

are also measurable, and

\[f(x)=f^{+}(x)-f^{-}(x).\]

If the integrals of \(f^{+}\) and \(f^{-}\) are both finite, then \(f\) is said to be _integrable_, and its integral is defined as \[\int f\,d\mu=\int f^{+}\,d\mu-\int f^{-}\,d\mu.\]

If of the two integrals one is finite and one infinite, then the integral of \(f\) is defined to be the appropriate infinite value; if both are infinite, the integral is not defined.

**Example 2.2.1**: Let \(\mathcal{X}\) be the closed interval \([a,\,b]\), \(\mathcal{A}\) be the class of Borel sets or of Lebesgue measurable sets in \(\mathcal{X}\), and \(\mu\) be Lebesgue measure. Then the integral of \(f\) with respect to \(\mu\) is written as \(\int_{a}^{b}\,f(x)\,dx\), and is called the Lebesgue integral of \(f\). This integral generalizes the Riemann integral in that it exists and agrees with the Riemann integral of \(f\) whenever the latter exists.

**Example 2.2.2**: Let \(\mathcal{X}\) be countable and consist of the points \(x_{1},\,x_{2},\,\ldots\,;\) let \(\mathcal{A}\) be the class of all subsets of \(\mathcal{X}\), and let \(\mu\) assign measure \(b_{i}\) to the point \(x_{i}\). Then \(f\) is integrable provided \(\sum f(x_{i})b_{i}\) converges absolutely, and \(\int f\,d\mu\) is given by this sum.

Let \(P^{X}\) be the probability distribution of a random variable \(X\), and let \(T\) be a real-valued statistic. If the function \(T(x)\) is integrable, its _expectation_ is defined by

\[E(T)=\int T(x)\,dP^{X}(x). \tag{2.7}\]

It will be seen from Lemma 2.3.2 in Section 2.3 that the integration can be carried out alternatively in \(t\)-space with respect to the distribution of \(T\) defined by (2.3), so that also

\[E(T)=\int t\,dP^{T}(t). \tag{2.8}\]

Definition (2.5) of the integral permits the basic convergence theorems.

**Theorem 2.2.1**: **Fatou's Lemma** _Let \(f_{n}\) be a sequence of measurable functions such that \(f_{n}(x)\geq 0\) and \(f_{n}(x)\to f(x)\), except possibly on a set of \(x\) values having \(\mu\) measure 0. Then,_

\[\int fd\mu\leq\liminf\int f_{n}d\mu\.\]

**Theorem 2.2.2**: _Let \(f_{n}\) be a sequence of measurable functions, and let \(f_{n}(x)\to f(x)\), except possibly on a set of \(x\) values having \(\mu\) measure 0. Then_

\[\int f_{n}\,d\mu\to\int f\,d\mu\]

_if any one of the following conditions holds:_

* **Lebesgue Monotone Convergence Theorem:** _the_ \(f_{n}\)_'s are nonnegative and the sequence is nondecreasing;__or_

* **Lebesgue Dominated Convergence Theorem:** _there exists an integrable function_ \(g\) _such that_ \(|f_{n}(x)|\leq g(x)\) _for_ \(n\) _and_ \(x\)_._

_or_

* **General Form:** _there exist_ \(g_{n}\) _and_ \(g\) _with_ \(|f_{n}|\leq g_{n}\)_,_ \(g_{n}(x)\to g(x)\) _except possibly on a_ \(\mu\) _null set, and_ \(\int g_{n}d\mu\to\int gd\mu\)_._

**Corollary 2.2.1**: **Vitali's Theorem** _Suppose \(f_{n}\) and \(f\) are real-valued measurable functions with \(f_{n}(x)\to f(x)\), except possibly on a set having \(\mu\) measure 0. Assume_

\[\limsup_{n}\int f_{n}^{2}(x)d\mu(x)\leq\int f^{2}(x)d\mu(x)<\infty\.\]

_Then,_

\[\int|f_{n}(x)-f(x)|^{2}d\mu(x)\to 0\ \.\]

For a proof of this result, see Theorem 6.1.3 of Hajek et al. (1999).

For any set \(A\in{\cal A}\), let \(I_{A}\) be its _indicator function_ defined by

\[I_{A}(x)=1\ {\rm or}\ 0\ \ \ \ \ as\ \ x\in A\ {\rm or}\ x\in A^{c}, \tag{2.9}\]

and let

\[\int_{A}f\,d\mu=\int fI_{A}\,d\mu. \tag{2.10}\]

If \(\mu\) is a measure and \(f\) a nonnegative measurable function over \(({\cal X},{\cal A})\), then

\[\nu(A)=\int_{A}f\,d\mu \tag{2.11}\]

defines a new measure over \(({\cal X},{\cal A})\). The fact that (2.11) holds for all \(A\in{\cal A}\) is expressed by writing

\[d\nu=f\,d\mu\ \ \ {\rm or}\ \ \ f=\frac{d\nu}{d\mu}. \tag{2.12}\]

Let \(\mu\) and \(\nu\) be two given \(\sigma\)-finite measures over \(({\cal X},{\cal A})\). If there exists a function \(f\) satisfying (2.12), it is determined through this relation up to sets of measure zero, since

\[\int_{A}f\,d\mu=\int_{A}g\,d\mu\ \ \ \ \ \ {\rm for\ all}\ \ \ A\in{\cal A}\]implies that \(f=g\) a.e. \(\mu\).3 Such an \(f\) is called the _Radon-Nikodym derivative_ of \(\nu\) with respect to \(\mu\), and in the particular case that \(\nu\) is a probability measure, the _probability density_ of \(\nu\) with respect to \(\mu\).

Footnote 3: A statement that holds for all points \(x\) except possibly on a set of \(\mu\)-measure zero is said to hold _almost everywhere_\(\mu\), abbreviated a.e. \(\mu\), or to hold a.e. (\(\mathcal{A}\), \(\mu\)) if it is desirable to indicate the \(\sigma\)-field over which \(\mu\) is defined.

The question of existence of a function \(f\) satisfying (2.12) for given measures \(\mu\) and \(\nu\) is answered in terms of the following definition. A measure \(\nu\) is _absolutely continuous_ with respect to \(\mu\) if

\[\mu(A)=0\ \ \ \text{implies}\ \ \ \nu(A)=0.\]

**Theorem 2.2.3**.: **(Radon-Nikodym)** _If \(\mu\) and \(\nu\) are \(\sigma\)-finite measures over (\(\mathcal{X}\), \(\mathcal{A}\)), then there exists a measurable function \(f\) satisfying (2.12) if and only if \(\nu\) is absolutely continuous with respect to \(\mu\)._

The _direct_ (or _Cartesian_) _product_\(A\times B\) of two sets \(A\) and \(B\) is the set of all pairs \((x,y)\) with \(x\in A\), \(y\in B\). Let (\(\mathcal{X}\), \(\mathcal{A}\)) and (\(\mathcal{Y}\), \(\mathcal{B}\)) be two measurable spaces, and let \(\mathcal{A}\times\mathcal{B}\) be the smallest \(\sigma\)-field containing all sets \(A\times B\) with \(A\in\mathcal{A}\) and \(B\in\mathcal{B}\). If \(\mu\) and \(\nu\) are two \(\sigma\)-finite measures over (\(\mathcal{X}\), \(\mathcal{A}\)) and (\(\mathcal{Y}\), \(\mathcal{B}\)), respectively, then there exists a unique measure \(\lambda=\mu\times\nu\) over (\(\mathcal{X}\times\mathcal{Y}\), \(\mathcal{A}\times\mathcal{B}\)), the _product_ of \(\mu\) and \(\nu\), such that for any \(A\in\mathcal{A}\), \(B\in\mathcal{B}\),

\[\lambda(A\times B)=\mu(A)\nu(B). \tag{2.13}\]

**Example 2.2.3**.: Let \(\mathcal{X}\), \(\mathcal{Y}\) be Euclidean spaces of \(m\) and \(n\) dimensions, and let \(\mathcal{A}\), \(\mathcal{B}\) be the \(\sigma\)-fields of Borel sets in these spaces. Then \(\mathcal{X}\times\mathcal{Y}\) is an (\(m+n\))-dimensional Euclidean space, and \(\mathcal{A}\times\mathcal{B}\) the class of its Borel sets.

**Example 2.2.4**.: Let \(Z=(X,Y)\) be a random variable defined over (\(\mathcal{X}\times\mathcal{Y}\), \(\mathcal{A}\times\mathcal{B}\)), and suppose that the random variables \(X\) and \(Y\) have distributions \(P^{X}\), \(P^{Y}\) over (\(\mathcal{X}\), \(\mathcal{A}\)) and (\(\mathcal{Y}\), \(\mathcal{B}\)). Then \(X\) and \(Y\) are said to be _independent_ if the probability distribution \(P^{Z}\) of \(Z\) is the product \(P^{X}\times P^{Y}\).

In terms of these concepts the reduction of a double integral to a repeated one is given by the following theorem.

**Theorem 2.2.4**.: **(Fubini)** _Let \(\mu\) and \(\nu\) be \(\sigma\)-finite measures over (\(\mathcal{X}\), \(\mathcal{A}\)) and (\(\mathcal{Y}\), \(\mathcal{B}\)) respectively, and let \(\lambda=\mu\times\nu\). If \(f(x,y)\) is integrable with respect to \(\lambda\), then_

1. _for almost all_ (\(\nu\)) _fixed_ \(y\)_, the function_ \(f(x,y)\) _is integrable with respect to_ \(\mu\)_,_
2. _the function_ \(\int f(x,y)\,d\mu(x)\) _is integrable with respect to_ \(\nu\)_, and_ \[\int f(x,y)\,d\lambda(x,y)=\int\left[\int f(x,y)\,d\mu(x)\right]\,d\nu(y).\] (2.14)

### Statistics and Subfields

According to the definition of Section 2.1, a statistic is a measurable transformation \(T\) from the sample space (\(\mathcal{X}\), \(\mathcal{A}\)) into a measurable space (\(\mathcal{T}\), \(\mathcal{B}\)). Such a transformation induces in the original sample space the subfield4

Footnote 4: We shall use this term in place of the more cumbersome “sub-\(\sigma\)-field”.

\[\mathcal{A}_{0}=T^{-1}(\mathcal{B})=\left\{T^{-1}(B):B\in\mathcal{B}\right\}. \tag{2.15}\]

Since the set \(T^{-1}[T(A)]\) contains \(A\) but is not necessarily equal to \(A\), the \(\sigma\)-field \(\mathcal{A}_{0}\) need not coincide with \(\mathcal{A}\) and hence can be a proper subfield of \(\mathcal{A}\). On the other hand, suppose for a moment that \(\mathcal{T}=T(\mathcal{X})\), that is, that the transformation \(T\) is onto rather than into \(\mathcal{T}\). Then

\[T\left[T^{-1}(B)\right]=B\quad\text{ for all }\quad B\in\mathcal{B}, \tag{2.16}\]

so that the relationship \(A_{0}=T^{-1}(B)\) establishes a 1:1 correspondence between the sets of \(\mathcal{A}_{0}\) and \(\mathcal{B}\), which is an isomorphism--that is, which preserves the set operations of intersection, union, and complementation. For most purposes it is therefore immaterial whether one works in the space (\(\mathcal{X}\), \(\mathcal{A}_{0}\)) or in (\(\mathcal{T}\), \(\mathcal{B}\)). These generate two equivalent classes of events, and therefore of measurable functions, possible decision procedures, etc. If the transformation \(T\) is only into \(\mathcal{T}\), the above 1:1 correspondence applies to the class \(\mathcal{B}^{\prime}\) of subsets of \(T^{\prime}=T(\mathcal{X})\) which belong to \(\mathcal{B}\), rather than to \(\mathcal{B}\) itself. However, any set \(B\in\mathcal{B}\) is equivalent to \(B^{\prime}=B\cap\mathcal{T}^{\prime}\) in the sense that any measure over (\(\mathcal{X}\), \(\mathcal{A}\)) assigns the same measure to \(B^{\prime}\) as to \(B\). Considered as classes of events, \(\mathcal{A}_{0}\) and \(\mathcal{B}\) therefore continue to be equivalent, with the only difference that \(\mathcal{B}\) contains several (equivalent) representations of the same event.

As an example, let \(\mathcal{X}\) be the real line and \(\mathcal{A}\) the class of Borel sets, and let \(T(x)=x^{2}\). Let \(\mathcal{T}\) be either the positive real axis or the whole real axis, and let \(\mathcal{B}\) be the class of Borel subsets of \(\mathcal{T}\). Then \(\mathcal{A}_{0}\) is the class of Borel sets that are symmetric with respect to the origin. When considering, for example, real-valued measurable functions, one would, when working in \(\mathcal{T}\)-space, restrict attention to measurable function of \(x^{2}\). Instead, one could remain in the original space, where the restriction would be to the class of even measurable functions of \(x\). The equivalence is clear. Which representation is more convenient depends on the situation.

That the correspondence between the sets \(A_{0}=T^{-1}(B)\in\mathcal{A}_{0}\) and \(B\in\mathcal{B}\) establishes an analogous correspondence between measurable functions defined over (\(\mathcal{X}\), \(\mathcal{A}_{0}\)) and (\(\mathcal{T}\), \(\mathcal{B}\)) is shown by the following lemma.

**Lemma 2.3.1**: _Let the statistic \(T\) from \(({\cal X},{\cal A})\) into \(({\cal T},{\cal B})\) induce the subfield \({\cal A}_{0}\). Then a real-valued \({\cal A}\)-measurable function \(f\) is \({\cal A}_{0}\)-measurable if and only if there exists a \({\cal B}\)-measurable function \(g\) such that_

\[f(x)=g[T(x)]\]

_for all \(x\)._

Proof. Suppose first that such a function \(g\) exists. Then the set

\[\{x:f(x)<r\}=T^{-1}(\{t:g(t)<r\})\]

is in \({\cal A}_{0}\), and \(f\) is \({\cal A}_{0}\)-measurable. Conversely, if \(f\) is \({\cal A}_{0}\)-measurable, then the sets

\[A_{in}=\left\{x:\frac{i}{2^{n}}<f(x)\leq\frac{i+1}{2^{n}}\right\},\ \ \ \ \ i=0,\pm 1,\pm 2,\ldots\]

are (for fixed \(n\)) disjoint sets in \({\cal A}_{0}\) whose union is \({\cal X}\), and there exist \(B_{in}\in{\cal B}\) such that \(A_{in}=T^{-1}(B_{in})\). Let

\[B_{in}^{*}=B_{in}\cap\{\bigcup_{j\neq i}B_{jn}\}^{c}\.\]

Since \(A_{in}\) and \(A_{jn}\) are mutually exclusive for \(i\neq j\), the set \(T^{-1}(B_{in}\cap B_{jn})\) is empty and so is the set \(T^{-1}(B_{in}\cap\{B_{in}^{*}\}^{c})\). Hence, for fixed \(n\), the sets \(B_{in}^{*}\) are disjoint, and still satisfy \(A_{in}=T^{-1}(B_{in}^{*})\). Defining

\[f_{n}(x)=\frac{i}{2^{n}}\ \ \ {\rm if}\ \ x\in A_{in},\ \ \ \ \ i=0\pm 1,\pm 2,\ldots,\]

one can write

\[f_{n}(x)=g_{n}[T(x)],\]

where

\[g_{n}(t)=\left\{\begin{array}{ll}\frac{i}{2^{n}}&{\rm for}\ t\in B_{in}^{*},\ \ \ i=0\pm 1,\pm 2,\ldots,\\ 0&{\rm otherwise}.\end{array}\right.\]

Since the functions \(g_{n}\) are \({\cal B}\)-measurable, the set \(B\) on which \(g_{n}(t)\) converges to a finite limit is in \({\cal B}\). Let \(R=T({\cal X})\) be the range of \(T\). Then for \(t\in R\),

\[\lim g_{n}[T(x)]=\lim\ f_{n}(x)=f(x)\]

for all \(x\in{\cal X}\) so that \(R\) is contained in \(B\). Therefore, the function \(g\) defined by \(g(t)=\lim g_{n}(t)\) for \(t\in B\) and \(g(t)=0\) otherwise possesses the required properties.

The relationship between integrals of the functions \(f\) and \(g\) above is given by the following lemma.

**Lemma 2.3.2**: _Let \(T\) be a measurable transformation from \((\mathcal{X},\mathcal{A})\) into \((\mathcal{T},\mathcal{B})\), \(\mu\) a \(\sigma\)-finite measure over \((\mathcal{X},\mathcal{A})\), and \(g\) a real-valued measurable function of \(t\). If \(\mu^{*}\) is the measure defined over \((\mathcal{T},\mathcal{B})\) by_

\[\mu^{*}(B)=\mu\left[T^{-1}(B)\right]\quad\text{ for all }\quad B\in\mathcal{ B}, \tag{2.17}\]

_then for any \(B\in\mathcal{B}\),_

\[\int_{T^{-1}(B)}g[T(x)]\,d\mu(x)=\int_{B}g(t)\,d\mu^{*}(t) \tag{2.18}\]

_in the sense that if either integral exists, so does the other and the two are equal._

Proof. Without loss of generality let \(B\) be the whole space \(\mathcal{T}\). If \(g\) is the indicator of a set \(B_{0}\in\mathcal{B}\), the lemma holds, since the left- and right-hand sides of (2.18) reduce respectively to \(\mu[T^{-1}(B_{0})]\) and \(\mu^{*}(B_{0})\), which are equal by the definition of \(\mu^{*}\). If follows that (2.18) holds successively for all simple functions, for all nonnegative measurable functions, and hence finally for all integrable functions.

### Conditional Expectation and Probability

If two statistics induce the same subfield \(\mathcal{A}_{0}\), they are equivalent in the sense of leading to equivalent classes of measurable events. This equivalence is particularly relevant to considerations of conditional probability. Thus if \(X\) is normally distributed with zero mean, the information carried by the statistics \(|X|\), \(X^{2}\), \(e^{-X^{2}}\), and so on, is the same. Given that \(|X|=t\), \(X^{2}=t^{2}\), \(e^{-X^{2}}=e^{-t^{2}}\), it follows that \(X\) is \(\pm t\), and any reasonable definition of conditional probability will assign probability \(\frac{1}{2}\) to each of these values. The general definition of conditional probability to be given below will in fact involve essentially only \(\mathcal{A}_{0}\) and not the range space \(\mathcal{T}\) of \(T\). However, when referred to \(\mathcal{A}_{0}\) alone the concept loses much of its intuitive meaning, and the gap between the elementary definition and that of the general case becomes unnecessarily wide. For these reasons it is frequently more convenient to work with a particular representation of a statistic, involving a definite range space \((\mathcal{T},\mathcal{B})\).

Let \(P\) be a probability measure over \((\mathcal{X},\mathcal{A})\), \(T\) a statistic with range space \((\mathcal{T},\mathcal{B})\), and \(\mathcal{A}_{0}\) the subfield it induces. Consider a nonnegative function \(f\) which is integrable \((\mathcal{A},\,P)\), that is, \(\mathcal{A}\)-measurable and \(P\)-integrable. Then \(\int_{A}f\,d\,P\) is defined for all \(A\in\mathcal{A}\) and therefore for all \(A_{0}\in\mathcal{A}_{0}\). If follows from the Radon-Nikodym Theorem (Theorem 2.2.3) that there exists a function \(f_{0}\) which is integrable \((\mathcal{A}_{0},\,P)\) and such that

\[\int_{A_{0}}f\,dP=\int_{A_{0}}f_{0}\,dP\quad\text{ for all }\quad A_{0}\in \mathcal{A}_{0}, \tag{2.19}\]and that \(f_{0}\) is unique (\(\mathcal{A}_{0}\), \(P\)). By Lemma 2.3.1, \(f_{0}\) depends on \(x\) only through \(T(x)\). In the example of a normally distributed variable \(X\) with zero mean, and \(T=X^{2}\), the function \(f_{0}\) is determined by (2.19) holding for all sets \(A_{0}\) that are symmetric with respect to the origin, so that \(f_{0}(x)=\frac{1}{2}[f(x)+f(-x)]\).

The function \(f_{0}\) defined through (2.19) is determined by two properties:

1. Its average value over any set \(A_{0}\) with respect to \(P\) is the same as that of \(f\);
2. It depends on \(x\) only through \(T(x)\) and hence is constant on the sets \(D_{x}\) over which \(T\) is constant.

Intuitively, what one attempts to do in order to construct such a function is to define \(f_{0}(x)\) as the conditional \(P\)-average of \(f\) over the set \(D_{x}\). One would thereby replace the single averaging process of integrating \(f\) represented by the left-hand side with a two-stage averaging process such as an iterated integral. Such a construction can actually be carried out when \(X\) is a discrete variable and in the regular case considered in Section 1.9; \(f_{0}(x)\) is then just the conditional expectation of \(f(X)\) given \(T(x)\). In general, it is not clear how to define this conditional expectation directly. Since it should, however, possess properties (i) and (ii), and since these through (2.19) determine \(f_{0}\) uniquely (\(\mathcal{A}_{0}\), \(P\)), we shall take \(f_{0}(x)\) of (2.19) as the general definition of the _conditional expectation_\(E[f(X)\mid T(x)]\). Equivalently, if \(f_{0}(x)=g[T(x)]\), one can write

\[E[f(X)\mid t]=E[f(X)\mid T=t]=g(t),\]

so that \(E[f(X)\mid t]\) is a \(\mathcal{B}\)-measurable function defined up to equivalence (\(\mathcal{B}\), \(P^{T}\)). In the relationship of integrals given in Lemma 2.3.2, if \(\mu=P^{X}\), then \(\mu^{*}=P^{T}\), and it is seen that the function \(g\) can be defined directly in terms of \(f\) through

\[\int_{T^{-1}(B)}f(x)\,dP^{X}(x)=\int_{B}g(t)\,dP^{T}(t)\quad\text{ for all }\quad B\in\mathcal{B}, \tag{2.20}\]

which is equivalent to (2.19).

So far, \(f\) has been assumed to be nonnegative. In the general case, the conditional expectation of \(f\) is defined as

\[E[f(X)\mid t]=E[f^{+}(X)\mid t]-E[f^{-}(X)\mid t].\]

**Example 2.4.1**.: (**Order statistics**) Let \(X_{1}\),..., \(X_{n}\) be identically and independently distributed random variables with continuous distribution function, and let

\[T(x_{1},\ldots,x_{n})=(x_{(1)},\ldots,x_{(n)}),\]

where \(x_{(1)}\leq\cdots\leq x_{(n)}\) denote the ordered \(x\)'s. Without loss of generality one can restrict attention to the points with \(x_{(1)}<\cdots<x_{(n)}\), since the probability of two coordinates being equal is \(0\). Then \(\mathcal{X}\) is the set of all \(n\)-tuples with distinct coordinates, \(\mathcal{T}\) the set of all ordered \(n\)-tuples, and \(\mathcal{A}\) and \(\mathcal{B}\) are the classes of Borel subsets of \(\mathcal{X}\) and 

[MISSING_PAGE_EMPTY:1656]

Equivalent to the statistic \(T(x)=(x_{(1)},\ldots,x_{(n)})\), the set of _order statistics_ is \(U(x)=\left(\sum x_{i},\sum x_{i}^{2},\ldots,\sum x_{i}^{n}\right)\). This is an immediate consequence of the fact, to be shown below, that if \(T(x^{0})=t^{0}\) and \(U(x^{0})=u^{0}\), then

\[T^{-1}\left(\left\{t^{0}\right\}\right)=U^{-1}\left(\left\{u^{0}\right\} \right)=S,\]

where \(\left\{t^{0}\right\}\) and \(\left\{u^{0}\right\}\) denote the sets consisting of the single point \(t^{0}\) and \(u^{0}\), respectively, and where \(S\) consists of the totality of points \(x=(x_{1},\ldots,x_{n})\) obtained by permuting the coordinates of \(x^{0}=(x_{1}^{0},\ldots,x_{n}^{0})\) in all possible ways.

That \(T^{-1}\left(\left\{t^{0}\right\}\right)=S\) is obvious. To see the corresponding fact for \(U^{-1}\), let

\[V(x)=\left(\sum_{i}x_{i},\sum_{i<j}x_{i}x_{j},\sum_{i<j<k}x_{i}x_{j}x_{k}, \ldots,x_{1}x_{2}\cdots x_{n}\right),\]

so that the components of \(V(x)\) are the elementary symmetric functions \(v_{1}=\sum x_{i},\ldots,v_{n}=x_{1}\ldots x_{n}\) of the \(n\) arguments \(x_{1},\ldots,x_{n}\). Then

\[(x-x_{1})\ldots(x-x_{n})=x^{n}-v_{1}x^{n-1}+v_{2}x^{n-2}-\cdots+(-1)^{n}v_{n}.\]

Hence \(V(x^{0})=v^{0}=(v_{1}^{0},\ldots,v_{n}^{0})\) implies that \(V^{-1}(\left\{v^{0}\right\})=S\). That then also \(U^{-1}(\left\{u^{0}\right\})=S\) follows from the 1:1 correspondence between \(u\) and \(v\) established by the relations (known as Newton's identities)5:

Footnote 5: For a proof of these relations, see for example Turnbull (1952), Section 32.

\[u_{k}-v_{1}u_{k-1}+v_{2}u_{k-2}-\cdots+(-1)^{k-1}v_{k-1}u_{1}+(-1)^{k}kv_{k}=0\]

for \(1\leq k\leq n\).

It is easily verified from the above definition that conditional expectation possesses most of the usual properties of expectation. It follows of course from the nonuniqueness of the definition that these properties can hold only (\(\mathcal{B}\), \(P^{T}\)). We state this formally in the following lemma.

**Lemma 2.4.1**: _If \(T\) is a statistic and the functions \(f\), \(g\),... are integrable (\(\mathcal{A}\), \(P\)), then a.e. \((\mathcal{B}\), \(P^{T})\)_

1. \(E[af(X)+bg(X)\mid t]=aE[f(X)\mid t]+bE[g(X)\mid t]\)_;_
2. \(E[h(T)f(X)\mid t]=h(t)E[f(X)\mid t]\)_;_
3. \(a\leq f(x)\leq b\)__\((\mathcal{A}\)_,_ \(P) _implies_ \(a\leq E[f(X)\mid t]\leq b\)_;_
4. \(|f_{n}|\leq g\)_,_ \(f_{n}(x)\to f(x)\)__\((\mathcal{A}\)_,_ \(P) _implies_ \(E[f_{n}(X)\mid t]\to E[f(X)\mid t]\)_._

A further useful result is obtained by specializing (2.20) to the case that \(B\) is the whole space \(\mathcal{T}\). One then has

**Lemma 2.4.2**: _If \(E[\,\mid\,f(X)\,\mid\,]<\infty\), and if \(g(t)=E[\,f(X)\,\mid\,t\,]\), then_

\[E[\,f(X)\,]=E[\,g(T)\,]\,, \tag{2.21}\]

_that is, the expectation can be obtained as the expected value of the conditional expectation._

Since \(P\{X\in A\}=E[\,I_{A}(X)\,]\), where \(I_{A}\) denotes the indicator of the set \(A\), it is natural to define the _conditional probability_ of \(A\) given \(T=t\) by

\[P(A\mid t)=E[\,I_{A}(X)\,\mid\,t\,]. \tag{2.22}\]

In view of (2.20) the defining equation for \(P(A\mid t)\) can therefore be written as

\[P^{X}\left(A\cap T^{-1}(B)\right) =\int_{A\cap T^{-1}(B)}\,dP^{X}(x) \tag{2.23}\] \[=\int_{B}P(A\mid t)\,dP^{T}(t)\,\,\,\,\,\,\,\,\,\,\,\text{for all }\,\,\,\,B\in\mathcal{B}.\]

It is an immediate consequence of Lemma 2.4.1 that subject to the appropriate null-set6 qualifications, \(P(A\mid t)\) possesses the usual properties of probabilities, as summarized in the following lemma.

Footnote 6: This term is used as an alternative to the more cumbersome “set of measure zero”.

**Lemma 2.4.3**: _If \(T\) is a statistic with range space \((\mathcal{T},\mathcal{B})\), and \(A,\,B,\,A_{1},\,A_{2},\,\ldots\) are sets belonging to \(\mathcal{A}\), then a.e. \((\mathcal{B},\,P^{T})\)_

1. \(0\leq P(A\mid t)\leq 1\)_;_
2. _if the sets_ \(A_{1},\,A_{2},\,\ldots\) _are mutually exclusive,_ \[P\left(\bigcup A_{i}\mid t\right)=\sum P(A_{i}\mid t);\]
3. \(A\subset B\) _implies_ \(P(A\mid t)\leq P(B\mid t)\)_._

According to definition (2.22), the conditional probability \(P(A\mid t)\) must be considered for fixed \(A\) as a \(\mathcal{B}\)-measurable function of \(t\). This is in contrast to the elementary definition in which one takes \(t\) as fixed and considers \(P(A\mid t)\) for varying \(A\) as a set function over \(\mathcal{A}\). Lemma 2.4.3 suggests the possibility that the interpretation of \(P(A\mid t)\) for fixed \(t\) as a probability distribution over \(\mathcal{A}\) may be valid also in the general case. However, the equality \(P(A_{1}\cup A_{2}\mid t)=P(A_{1}\mid t)+P(A_{2}\mid t)\), for example, can break down on a null set that may vary with \(A_{1}\) and \(A_{2}\), and the union of all these null sets need no longer have measure zero.

For an important class of cases, this difficulty can be overcome through the nonuniqueness of the functions \(P(A\mid t)\), which for each fixed \(A\) are determined only up to sets of measure zero in \(t\). Since all determinations of these functions areequivalent, it is enough to find a specific determination for each \(A\) so that for each fixed \(t\) these determinations jointly constitute a probability distribution over \(\mathcal{A}\). This possibility is illustrated by Example 2.4.1, in which the conditional probability distribution given \(T(x)=t\) can be taken to assign probability \(1/n!\) to each of the \(n!\) points satisfying \(T(x)=t\). Sufficient conditions for the existence of such conditional distributions will be given in the next section. For counterexamples see Blackwell and Dubins (1975).

### Conditional Probability Distributions

We shall now investigate the existence7 of conditional probability distributions under the assumption, satisfied in most statistical applications, that \(\mathcal{X}\) is a Borel set in a Euclidean space. We shall then say for short that \(\mathcal{X}\) is Euclidean and assume that, unless otherwise stated, \(\mathcal{A}\) is the class of Borel subsets of \(\mathcal{X}\).

Footnote 7: This section may be omitted at first reading. Its principal application is in the proof of Lemma 2.7.2(ii) in Section 2.7, which in turn is used only in the proof of Theorem 4.4.1.

**Theorem 2.5.1**: _If \(\mathcal{X}\) is Euclidean, there exist determinations of the functions \(P(A\mid t)\) such that for each \(t\), \(P(A\mid t)\) is a probability measure over \(\mathcal{A}\)._

Proof. By setting equal to \(0\) the probability of any Borel set in the complement of \(\mathcal{X}\), one can extend the given probability measure to the class of all Borel sets and can therefore assume without loss of generality that \(\mathcal{X}\) is the full Euclidean space. For simplicity we shall give the proof only in the one-dimensional case. For each real \(x\) put \(F(x,t)=P((-\infty,x]\mid t)\) for some version of this conditional probability function, and let \(r_{1},r_{2},\ldots\) denote the set of all rational numbers in some order. Then \(r_{i}<r_{j}\) implies that \(F(r_{i},t)\leq F(r_{j},t)\) for all \(t\) except those in a null set \(N_{ij}\), and hence that \(F(x,t)\) is nondecreasing in \(x\) over the rationals for all \(t\) outside of the null set \(N^{\prime}=\bigcup N_{ij}\). Similarly, it follows from Lemma 2.4.1(iv) that for all \(t\) not in a null set \(N^{\prime\prime}\), as \(n\) tends to infinity lim \(F(r_{i}+1/n,t)=F(r_{i},t)\) for \(i=1\), \(2\), \(\ldots\), lim \(F(n,t)=1\), and lim \(F(-n,t)=0\). Therefore, for all \(t\) outside of the null set \(N^{\prime}\cup N^{\prime\prime}\), \(F(x,t)\) considered as a function of \(x\) is properly normalized, monotone, and continuous on the right over the rationals. For \(t\) not in \(N^{\prime}\cup N^{\prime\prime}\) let \(F^{*}(x,t)\) be the unique function that is continuous on the right in \(x\) and agrees with \(F(x,t)\) for all rational \(x\). Then \(F^{*}(x,t)\) is a cumulative distribution function and therefore determines a probability measure \(P^{*}(A\mid t)\) over \(\mathcal{A}\). We shall now show that \(P^{*}(A\mid t)\) is a conditional probability of \(A\) given \(t\), by showing that for each fixed \(A\) it is a \(\mathcal{B}\)-measurable function of \(t\) satisfying (2.23). This will be accomplished by proving that for each fixed \(A\in\mathcal{A}\)

\[P^{*}(A\mid t)=P(A\mid t)\hskip 14.226378pt(\mathcal{B},P^{T}).\]By definition of \(P^{*}\) this is true whenever \(A\) is one of the sets \((-\infty,\,x]\) with \(x\) rational. It holds next when \(A\) is an interval \((a,\,b]=(-\infty,\,b]-(-\infty,\,a]\) with \(a,\,b\) rational, since \(P^{*}\) is a measure and \(P\) satisfies Lemma 2.4.3(ii). Therefore, the desired equation holds for the field \(\mathcal{F}\) of all sets \(A\) which are finite unions of intervals \((a_{i},\,b_{i}]\) with rational end points. Finally, the class of sets for which the equation holds is a monotone class (see Problem 2.1) and hence contains the smallest \(\sigma\)-field containing \(\mathcal{F}\), which is \(\mathcal{A}\). The measure \(P^{*}(A\mid t)\) over \(\mathcal{A}\) was defined above for all \(t\) not in \(N^{\prime}\cup N^{\prime\prime}\). However, since neither the measurability of a function nor the values of its integrals are affected by its values on a null set, one can take arbitrary probability measures over \(\mathcal{A}\) for \(t\) in \(N^{\prime}\cup N^{\prime\prime}\) and thereby complete the determination.

If \(X\) is a vector-valued random variable with probability distribution \(P^{X}\) and \(T\) is a statistic defined over \((\mathcal{X},\,\mathcal{A})\), let \(P^{X|t}\) denote any version of the family of conditional distributions \(P(A\mid t)\) over \(\mathcal{A}\) guaranteed by Theorem 2.5.1. The connection with conditional expectation is given by the following theorem.

**Theorem 2.5.2**: _If \(X\) is a vector-valued random variable and \(E|f(X)|<\infty\), then_

\[E[f(X)\mid t]=\int\,f(x)\,dP^{X|t}(x)\ \ \ \ \ (\mathcal{B},\,P^{T}). \tag{2.24}\]

Proof. Equation (2.24) holds if \(f\) is the indicator of any set \(A\in\mathcal{A}\). It then follows from Lemma 2.4.1 that it also holds for any simple function and hence for any integrable function.

The determination of the conditional expectation \(E[f(X)\mid t]\) given by the right-hand side of (2.24) possesses for each \(t\) the usual properties of an expectation, (i), (iii), and (iv) of Lemma 2.4.1, which previously could be asserted only up to sets of measure zero depending on the functions \(f,\,g,\ldots\) involved. Under the assumptions of Theorem 2.5.1 a similar strengthening is possible with respect to (ii) of Lemma 2.4.1, which can be shown to hold except possibly on a null set \(N\) not depending on the function \(h\). It will be sufficient for the present purpose to prove this under the additional assumption that the range space of the statistic \(T\) is also Euclidean. For a proof without this restriction, see for example Billingsley (1995).

**Theorem 2.5.3**: _If \(T\) is a statistic with Euclidean domain and range spaces \((\mathcal{X},\,\mathcal{A})\) and \((\mathcal{T},\,\mathcal{B})\), there exists a determination \(P^{X|t}\) of the conditional probability distribution and a null set \(N\) such that the conditional expectation computed by_

\[E[f(X)\mid t]=\int\,f(x)\,dP^{X|t}(x)\]

_satisfies for all \(t\notin N\)._

\[E[h(T)f(X)\mid t]=h(t)E[f(X)\mid t]. \tag{2.25}\]

Proof. For the sake of simplicity and without essential loss of generality suppose that \(T\) is real-valued. Let \(P^{X|t}(A)\) be a probability distribution over \(\mathcal{A}\) for each \(t\)the existence of which is guaranteed by Theorem 2.5.1. For \(B\in\mathcal{B}\), the indicator function \(I_{B}(t)\) is \(\mathcal{B}\)-measurable and

\[\int_{B^{\prime}}I_{B}(t)\,dP^{T}(t)=P^{T}(B^{\prime}\cap B)=P^{X}( T^{-1}B^{\prime}\cap T^{-1}B)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\text{for all}\quad B^{\prime}\in\mathcal{B}.\]

Thus by (2.20)

\[I_{B}(t)=P^{X|t}\left(T^{-1}B\right)\qquad\text{a.e. }P^{T}.\]

Let \(B_{n}\), \(n=1\), \(2\),..., be the intervals of \(\mathcal{T}\) with rational end points. Then there exists a \(P\)-null set \(N=\cup N_{n}\) such that for \(t\notin N\)

\[I_{B_{n}}(t)=P^{X|t}\left(T^{-1}B_{n}\right)\]

for all \(n\). For fixed \(t\notin N\), the two set functions \(P^{X|t}\left(T^{-1}B\right)\) and \(I_{B}(t)\) are probability distributions over \(\mathcal{B}\), the latter assigning probability \(1\) or \(0\) to a set as it does or does not contain the point \(t\). Since these distributions agree over the rational intervals \(B_{n}\), they agree for all \(B\in\mathcal{B}\). In particular, for \(t\notin N\), the set consisting of the single point \(t\) is in \(\mathcal{B}\), and if

\[A^{(t)}=\{x:T(x)=t\},\]

it follows that for all \(t\notin N\)

\[P^{X|t}\left(A^{(t)}\right)=1. \tag{2.26}\]

Thus

\[\int h[T(x)]f(x)\,dP^{X|t}(x) =\int_{A^{(t)}}h[T(x)]f(x)\,dP^{X|t}(x)\] \[=h(t)\int f(x)\,dP^{X|t}(x)\]

for \(t\notin N\), as was to be proved.

It is a consequence of Theorem 2.5.3 that for all \(t\notin N\), \(E[h(T)\mid t]=h(t)\) and hence in particular \(P(T\in B\mid t)=1\) or \(0\) as \(t\in B\) or \(t\notin B\).

The conditional distributions \(P^{X|t}\) still differ from those of the elementary case considered in Section 1.9, in being defined over \((\mathcal{X},\,\mathcal{A})\) rather than over the set \(A^{(t)}\) and the \(\sigma\)-field \(\mathcal{A}^{(t)}\) of its Borel subsets. However, (2.26) implies that for \(t\notin N\)

\[P^{X|t}(A)=P^{X|t}(A\cap A^{(t)}).\]

The calculations of conditional probabilities and expectations are therefore unchanged if for \(t\notin N\), \(P^{X|t}\) is replaced by the distribution \(\tilde{P}^{X|t}\), which is defined over \((A^{(t)},\,\mathcal{A}^{(t)})\) and which assigns to any subset of \(A^{(t)}\) the same probability as \(P^{X|t}\).

Theorem 2.5.3 establishes for all \(t\notin N\) the existence of conditional probability distributions \(\bar{P}^{X|t}\), which are defined over \((A^{(t)},\mathcal{A}^{(t)})\) and which by Lemma 2.4.2 satisfy

\[E[f(X)]=\int_{\mathcal{T}-N}\left[\int_{A^{(t)}}f(x)\,d\,P^{(X|t)}(x)\right]\,dP ^{T}(t) \tag{2.27}\]

for all integrable functions \(f\). Conversely, consider any family of distributions satisfying (2.27), and the experiment of observing first \(T\), and then, if \(T=t\), a random quantity with distribution \(\bar{P}^{X|t}\). The result of this two-stage procedure is a point distributed over \((\mathcal{X},\mathcal{A})\) with the same distribution as the original \(X\). Thus \(\bar{P}^{X|t}\) satisfies this "functional" definition of conditional probability.

If \((\mathcal{X},\mathcal{A})\) is a product space \((\mathcal{T}\times\mathcal{Y},\mathcal{B}\times\mathcal{C})\), then \(A^{(t)}\) is the product of \(\mathcal{Y}\) with the set consisting of the single point \(t\). For \(t\notin N\), the conditional distribution \(\bar{P}^{X|t}\) then induces a distribution over \((\mathcal{Y},\mathcal{C})\), which in analogy with the elementary case will be denoted by \(P^{Y|t}\). In this case, the definition can be extended to all of \(\mathcal{T}\) by letting \(P^{Y|t}\) assign probability \(1\) to a common specified point \(y_{0}\) for all \(t\in N\). With this definition, (2.27) becomes

\[Ef(T,Y)=\int_{\mathcal{T}}\left[\int_{\mathcal{Y}}f(t,y)\,dP^{Y|t}(y)\right]\,dP ^{T}(t). \tag{2.28}\]

As an application, we shall prove the following lemma, which will be used in Section 2.7.

**Lemma 2.5.1**: _Let \((\mathcal{T},\mathcal{B})\) and \((\mathcal{Y},\mathcal{C})\) be Euclidean spaces, and let \(P_{0}^{T,Y}\) be a distribution over the product space \((\mathcal{X},\mathcal{A})=(\mathcal{T}\times\mathcal{Y},\mathcal{B}\times \mathcal{C})\). Suppose that another distribution \(P_{1}\) over \((\mathcal{X},\mathcal{A})\) is such that_

\[d\,P_{1}(t,\,y)=a(y)b(t)\,dP_{0}(t,\,y),\]

_with \(a(y)>0\) for all \(y\). Then under \(P_{1}\) the marginal distribution of \(T\) and a version of the conditional distribution of \(Y\) given \(t\) are given by_

\[dP_{1}^{T}(t)=b(t)\left[\int a(y)\,dP_{0}^{Y|t}(y)\right]\,dP_{0}^{T}(t)\]

_and_

\[dP_{1}^{Y|t}(y)=\frac{a(y)\,dP_{0}^{Y|t}(y)}{\int_{\mathcal{Y}}a(y^{\prime})\, dP_{0}^{Y|t}(y^{\prime})}.\]

Proof. The first statement of the lemma follows from the equation

\[P_{1}\{T\in B\}=E_{1}\left[I_{B}(T)\right] =E_{0}\left[I_{B}(T)a(Y)b(T)\right]\] \[=\int_{B}b(T)\left[\int_{\mathcal{Y}}a(y)\,dP_{0}^{Y|t}(y)\right] \,dP_{0}^{T}(t).\]To check the second statement, one need only to show that for any integrable \(f\) the expectation \(E_{1}f(Y,T)\) satisfies (2.28), which is immediate. The denominator of \(dP_{1}^{Y|_{t}}\) is positive, since \(a(y)>0\) for all \(y\).

### Characterization of Sufficiency

We can now generalize the definition of sufficiency given in Section 1.9. If \(\mathcal{P}=\{P_{\theta},\,\theta\in\Omega\}\) is any family of distributions defined over a common sample space \((\mathcal{X},\mathcal{A})\), a statistic \(T\) is _sufficient_ for \(\mathcal{P}\) (or for \(\theta\)) if for each \(A\) in \(\mathcal{A}\) there exists a determination of the conditional probability function \(P_{\theta}(A\mid t)\) that is independent of \(\theta\). As an example suppose that \(X_{1},\,\ldots,\,X_{n}\) are identically and independently distributed with continuous distribution function \(F_{\theta},\,\theta\in\Omega\). Then it follows from Example 2.4.1 that the set of order statistics \(T(X)=(X_{(1)},\,\ldots,\,X_{(n)})\) is sufficient for \(\theta\).

**Theorem 2.6.1**: _If \(\mathcal{X}\) is Euclidean, and if the statistic \(T\) is sufficient for \(\mathcal{P}\), then there exist determinations of the conditional probability distributions \(P_{\theta}(A\mid t)\) which are independent of \(\theta\) and such that for each fixed \(t,\,P_{\theta}(A\mid t)\) is a probability measure over \(\mathcal{A}\)._

Proof. This is seen from the proof of Theorem 2.5.1. By the definition of sufficiency one can, for each rational number \(r\), take the functions \(F(r,t)\) to be independent of \(\theta\), and the resulting conditional distributions will then also not depend on \(\theta\).

In Chapter 1, the definition of sufficiency was justified by showing that in a certain sense a sufficient statistic contains all the available information. In view of Theorem 2.6.1, the same justification applies quite generally when the sample space is Euclidean. With the help of a random mechanism one can then construct from a sufficient statistic \(T\) a random vector \(X^{\prime}\) having the same distribution as the original sample vector \(X\). Another generalization of the earlier result, not involving the restriction to a Euclidean sample space, is given in Problem 2.13.

The factorization criterion of sufficiency, derived in Chapter 1, can be extended to any _dominated_ family of distributions, that is, any family \(\mathcal{P}=\{P_{\theta},\,\theta\in\Omega\}\) possessing probability densities \(p_{\theta}\) with respect to some \(\sigma\)-finite measure \(\mu\) over \((\mathcal{X},\mathcal{A})\). The proof of this statement is based on the existence of a probability distribution \(\lambda=\sum c_{i}\,P_{\theta_{i}}\) (Theorem 2.2.3 of the Appendix), which is _equivalent_ to \(\mathcal{P}\) in the sense that for any \(A\in\mathcal{A}\)

\[\lambda(A)=0\ \ \ \mbox{if and only if}\ \ \ P_{\theta}=0\ \ \ \mbox{for all }\theta\in\Omega. \tag{2.29}\]

**Theorem 2.6.2**: _Let \(\mathcal{P}=\{P_{\theta},\,\theta\in\Omega\}\) be a dominated family of probability distributions over \((\mathcal{X},\mathcal{A})\), and let \(\lambda=\sum c_{i}\,P_{\theta_{i}}\) satisfy (2.29). Then a statistic \(T\) with range space \((\mathcal{T},\mathcal{B})\) is sufficient for \(\mathcal{P}\) if and only if there exist nonnegative \(\mathcal{B}\)-measurable functions \(g_{\theta}(t)\) such that_

\[d\,P_{\theta}(x)=g_{\theta}[T(x)]\,d\lambda(x) \tag{2.30}\]

_for all \(\theta\in\Omega\)._

Proof. Let \(\mathcal{A}_{0}\) be the subfield induced by \(T\), and suppose that \(T\) is sufficient for \(\theta\). Then for all \(\theta\in\Omega\), \(A_{0}\in\mathcal{A}_{0}\), and \(A\in\mathcal{A}\)

\[\int_{A_{0}}P\;(A\mid T(x))\;dP_{\theta}(x)=P_{\theta}(A\cap A_{0}),\]

and since \(\lambda=\sum c_{i}\,P_{\theta_{i}}\),

\[\int_{A_{0}}P\;(A\mid T(x))\;d\lambda(x)=\lambda(A\cap A_{0}),\]

so that \(P\,(A\mid T(x))\) serves as conditional probability function also for \(\lambda\). Let \(g_{\theta}(T(x))\) be the Radon-Nikodym derivative \(dP_{\theta}(x)/d\lambda(x)\) for \((\mathcal{A}_{0},\,\lambda)\). To prove (2.30) it is necessary to show that \(g_{\theta}(T(x))\) is also the derivative of \(P_{\theta}\) for \((\mathcal{A},\,\lambda)\). If \(A_{0}\) is put equal to \(\mathcal{X}\) in the first displayed equation, this follows from the relation

\[P_{\theta}(A) =\int P\;(A\mid T(x))\;dP_{\theta}(x)=\int E_{\lambda}\,[I_{A}(x )\mid T(x)]\;dP_{\theta}(x)\] \[=\int E_{\lambda}\,[I_{A}(x)\mid T(x)]\,g_{\theta}(T(x))\,d \lambda(x)\] \[=\int E_{\lambda}\,[g_{\theta}(T(x))I_{A}(x)\mid T(x)]\;d\lambda(x)\] \[=\int g_{\theta}(T(x))I_{A}(x)\,d\lambda(x)=\int_{A}g_{\theta}(T( x))\,d\lambda(x).\]

Here the second equality uses the fact, established at the beginning of the proof, that \(P(A\mid T(x))\) is also the conditional probability for \(\lambda\); the third equality holds because the function being integrated is \(\mathcal{A}_{0}\)-measurable and because \(dP_{\theta}=g_{\theta}\,d\lambda\) for \((\mathcal{A}_{0},\,\lambda)\); the fourth is an application of Lemma 2.4.1(ii); and the fifth employs the defining property of conditional expectation.

Suppose conversely that (2.30) holds. We shall then prove that the conditional probability function \(P_{\lambda}(A\mid t)\) serves as a conditional probability function for all \(P\in\mathcal{P}\). Let \(g_{\theta}(T(x))=\,dP_{\theta}(x)/\,d\lambda(x)\) on \(\mathcal{A}\) and for fixed \(A\) and \(\theta\) define a measure \(\nu\) over \(\mathcal{A}\) by the equation \(d\nu=I_{A}\,dP_{\theta}\). Then over \(\mathcal{A}_{0},\;d\nu(x)/\,dP_{\theta}(x)=E_{\theta}[I_{A}(X)\mid T(x)]\), and therefore

\[\frac{d\nu(x)}{d\lambda(x)}=\,P_{\theta}[A\mid T(x)]g_{\theta}(T(x))\quad\; \text{ over }\mathcal{A}_{0}.\]On the other hand, \(d\nu(x)/d\lambda(x)=I_{A}(x)g_{\theta}(T(x))\) over \(\mathcal{A}\), and hence

\[\frac{d\nu(x)}{d\lambda(x)} = E_{\lambda}[I_{A}(X)g_{\theta}(T(X))\mid T(x)]\] \[= P_{\lambda}[A\mid T(x)]g_{\theta}(T(x))\quad\quad\text{over } \mathcal{A}_{0}.\]

It follows that \(P_{\lambda}(A\mid T(x))g_{\theta}(T(x))=P_{\theta}(A\mid T(x))g_{\theta}(T(x))\) (\(\mathcal{A}_{0},\,\lambda\)) and hence \((\mathcal{A}_{0},\,P_{\theta})\). Since \(g_{\theta}(T(x))\neq 0\) (\(\mathcal{A}_{0},\,P_{\theta}\)), this shows that \(P_{\theta}(A\mid T(x))=P_{\lambda}(A\mid T(x))\) (\(\mathcal{A}_{0},\,P_{\theta}\)), and hence that \(P_{\lambda}(A\mid T(x))\) is a determination of \(P_{\theta}(A\mid T(x))\).

Instead of the above formulation, which explicitly involves the distribution \(\lambda\), it is sometimes more convenient to state the result with respect to a given dominating measure \(\mu\).

**Corollary 2.6.1**: **(Factorization Theorem)** _If the distributions \(P_{\theta}\) of \(\mathcal{P}\) have probability densities \(p_{\theta}=\,d\,P_{\theta}/d\mu\) with respect to a \(\sigma\)-finite measure \(\mu\), then \(T\) is sufficient for \(\mathcal{P}\) if and only if there exist nonnegative \(\mathcal{B}\)-measurable functions \(g_{\theta}\) on \(T\) and a nonnegative \(\mathcal{A}\)-measurable function \(h\) on \(\mathcal{X}\) such that_

\[p_{\theta}(x)=g_{\theta}[T(x)]h(x)\quad\quad(\mathcal{A},\,\mu). \tag{2.31}\]

Proof. Let \(\lambda=\sum c_{i}\,P_{\theta_{i}}\) satisfy (2.29). Then if \(T\) is sufficient, (2.31) follows from (2.30) with \(h=d\lambda/d\mu\). Conversely, if (2.31) holds

\[d\lambda(x)=\sum c_{i}\,g_{\theta_{i}}[T(x)]h(x)\,d\mu(x)=k[T(x)]h(x)\,d\mu(x)\]

and therefore \(d\,P_{\theta}(x)=g_{\theta}^{*}(T(x))\,d\lambda(x)\) where \(g_{\theta}^{*}(t)=g_{\theta}(t)/k(t)\) when \(k(t)>0\) and may be defined arbitrarily when \(k(t)=0\).

For extensions of the factorizations theorem to undominated families, see Ghosh et al. (1981) and the literature cited there.

### Exponential Families

An important family of distributions which admits a reduction by means of sufficient statistics is the _exponential family_, defined by probability densities of the form

\[p_{\theta}(x)=C(\theta)\exp\left[\sum_{j=1}^{k}Q_{j}(\theta)T_{j}(x)\right]h(x) \tag{2.32}\]

with respect to a \(\sigma\)-finite measure \(\mu\) over a Euclidean sample space (\(\mathcal{X},\,\mathcal{A}\)). Particular cases are the distributions of a sample \(X=(X_{1},\,\ldots,\,X_{n})\) from a binomial, Poisson,or normal distribution. In the binomial case, for example, the density (with respect to counting measure) is

\[{n\choose x}p^{x}(1-p)^{n-x}=(1-p)^{n}\exp\left[x\log\left(\frac{p}{1-p}\right) \right]{n\choose x}.\]

**Example 2.7.1**: If \(Y_{1},\ldots,Y_{n}\) are independently distributed, each with density (with respect to Lebesgue measure)

\[p_{\sigma}(y)=\frac{y^{\left[(f/2)-1\right]}\exp\left[-y/\left(2\sigma^{2} \right)\right]}{\left(2\sigma^{2}\right)^{f/2}\Gamma(f/2)},\ \ \ \ \ y>0, \tag{2.33}\]

then the joint distribution of the \(Y\)'s constitutes an exponential family. For \(\sigma=1\), (2.33) is the density of the \(\chi^{2}\)-distribution with \(f\) degrees of freedom, in particular, for \(f\) an integer this is the density of \(\sum_{j=1}^{f}X_{j}^{2}\), where the \(X\)'s are a sample from the normal distribution \(N(0,1)\).

**Example 2.7.2**: Consider \(n\) independent trials, each of them resulting in one of the \(s\) outcomes \(E_{1},\ldots,E_{s}\) with probabilities \(p_{1},\ldots,p_{s}\), respectively. If \(X_{ij}\) is \(1\) when the outcome of the \(i\)th trial is \(E_{j}\) and \(0\) otherwise, the joint distribution of the \(X\)'s is

\[P\{X_{11}=x_{11},\ldots,X_{ns}\}=p_{1}^{\sum x_{i1}}p_{2}^{\sum x_{i2}}\cdots p _{s}^{\sum x_{ix}},\]

where all \(x_{ij}=0\) or \(1\) and \(\sum_{j}x_{ij}=1\). This forms an exponential family with \(T_{j}(x)=\sum_{i=1}^{n}x_{ij}\) (\(j=1,\ldots,s-1\)). The joint distribution of the \(T\)'s is the multinomial distribution \(M(n;\,p_{1},\ldots,p_{s})\) given by

\[P\{T_{1}=t_{1},\ldots,T_{s-1}=t_{s-1}\} \tag{2.34}\]

\[=\frac{n!}{t_{1}!\ldots t_{s-1}!(n-t_{1}-\cdots-t_{s-1})!}\]

\[\times p_{1}^{t_{1}}\ldots p_{s-1}^{t_{s-1}}(1-p_{1}-\cdots-p_{s-1})^{n-t_{1}- \cdots-t_{s-1}}\.\ \blacksquare\]

If \(X_{1},\ldots,X_{n}\) is a sample from a distribution with density (2.32), the joint distribution of the \(X\)'s constitutes an exponential family with the sufficient statistics \(\sum_{i=1}^{n}T_{j}(X_{i}),\ j=1,\ldots,k\). Thus there exists a \(k\)-dimensional sufficient statistic for \((X_{1},\ldots,X_{n})\) regardless of the sample size. Suppose conversely that \(X_{1},\ldots,X_{n}\) is a sample from a distribution with some density \(p_{\theta}(x)\) and that the set over which this density is positive is independent of \(\theta\). Then under regularity assumptions which make the concept of dimensionality meaningful, if there exists a \(k\)-dimensional sufficient statistic with \(k<n\), the densities \(p_{\theta}(x)\) constitute an exponential family. For a proof of this result, see Darmois (1935), Koopman (1936), and Pitman (1937,1938a). Regularity conditions of the result are discussed in Barankin and Maitra (1963), Brown (1964), Barndorff-Nielsen and Pedersen (1968), and Hipp (1974).

Employing a more natural parametrization and absorbing the factor \(h(x)\) into \(\mu\), we shall write an exponential family in the form \(d\,P_{\theta}(x)=p_{\theta}(x)\,d\mu(x)\) with

\[p_{\theta}(x)=C(\theta)\exp\left[\,\sum_{j=1}^{k}\theta_{j}T_{j}(x)\,\right]. \tag{2.35}\]

For suitable choice of the constant \(C(\theta)\), the right-hand side of (2.35) is a probability density provided its integral is finite. The set \(\Omega\) of parameter points \(\theta=(\theta_{1},\,\ldots,\,\theta_{k})\) for which this is the case is the _natural parameter space_ of the exponential family (2.35).

Optimum tests of certain hypotheses concerning any \(\theta_{j}\) are obtained in Chapter 4. We shall now consider some properties of exponential families required for this purpose.

**Lemma 2.7.1**: _The natural parameter space of an exponential family is convex._

Proof. Let \((\theta_{1},\,\ldots,\,\theta_{k})\) and \((\theta^{\prime}_{1},\,\ldots,\,\theta^{\prime}_{k})\) be two parameter points for which the integral of (2.35) is finite. Then by Holder's inequality,

\[\int\exp\left[\,\sum\left[\alpha\theta_{j}+(1-\alpha)\theta^{ \prime}_{j}\right]T_{j}(x)\,\right]d\mu(x)\] \[\qquad\leq\left[\int\exp\left[\,\sum\theta_{j}T_{j}(x)\,\right]d \mu(x)\,\right]^{\alpha}\left[\int\exp\left[\,\sum\theta^{\prime}_{j}T_{j}(x) \,\right]d\mu(x)\,\right]^{1-\alpha}<\infty\]

for any \(0<\alpha<1\).

If the convex set \(\Omega\) lies in a linear space of dimension \(<k\), then (2.35) can be rewritten in a form involving fewer than \(k\) components of \(T\). We shall therefore, without loss of generality, assume \(\Omega\) to be \(k\)-dimensional.

It follows from the factorization theorem that \(T(x)=(T_{1}(x),\,\ldots,\,T_{k}(x))\) is sufficient for \(\mathcal{P}=\{P_{\theta},\,\theta\in\Omega\}\).

**Lemma 2.7.2**: _Let \(X\) be distributed according to the exponential family_

\[d\,P^{T}_{\theta,\vartheta}(x)=C(\theta,\,\vartheta)\exp\left[\,\sum_{i=1}^{r} \theta_{i}U_{i}(x)+\sum_{j=1}^{s}\vartheta_{j}T_{j}(x)\,\right]d\mu(x).\]

_Then there exist measures \(\lambda_{\theta}\) and \(\nu_{t}\) over \(s\)- and \(r\)-dimensional Euclidean space respectively such that_

1. _the distribution of_ \(T=(T_{1},\,\ldots,\,T_{s})\) _is an exponential family of the form_ \[d\,P^{T}_{\theta,\vartheta}(t)=C(\theta,\,\vartheta)\exp\left(\sum_{j=1}^{s} \vartheta_{j}t_{j}\right)\,d\lambda_{\theta}(t),\] (2.36)_the conditional distribution of_ \(U=(U_{1},\ldots,U_{r})\) _given_ \(T=t\) _is an exponential family of the form_ \[d\,P^{U|t}_{\theta_{i}}(u)=C(\theta)\exp\left(\sum_{i=1}^{r}\theta_{i}u_{i} \right)\,d\nu_{t}(u),\] (2.37) _and hence in particular is independent of_ \(\vartheta\)_._

Proof. Let \((\theta^{0},\vartheta^{0})\) be a point of the natural parameter space, and let \(\mu^{*}=P^{X}_{\theta^{0},\vartheta^{0}}\). Then

\[d\,P^{X}_{\theta^{0},\vartheta^{0}}(x) = \frac{C(\theta,\vartheta)}{C(\theta^{0},\vartheta^{0})}\] \[\times\exp\left[\,\sum_{i=1}^{r}(\theta_{i}-\theta_{i}^{0})U_{i} (x)+\sum_{j=1}^{s}(\vartheta_{j}-\vartheta_{j}^{0})T_{j}(x)\,\right]d\mu^{*}(x),\]

and the result follows from Lemma 2.5.1, with

\[d\lambda_{\theta}(t)=\exp\left(-\sum\vartheta_{i}^{0}t_{i}\right)\left[\int \exp\left[\,\sum_{i=1}^{r}(\theta_{i}-\theta_{i}^{0})u_{i}\,\right]dP^{U|t}_{ \theta^{0},\vartheta^{0}}(u)\,\right]dP^{T}_{\theta^{0},\vartheta^{0}}(t)\]

and

\[d\nu_{t}(u)=\exp\left(-\sum\theta_{i}^{0}u_{i}\right)\,dP^{U|t}_{\theta^{0}, \vartheta^{0}}(u).\,\,\blacksquare\]

**Theorem 2.7.1**: _Let \(\phi\) be any function on \((\mathcal{X},\mathcal{A})\) for which the integral_

\[\int\phi(x)\exp\left[\,\sum_{j=1}^{k}\theta_{j}T_{j}(x)\,\right]d\mu(x) \tag{2.38}\]

_considered as a function of the complex variables \(\theta_{j}=\xi_{j}+i\eta_{j}\)\((j=1,\ldots,k)\) exists for all \((\xi_{1},\ldots,\xi_{k})\in\Omega\) and is finite. Then_

* _the integral is an analytic function of each of the_ \(\theta\)_'s in the region_ \(R\) _of parameter points for which_ \((\xi_{1},\ldots,\xi_{k})\) _is an interior point of the natural parameter space_ \(\Omega\)_;_
* _the derivatives of all orders with respect to the_ \(\theta\)_'s of the integral (_2.38_) can be computed under the integral sign._

Proof. Let \((\xi_{1},\ldots,\xi_{k})\) be any fixed point in the interior of \(\Omega\), and consider one of the variables in question, say \(\theta_{1}\). Breaking up the factor

\[\phi(x)\exp\left[\left(\xi_{2}^{0}+i\eta_{2}^{0}\right)T_{2}(x)+\cdots+\left( \xi_{k}^{0}+i\eta_{k}^{0}\right)T_{k}(x)\right]\]into its real and complex parts and each of these into its positive and negative parts, and absorbing this factor in each of the four terms thus obtained into the measure \(\mu\), one sees that as a function of \(\theta_{1}\) the integral (2.38) can be written as

\[\int\exp\left[\theta_{1}T_{1}(x)\right]\,d\mu_{1}(x)-\int\exp\left[ \theta_{1}T_{1}(x)\right]\,d\mu_{2}(x)\] \[\qquad+i\int\exp\left[\theta_{1}T_{1}(x)\right]\,d\mu_{3}(x)-i \int\exp\left[\theta_{1}T_{1}(x)\right]\,d\mu_{4}(x).\]

It is therefore sufficient to prove the result for integrals of the form

\[\psi(\theta_{1})=\int\exp\left[\theta_{1}T_{1}(x)\right]\,d\mu(x).\]

Since \((\xi_{1}^{0},\ldots,\xi_{k}^{0})\) is in the interior of \(\Omega\), there exists \(\delta>0\) such that \(\psi(\theta_{1})\) exists and is finite for all \(\theta_{1}\) with \(|\xi_{1}-\xi_{1}^{0}|\leq\delta\). Consider the difference

\[\frac{\psi(\theta_{1})-\psi(\theta_{1}^{0})}{\theta_{1}-\theta_{1}^{0}}=\int \frac{\exp\left[\theta_{1}T_{1}(x)\right]-\exp\left[\theta_{1}^{0}T_{1}(x) \right]}{\theta_{1}-\theta_{1}^{0}}\,d\mu(x).\]

The integrand can be written as

\[\exp\left[\theta_{1}^{0}T_{1}(x)\right]\left[\frac{\exp\left[(\theta_{1}- \theta_{1}^{0})T_{1}(x)\right]-1}{\theta_{1}-\theta_{1}^{0}}\right].\]

Applying to the second factor the inequality

\[\left|\frac{\exp(az)-1}{z}\right|\leq\frac{\exp(\delta|a|)}{\delta}\quad\quad \text{for}\quad|z|\leq\delta,\]

the integrand is seen to be bounded above in absolute value by

\[\frac{1}{\delta}\left|\exp\left(\theta_{1}^{0}T_{1}+\delta|T_{1}|\right)\right| \leq\frac{1}{\delta}\left|\exp\left[\left(\theta_{1}^{0}+\delta\right)T_{1} \right]+\exp\left[\left(\theta_{1}^{0}-\delta\right)T_{1}\right]\right|\]

for \(|\theta_{1}-\theta_{1}^{0}|\leq\delta\). Since the right-hand side integrable, it follows from the Lebesgue Dominated Convergence Theorem [Theorem 2.2.2(ii)] that for any sequence of points \(\theta_{1}^{(n)}\) tending to \(\theta_{1}^{0}\), the difference quotient of \(\psi\) tends to

\[\int T_{1}(x)\exp\left[\theta_{1}^{0}T_{1}(x)\right]\,d\mu(x).\]

This completes the proof of (i), and proves (ii) for the first derivative. The proof for the higher derivatives is by induction and is completely analogous.

### Problems

#### 2.1.1 Section 2.1

**Problem 2.1**: _Monotone class._ A class \(\mathcal{F}\) of subsets of a space is a _field_ if it contains the whole space and is closed under complementation and under finite unions; a class \(\mathcal{M}\) is _monotone_ if the union and intersection of every increasing and decreasing sequence of sets of \(\mathcal{M}\) is again in \(\mathcal{M}\). The smallest monotone class \(\mathcal{M}_{0}\) containing a given field \(\mathcal{F}\) coincides with the smallest \(\sigma\)-field \(\mathcal{A}\) containing \(\mathcal{F}\). [One proves first that \(\mathcal{M}_{0}\) is a field. To show, for example, that \(A\cap B\in\mathcal{M}_{0}\) when \(A\) and \(B\) are in \(\mathcal{M}_{0}\), consider, for a fixed set \(A\in\mathcal{F}\), the class \(\mathcal{M}_{A}\) of all \(B\) in \(\mathcal{M}_{0}\) for which \(A\cap B\in\mathcal{M}_{0}\). Then \(\mathcal{M}_{A}\) is a monotone class containing \(\mathcal{F}\), and hence \(\mathcal{M}_{A}=\mathcal{M}_{0}\). Thus \(A\cap B\in\mathcal{M}_{A}\) for all \(B\). The argument can now be repeated with a fixed set \(B\in\mathcal{M}_{0}\) and the class \(\mathcal{M}_{B}\) of sets \(A\) in \(\mathcal{M}_{0}\) for which \(A\cap B\in\mathcal{M}_{0}\). Since \(\mathcal{M}_{0}\) is a field and monotone, it is a \(\sigma\)-field containing \(\mathcal{F}\) and hence contains \(\mathcal{A}\). But any \(\sigma\)-field is a monotone class so that also \(\mathcal{M}_{0}\) is contained in \(\mathcal{A}\).]

#### 2.2.1 Section 2.2

**Problem 2.2**: Prove Corollary 2.2.1 using Theorems 2.2.1 and 2.2.2.

**Problem 2.3**: _Radon-Nikodym derivatives._

(i) If \(\lambda\) and \(\mu\) are \(\sigma\)-finite measures over (\(\mathcal{X}\), \(\mathcal{A}\)) and \(\mu\) is absolutely continuous with respect to \(\lambda\), then

\[\int f\,d\mu=\int f\frac{d\mu}{d\lambda}\,d\lambda\]

for any \(\mu\)-integrable function \(f\).

(ii) If \(\lambda\), \(\mu\), and \(\nu\) are \(\sigma\)-finite measures over (\(\mathcal{X}\), \(\mathcal{A}\)) such that \(\nu\) is absolutely continuous with respect to \(\mu\) and \(\mu\) with respect to \(\lambda\), then

\[\frac{d\nu}{d\lambda}=\frac{d\nu}{d\mu}\frac{d\mu}{d\lambda}\qquad\mbox{a.e. }\lambda.\]

(iii) If \(\mu\) and \(\nu\) are \(\sigma\)-finite measures,, which are _equivalent_ in the sense that each is absolutely continuous with respect to the other, then

\[\frac{d\nu}{d\mu}=\left(\frac{d\mu}{d\nu}\right)^{-1}\qquad\mbox{a.e. }\mu,\nu.\]

[MISSING_PAGE_EMPTY:1671]

2. Let \(G=\{g_{1},\ldots,g_{r}\}\) be any group of permutations of the coordinates \(x_{1},\ldots,x_{n}\) of a point \(x\) in \(n\)-space, and denote by \(gx\) the point obtained by applying \(g\) to the coordinates of \(x\). Let \(\mathcal{P}\) be any family of distributions \(P\) of \(X=(X_{1},\ldots,X_{n})\) such that \[P\{gX\in A\}=P\{X\in A\}\quad\text{ for all }\;g\in G.\] (2.39) For any point \(x\) let \(t=T(x)\) be any rule that selects a unique point from the \(r\) points \(g_{k}x,k=1,\ldots,r\) (for example the smallest first coordinate if this defines it uniquely, otherwise also the smallest second coordinate, etc.). Then \[E[f(X)\mid t]=\frac{1}{r}\sum_{k=1}^{r}f(g_{k}t).\] (iii) Suppose that in (ii) the distributions \(P\) do not satisfy the invariance condition (2.39) but are given by \[d\,P(x)=h(x)\,d\mu(x),\] where \(\mu\) is invariant in the sense that \(\mu\{x:gx\in A\}=\mu(A)\). Then \[E[f(X)\mid t]=\frac{\sum_{k=1}^{r}f(g_{k}t)h(g_{k}t)}{\sum_{k=1}^{r}h(g_{k}t)}.\]

### _Section 2.5_

**Problem 2.7**: Prove Theorem 2.5.1 for the case of an \(n\)-dimensional sample space. [The condition that the cumulative distribution function is nondecreasing is replaced by \(P\{x_{1}<X_{1}\leq x_{1}^{\prime},\ldots,x_{n}<X_{n}\leq x_{n}^{\prime}\}\geq 0\); the condition that it is continuous on the right can be stated as \(\lim_{m\to\infty}F(x_{1}+1/m,\ldots,x_{n}+1/m)=F(x_{1},\ldots,x_{n})\).]

**Problem 2.8**: Let \(\mathcal{X}=\mathcal{Y}\times\mathcal{T}\), and suppose that \(P_{0}\), \(P_{1}\) are two probability distributions given by

\[dP_{0}(y,t) = f(y)g(t)\,d\mu(y)\,d\nu(t),\] \[dP_{1}(y,t) = h(y,t)\,d\mu(y)\,d\nu(t),\]

where \(h(y,t)/f(y)g(t)<\infty\). Then under \(P_{1}\) the probability density of \(Y\) with respect to \(\mu\) is

\[p_{1}^{Y}(y)=f(y)E_{0}\left[\frac{h(y,T)}{f(y)g(T)}\,\Big{|}\,Y=y\right].\]

[MISSING_PAGE_FAIL:71]

is also \({\cal A}_{0}\)-measurable. (ii): Let \(\lambda=\sum_{j=1}^{\infty}c_{j}P_{\theta_{j}}\) be equivalent to \({\cal P}\). Then pairwise sufficiency of \(T\) implies for any \(\theta_{0}\) that \(dP_{\theta_{0}}/(dP_{\theta_{0}}+d\lambda)\) and hence \(dP_{\theta_{0}}/d\lambda\) is a measurable function of \(T\).]

**Problem 2.12**: If a statistic \(T\) is sufficient for \({\cal P}\), then for every function \(f\) which is \(({\cal A},\,P_{\theta})\)-integrable for all \(\theta\in\Omega\) there exists a determination of the conditional expectation function \(E_{\theta}[f(X)\mid t]\) that is independent of \(\theta\). [If \({\cal X}\) is Euclidean, this follows from Theorems 2.5.2 and 2.6.1. In general, if \(f\) is nonnegative there exists a nondecreasing sequence of simple nonnegative functions \(f_{n}\) tending to \(f\). Since the conditional expectation of a simple function can be taken to be independent of \(\theta\) by Lemma 2.4.1(i), the desired result follows from Lemma 2.4.1(iv).]

**Problem 2.13**: For a decision problem with a finite number of decisions, the class of procedures depending on a sufficient statistic \(T\) only is essentially complete. [For Euclidean sample spaces this follows from Theorem 2.5.1 without any restriction on the decision space. For the present case, let a decision procedure be given by \(\delta(x)=(\delta^{(1)}(x),\,\ldots,\,\delta^{(m)}(x))\) where \(\delta^{(i)}(x)\) is the probability with which decision \(d_{i}\) is taken when \(x\) is observed. If \(T\) is sufficient and \(\eta^{(i)}(t)=E[\delta^{(i)}(X)\mid t]\), the procedures \(\delta\) and \(\eta\) have identical risk functions.] [More general versions of this result are discussed, for example, by Elfving (1952), Bahadur (1955), Burkholder (1961), LeCam (1964), and Roy and Ramamoorthi (1979).]

**Problem 2.14**: Let \(X_{i}\) (\(i=1,\,\ldots,\,s\)) be independently distributed with Poisson distribution \(P(\lambda_{i})\), and let \(T_{0}=\sum X_{j}\), \(T_{i}=X_{i}\), \(\lambda=\sum\lambda_{j}\). Then \(T_{0}\) has the Poisson distribution \(P(\lambda)\), and the conditional distribution of \(T_{1},\,\ldots,\,T_{s-1}\) given \(T_{0}=t_{0}\) is the multinomial distribution (2.34) with \(n=t_{0}\) and \(p_{i}=\lambda_{i}/\lambda\).

**Problem 2.15**: _Life testing._ Let \(X_{1},\,\ldots,\,X_{n}\) be independently distributed with exponential density \((2\theta)^{-1}e^{-x/2\theta}\) for \(x\geq 0\), and let the ordered \(X\)'s be denoted by \(Y_{1}\leq Y_{2}\leq\cdots\leq Y_{n}\). It is assumed that \(Y_{1}\) becomes available first, then \(Y_{2}\), and so on, and that observation is continued until \(Y_{r}\) has been observed. This might arise, for example, in life testing where each \(X\) measures the length of life of, say, an electron tube, and \(n\) tubes are being tested simultaneously. Another application is to the disintegration of radioactive material, where \(n\) is the number of atoms, and observation is continued until \(r\)\(\alpha\)-particles have been emitted.

* The joint distribution of \(Y_{1},\,\ldots,\,Y_{r}\) is an exponential family with density \[\frac{1}{(2\theta)^{r}}\,\frac{n!}{(n-r)!}\exp\left[-\frac{\sum\limits_{i=1} ^{r}y_{i}+(n-r)y_{r}}{2\theta}\right],\qquad 0\leq y_{1}\leq\cdots\leq y_{r}.\]* The distribution of \([\sum_{i=1}^{r}Y_{i}+(n-r)Y_{r}]/\theta\) is \(\chi^{2}\) with \(2r\) degrees of freedom.
* Let \(Y_{1}\), \(Y_{2},\ldots\) denote the time required until the first, second, \(\ldots\) event occurs in a Poisson process with parameter \(1/2\theta^{\prime}\) (see Problem 1.1). Then \(Z_{1}=Y_{1}/\theta^{\prime}\), \(Z_{2}=(Y_{2}-Y_{1})/\theta^{\prime}\), \(Z_{3}=(Y_{3}-Y_{2})/\theta^{\prime}\), \(\ldots\) are independently distributed as \(\chi^{2}\) with \(2\) degrees of freedom, and the joint density \(Y_{1}\), \(\ldots\), \(Y_{r}\) is an exponential family with density \[\frac{1}{(2\theta^{\prime})^{r}}\exp\left(-\frac{y_{r}}{2\theta^{\prime}} \right),\hskip 19.916929pt0\leq y_{1}\leq\cdots\leq y_{r}.\] The distribution of \(Y_{r}/\theta^{\prime}\) is again \(\chi^{2}\) with \(2r\) degrees of freedom.
* The same model arises in the application to life testing if the number \(n\) of tubes is held constant by replacing each burned-out tube with a new one, and if \(Y_{1}\) denotes the time at which the first tube burns out, \(Y_{2}\) the time at which the second tube burns out, and so on, measured from some fixed time.

[(ii): The random variables \(Z_{i}=(n-i+1)(Y_{i}-Y_{i-1})/\theta\) (\(i=1,2,\ldots,r\)) are independently distributed as \(\chi^{2}\) with \(2\) degrees of freedom, and \([\sum_{i=1}^{r}Y_{i}+(n-r)Y_{r}/\theta=\sum_{i=1}^{r}Z_{i}\).]

**Problem 2.16**: For any \(\theta\) which is an interior point of the natural parameter space, the expectations and covariances of the statistics \(T_{j}\) in the exponential family (2.35) are given by

\[E\left[T_{j}(X)\right]=-\frac{\partial\log C(\theta)}{\partial\theta_{j}} \hskip 19.916929pt(j=1,\ldots,k),\]

\[E\left[T_{i}(X)T_{j}(X)\right]-\left[ET_{i}(X)ET_{j}(X)\right]=-\frac{ \partial^{2}\log C(\theta)}{\partial\theta_{i}\partial\theta_{j}}\hskip 19.916929pt (i,j=1,\ldots,k).\]

**Problem 2.17**: Let \(\Omega\) be the natural parameter space of the exponential family (2.35), and for any fixed \(t_{r+1}\), \(\ldots\), \(t_{k}\) (\(r<k\)) let \(\Omega^{\prime}_{\theta_{1}\ldots\theta_{r}}\) be the natural parameter space of the family of conditional distributions given \(T_{r+1}=t_{r+1}\), \(\ldots\), \(T_{k}=t_{k}\).

* Then \(\Omega^{\prime}_{\theta_{1},\ldots,\theta_{r}}\) contains the projection \(\Omega_{\theta_{1},\ldots,\theta_{r}}\) of \(\Omega\) onto \(\theta_{1}\), \(\ldots\), \(\theta_{r}\).
* An example in which \(\Omega_{\theta_{1},\ldots,\theta_{r}}\) is a proper subset of \(\Omega^{\prime}_{\theta_{1},\ldots,\theta_{r}}\) is the family of densities \[p_{\theta_{1}\theta_{2}}(x,y)=C(\theta_{1},\theta_{2})\exp(\theta_{1}x+\theta _{2}y-xy),\hskip 19.916929ptx,y>0.\]

### Notes

The theory of measure and integration in abstract spaces and its application to probability theory, including in particular conditional probability and expectation, is treated in a number of books, among them Dudley (1989), Williams (1991), and Billingsley (1995). The material on sufficient statistics and exponential families is complemented by the corresponding sections in Lehmann and Casella (1998). Much fuller treatments of exponential families (as well as sufficiency) are provided by Barndorff-Nielsen (1978) and Brown (1986).

## Chapter 3 Uniformly Most Powerful Tests

### 3.1 Stating the Problem

We now begin the study of the statistical problem that forms the principal subject of this book, the problem of hypothesis testing. As the term suggests, one wishes to decide whether or not some hypothesis that has been formulated is correct. The choice here lies between only two decisions: accepting or rejecting the hypothesis. A decision procedure for such a problem is called a _test_ of the hypothesis in question.

The decision is to be based on the value of a certain random variable \(X\), the distribution \(P_{\theta}\) of which is known to belong to a class \(\mathcal{P}=\{P_{\theta},\,\theta\in\Omega\}\). We shall assume that if \(\theta\) were known, one would also know whether or not the hypothesis is true. The distributions of \(\mathcal{P}\) can then be classified into those for which the hypothesis is true and those for which it is false. The resulting two mutually exclusive classes are denoted by \(H\) and \(K\), and the corresponding subsets of \(\Omega\) by \(\Omega_{H}\) and \(\Omega_{K}\) respectively, so that \(H\,\cup\,K\,=\mathcal{P}\) and \(\Omega_{H}\,\cup\,\Omega_{K}\,=\Omega\). Mathematically, the hypothesis is equivalent to the statement that \(P_{\theta}\) is an element of \(H\). It is therefore convenient to identify the hypothesis with this statement and to use the letter \(H\) also to denote the hypothesis. Analogously we call the distributions in \(K\) the alternatives to \(H\), so that \(K\) is the _class of alternatives_.

Let the decisions of accepting or rejecting \(H\) be denoted by \(d_{0}\) and \(d_{1}\) respectively. A nonrandomized test procedure assigns to each possible value \(x\) of \(X\) one of these two decisions and thereby divides the sample space into two complementary regions \(A\) and \(R\). If \(X\) falls into \(A\), the hypothesis is accepted; otherwise it is rejected. The set \(A\) is called the region of acceptance, and the set \(R\) the region of rejection or _critical_ region.

When performing a test one may arrive at the correct decision, or one may commit one of two errors: rejecting the hypothesis when it is true (error of the first kind or Type 1 error) or accepting it when it is false (error of the second kind or Type 2 error). The consequences of these are often quite different. For example, if one tests for the presence of some disease, incorrectly deciding on the necessity of treatmentmay cause the patient discomfort and financial loss. On the other hand, failure to diagnose the presence of the ailment may lead to the patient's death.

It is desirable to carry out the test in a manner which keeps the probabilities of the two types of error to a minimum. Unfortunately, when the number of observations is given, both probabilities cannot be controlled simultaneously. It is customary therefore to assign a bound to the probability of incorrectly rejecting \(H\) when it is true and to attempt to minimize the other probability subject to this condition. Thus one selects a number \(\alpha\) between 0 and 1, called the _level of significance_, and imposes the condition that

\[P_{\theta}\{\delta(X)=d_{1}\}=P_{\theta}\{X\in R\}\leq\alpha\quad\text{ for all }\ \ \theta\in\Omega_{H}. \tag{3.1}\]

Subject to this condition, it is desired to minimize \(P_{\theta}\{\delta(X)=d_{0}\}\) for \(\theta\) in \(\Omega_{K}\) or, equivalently, to maximize

\[P_{\theta}\{\delta(X)=d_{1}\}=P_{\theta}\{X\in R\}\quad\text{ for all }\ \ \theta\in\Omega_{K}. \tag{3.2}\]

Although (3.1) usually implies that

\[\sup_{\Omega_{H}}P_{\theta}\{X\in R\}=\alpha, \tag{3.3}\]

it is convenient to introduce a term for the left-hand side of (3.3): it is called the _size_ of the test or critical region \(R\). The condition (3.1) therefore restricts consideration to test whose size does not exceed the given level of significance. The probability of rejection (3.2) evaluated for a given \(\theta\) in \(\Omega_{K}\) is called the _power_ of the test against the alternative \(\theta\). Considered as a function of \(\theta\) for all \(\theta\in\Omega\), the probability (3.2) is called the _power function_ of the test and is denoted by \(\beta(\theta)\).

Although we may formally decide between accepting \(H\) when \(X\in A\) and rejecting \(H\) when \(X\in R\), it must be emphasized that "accepting" \(H\) does not prove that \(H\) is true. Failure to reject \(H\) may result from insufficient data or poor power, so that accepting \(H\) should be interpreted as the data provide insufficient evidence against the null hypothesis.

The choice of a level of significance \(\alpha\) is usually somewhat arbitrary, since in most situations there is no precise limit to the probability of an error of the first kind that can be tolerated.1 Standard values, such as 0.01 or 0.05, were originally chosen to effect a reduction in the tables needed for carrying out various tests. By habit, and because of the convenience of standardization in providing a common frame of reference, these values gradually became entrenched as the conventional levels to use. This is unfortunate, since the choice of significance level should also take into consideration the power that the test will achieve against the alternatives of interest. There is little point in carrying out an experiment which has only a small chance of detecting the effect being sought when it exists. Surveys by Cohen (1962) and Freiman et al. (1978) suggest that this is in fact the case for many studies. Ideally, the sample size should then be increased to permit adequate values for both significance level and power. If that is not feasible one may wish to use higher values of \(\alpha\) than the customary ones. The opposite possibility, that one would like to decrease \(\alpha\), arises when the latter is so close to 1 that \(\alpha\) can be lowered appreciably without a significant loss of power (cf. Problem 3.11). Rules for choosing \(\alpha\) in relation to the attainable power are discussed by Lehmann (1958), Arrow (1960), and Sanathanan (1974), and from a Bayesian point of view by Savage (1962, pp. 64-66). See also Rosenthal and Rubin (1985).

Another consideration that may enter into the specification of a significance level is the attitude toward the hypothesis before the experiment is performed. If one firmly believes the hypothesis to be true, extremely convincing evidence will be required before one is willing to give up this belief, and the significance level will accordingly be set very low. (A low significance level results in the hypothesis being rejected only for a set of values of the observations whose total probability under hypothesis is small, so that such values would be most unlikely to occur if \(H\) were true.)

Let us next consider the structure of a randomized test. For any values \(x\), such a test chooses between the two decisions, rejection or acceptance, with certain probabilities that depend on \(x\) and will be denoted by \(\phi(x)\) and \(1-\phi(x)\) respectively. If the value of \(X\) is \(x\), a random experiment is performed with two possible outcomes \(R\) and \(\tilde{R}\), the probabilities of which are \(\phi(x)\) and \(1-\phi(x)\). If in this experiment \(R\) occurs, the hypothesis is rejected, otherwise it is accepted. A randomized test is therefore completely characterized by a function \(\phi\), the _critical function_, with \(0\leq\phi(x)\leq 1\) for all \(x\). If \(\phi\) takes on only the values 1 and 0, one is back in the case of a nonrandomized test. The set of points \(x\) for which \(\phi(x)=1\) is then just the region of rejection, so that in a nonrandomized test \(\phi\) is simply the indicator function of the critical region.

If the distribution of \(X\) is \(P_{\theta}\), and the critical function \(\phi\) is used, the probability of rejection is

\[E_{\theta}\phi(X)=\int\phi(x)\,d\,P_{\theta}(x),\]

the conditional probability \(\phi(x)\) of rejection given \(x\), integrated with respect to the probability distribution of \(X\). The problem is to select \(\phi\) so as to maximize the power

\[\beta_{\phi}(\theta)=E_{\theta}\phi(X)\quad\quad\mbox{for all}\quad\theta\in \Omega_{K} \tag{3.4}\]

subject to the condition

\[E_{\theta}\phi(X)\leq\alpha\quad\quad\mbox{for all}\quad\theta\in\Omega_{H}. \tag{3.5}\]

A level \(\alpha\) test that maximizes (3.4) is called a _most powerful_ (MP) level \(\alpha\) test. The same difficulty now arises that presented itself in the general discussion of Chapter 1. Typically, the test that maximizes the power against a particular alternative in depends on this alternative, so that some additional principal has to be introduced to define what is meant by an optimum test. There is one important exception: if \(K\) contains only one distribution, that is, if one is concerned with a single alternative, the problem is completely specified by (3.4) and (3.5). It then reduces to the mathematical problem of maximizing an integral subject to certain side conditions. The theory of this problem, and its statistical applications, constitutes the principle subject of the present chapter. In special cases it may of course turn out that the same test maximizes the power of all alternatives in \(K\) even when there is more than one. Examples of such _uniformly most powerful_ (UMP) tests will be given in Sections 3.4 and 3.7.

In the above formulation the problem can be considered a special case of the general decision problem with two types of losses. Corresponding to the two kinds of error, one can introduce the two-component loss functions,

\[\begin{array}{llll}L_{1}(\theta,d_{1})=1\;\;\mbox{or}\;\;0&\mbox{as}\;\;\;\; \;\theta\in\Omega_{H}\;\mbox{or}\;\theta\in\Omega_{K},\\ L_{1}(\theta,d_{0})=0&\mbox{for all}\;\;\theta\end{array}\]

and

\[\begin{array}{llll}L_{2}(\theta,d_{0})=0\;\;\mbox{or}\;\;1&\mbox{as}\;\;\;\; \;\theta\in\Omega_{H}\;\mbox{or}\;\theta\in\Omega_{K},\\ L_{2}(\theta,d_{1})=0&\mbox{for all}\;\;\theta\;.\end{array}\]

With this definition the minimization of \(EL_{2}(\theta,\delta(X))\) subject to the restriction \(EL_{1}(\theta,\delta(X))\leq\alpha\) is exactly equivalent to the problem of hypothesis testing as given above.

The formal loss functions \(L_{1}\) and \(L_{2}\) clearly do not represent in general the true losses. The loss resulting from an incorrect acceptance of the hypothesis, for example, will not be the same for all alternatives. The more the alternative differs from the hypothesis, the more serious are the consequences of such an error. As was discussed earlier, we have purposely foregone the more detailed approach implied by this criticism. Rather than working with a loss function which in practice one does not know, it seems preferable to base the theory on the simpler and intuitively appealing notion of error. It will be seen later that at least some of the results can be justified also in the more elaborate formulation.

### The Neyman-Pearson Fundamental Lemma

A class of distributions is called _simple_ if it contains a single distribution, and otherwise it is said to be _composite_. The problem of hypothesis testing is completely specified by (3.4) and (3.5) if \(K\) is simple. Its solution is easiest and can be given explicitly when the same is true of \(H\). Let the distributions under a simple hypothesis \(H\) and alternative \(K\) be \(P_{0}\) and \(P_{1}\), and suppose for a moment that these distributions are discrete with \(P_{i}\{X=x\}=P_{i}(x)\) for \(i=0,1\). If at first one restricts attention to nonrandomized tests, the optimum test is defined as the critical region \(S\) satisfying \[\sum_{x\in S}P_{0}(x)\leq\alpha \tag{3.6}\]

and

\[\sum_{x\in S}P_{1}(x)=\text{maximum}\.\]

It is easy to see which points should be included in \(S\). To each point are attached two values, its probability under \(P_{0}\) and under \(P_{1}\). The selected points are to have a total value not exceeding \(\alpha\) on the one scale, and as large as possible on the other. This is a situation that occurs in many contexts. A buyer with a limited budget who wants to get "the most for his money" will rate the items according to their _value per dollar_. In order to travel a given distance in the shortest possible time, one must choose the quickest mode of transportation, that is, the one that yields the largest number of _miles per hour_. Analogously in the present problem the most valuable points \(x\) are those with the highest value of

\[r(x)=\frac{P_{1}(x)}{P_{0}(x)}.\]

The points are therefore rated according to the value of this ratio and selected for \(S\) in this order, as many as one can afford under restriction (3.6). Formally this means that \(S\) is the set of all points \(x\) for which \(r(x)>c\), where \(c\) is determined by the condition

\[P_{0}\{X\in S\}=\sum_{x:r(x)>c}P_{0}(x)=\alpha\.\]

Here a difficulty is seen to arise. It may happen that when a certain point is included, the value \(\alpha\) has not yet been reached but that it would be exceeded if the point were also included. The exact value \(\alpha\) can then either not be achieved at all, or it can be attained only by breaking the preference order established by \(r(x)\). The resulting optimization problem has no explicit solution. (Algorithms for obtaining the maximizing set \(S\) are given by the theory of linear programming.) The difficulty can be avoided, however, by a modification which does not require violation of the \(r\)-order and which does lead to a simple explicit solution, namely by permitting randomization.2 This makes it possible to split the next point, including only a portion of it, and thereby to obtain the exact value \(\alpha\) without breaking the order of preference that has been established for inclusion of the various sample points. These considerations are formalized in the following theorem, the _fundamental lemma of Neyman and Pearson_.

Footnote 2: In practice, typically neither the breaking of the \(r\)-order nor randomization is considered acceptable. The common solution, instead, is to adopt a value of \(\alpha\) that can be attained exactly and therefore does not present this problem.

**Theorem 3.2.1**: _Let \(P_{0}\) and \(P_{1}\) be probability distributions possessing densities \(p_{0}\) and \(p_{1}\) respectively with respect to a measure \(\mu\).3_

Footnote 3: There is no loss of generality in this assumption, since one can take \(\mu=P_{0}+P_{1}\).

(i) Existence. _For testing \(H:p_{0}\) against the alternative \(K:p_{1}\) there exists a test \(\phi\) and a constant \(k\) such that_

\[E_{0}\phi(X)=\alpha \tag{3.7}\]

_and_

\[\phi(x)=\left\{\begin{array}{ll}1\ when&p_{1}(x)>kp_{0}(x),\\ 0\ when&p_{1}(x)<kp_{0}(x).\end{array}\right. \tag{3.8}\]

(ii) Sufficient condition for a most powerful test. _If a test satisfies (3.7) and (3.8) for some \(k\), then it is most powerful for testing \(p_{0}\) against \(p_{1}\) at level \(\alpha\)._

(iii) Necessary condition for a most powerful test. _If \(\phi\) is most powerful at level \(\alpha\) for testing \(p_{0}\) against \(p_{1}\), then for some \(k\) it satisfies (3.8)_ a.e. \(\mu\). _It also satisfies (3.7) unless there exists a test of size \(<\alpha\) and with power \(1\)._

Proof. For \(\alpha=0\) and \(\alpha=1\) the theorem is easily seen to be true provided the value \(k=+\infty\) is admitted in (3.8) and \(0\cdot\infty\) is interpreted as \(0\). Throughout the proof we shall therefore assume \(0<\alpha<1\).

(i): Let \(\alpha(c)=P_{0}\{p_{1}(X)>cp_{0}(X)\}\). Since the probability is computed under \(P_{0}\), the inequality needs to be considered only for the set where \(p_{0}(x)>0\), so that \(\alpha(c)\) is the probability that the random variable \(p_{1}(X)/p_{0}(X)\) exceeds \(c\). Thus \(1-\alpha(c)\) is a cumulative distribution function, and \(\alpha(c)\) is nonincreasing and continuous on the right, \(\alpha(c^{-})-\alpha(c)=P_{0}\{p_{1}(X)/p_{0}(X)=c\}\), \(\alpha(-\infty)=1\), and \(\alpha(\infty)=0\). Given any \(0<\alpha<1\), let \(c_{0}\) be such that \(\alpha(c_{0})\leq\alpha\leq\alpha(c_{0}^{-})\), and consider the test \(\phi\) defined by

\[\phi(x)=\left\{\begin{array}{ll}1&\mbox{when }p_{1}(x)>c_{0}p_{0}(x),\\ \frac{\alpha-\alpha(c_{0})}{\alpha(c_{0}^{-})-\alpha(c_{0})}&\mbox{when }p_{1}(x)=c_{0}p_{0}(x),\\ 0&\mbox{when }p_{1}(x)<c_{0}p_{0}(x).\end{array}\right.\]

Here the middle expression is meaningful unless \(\alpha(c_{0})=\alpha(c_{0}^{-})\); since then \(P_{0}\{p_{1}(X)=c_{0}p_{0}(X)\}=0\), \(\phi\) is defined a.e. The size of \(\phi\) is

\[E_{0}\phi(X)=P_{0}\left\{\frac{p_{1}(X)}{p_{0}(X)}>c_{0}\right\}+\frac{\alpha- \alpha(c_{0})}{\alpha(c_{0}^{-})-\alpha(c_{0})}P_{0}\left\{\frac{p_{1}(X)}{p_ {0}(X)}=c_{0}\right\}=\alpha,\]

so that \(c_{0}\) can be taken as the \(k\) of the theorem.

(ii): Suppose that \(\phi\) is a test satisfying (3.7) and (3.8) and that \(\phi^{*}\) is any other test with \(E_{0}\phi^{*}(X)\leq\alpha\). Denote by \(S^{+}\) and \(S^{-}\) the sets in the sample space where \(\phi(x)-\phi^{*}(x)>0\) and \(<0\), respectively. If \(x\) is in \(S^{+}\), \(\phi(x)\) must be \(>0\) and \(p_{1}(x)\geq kp_{0}(x)\). In the same way \(p_{1}(x)\leq kp_{0}(x)\) for all \(x\) in \(S^{-}\), and hence

\[\int(\phi-\phi^{*})(p_{1}-kp_{0})\,d\mu=\int_{S^{+}\cup S^{-}}(\phi-\phi^{*})(p _{1}-kp_{0})\,d\mu\geq 0.\]The difference in power between \(\phi\) and \(\phi*\) therefore satisfies

\[\int(\phi-\phi^{*})\,p_{1}\,d\mu\geq k\int(\phi-\phi^{*})\,p_{0}\,d\mu\geq 0,\]

as was to be proved.

(iii): Let \(\phi^{*}\) be most powerful at level \(\alpha\) for testing \(p_{0}\) against \(p_{1}\), and let \(\phi\) satisfy (3.7) and (3.8). Let \(S\) be the intersection of the set \(S^{+}\cup S^{-}\), on which \(\phi\) and \(\phi^{*}\) differ, with the set \(\{x:p_{1}(x)\neq kp_{0}(x)\}\), and suppose that \(\mu(S)>0\). Since \((\phi-\phi^{*})(p_{1}-kp_{0})\) is positive on \(S\), it follows from Problem 2.4 that

\[\int_{S^{+}\cup S^{-}}(\phi-\phi^{*})(p_{1}-kp_{0})\,d\mu=\int_{S}(\phi-\phi^{* })(p_{1}-kp_{0})\,d\mu>0\]

and hence \(\phi\) is more powerful against \(p_{1}\) than \(\phi^{*}\). This is a contradiction, and therefore \(\mu(S)=0\), as was to be proved.

If \(\phi^{*}\) were of size \(<\alpha\) and power \(<1\), it would be possible to include in the rejection region additional points or portions of points and thereby to increase the power until either the power is 1 or the size is \(\alpha\). Thus either \(E_{0}\phi^{*}(X)=\alpha\) or \(E_{1}\phi^{*}(X)=1\).

The proof of part (iii) shows that the most powerful test is uniquely determined by (3.7) and (3.8) except on the set on which \(p_{1}(x)=kp_{0}(x)\). On this set, \(\phi\) can be defined arbitrarily provided the resulting test has size \(\alpha\). Actually, we have shown that it is always possible to define \(\phi\) to be constant over this boundary set. In the trivial case when there exists a test of power 1, the constant \(k\) of (3.8) is 0, and one will accept \(H\) for all points for which \(p_{1}(x)=kp_{0}(x)\) even though the test may then have size \(<\alpha\).

It follows from these remarks that the most powerful test is determined uniquely (up to sets of measure zero) by (3.7) and (3.8) whenever the set on which \(p_{1}(x)=kp_{0}(x)\) has \(\mu\)-measure zero. This unique test is then clearly nonrandomized. More generally, it is seen that randomization is not required except possibly on the boundary set, where it may be necessary to randomize in order to get the size equal to \(\alpha\). When there exists a test of power 1, (3.7) and (3.8) will determine a most powerful test, but it may not be unique in that there may exist a test also most powerful and satisfying (3.7) and (3.8) for some \(\alpha^{\prime}<\alpha\).

**Corollary 3.2.1**: _Let \(\beta\) denote the power of the most powerful level-\(\alpha\) test \((0<\alpha<1)\) for testing \(P_{0}\) against \(P_{1}\). Then \(\alpha<\beta\) unless \(P_{0}=P_{1}\)._

Proof. Since the level-\(\alpha\) test given by \(\phi(x)\equiv\alpha\) has power \(\alpha\), it is seen that \(\alpha\leq\beta\). If \(\alpha=\beta<1\), the test \(\phi(x)\equiv\alpha\) is most powerful and by Theorem 3.2.1(iii) must satisfy (3.8). Then \(p_{0}(x)=p_{1}(x)\) a.e. \(\mu\) and hence \(P_{0}=P_{1}\).

An alternative method for proving some of the results of this section is based on the following geometric representation of the problem of testing a simple hypothesis against a simple alternative. Let \(N\) be the set of all points \((\alpha,\beta)\) for which there exists a test \(\phi\) such that

\[\alpha=E_{0}\phi(X),\qquad\beta=E_{1}\phi(X).\]

This set is convex, contains the points (0,0) and (1,1), and is symmetric with respect to the point \((\frac{1}{2},\,\frac{1}{2})\) in the sense that with any point \((\alpha,\beta)\) it also contains the point \((1-\alpha,1-\beta)\). In addition, the set \(N\) is closed. [This follows from the weak compactness theorem for critical functions, Theorem A.5.1 of the Appendix; the argument is the same as that in the proof of Theorem 3.6.1(i).]

For each value \(0<\alpha_{0}<1\), the level-\(\alpha_{0}\) tests are represented by the points whose abscissa is \(\leq\alpha_{o}\). The most powerful of these tests (whose existence follows from the fact that \(N\) is closed) corresponds to the point on the upper boundary of \(N\) with abscissa \(\alpha_{0}\). This is the only point corresponding to a most powerful level-\(\alpha_{0}\) test unless there exists a point \((\alpha,1)\) in \(N\) with \(\alpha<\alpha_{0}\) (Figure 3.1_b_).

As an example of this geometric approach, consider the following alternative proof of Corollary 3.2.1. Suppose that for some \(0<\alpha_{0}<1\) the power of the most powerful level-\(\alpha_{0}\) test is \(\alpha_{0}\). Then it follows from the convexity of \(N\) that \((\alpha,\beta)\in N\) implies \(\beta\leq\alpha\), and hence from the symmetry of \(N\) that \(N\) consists exactly of the line segment connecting the points (0,0) and (1,1). This means that \(\int\phi p_{o}\,d\mu=\int\phi p_{1}\,d\mu\) for all \(\phi\) and hence that \(p_{0}=p_{1}\) (a.e.\(\mu\)), as was to be proved. A proof of Theorem 3.2.1 along these lines is given in a more general setting in the proof of Theorem 3.6.1.

**Example 3.2.1**: **(Normal Location Model)** Suppose \(X\) is an observation from \(N(\xi,\sigma^{2})\), with \(\sigma^{2}\) known. The null hypothesis specifies \(\xi=0\) and the alternative specifies \(\xi=\xi_{1}\) for some \(\xi_{1}>0\). Then, the likelihood ratio is given by

\[\frac{p_{1}(x)}{p_{0}(x)}=\frac{\exp[-\frac{1}{2\sigma^{2}}(x-\xi_{1})^{2}]}{ \exp[-\frac{1}{2\sigma^{2}}x^{2}]}=\exp[\frac{\xi_{1}x}{\sigma^{2}}-\frac{\xi _{1}^{2}}{2\sigma^{2}}]. \tag{3.9}\]

Figure 3.1: Possible Values of \((\alpha,\beta)\) for Varying \(\phi\)

Since the exponential function is strictly increasing and \(\xi_{1}>0\), the set of \(x\) where \(p_{1}(x)/p_{0}(x)>k\) is equivalent to the set of \(x\) where \(x>k^{\prime}\). In order to determine \(k^{\prime}\), the level constraint

\[P_{0}\{X>k^{\prime}\}=\alpha\]

must be satisfied, and so \(k^{\prime}=\sigma z_{1-\alpha}\), where \(z_{1-\alpha}\) is the \(1-\alpha\) quantile of the standard normal distribution. Therefore, the most powerful (MP) level \(\alpha\) test rejects if \(X>\sigma z_{1-\alpha}\). Several points are worth mentioning. First, the MP level \(\alpha\) test is unique up to sets of Lebesgue measure 0, by the Neyman-Pearson Lemma, Theorem 3.2.1 (iii). Second, since this test is MP for any alternative \(\xi_{1}>0\), the test is UMP level \(\alpha\) against the composite alternatives \(\xi_{1}>0\). Third, by a similar argument, the test that rejects if \(X<\sigma z_{\alpha}\) is UMP level \(\alpha\) against the composite alternatives \(\xi_{1}<0\). Finally, it follows that no UMP level \(\alpha\) test exists against all two-sided alternatives \(\xi_{1}\neq 0\).

### \(p\)-values

Testing at a fixed level \(\alpha\), as described in Sections 3.1 and 3.2, is one of two standard (non-Bayesian) approaches to the evaluation of hypotheses. To explain the other, suppose that, under \(P_{0}\), the distribution of \(p_{1}(X)/p_{0}(X)\) is continuous. Then, the most powerful level \(\alpha\) test is nonrandomized and rejects if \(p_{1}(X)/p_{0}(X)>k\), where \(k=k(\alpha)\) is determined by (3.7). For varying \(\alpha\), the resulting tests provide an example of the typical situation in which the rejection regions \(R_{\alpha}\) are nested in the sense that

\[R_{\alpha}\subseteq R_{\alpha^{\prime}}\quad\text{if }\alpha<\alpha^{\prime}. \tag{3.10}\]

When this is the case,4 it is good practice to determine not only whether the hypothesis is accepted or rejected at the given significance level, but also to determine the smallest significance level, or more formally

Footnote 4: See Problems 3.17 and 3.63 for examples where optimal nonrandomized tests need not be nested.

\[\hat{p}=\hat{p}(X)=\inf\{\alpha:\ X\in R_{\alpha}\}\, \tag{3.11}\]

at which the hypothesis would be rejected for the given observation. This number, the so-called \(p\)_-value_ gives an idea of how strongly the data contradict the hypothesis.5 It also enables others to reach a verdict based on the significance level of their choice.

**Example 3.3.1**: **(Continuation of Example)** 3.2.1 Let \(\Phi\) denote the standard normal c.d.f. Then, the rejection region can be written as

\[R_{\alpha}=\{X:\ X>\sigma z_{1-\alpha}\}=\{X:\ \Phi(\frac{X}{\sigma})>1-\alpha\}= \{X:\ 1-\Phi(\frac{X}{\sigma})<\alpha\}\.\]

For a given observed value of \(X\), the inf over all \(\alpha\) where the last inequality holds is

\[\hat{p}=1-\Phi(\frac{X}{\sigma})\.\]

Alternatively, the \(p\)-value is \(P_{0}\{X\geq x\}\), where \(x\) is the observed value of \(X\). Note that, under \(\xi=0\), the distribution of \(\hat{p}\) is given by

\[P_{0}\{\hat{p}\leq u\}=P_{0}\{1-\Phi(\frac{X}{\sigma})\leq u\}=P_{0}\{\Phi( \frac{X}{\sigma})\geq 1-u\}=u\,\]

because \(\Phi(X/\sigma)\) is uniformly distributed on (0,1) (see Problem 3.22); therefore, \(\hat{p}\) is uniformly distributed on (0,1).

A general property of \(p\)-values is given in the following lemma, which applies to both simple and composite null hypotheses.

**Lemma 3.3.1**: _Suppose \(X\) has distribution \(P_{\theta}\) for some \(\theta\in\Omega\), and the null hypothesis \(H\) specifies \(\theta\in\Omega_{H}\). Assume the rejection regions satisfy (3.10). (i) If_

\[\sup_{\theta\in\Omega_{H}}P_{\theta}\{X\in R_{\alpha}\}\leq\alpha\quad\mbox{ for all }0<\alpha<1, \tag{3.12}\]

_then the distribution of \(\hat{p}\) under \(\theta\in\Omega_{H}\) satisfies_

\[P_{\theta}\{\hat{p}\leq u\}\leq u\quad\mbox{for all }0\leq u\leq 1. \tag{3.13}\]

_(ii) If, for \(\theta\in\Omega_{H}\),_

\[P_{\theta}\{X\in R_{\alpha}\}=\alpha\quad\mbox{for all }0<\alpha<1\, \tag{3.14}\]

_then_

\[P_{\theta}\{\hat{p}\leq u\}=u\quad\mbox{for all }0\leq u\leq 1\ ;\]

_i.e., \(\hat{p}\) is uniformly distributed over \((0,1)\)._

Proof. (i) If \(\theta\in\Omega_{H}\), then the event \(\{\hat{p}\leq u\}\) implies \(\{X\in R_{v}\}\) for all \(u<v\). The result follows by letting \(v\to u\).

(ii) Since the event \(\{X\in R_{u}\}\) implies \(\{\hat{p}\leq u\}\), it follows that

\[P_{\theta}\{\hat{p}\leq u\}\geq P_{\theta}\{X\in R_{u}\}\.\]

Therefore, if (3.14) holds, then \(P_{\theta}\{\hat{p}\leq u\}\geq u\), and the result follows from (i).

**Example 3.3.2**: Suppose \(X\) takes values \(1,2,\ldots,10\). Under \(H\), the distribution is uniform, i.e., \(p_{0}(j)=\frac{1}{10}\) for \(j=1,\ldots,10\). Under \(K\), suppose \(p_{1}(j)=j/55\). The MP level \(\alpha=i/10\) test rejects if \(X\geq 11-i\). However, unless \(\alpha\) is a multiple of \(1/10\), the MP level \(\alpha\) test is randomized. If we want to restrict attention to nonrandomized procedures, consider the conservative approach by defining

\[R_{\alpha}=\{X\geq 11-i\}\ \ \ \mbox{if}\ \frac{i}{10}\leq\alpha<\frac{i+1}{10}\.\]

If the observed value of \(X\) is \(x\), then the \(p\)-value is given by \((11-x)/10\). Then, the distribution of \(\hat{p}\) under \(H\) is given by

\[P\{\hat{p}\leq u\}=P\{\frac{11-X}{10}\leq u\}=P\{X\geq 11-10u\}\leq u\, \tag{3.15}\]

and the last inequality is an equality if and only if \(u\) is of the form \(i/10\) for some integer \(i=0,1,\ldots,10\), i.e., the levels for which the MP test is nonrandomized (Problem 3.21).

In general, we say a \(p\)-value is valid if it satisfies (3.13) for all \(\theta\in\Omega_{H}\), even if \(\hat{p}\) is not specified through a family of rejection regions. If (3.13) holds, then the test that rejects \(H\) if \(\hat{p}\leq\alpha\) is level \(\alpha\). A direct approach is given next.

**Example 3.3.3**: **(Constructing \(P\)-values for a Simple \(H\))** Suppose for a given family indexed by \(\theta\in\Omega\), the problem is to test a simple null hypothesis \(\theta=\theta_{0}\). Here \(\theta\) is quite general; it can be real-valued, vector-valued, or even be function-valued. (As an example, if \(\theta\) corresponds to a c.d.f. on the real line, \(\theta_{0}\) could specify the uniform distribution.) Let \(T=T(X)\) be any real-valued test statistic, and let its c.d.f. be denoted by \(F_{\theta}(\cdot)\). Then, \(\hat{p}=F_{\theta_{0}}(T)\) serves as a valid \(p\)-value for testing the null hypothesis \(\theta=\theta_{0}\). To see why, note that if \(Y\) is any real-valued random variable with c.d.f. \(G(\cdot)\), then (Problem 3.23)

\[P\{G(Y)\leq u\}\leq u\ \ \ \mbox{for all}\ 0\leq u\leq 1\.\]

Hence, for any \(0\leq u\leq 1\),

\[P_{\theta_{0}}\{\hat{p}\leq u\}=P_{\theta_{0}}\{F_{\theta_{0}}(T)\leq u\}\leq u\,\]

verifying (3.13). A test based on \(F_{\theta_{0}}(T)\) is appropriate if small values of \(T\) indicate departures from \(H\). Similarly, if large values of \(T\) indicate departures from \(H\), then \(1-F_{\theta_{0}}^{-}(T)\) is a valid \(p\)-value, where \(F_{\theta_{0}}^{-}(t)=P_{\theta_{0}}\{T<t\}\).

\(P\)-values, with the additional information they provide, are typically more appropriate than fixed levels in scientific problems, whereas a fixed predetermined \(\alpha\) is unavoidable when acceptance or rejection of \(H\) implies an imminent concrete decision. A review of some of the issues arising in this context, with references to theliterature, are given in Kruskal (1978), Wasserstein and Lazar (2016), and Kuffner and Walker (2019).

### Distributions with Monotone Likelihood Ratio

The case that both the hypothesis and the class of alternatives are simple is mainly of theoretical interest, since problems arising in applications typically involve a parametric family of distributions depending on one or more parameters. In the simplest situation of this kind the distributions depend on a single real-valued parameter \(\theta\), and the hypothesis is one sided, say \(H:\theta\leq\theta_{0}\). In general, the most powerful test of \(H\) against an alternative \(\theta_{1}>\theta_{0}\) depends on \(\theta_{1}\) and is then not UMP. However, a UMP test does exist if an additional assumption is satisfied. The real-valued parameter family of densities \(p_{\theta}(x)\) is said to have _monotone likelihood ratio6_ if there exists a real-valued function \(T(x)\) such that for any \(\theta<\theta^{\prime}\) the distributions \(P_{\theta}\) and \(P_{\theta^{\prime}}\) are distinct, and the ratio \(p_{\theta^{\prime}}(x)/p_{\theta}(x)\) is a nondecreasing function of \(T(x)\).

Footnote 6: This definition is in terms of specific versions of the densities \(p_{\theta}\). If instead the definition is to be given in terms of the distribution \(P_{\theta}\), various null-set considerations enter which are discussed in Pfanzagl (1967).

**Theorem 3.4.1**: _Let \(\theta\) be a real parameter, and let the random variable \(X\) have probability density \(p_{\theta}(x)\) with monotone likelihood ratio in \(T(x)\)._

_For testing \(H:\theta\leq\theta_{0}\) against \(K:\theta>\theta_{0}\), there exists a UMP test, which is given by_

\[\phi(x)=\left\{\begin{array}{ll}1\ when\ T(x)>C,\\ \gamma\ when\ T(x)=C,\\ 0\ when\ T(x)<C,\end{array}\right. \tag{3.16}\]

_where \(C\) and \(\gamma\) are determined by_

\[E_{\theta_{0}}\phi(X)=\alpha. \tag{3.17}\]

_The power function_

\[\beta(\theta)=E_{\theta}\phi(X)\]

_of this test is strictly increasing for all points \(\theta\) for which \(0<\beta(\theta)<1\)._

_For all \(\theta^{\prime}\), the test determined by (3.16) and (3.17) is UMP for testing \(H^{\prime}:\theta\leq\theta^{\prime}\) against \(K^{\prime}:\theta>\theta^{\prime}\) at level \(\alpha^{\prime}=\beta(\theta^{\prime})\)._

_For any \(\theta<\theta_{0}\) the test minimizes \(\beta(\theta)\) (the probability of an error of the first kind) among all tests satisfying (3.17)._

Proof. (i) and (ii): Consider first the hypothesis \(H_{0}:\theta=\theta_{0}\) and some simple alternative \(\theta_{1}>\theta_{0}\). The most desirable points for rejection are those for which \(r(x)=p_{\theta_{1}}(x)/p_{\theta_{0}}(x)=g[T(x)]\) is sufficiently large. If \(T(x)<T(x^{\prime})\), then \(r(x)\leq r(x^{\prime})\)and \(x^{\prime}\) is at least as desirable as \(x\). Thus the test which rejects for large values of \(T(x)\) is most powerful. As in the proof of Theorem 3.2.1(i), it is seen that there exist \(C\) and \(\gamma\) such that (3.16) and (3.17) hold. By Theorem 3.2.1(ii), the resulting test is also most powerful for testing \(P_{\theta^{\prime}}\) against \(P_{\theta^{\prime\prime}}\) at level \(\alpha^{\prime}=\beta(\theta^{\prime})\) provided \(\theta^{\prime}<\theta^{\prime\prime}\). Part (ii) of the present theorem now follows from Corollary 3.2.1. Since \(\beta(\theta)\) is therefore nondecreasing the test satisfies

\[E_{\theta}\phi(X)\leq\alpha\quad\text{ for }\quad\theta\leq\theta_{0}. \tag{3.18}\]

The class of tests satisfying (3.18) is contained in the class satisfying \(E_{\theta_{0}}\phi(X)\leq\alpha\). Since the given test maximizes \(\beta(\theta_{1})\) within this wider class, it also maximizes \(\beta(\theta_{1})\) subject to (3.18); since it is independent of the particular alternative \(\theta_{1}>\theta_{0}\) chosen, it is UMP against \(K\).

(iii) is proved by an analogous argument.

(iv) follows from the fact that the test which minimizes the power for testing a simple hypothesis against a simple alternative is obtained by applying the fundamental lemma (Theorem 3.2.1) with all inequalities reversed.

By interchanging inequalities throughout, one obtains in an obvious manner the solution of the dual problem, \(H:\theta\geq\theta_{0}\), \(K:\theta<\theta_{0}\).

The proof of (i) and (ii) exhibits the basic property of families with monotone likelihood ratio: every pair of parameter values \(\theta_{0}<\theta_{1}\) establishes essentially the same preference order of the sample points (in the sense of the preceding section). A few examples of such families, and hence of UMP one-sided tests, will be given below. However, the main applications of Theorem 3.4.1 will come later, when such families appear as the set of conditional distributions given a sufficient statistic (Chapters 4 and 5) and as distributions of a maximal invariant (Chapters 6 and 7).

**Example 3.4.1**: **(Hypergeometric)** From a lot containing \(N\) items of a manufactured product, a sample of size \(n\) is selected at random, and each item in the sample is inspected. If the total number of defective items in the lot is \(D\), the number \(X\) of defectives found in the sample has the _hypergeometric_ distribution

\[P\{X=x\}=P_{D}(x)=\frac{{D\choose x}{N-D\choose n-x}}{{N\choose n}},\quad\max( 0,n+D-N)\leq x\leq\min(n,D).\]

Interpreting \(P_{D}(x)\) as a density with respect to the measure \(\mu\) that assigns to any set on the real line as measure the number of integers \(0,1,2,\ldots\) that it contains, and nothing that for values of \(x\) within its range

\[\frac{P_{D+1}(x)}{P_{D}(x)}=\left\{\begin{array}{ll}\frac{D+1}{N-D}\frac{N-D -n+x}{D+1-x}&\text{if }n+D+1-N\leq x\leq D,\\ 0\text{ or }\infty&\text{if }x=n+D-N\text{ or }D+1,\end{array}\right.\]

it is seen that the distributions satisfy the assumption of monotone likelihood ratios with \(T(x)=x\). Therefore there exists a UMP test for testing the hypothesis \(H:D\leq D_{0}\) against \(K:D>D_{0}\), which rejects \(H\) when \(X\) is too large, and an analogous test for testing \(H^{\prime}:D\geq D_{0}\).

An important class of families of distributions that satisfy the assumptions of Theorem 3.4.1 are the _one-parameter exponential families_.

Corollary 3.4.1: _Let \(\theta\) be a real parameter, and let \(X\) have probability density (with respect to some measure \(\mu\))_

\[p_{\theta}(x)=C(\theta)e^{Q(\theta)T(x)}h(x),\]

_where \(Q\) is strictly monotone. Then there exists a UMP test \(\phi\) for testing \(H:\theta\leq\theta_{0}\) against \(K:\theta>\theta_{0}\). If \(Q\) is increasing,_

\[\phi(x)=1,\gamma,0\quad\text{ as }\ \ T(x)>,=,<C,\]

_where \(C\) and \(\gamma\) are determined by \(E_{\theta_{0}}\phi(X)=\alpha\). If \(Q\) is decreasing, the inequalities are reversed._

A converse of Corollary 3.4.1 is given by Pfanzagl (1968), who shows under weak regularity conditions that the existence of UMP tests against one-sided alternatives for all sample sizes and one value of \(\alpha\) implies an exponential family.

As in Example 3.4.1, we shall denote the right-hand side of (3.19) by \(P_{\theta}(x)\) instead of \(p_{\theta}(x)\) when it is a probability, that is, when \(X\) is discrete and \(\mu\) is counting measure.

Example 3.4.2: (Binomial) The binomial distributions \(b(p,n)\) with

\[P_{p}(x)={n\choose x}p^{x}(1-p)^{n-x}\]

satisfy (3.19) with \(T(x)=x\), \(\theta=p\), \(Q(p)=\log[p/(1-p)]\). The problem of testing \(H:p\geq p_{0}\) arises, for instance, in the situation of Example 3.4.1 if one supposes that the production process is in statistical control, so that the various items constitute independent trials with constant probability \(p\) of being defective. The number of defectives \(X\) in a sample of size \(n\) is then sufficient statistic for the distribution of the variables \(X_{i}\) (\(i=1,\ldots,n\)), where \(X_{i}\) is 1 or 0 as the \(i\)th item drawn is defective or not, and \(X\) is distributed as \(b(p,n)\). There exists therefore a UMP test of \(H\), which rejects \(H\) when \(X\) is too small.

An alternative sampling plan which is sometimes used in binomial situations is _inverse binomial sampling_. Here the experiment is continued until a specified number \(m\) of successes--for example, cures effected by some new medical treatment--has been obtained. If \(Y_{i}\) denotes the number of trials after the (\(i-1\))st success, up to but not including the \(i\)th success, the probability that \(Y_{i}=y\) is \(pq^{y}\) for \(y=0\), 1, \(\ldots\), so that the joint distribution of \(Y_{1}\), \(\ldots\), \(Y_{m}\) is

\[P_{p}(y_{1},\ldots,y_{m})=p^{m}q^{\sum y_{k}},\quad\quad y_{k}=0,1,\ldots, \quad k=1,\ldots,m.\]This is an exponential family with \(T(y)=\sum y_{i}\) and \(Q(p)=\log(1-p)\). Since \(Q(p)\) is a decreasing function of \(p\), the UMP test of \(H:p\leq p_{0}\) rejects \(H\) when \(T\) is too small. This is what one would expect, since the realization of \(m\) successes in only a few more than \(m\) trials indicates a high value of \(p\). The test statistic \(T\), which is the number of trials required in excess of \(m\) to get \(m\) successes, has the negative binomial distribution (Problem 1.1(i))

\[P(t)=\binom{m+t-1}{m-1}p^{m}q^{t},\qquad t=0,1,\ldots...\]

**Example 3.4.3**: **(Poisson)** If \(X_{1},\ldots,X_{n}\) are independent Poisson variables with \(E(X_{i})=\lambda\), their joint distribution is

\[P_{\lambda}(x_{1},\ldots,x_{n})=\frac{\lambda^{x_{1}+\cdots+x_{n}}}{x_{1}! \cdots x_{n}!}e^{-n\lambda}.\]

This constitutes an exponential family with \(T(x)=\sum x_{i}\), and \(Q(\lambda)=\log\lambda\). One-sided hypotheses concerning \(\lambda\) might arise if \(\lambda\) is a bacterial density and the \(X\)'s are a number of bacterial counts, or if the \(X\)'s denote the number of \(\alpha\)-particles produced in equal time intervals by a radioactive substance, etc. The UMP test of the hypothesis \(\lambda\leq\lambda_{0}\) rejects when \(\sum X_{i}\) is too large. Here the test statistic \(\sum X_{i}\) has itself a Poisson distribution with parameter \(n\lambda\).

Instead of observing the radioactive material for given time periods or counting the number of bacteria in given areas of a slide, one can adopt an inverse sampling method. The experiment is then continued, or the area over which the bacteria are counted is enlarged, until a count of \(m\) has been obtained. The observations consist of the times \(T_{1}\),..., \(T_{m}\) that it takes for the first occurrence, from the first to the second, and so on. If one is dealing with a Poisson process and the number of occurrences in a time or space interval \(\tau\) has the distribution

\[P(x)=\frac{(\lambda\tau)^{x}}{x!}e^{-\lambda\tau},\qquad x=0,1,\ldots,\]

then the observed times are independently distributed, each with the exponential density \(\lambda e^{-\lambda t}\) for \(t\geq 0\) (Problem 1.1(ii)). The joint densities

\[p_{\lambda}(t_{1},\ldots,t_{m})=\lambda^{m}\exp\left(-\lambda\sum_{i=1}^{m}t_ {i}\right),\qquad t_{1},\ldots,t_{m}\geq 0,\]

form an exponential family with \(T(t_{1},\ldots,t_{m})=\sum t_{i}\) and \(Q(\lambda)=-\lambda\). The UMP test of \(H:\lambda\leq\lambda_{0}\) rejects when \(T=\sum T_{i}\) is too small. Since \(2\lambda T_{i}\) has density \(\frac{1}{2}e^{-u/2}\) for \(u\geq 0\), which is the density of a \(\chi^{2}\)-distribution with \(2\) degrees of freedom, \(2\lambda T\) has a \(\chi^{2}\)-distribution with \(2m\) degrees of freedom. The boundary of the rejection region can therefore be determined from a table of \(\chi^{2}\).

The formulation of the problem of hypothesis testing given at the beginning of the chapter takes account of the losses resulting from wrong decisions only in terms of the two types of error. To obtain a more detailed description of the problem of testing \(H:\theta\leq\theta_{0}\) against the alternatives \(\theta>\theta_{0}\), one can consider it as a decision problem with the decisions \(d_{0}\) and \(d_{1}\) of accepting and rejecting \(H\) and a loss function \(L(\theta,d_{i})=L_{i}(\theta)\). Typically, \(L_{0}(\theta)\) will be \(0\) for \(\theta\leq\theta_{0}\) and strictly increasing for \(\theta\geq\theta_{0}\), and \(L_{1}(\theta)\) will be strictly decreasing for \(\theta\leq\theta_{0}\) and equal to \(0\) for \(\theta\geq\theta_{0}\). The difference then satisfies

\[L_{1}(\theta)-L_{0}(\theta)\mathop{>}_{<}^{>}0\quad\text{ as }\quad\theta \mathop{<}_{>}^{<}\theta_{0}. \tag{3.20}\]

The following theorem is a special case of complete class results of Karlin and Rubin (1956) and Brown et al. (1976).

**Theorem 3.4.2**: (i) _Under the assumptions of Theorem 3.4.1, the family of tests given by (3.16) and (3.17) with \(0\leq\alpha\leq 1\) is essentially complete provided the loss function satisfies (3.20)._

(ii) _This family is also minimal essentially complete if the set of points \(x\) for which \(p_{\theta}(x)>0\) is independent of \(\theta\)._

Proof. (i): The risk function of any test \(\phi\) is

\[R(\theta,\phi) =\int\,p_{\theta}(x)\{\phi(x)L_{1}(\theta)+[1-\phi(x)]L_{0}( \theta)\}\,d\mu(x)\] \[=\int\,p_{\theta}(x)\{L_{0}(\theta)+[L_{1}(\theta)-L_{0}(\theta) ]\phi(x)\}\,d\mu(x),\]

and hence the difference of two risk functions is

\[R(\theta,\phi^{\prime})-R(\theta,\phi)=[L_{1}(\theta)-L_{0}(\theta)]\int(\phi^ {\prime}-\phi)\,p_{\theta}\,d\mu.\]

This is \(\leq 0\) for all \(\theta\) if

\[\beta_{\phi^{\prime}}(\theta)-\beta_{\phi}(\theta)=\int(\phi^{\prime}-\phi)\,p _{\theta}\,d\mu\mathop{\geqq}_{\llq 0}\quad\text{ for }\quad\theta\mathop{\geqq}_{\llq 0}.\]

Given any test \(\phi\), let \(E_{\theta_{0}}\phi(X)=\alpha\). It follows from Theorem 3.4.1(i) that there exists a UMP level-\(\alpha\) test \(\phi^{\prime}\) for testing \(\theta=\theta_{0}\) against \(\theta>\theta_{0}\), which satisfies (3.16) and (3.17). By Theorem 3.4.1(iv), \(\phi^{\prime}\) also minimizes the power for \(\theta<\theta_{0}\). Thus the two risk functions satisfy \(R(\theta,\phi^{\prime})\leq R(\theta,\phi)\) for all \(\theta\), as was to be proved.

(ii): Let \(\phi_{\alpha}\) and \(\phi_{\alpha^{\prime}}\) be of sizes \(\alpha<\alpha^{\prime}\) and UMP for testing \(\theta_{0}\) against \(\theta>\theta_{0}\). Then \(\beta_{\phi_{\alpha}}(\theta)<\beta_{\phi_{\alpha^{\prime}}}(\theta)\) for all \(\theta>\theta_{0}\) unless \(\beta_{\phi_{\alpha}}(\theta)=1\). By considering the problem of testing \(\theta=\theta_{0}\) against \(\theta<\theta_{0}\), it is seen analogously that this inequality also holds for all \(\theta<\theta_{0}\) unless \(\beta_{\phi_{\alpha^{\prime}}}(\theta)=0\). Since the exceptional possibilities are excluded by the assumptions, it follows that \(R(\theta,\phi^{\prime})>\stackrel{{<}}{{>}}R(\theta,\phi)\) as \(\theta\stackrel{{>}}{{<}}\theta_{0}\). Hence each of the two risk functions is better than the other for some values of \(\theta\).

The class of tests previously derived as UMP at the various significance levels \(\alpha\) is now seen to constitute an essentially complete class for a much more general decision problem, in which the loss function is only required to satisfy certain broad qualitative conditions. From this point of view, the formulation involving the specification of a level of significance can be considered a simple way of selecting a particular procedure from an essentially complete family.

The property of monotone likelihood ratio defines a very strong ordering of a family of distributions. For later use, we consider also the following somewhat weaker definition. A family of cumulative distribution functions \(F_{\theta}\) on the real line is said to be _stochastically increasing_ (and the same term is applied to random variables possessing these distributions) if the distributions are distinct and if \(\theta<\theta^{\prime}\) implies \(F_{\theta}(x)\geq F_{\theta^{\prime}}(x)\) for all \(x\). If then \(X\) and \(X^{\prime}\) have distributions \(F_{\theta}\) and \(F^{\prime}_{\theta}\), respectively, it follows that \(P\{X>x\}\leq P\{X^{\prime}>x\}\) for all \(x\) so that \(X^{\prime}\) tends to have larger values than \(X\). In this case the variable \(X^{\prime}\) is said to be _stochastically larger_ than \(X\). This relationship is made more intuitive by the following characterization of the stochastic ordering of two distributions.

**Lemma 3.4.1**: _Let \(F_{0}\) and \(F_{1}\) be two cumulative distribution functions on the real line. Then \(F_{1}(x)\leq F_{0}(x)\) for all \(x\) if and only if there exist two nondecreasing functions \(f_{0}\) and \(f_{1}\), and a random variable \(V\) such that_ (a)_\(f_{0}(v)\leq f_{1}(v)\) for all \(v\), and_ (b) _the distributions of \(f_{0}(V)\) and \(f_{1}(V)\) are \(F_{0}\) and \(F_{1}\), respectively._

Proof. Suppose first that the required \(f_{0}\), \(f_{1}\) and \(V\) exist. Then

\[F_{1}(x)=P\{f_{1}(V)\leq x\}\leq P\{f_{0}(V)\leq x\}=F_{0}(x)\]

for all \(x\). Conversely, suppose that \(F_{1}(x)\leq F_{0}(x)\) for all \(x\), and let \(f_{i}(y)=\inf\{x:F_{i}(x-0)\leq y\leq F_{1}(x)\},i=0,1\). These functions are nondecreasing and for \(f_{i}=f\), \(F_{i}=F\) satisfy

\[f[F(x)]\leq x\mbox{ and }F[f(y)]\geq y\mbox{\ \ \ \ \ for all }x\mbox{ and }y.\]

It follows that \(y\leq F(x_{0})\) implies \(f(y)\leq f[F(x_{0})]\leq x_{0}\) and that conversely \(f(y)\leq x_{0}\) implies \(F[f(y)]\leq F(x_{0})\) and hence \(y\leq F(x_{0})\), so that the two inequalities \(f(y)\leq x_{0}\) and \(y\leq F(x_{0})\) are equivalent. Let \(V\) be uniformly distributed on (0,1). Then \(P\{f_{i}(V)\leq x\}=P\{V\leq F_{i}(x)\}=F_{i}(x)\). Since \(F_{i}(x)\leq F_{0}(x)\) for all \(x\) implies \(f_{0}(y)\leq f_{1}(y)\) for all \(y\), this completes the proof.

One of the simplest examples of a stochastically ordered family is a location parameter family, that is, a family satisfying

\[F_{\theta}(x)=F(x-\theta).\]To see that this is stochastically increasing, let \(X\) be a random variable with distribution \(F(x)\). Then \(\theta<\theta^{\prime}\) implies

\[F(x-\theta)=P\{X\leq x-\theta\}\geq P\{X\leq x-\theta^{\prime}\}=F(x-\theta^{ \prime}),\]

as was to be shown.

Another example is furnished by families with monotone likelihood ratio. This is seen from the following lemma, which establishes some basic properties of these families.

**Lemma 3.4.2**: _Let \(p_{\theta}(x)\) be a family of densities on the real line with monotone likelihood ratio in \(x\)._

(i) _If \(\psi\) is a nondecreasing function of \(x\), then \(E_{\theta}\psi(X)\) is a nondecreasing function of \(\theta\); if \(X_{1},\ldots,X_{n}\) are independently distributed with density \(p_{\theta}\) and \(\psi^{\prime}\) is a function of \(x_{1},\ldots,x_{n}\) which is nondecreasing in each of its arguments, then \(E_{\theta}\psi^{\prime}(X_{1},\ldots,X_{n})\) is a nondecreasing function of \(\theta\)._

(ii) _For any \(\theta<\theta^{\prime}\), the cumulative distribution functions of \(X\) under \(\theta\) and \(\theta^{\prime}\) satisfy_

\[F_{\theta^{\prime}}(x)\leq F_{\theta}(x)\quad\mbox{ for all }x\,.\]

(iii) _Let \(\psi\) be a function with a single change of sign. More specifically, suppose there exists a value \(x_{0}\) such that \(\psi(x)\leq 0\) for \(x<x_{0}\) and \(\psi(x)\geq 0\) for \(x\geq x_{0}\). Then there exists \(\theta_{0}\) such that \(E_{\theta}\psi(X)\leq 0\) for \(\theta<\theta_{0}\) and \(E_{\theta}\psi(X)\geq 0\) for \(\theta>\theta_{0}\), unless \(E_{\theta}\psi(X)\) is either positive for all \(\theta\) or negative for all \(\theta\)._

(iv) _Suppose that \(p_{\theta}(x)\) is positive for all \(\theta\) and all \(x\), that \(p_{\theta^{\prime}}(x)/p_{\theta}(x)\) is strictly increasing in \(x\) for \(\theta<\theta^{\prime}\), and that \(\psi(x)\) is as in_ (iii) _and is \(\neq 0\) with positive probability. If \(E_{\theta o}\psi(X)=0\), then \(E_{\theta}\psi(X)<0\) for \(\theta<\theta_{0}\) and \(>0\) for \(\theta>\theta_{0}\)._

Proof. (i): Let \(\theta<\theta^{\prime}\), and let \(A\) and \(B\) be the sets for which \(p_{\theta^{\prime}}(x)<p_{\theta}(x)\) and \(p_{\theta^{\prime}}(x)>p_{\theta}(x)\) respectively. If \(a=\sup_{A}\psi(x)\) and \(b=\inf_{B}\psi(x)\), then \(b-a\geq 0\) and

\[\int\psi(p_{\theta^{\prime}}-p_{\theta})\,d\mu \geq a\int_{A}(p_{\theta^{\prime}}-p_{\theta})\,d\mu+b\int_{B}(p_{ \theta^{\prime}}-p_{\theta})\,d\mu\] \[= (b-a)\int_{B}(p_{\theta^{\prime}}-p_{\theta})\,d\mu\geq 0,\]

which proves the first assertion. The result for general \(n\) follows by induction.

(ii): This follows from (i) by letting \(\psi(x)=1\) for \(x>x_{0}\) and \(\psi(x)=0\) otherwise.

(iii): We shall show first that for any \(\theta^{\prime}<\theta^{\prime\prime}\), \(E_{\theta^{\prime}}\psi(X)>0\) implies \(E_{\theta^{\prime}}\psi(X)\geq 0\). If \(p_{\theta^{\prime\prime}}(x_{0})/p_{\theta^{\prime}}(x_{0})=\infty\), then \(p_{\theta^{\prime\prime}}(x)=0\) for \(x\geq x_{0}\) and hence \(E_{\theta^{\prime}}\psi(X)\leq 0\). Suppose therefore that \(p_{\theta^{\prime\prime}}(x_{0})/p_{\\[E_{\theta^{\prime}}\psi(X) \geq\int_{\bar{S}}\psi\frac{p_{\theta^{\prime\prime}}}{p_{\theta^{ \prime}}}p_{\theta^{\prime}}\,d\mu\] \[\geq\int_{-\infty}^{x_{0}-}c\psi p_{\theta^{\prime}}\,d\mu+\int_{x _{0}}^{\infty}c\psi p_{\theta^{\prime}}\,d\mu=cE_{\theta^{\prime}}\psi(X)\geq 0.\]

The result now follows by letting \(\theta_{0}=\inf\{\theta:E_{\theta}\psi(X)>0\}\).

(iv): The proof is analogous to that of (iii).

Part (ii) of the lemma shows that any family of distributions with monotone likelihood ratio in \(x\) is stochastically increasing. That the converse does not hold is shown for example by the Cauchy densities

\[\frac{1}{\pi}\frac{1}{1+(x-\theta)^{2}}.\]

The family is stochastically increasing, since \(\theta\) is a location parameter; however, the likelihood ratio is not monotone. Conditions under which a location parameter family possesses monotone likelihood ratio are given in Example 8.2.1.

Lemma 3.4.2 is a special case of a theorem of Karlin (1957, 1968) relating the number of sign changes of \(E_{\theta}\psi(X)\) to those of \(\psi(x)\) when the densities \(p_{\theta}(x)\) are _totally positive_ (defined in Problem 3.55). The application of totally positive--or equivalently, variation diminishing--distributions to statistics is discussed by Brown et al. (1981); see also Problem 3.58.

### Confidence Bounds

The theory of UMP one-sided tests can be applied to the problem of obtaining a lower or upper bound for a real-valued parameter \(\theta\). The problem of setting a lower bound arises, for example, when \(\theta\) is the breaking strength of a new alloy; that of setting an upper bound, when \(\theta\) is the toxicity of drug or the probability of an undesirable event. The discussion of lower and upper bounds is completely parallel, and it is therefore enough to consider the case of a lower bound, say \(\underline{\theta}\).

Since \(\underline{\theta}=\underline{\theta}(X)\) will be a function of the observations, it cannot be required to fall below \(\theta\) with certainty, but only with specified high probability. One selects a number \(1-\alpha\), the _confidence level_, and restricts attention to bounds \(\theta\) satisfying

\[P_{\theta}\{\underline{\theta}(X)\leq\theta\}\geq 1-\alpha\quad\text{ for all }\theta. \tag{3.21}\]

The function \(\underline{\theta}\) is called a lower _confidence bound_ for \(\theta\) at confidence level \(1-\alpha\); the infimum of the left-hand side of (3.21), which in practice will be equal to \(1-\alpha\), is called the _confidence coefficient_ of \(\underline{\theta}\).

Subject to (3.21), \(\underline{\theta}\) should underestimate \(\theta\) by as little as possible. One can ask, for example, that the probability of \(\underline{\theta}\) falling below any \(\theta^{\prime}<\theta\) should be a minimum.

\(A\) function \(\underline{\theta}\) for which

\[P_{\theta}\{\underline{\theta}(X)\leq\theta^{\prime}\}=\ \mbox{minimum} \tag{3.22}\]

for all \(\theta^{\prime}<\theta\) subject to (3.21) is a uniformly _most accurate_ lower confidence bound for \(\theta\) at confidence level \(1-\alpha\).

Let \(L(\theta,\underline{\theta})\) be a measure of the loss resulting from underestimating \(\theta\), so that for each fixed \(\theta\) the function \(L(\theta,\underline{\theta})\) is defined and nonnegative for \(\underline{\theta}<\theta\), and is nonincreasing in this second argument. One would then wish to minimize

\[E_{\theta}L(\theta,\underline{\theta}) \tag{3.23}\]

subject to (3.21). It can be shown that a uniformly most accurate lower confidence bound \(\underline{\theta}\) minimizes (3.23) subject to (3.21) for every such loss function \(L\). (See Problem 3.49.)

The derivation of uniformly most accurate confidence bounds is facilitated by introducing the following more general concept, which will be considered in more detail in Chapter 5. A family of subsets \(S(x)\) of the parameter space \(\Omega\) is said to constitute a family of _confidence sets_ at confidence level \(1-\alpha\) if

\[P_{\theta}\{\theta\in S(X)\}\geq 1-\alpha\ \ \ \ \ \mbox{for all}\ \ \ \theta\in\Omega, \tag{3.24}\]

that is, if the random sets \(S(X)\) covers the true parameter point with probability \(\geq 1-\alpha\). A lower confidence bound corresponds to the special case that \(S(x)\) is a one-sided interval

\[S(x)=\{\theta:\underline{\theta}(x)\leq\theta<\infty\}.\]

**Theorem 3.5.1**: (i) _For each \(\theta_{0}\in\Omega\) let \(A(\theta_{0})\) be the acceptance region of a level-\(\alpha\) test for testing \(H(\theta_{0}):\theta=\theta_{0}\), and for each sample point \(x\) let \(S(x)\) denote the set of parameter values_

\[S(x)=\{\theta:x\in A(\theta),\,\theta\in\Omega\}.\]

_Then \(S(x)\) is a family of confidence sets for \(\theta\) at confidence level \(1-\alpha\)._

(ii) _If for all \(\theta_{0}\), \(A(\theta_{0})\) is UMP for testing \(H(\theta_{0})\) at level \(\alpha\) against the alternatives \(K(\theta_{0})\), then for each \(\theta_{0}\notin\Omega\), \(S(X)\) minimizes the probability_

\[P_{\theta}\{\theta_{0}\in S(X)\}\ \ \ \ \mbox{for all}\ \ \ \theta\in K(\theta_{0})\]

_among all level \(1-\alpha\) families of confidence sets for \(\theta\)._

Proof. (i) By definition of \(S(x)\),

\[\theta\in S(x)\ \ \ \ \ \mbox{if and only if}\ \ \ x\in A(\theta), \tag{3.25}\]

and hence \[P_{\theta}\{\theta\in S(X)\}=P_{\theta}\{X\in A(\theta)\}\geq 1-\alpha.\]

(ii) If \(S^{*}(x)\) is any other family of confidence sets at level \(1-\alpha\), and if \(A^{*}(\theta)=\{x:\theta\in S^{*}(x)\}\), then

\[P_{\theta}\{X\in A^{*}(\theta)\}=P_{\theta}\{\theta\in S^{*}(X)\}\geq 1-\alpha\]

so that \(A^{*}(\theta_{0})\) is the acceptance region of a level-\(\alpha\) test of \(H(\theta_{0})\). It follows from the assumed property of \(A(\theta_{0})\) that for any \(\theta\in K(\theta_{0})\)

\[P_{\theta}\{X\in A^{*}(\theta_{0})\}\geq P_{\theta}\{X\in A(\theta_{0})\}\]

and hence that

\[P_{\theta}\{\theta_{0}\in S^{*}(X)\}\geq P_{\theta}\{\theta_{0}\in S(X)\},\]

as was to be proved.

The equivalence (3.25) shows the structure of the confidence sets \(S(x)\) as the totality of parameter values \(\theta\) for which the hypothesis \(H(\theta)\) is accepted when \(x\) is observed. A confidence set can therefore be viewed as a combined statement regarding the tests of the various hypotheses \(H(\theta)\), which exhibits the values for which the hypothesis is accepted [\(\theta\in S(x)\)] and those for which it is rejected [\(\theta\in\tilde{S}(x)\)]. Such a method of constructing confidence sets for parameters is known as "test inversion".

Note that a lower confidence bound \(\underline{\theta}\) satisfying (3.21) corresponds to the interval [\(\underline{\theta}\), \(\infty\)). However, one can typically also conclude the open interval (\(\underline{\theta}\), \(\infty\)) contains the true \(\theta\) with probability at least \(1-\alpha\). This occurs under the conditions of Corollary 3.5.1 below. So, the resulting confidence interval may or may not include the endpoint, and those obtained through "test inversion" will include the endpoint or not depending on the exact construction of the tests. The first definition where the confidence set includes the lower endpoint was a convenient way to initiate the discussion. In practice, confidence intervals are usually presented with the endpoints included. Certainly, if the open interval satisfies the coverage constraint, so does its closure.

**Corollary 3.5.1**: _Let the family of densities \(p_{\theta}(x)\), \(\theta\in\Omega\), have monotone likelihood ratio in \(T(x)\), and suppose that the cumulative distribution function \(F_{\theta}(t)\) of \(T=T(X)\) is a continuous function in each of the variables \(t\) and \(\theta\) when the other is fixed._

(i) _There exists a uniformly most accurate confidence bound \(\underline{\theta}\) for \(\theta\) at each confidence level \(1-\alpha\)._

(ii) _If \(x\) denotes the observed values of \(X\) and \(t=T(x)\), and if the equation_

\[F_{\theta}(t)=1-\alpha \tag{3.26}\]

_has a solution \(\theta=\hat{\theta}\) in \(\Omega\) then this solution is unique and \(\underline{\theta}(x)=\hat{\theta}\)._Proof. (i): There exists for each \(\theta_{0}\) a constant \(C(\theta_{0})\) such that

\[P_{\theta_{0}}\{T>C(\theta_{0})\}=\alpha,\]

and by Theorem 3.4.1, \(T>C(\theta_{0})\) is a UMP level-\(\alpha\) rejection region for testing \(\theta=\theta_{0}\) against \(\theta>\theta_{0}\). By Corollary 3.2.1, the power of this test against any alternative \(\theta_{1}>\theta_{0}\) exceeds \(\alpha\), and hence \(C(\theta_{0})<C(\theta_{1})\) so that the function \(C\) is strictly increasing; it is also continuous. Let \(A(\theta_{0})\) denote the acceptance region \(T\leq C(\theta_{0})\), and let \(S(x)\) be defined by (3.25). If follows from the monotonicity of the function \(C\) that \(S(x)\) consists of those values \(\theta\in\Omega\) which satisfy \(\underline{\theta}\leq\theta\), where

\[\underline{\theta}=\inf\{\theta:T(x)\leq C(\theta)\}.\]

By Theorem 3.5.1, the sets \(\{\theta:\underline{\theta}(x)\leq\theta\}\), restricted to possible values of the parameter, constitute a family of confidence sets at level \(1-\alpha\), which minimize \(P_{\theta}\{\underline{\theta}\leq\theta^{\prime}\}\) for all \(\theta\in K(\theta^{\prime})\), that is, for all \(\theta>\theta^{\prime}\). This shows \(\underline{\theta}\) to be a uniformly most accurate confidence bound for \(\theta\).

(ii): It follows from Corollary 3.2.1 that \(F_{\theta}(t)\) is a strictly decreasing function of \(\theta\) at any point \(t\) for which \(0<F_{\theta}(t)<1\), and hence that (3.26) can have at most one solution. Suppose now that \(t\) is the observed value of \(T\) and that the equation \(F_{\theta}(t)=1-\alpha\) has the solution \(\hat{\theta}\in\Omega\). Then \(F_{\hat{\theta}}(t)=1-\alpha\), and by definition of the function \(C\), \(C(\hat{\theta})=t\). The inequality \(t\leq C(\theta)\) is then equivalent to \(C(\hat{\theta})\leq C(\theta)\) and hence to \(\hat{\theta}\leq\theta\). It follows that \(\underline{\theta}=\hat{\theta}\), as was to be proved.

Under the same assumptions, the corresponding upper confidence bound with confidence coefficient \(1-\alpha\) is the solution \(\bar{\theta}\) of the equation \(P_{\theta}\{T\geq t\}=1-\alpha\) or equivalently of \(F_{\theta}(t)=\alpha\).

**Example 3.5.1**: **(Exponential waiting times)** To determine an upper bound for the degree of radioactivity \(\lambda\) of a radioactive substance, the substance is observed until a count of \(m\) has been obtained on a Geiger counter. Under the assumptions of Example 3.4.3, the joint probability density of the times \(T_{i}(i=1,\ldots,m)\) elapsing between the \((i-1)\)st count and the \(i\)th one is

\[p(t_{1},\ldots,t_{m})=\lambda^{m}e^{-\lambda\sum t_{i}},\ \ \ \ \ t_{1},\ldots,t_{m}\geq 0.\]

If \(T=\sum T_{i}\) denotes the total time of observation, then \(2\lambda T\) has a \(\chi^{2}\)-distribution with \(2m\) degrees of freedom, and, as was shown in Example 3.4.3, the acceptance region of the most powerful test of \(H(\lambda_{0}):\lambda=\lambda_{0}\) against \(\lambda<\lambda_{0}\) is \(2\lambda_{0}T\leq C\), where \(C\) is determined by the equation

\[\int_{0}^{C}\chi^{2}_{2m}=1-\alpha\.\]The set \(S(t_{1},\ldots,t_{m})\) defined by (3.25) is then the set of values \(\lambda\) such that \(\lambda\leq C/2T\), and it follows from Theorem 3.5.1 that \(\tilde{\lambda}=C/2T\) is a uniformly most accurate upper confidence bound for \(\lambda\). This result can also be obtained through Corollary 3.5.1.

If the variables \(X\) or \(T\) are discrete, Corollary 3.5.1 cannot be applied directly since the distribution functions \(F_{\theta}(t)\) are not continuous, and for most values of \(\theta_{0}\) the optimum test of \(H:\theta=\theta_{0}\) is randomized. However, any randomized test based on \(X\) has the following representation as a nonrandomized test depending on \(X\) and an independent variable \(U\) distributed uniformly over \((0,1)\). Given a critical function \(\phi\), consider the rejection region

\[R=\{(x,u):u\leq\phi(x)\}.\]

Then

\[P\{(X,U)\in R\}=P\{U\leq\phi(X)\}=E_{\phi}(X),\]

whatever the distribution of \(X\), so that \(R\) has the same power function as \(\phi\) and the two tests are equivalent. The pair of variables \((X,U)\) has a particularly simple representation when \(X\) is integer-valued. In this case the statistic

\[T=X+U\]

is equivalent to the pair \((X,U)\), since with probability 1

\[X=[T],\qquad U=T-[T],\]

where \([T]\) denotes the largest integer \(\leq T\). The distribution of \(T\) is continuous, and confidence bounds can be based on this statistic.

**Example 3.5.2**: **(Binomial)** An upper bound is required for a binomial probability \(p\)--for example, the probability that a batch of polio vaccine manufactured according to a certain procedure contains any live virus. Let \(X_{1},\ldots,X_{n}\) denote the outcome of \(n\) trials, \(X_{i}\) being 1 or 0 with probabilities \(p\) and \(q\) respectively, and let \(X=\sum X_{i}\). Then \(T=X+U\) has probability density

\[\binom{n}{[t]}p^{[t]}q^{n-[t]},\qquad 0\leq t<n+1.\]

This satisfies the conditions of Corollary 3.5.1, and the upper confidence bound \(\bar{p}\) is therefore the solution, if it exists, of the equation

\[P_{p}\{T<t\}=\alpha\, \tag{3.27}\]

where \(t\) is the observed value of \(T\). A solution does exist for all values \(\alpha\leq t\leq n+\alpha\). For \(n+\alpha<t\), the hypothesis \(H(p_{0}):p=p_{0}\) is accepted against the alternative \(p<p_{0}\) for all values of \(p_{0}\) and hence \(\bar{p}=1\). For \(t<\alpha\), \(H(p_{0})\) is rejected for all values of \(p_{0}\) and the confidence set \(S(t)\) is therefore empty. Consider instead the sets \(S^{*}(t)\) which are equal to \(S(t)\) for \(t\geq\alpha\) and which for \(t<\alpha\) consist of the single point \(p=0\). They are also confidence sets at level \(1-\alpha\), since for all \(p\),

\[P_{p}\{p\in S^{*}(T)\}\geq P_{p}\{p\in S(T)\}=1-\alpha.\]

On the other hand, \(P_{p}\{p^{\prime}\in S^{*}(T)\}=P_{p}\{p^{\prime}\in S(T)\}\) for all \(p^{\prime}>0\) and hence

\[P_{p}\{p^{\prime}\in S^{*}(T)\}=P_{p}\{p^{\prime}\in S(T)\}\quad\text{ for all }\quad p^{\prime}>p.\]

Thus the family of sets \(S^{*}(t)\) minimizes the probability of covering \(p^{\prime}\) for all \(p^{\prime}>p\) at confidence level \(1-\alpha\). The associated confidence bound \(\bar{p}^{*}(t)=\bar{p}(t)\) for \(t\geq\alpha\) and \(\bar{p}^{*}(t)=0\) for \(t<\alpha\) is therefore a uniformly most accurate upper confidence bound for \(p\) at level \(1-\alpha\).

In practice, so as to avoid randomization and obtain a bound not dependent on the extraneous variable \(U\), one usually replaces \(T\) by \(X+1=[T]+1\). Since \(\bar{p}^{*}(t)\) is a nondecreasing function of \(t\), the resulting upper confidence bound \(\bar{p}^{*}([t]+1)\) is then somewhat larger than necessary; as a compensation it also gives a correspondingly higher probability of not falling below the true \(p\).

Equivalently, rather than finding the solution to (3.27), first note that for \(x=[t]\),

\[P_{p}\{T<t\}\leq P_{p}\{T<x+1\}=P_{p}\{X\leq x\}\.\]

Therefore, a conservative solution is to find the value of \(p\), say \(\hat{p}_{U}\), satisfying

\[P_{p}\{X\leq x\}=\alpha\,\]

where \(x\) is the observed value of \(X\). When \(x=n\), there is no solution, but then \(\hat{p}_{U}=1\) serves as the upper confidence bound. Otherwise, the value \(\hat{p}_{U}\) of \(p\) for which

\[\sum_{j=0}^{x}{n\choose j}p^{j}(1-p)^{n-j}=\alpha\]

serves as an upper \(1-\alpha\) confidence bound for \(p\). For \(x=0\), the solution is \(\hat{p}_{U}=1-\alpha^{1/n}\). In fact, for \(0<x<n\), \(\hat{p}_{U}\) can be expressed as the \(1-\alpha\) quantile of the Beta distribution with parameters \(x+1\) and \(n-x\) (Problem 3.48). Similarly, a lower \(1-\alpha\) confidence bound is the value \(\hat{p}_{L}\) satisfying

\[\sum_{j=x}^{n}{n\choose j}p^{j}(1-p)^{n-j}=\alpha\.\]

When \(0<x<n\), \(\hat{p}_{L}\) can be expressed as the \(\alpha\) quantile of the Beta distribution with parameters \(x\) and \(n-x+1\). If \(x=0\), then we take \(\hat{p}_{U}=0\), and if \(x=n\)the solution is \(\alpha^{1/n}\). The interval \([\hat{p}_{L},\,\hat{p}_{U}]\) then serves as a level \(1-2\alpha\) confidence interval for \(p\).

The conservative solution \(\hat{p}_{U}\) dates back to Clopper and Pearson (1934). References to tables for the confidence bounds and a careful discussion of various approximations can be found in Hall (1982) and Blyth (1986). Two-sided intervals will be discussed in Example 5.5.4. Large-sample approaches will be discussed in Example 11.3.4.

More generally, when \(X\) or \(T\) is discrete, a reasonable approach to constructing a \(1-\alpha\) upper confidence bound for some parameter \(\theta\) that avoids randomization is the following (whether or not \(T\) is optimal in any sense). Let \(F_{\theta}(t)=P_{\theta}\{T\leq t\}\). Assume, for fixed \(t\), \(F_{\theta}(t)\) is continuous and strictly monotone decreasing in \(\theta\). Suppose \(\tilde{\theta}\) satisfies

\[F_{\tilde{\theta}}(t)=\alpha\, \tag{3.28}\]

where \(t\) is the observed value of \(T\).7 Then, \(\tilde{\theta}\) serves as a level \(1-\alpha\) upper confidence bound for \(\theta\). In Figure 3.2, \(F_{\theta}(t)\) is plotted as a function of \(\theta\) with \(T=t\) fixed. The solution to (3.28) is shown as \(\tilde{\theta}\). Alternatively, Figure 3.3 shows how the confidence bounds may be obtained from the inverse function \(F_{\theta}^{-1}(\alpha)\). (In Figure 3.3, the functions displayed are linear in \(\theta\), as would be the case in a location model, though this is not generally the case.)

Footnote 7: An explicit solution for the value of \(\theta\) satisfying (3.28) may be unavailable, and one may resort to approximate numerical approaches, such as by discretization of \(\theta\) or the “automatic percentile” method of DiCiccio and Romano (1989).

The confidence bounds can also be derived simply from "test inversion". Indeed, the test of \(H(\theta_{0})\) against \(\theta<\theta_{0}\) that rejects \(H(\theta_{0})\) for small values of \(T\) has (possibly conservative) \(p\)-value given by \(F_{\theta_{0}}(t)\); see Example 3.3. Therefore, any \(\theta_{0}\) for which \(F_{\theta_{0}}(t)>\alpha\) should be included in the confidence region, while any \(\theta_{0}\) for which \(F_{\theta_{0}}(t)\leq\alpha\) should not be included in the confidence region. Such an approach is consistent with the solution to (3.28) when such a solution exists, but it also applies if no solution exists (as can happen in the binomial example). Similarly, a level \(1-\alpha\) lower confidence band for \(\theta\), \(\underline{\theta}\) may be obtained as the solution to the equation \(F_{\theta}^{-}(t)=1-\alpha\); if no solution exists, the region consists of all \(\theta_{0}\) for which \(F_{\theta_{0}}^{-}(t)<1-\alpha\).

Let \(\underline{\theta}\) and \(\tilde{\theta}\) be lower and upper bounds for \(\theta\) with confidence coefficients \(1-\alpha_{1}\) and \(1-\alpha_{2}\) respectively and suppose that \(\underline{\theta}(x)<\tilde{\theta}(x)\) for all \(x\). This will be the case under the assumptions of Corollary 3.5.1 if \(\alpha_{1}+\alpha_{2}<1\). The intervals \((\underline{\theta},\tilde{\theta})\) are then _confidence intervals_ for \(\theta\) with confidence coefficient \(1-\alpha_{1}-\alpha_{2}\); that is, they contain the true parameter value with probability \(1-\alpha_{1}-\alpha_{2}\), since

\[P_{\theta}\{\underline{\theta}\leq\theta\leq\tilde{\theta}\}=1-\alpha_{1}- \alpha_{2}\quad\quad\text{for all }\theta.\]If \(\underline{\theta}\) and \(\bar{\theta}\) are uniformly most accurate, they minimize \(E_{\theta}L_{1}(\theta,\underline{\theta})\) and \(E_{\theta}L_{2}(\theta,\bar{\theta})\) at their respective levels for any function \(L_{1}\) that is nonincreasing in \(\underline{\theta}\) for \(\underline{\theta}<\theta\) and \(0\) for \(\underline{\theta}\geq\theta\) and any \(L_{2}\) that is nondecreasing in \(\bar{\theta}\) for \(\bar{\theta}>\theta\) and \(0\) for \(\bar{\theta}\leq\theta\). Letting

\[L(\theta;\underline{\theta},\bar{\theta})=L_{1}(\theta,\underline{\theta})+L_{ 2}(\theta,\bar{\theta}),\]

the intervals \((\underline{\theta},\bar{\theta})\) therefore minimize \(E_{\theta}L(\theta;\underline{\theta},\bar{\theta})\) subject to

\[P_{\theta}\{\theta>\theta\}\leq\alpha_{1},\qquad P_{\theta}\{\bar{\theta}< \theta\}\leq\alpha_{2}.\]

Figure 3.2: Confidence Bounds from \(F_{\theta}\)

An example of such a loss function is

\[L(\theta;\,\underline{\theta},\,\bar{\theta})=\left\{\begin{array}{l}\bar{ \theta}-\underline{\theta}\mbox{ if }\underline{\theta}\leq\theta\leq\bar{\theta},\\ \bar{\theta}-\bar{\theta}\mbox{ if }\bar{\theta}<\underline{\theta},\\ \theta-\underline{\theta}\mbox{ if }\bar{\theta}<\bar{\theta},\end{array}\right.\]

which provides a natural measure of the accuracy of the intervals. Other possible measures are the actual length \(\bar{\theta}-\underline{\theta}\) of the intervals, or, for example, \(a(\theta-\underline{\theta})^{2}+b(\bar{\theta}-\theta)^{2}\), which gives an indication of the distance of the two endpoints from the true value.8

Footnote 8: Proposed by Wolfowitz (1950).

An important limiting case corresponds to the levels \(\alpha_{1}=\alpha_{2}=\frac{1}{2}\). Under the assumptions of Corollary 3.5.1 and if the region of positive density is independent of \(\theta\) so that tests of power 1 are impossible when \(\alpha<1\), the upper and lower confidence bounds \(\bar{\theta}\) and \(\underline{\theta}\) coincide in this case. The common bound satisfies

\[P_{\theta}\{\underline{\theta}\leq\theta\}=P_{\theta}\{\underline{\theta}\geq \theta\}=\frac{1}{2},\]

and the estimate \(\underline{\theta}\) of \(\theta\) is therefore as likely to underestimate as to overestimate the true value. An estimate with this property is said to be _median unbiased_. (For the relation of this to other concepts of unbiasedness, see Problem 1.3.) It follows from the above result for arbitrary \(\alpha_{1}\) and \(\alpha_{2}\) that among all median unbiased estimates, \(\underline{\theta}\) minimizes \(EL(\theta,\,\underline{\theta})\) for any _monotone_ loss function, that is, any loss function which for fixed \(\theta\) has a minimum of 0 at \(\underline{\theta}=\theta\) and is nondecreasing as \(\underline{\theta}\) moves away from \(\theta\) in either direction. By taking in particular \(L(\theta,\,\underline{\theta})=0\) when \(|\theta-\underline{\theta}|\leq\triangle\) and \(=1\) otherwise, it is seen that among all median unbiased estimates, \(\underline{\theta}\) minimizes the probability of differing from \(\theta\) by more than any given amount; more generally it maximizes the probability

\[P_{\theta}\{-\triangle_{1}\leq\theta-\underline{\theta}<\triangle_{2}\}\]

for any \(\triangle_{1}\), \(\triangle_{2}\geq 0\).

A more detailed assessment of the position of \(\theta\) than that provided by confidence bounds or intervals corresponding to a fixed level \(\gamma=1-\alpha\) is obtained by stating confidence bounds for a number of levels, for example upper confidence bounds corresponding to values such as \(\gamma=0.05\), \(0.1\), \(0.25\), \(0.5\), \(0.75\), \(0.9\), \(0.95\). These constitute a set of _standard confidence bounds_,9 from which different specific intervals or bounds can be obtained in the obvious manner.

### A Generalization of the Fundamental Lemma

The following is a useful extension of Theorem 3.2.1 to the case of more than one side condition.

**Theorem 3.6.1**: _Let \(f_{1},\ldots,\)\(f_{m+1}\) be real-valued functions defined on a Euclidean space \(\mathcal{X}\) and integrable \(\mu\), and suppose that for given constants \(c_{1},\ldots,c_{m}\) there exists a critical function \(\phi\) satisfying_

\[\int\phi f_{i}\,d\mu=c_{i},\ \ \ \ \ i=1,\ldots,m. \tag{3.29}\]

_Let \(\mathcal{C}\) be the class of critical functions \(\phi\) for which (3.29) holds._

1. _Among all members of_ \(\mathcal{C}\) _there exists one that maximizes_ \[\int\phi f_{m+1}\,d\mu.\]
2. _A sufficient condition for a member of_ \(\mathcal{C}\) _to maximize_ \[\int\phi f_{m+1}\,d\mu\] _is the existence of constants_ \(k_{1},\ldots,k_{m}\) _such that_ \[\phi(x)=1\ \ \ \ \ {\rm when}\ \ \ f_{m+1}(x)>\ \ \ \sum_{i=1}^{m}k_{i}\,f_{i}(x),\] (3.30) \[\phi(x)=0\ \ \ \ \ {\rm when}\ \ \ f_{m+1}(x)<\ \ \ \sum_{i=1}^{m}k_{i}\,f_{i}(x).\]
3. _If a member of_ \(\mathcal{C}\) _satisfies (_3.30_) with_ \(k_{1},\ldots,k_{m}\geq 0\)_, then it maximizes_ \[\int\phi f_{m+1}\,d\mu\] _among all critical functions satisfying_ \[\int\phi f_{i}\,d\mu\leq c_{i},\ \ \ \ \ i=1,\ldots,m.\] (3.31)
4. _The set_ \(M\) _of points in_ \(m\)_-dimensional space whose coordinates are_ \[\left(\int\phi f_{1}\,d\mu,\ldots,\int\phi f_{m}\,d\mu\right)\]for some critical function \(\phi\) is convex and closed. If \((c_{1},\,\ldots,c_{m})\) is an inner point10 of \(M\), then there exist constants \(k_{1},\,\ldots,k_{m}\) and a test \(\phi\) satisfying (3.29) and (3.30), and a necessary condition for a member of \(\mathcal{C}\) to maximize_

Footnote 10: A discussion of the problem when this assumption is not satisfied is given by Dantzig and Wald (1951).

\[\int\phi\,f_{m+1}\,d\mu\]

_is that (3.30) holds a.e. \(\mu\)._

Here the term "inner point of \(M\)" in statement (iv) can be interpreted as meaning a point interior to \(M\) relative to \(m\)-space or relative to the smallest linear space (of dimension \(\leq m\)) containing \(M\). The theorem is correct with both interpretations but is stronger with respect to the latter, for which it will be proved.

We also note that exactly analogous results hold for the minimization of \(\int\phi\,f_{m+1}\,d\mu\).

Proof. (i): Let \(\{\phi_{n}\}\) be a sequence of functions in \(\mathcal{C}\) such that \(\int\phi_{n}\,f_{m+1}\,d\mu\) tends to \(\sup_{\phi}\int\phi\,f_{m+1}\,d\mu\). By the weak compactness theorem for critical functions (Theorem 3.4.2 of the Appendix), there exists a subsequence \(\{\phi_{n_{i}}\}\) and a critical function \(\phi\) such that

\[\int\phi_{n_{i}}\,f_{k}\,d\mu\to\quad\int\phi\,f_{k}\,d\mu\quad\text{ for }\;\;k=1,\,\cdots,\,m+1.\]

It follows that \(\phi\) is in \(\mathcal{C}\) and maximizes the integral with respect to \(f_{m+1}\,d\mu\) within \(\mathcal{C}\).

(ii) and (iii) are proved exactly as was part (ii) of Theorem 3.2.1.

(iv): That \(M\) is closed follows again from the weak compactness theorem, and its convexity is a consequence of the fact that if \(\phi_{1}\) and \(\phi_{2}\) are critical functions, so is \(\alpha\phi_{1}+(1-\alpha)\phi_{2}\) for any \(0\leq\alpha\leq 1\). If \(N\) (see Figure 3.4) is the totality of points in (\(m+1\))-dimensional space with coordinates

\[\left(\int\phi\,f_{1}\,d\mu,\,\ldots,\int\phi\,f_{m+1}\,d\mu\right),\]

where \(\phi\) ranges over the class of all critical functions, then \(N\) is convex and closed by the same argument. Denote the coordinates of a general point in \(M\) and \(N\) by \((u_{1},\,\ldots,\,u_{m})\) and \((u_{1},\,\ldots,\,u_{m+1})\) respectively. The points of \(N\), the first \(m\) coordinates of which are \(c_{1},\,\ldots,\,c_{m}\), form a closed interval \([c^{*},\,c^{**}]\).

Assume first that \(c^{*}<c^{**}\). Since \((c_{1},\,\ldots,\,c_{m},\,c^{**})\) is a boundary point of \(N\), there exists a hyperplane \(\prod\) through it such that every point on \(N\) lies below or on \(\prod\). Let the equation of \(\prod\) be

\[\sum_{i=1}^{m+1}k_{i}u_{i}=\quad\sum_{i=1}^{m}k_{i}c_{i}+k_{m+1}c^{**}.\]

Since \((c_{1},\,\ldots,\,c_{m})\) is an inner point of \(M\), the coefficient \(k_{m+1}\neq 0\). To see this, let \(c^{*}<c<c^{**}\), so that \((c_{1},\,\ldots c_{m},\,c)\) is an inner point of \(N\). Then there exists a sphere with this point as center lying entirely in \(N\) and hence below \(\prod\). It follows that the point \((c_{1},\,\ldots c_{m},\,c)\) does not lie on \(\prod\) and hence that \(k_{m+1}\neq 0\). We may therefore take \(k_{m+1}=-1\) and see that for any point of \(N\)

\[u_{m+1}-\sum_{i=1}^{m}k_{i}u_{i}\leq c_{m+1}^{**}-\sum_{i=1}^{m}k_{i}c_{i}.\]

That is, all critical functions \(\phi\) satisfy

\[\int\phi\left(f_{m+1}-\sum_{i=1}^{m}k_{i}\,f_{i}\right)\,d\mu\leq\int\phi^{**} \left(f_{m+1}-\sum_{i=1}^{m}k_{i}\,f_{i}\right)\,d\mu,\]

Figure 3.4: The sets \(M\) and \(N\)

where \(\phi^{{}^{\ast\ast}}\) is the test giving rise to the point \((c_{1},\,\ldots,\,c_{m},\,c^{{}^{\ast\ast}})\). Thus \(\phi^{{}^{\ast\ast}}\) is the critical function that maximizes the left-hand side of this inequality. Since the integral in question is maximized by putting \(\phi\) equal to 1 when the integrand is positive and equal to 0 when it is negative, \(\phi^{{}^{\ast\ast}}\) satisfies (3.30) a.e. \(\mu\).

If \(c^{\ast}=c^{{}^{\ast\ast}}\), let \((c^{\prime}_{1},\,\ldots,\,c^{\prime}_{m})\) be any point of \(M\) other than \((c_{1},\,\ldots,\,c_{m})\). We shall now show that there exists exactly one real number \(c^{\prime}\) such that \((c^{\prime}_{1},\,\ldots,\,c^{\prime}_{m},\,c^{\prime})\) is in \(N\). Suppose to the contrary that \((c^{\prime}_{1},\,\ldots,\,c^{\prime}_{m},\,c^{\prime})\)and \((c^{\prime}_{1},\,\ldots,\,c^{\prime}_{m},\,\bar{c}^{\prime})\) are both in \(N\), and consider any point \((c^{\prime\prime}_{1},\,\ldots,\,c^{\prime\prime}_{m},\,c^{\prime\prime})\) of \(N\) such that \((c_{1},\,\ldots,\,c_{m})\) is an interior point of the line segment joining \((c^{\prime}_{1},\,\ldots,\,c^{\prime}_{m})\) and \((c^{\prime\prime}_{1},\,\ldots,\,c^{\prime\prime}_{m})\). Such a point exists since \((c_{1},\,\ldots,\,c_{m})\) is an inner point of \(M\). Then the convex set spanned by the three points \((c^{\prime}_{1},\,\ldots,\,c^{\prime}_{m},\,\underline{c}^{\prime})\), \((c^{\prime}_{1},\,\ldots,\,c^{\prime}_{m},\,\bar{c}^{\prime})\), and \((c^{\prime\prime}_{1},\,\ldots,\,c^{\prime\prime}_{m},\,c^{\prime\prime})\) is contained in \(N\) and contains points \((c_{1},\,\ldots,\,c_{m},\,\underline{c})\) and \((c_{1},\,\ldots,\,c_{m},\,\bar{c})\) with \(\underline{c}<\bar{c}\), which is a contradiction. Since \(N\) is convex, contains the origin, and has at most one point on any vertical line \(u_{1}=c^{\prime}_{1},\,\ldots,\quad u_{m}=c^{\prime}_{m}\), it is contained in a hyperplane, which passes through the origin and is not parallel to the \(u_{m+1}\)-axis. It follows that

\[\int\phi f_{m+1}\,d\mu=\sum_{i=1}^{m}k_{i}\int\phi f_{i}\,d\mu\]

for all \(\phi\). This arises of course only in the trivial case that

\[f_{m+1}=\sum_{i=1}^{m}k_{i}\,f_{i}\quad\mbox{ a.e. $\mu$,}\]

and (3.30) is satisfied vacuously.

**Corollary 3.6.1**: _Let \(p_{1},\,\ldots,\,p_{m}\), \(p_{m+1}\) be probability densities with respect to a measure \(\mu\), and let \(0<\alpha<1\). Then there exists a test \(\phi\) such that \(E_{i}\phi(X)=\alpha\)\((i=1,\,\ldots,\,m)\) and \(E_{m+1}\phi(X)>\alpha\), unless \(p_{m+1}=\sum_{i=1}^{m}k_{i}\,p_{i}\), a.e. \(\mu\)._

Proof. The proof will be by induction over \(m\). For \(m=1\) the result reduces to Corollary 3.2.1. Assume now that it has been proved for any set of \(m\) distributions, and consider the case of \(m+1\) densities \(p_{1},\,\ldots,\,p_{m+1}\). If \(p_{1},\,\ldots,\,p_{m}\) are linearly dependent, the number of \(p_{i}\) can be reduced and the result follows from the induction hypothesis. Assume therefore that \(p_{1},\,\ldots,\,p_{m}\) are linearly independent. Then for each \(j=1,\,\ldots,\,m\) there exist by the induction hypothesis tests \(\phi_{j}\) and \(\phi^{\prime}_{j}\) such that \(E_{i}\phi_{j}(X)=E_{i}\phi^{\prime}_{j}(X)=\alpha\) for all \(i=1,\,\ldots,\,j-1,\,j+1,\,\ldots,\,m\) and \(E_{j}\phi_{j}(X)<\alpha<E_{j}\phi^{\prime}_{j}(X)\). It follows that the point of \(m\)-space for which all \(m\) coordinates are equal to \(\alpha\) is an inner point of \(M\), so that Theorem 3.6.1(iv) is applicable. The test \(\phi(x)\equiv\alpha\) is such that \(E_{i}\phi(X)=\alpha\) for \(i=1,\,\ldots,\,m\). If among all tests satisfying the side conditions this one is most powerful, it has to satisfy (3.30). Since \(0<\alpha<1\), this implies\[p_{m+1}=\sum_{i=1}^{m}k_{i}\,p_{i}\quad\quad\mbox{a.e.}\mu,\]

as was to be proved.

The most useful parts of Theorems 3.2.1 and 3.6.1 are the parts (ii), which give sufficient conditions for a critical function to maximize an integral subject to certain side conditions. These results can be derived very easily as follows by the method of undetermined multipliers.

**Lemma 3.6.1**: _Let \(F_{1},\ldots,\,F_{m+1}\) be real-valued functions defined over a space \(U\), and consider the problem of maximizing \(F_{m+1}(u)\) subject to \(F_{i}(u)=c_{i}\) (\(i=1,\ldots,m\)). A sufficient condition for a point \(u^{0}\) satisfying the side conditions to be a solution of the given problem is that among all points of \(U\) it maximizes_

\[F_{m+1}(u)-\sum_{i=1}^{m}k_{i}\,F_{i}(u)\]

_for some \(k_{1},\ldots,\,k_{m}\)._

When applying the lemma one usually carries out the maximization for arbitrary \(k\)'s, and then determines the constants so as to satisfy the side conditions.

Proof. If \(u\) is any point satisfying the side conditions, then

\[F_{m+1}(u)-\sum_{i=1}^{m}k_{i}\,F_{i}(u)\leq F_{m+1}(u^{0})-\sum_{i=1}^{m}k_{i }\,F_{i}(u^{0}),\]

and hence \(F_{m+1}(u)\leq F_{m+1}(u^{0})\).

As an application consider the problem treated in Theorem 3.6.1. Let \(U\) be the space of critical functions \(\phi\), and let \(F_{i}(\phi)=\int\phi f_{i}\,d\mu\). Then a sufficient condition for \(\phi\) to maximize \(F_{m+1}(\phi)\), subject to \(F_{i}(\phi)=c_{i}\), is that it maximizes \(F_{m+1}(\phi)-\sum k_{i}\,F_{i}(\phi)=\int(f_{m+1}-\sum k_{i}\,f_{i})\phi\,d\mu\). This is achieved by setting \(\phi(x)=1\) or \(0\) as \(f_{m+1}(x)>\) or \(<\sum k_{i}\,f_{i}(x)\).

### Two-Sided Hypotheses

UMP tests exist not only for one sided but also for certain two-sided hypotheses of the form

\[H:\theta\leq\theta_{1}\mbox{ or }\theta\geq\theta_{2}\quad\quad(\theta_{1}< \theta_{2}). \tag{3.32}\]

This problem arises when trying to demonstrate equivalence (or sometimes called bioequivalence) of treatments; for example, a new drug may be declared equivalent to the current standard drug if the difference in therapeutic effect is small, meaning \(\theta\) lies in a small interval about \(0\). Such testing problems also occur when one wishes to determine whether given specifications have been met concerning the proportion of an ingredient in a drug or some other compound, or whether a measuring instrument, for example a scale, is properly balanced. One then sets up the hypothesis that \(\theta\) does not lie within the required limits, so that an error of the first kind consists in declaring \(\theta\) to be satisfactory when in fact it is not. In practice, the decision to accept \(H\) will typically be accompanied by a statement of whether \(\theta\) is believed to be \(\leq\theta_{1}\) or \(\geq\theta_{2}\). The implications of \(H\) are, however, frequently sufficiently important so that acceptance will in any case be followed by a more detailed investigation. If a manufacturer tests each precision instrument before releasing it and the test indicates an instrument to be out of balance, further work will be done to get it properly adjusted. If in a scientific investigation the inequalities \(\theta\leq\theta_{1}\) and \(\theta\geq\theta_{2}\) contradict some assumptions that have been formulated, a more complex theory may be needed and further experimentation will be required. In such situations there may be only two basic choices, to act as if \(\theta_{1}<\theta<\theta_{2}\) or to carry out some further investigation, and the formulation of the problem as that of testing the hypothesis \(H\) may be appropriate. In the present section, the existence of a UMP test of \(H\) will be proved for one-parameter exponential families.

**Theorem 3.7.1**: (i) _For testing the hypothesis \(H:\theta\leq\theta_{1}\) or \(\theta\geq\theta_{2}\) (\(\theta_{1}<\theta_{2}\)) against the alternatives \(K:\theta_{1}<\theta<\theta_{2}\) in the one-parameter exponential family (3.19) there exists a UMP test given by_

\[\phi(x)=\left\{\begin{array}{l}1\mbox{ when }C_{1}<T(x)<C_{2}\;\;\;(C_{1}<C _{2}),\\ \gamma_{i}\mbox{ when }T(x)=C_{i},\;\;\;i=1,2,\\ 0\mbox{ when }T(x)<C_{1}\mbox{ or }>C_{2},\end{array}\right. \tag{3.33}\]

_where the \(C\)'s and \(\gamma\)'s are determined by_

\[E_{\theta_{1}}\phi(X)=E_{\theta_{2}}\phi(X)=\alpha. \tag{3.34}\]

* _This test minimizes_ \(E_{\theta}\phi(X)\) _subject to (_3.34_) for all_ \(\theta<\theta_{1}\) _and_ \(>\theta_{2}\)_._
* _For_ \(0<\alpha<1\) _the power function of this test has a maximum at a point_ \(\theta_{0}\) _between_ \(\theta_{1}\) _and_ \(\theta_{2}\) _and decreases strictly as_ \(\theta\) _tends away from_ \(\theta_{0}\) _in either direction, unless there exist two values_ \(t_{1},t_{2}\) _such that_ \(P_{\theta}\{T(X)=t_{1}\}+P_{\theta}\{T(X)=t_{2}\}=1\) _for all_ \(\theta\)_._

Proof. (i): One can restrict attention to the sufficient statistic \(T=T(X)\), the distribution of which by Lemma 2.7.2 is

\[d\,P_{\theta}(t)=C(\theta)e^{Q(\theta)t}d\nu(t),\]

where \(Q(\theta)\) is assumed to be strictly increasing. Let \(\theta_{1}<\theta^{\prime}<\theta_{2}\), and consider first the problem of maximizing \(E_{\theta^{\prime}}\psi(T)\) subject to (3.34) with \(\phi(x)=\psi[T(x)]\). If \(M\) denotes the set of all points \(E_{\theta_{1}}\psi(T)\), \(E_{\theta_{2}}\psi(T))\) as \(\psi\) ranges over the totality of critical 

[MISSING_PAGE_FAIL:108]

**Lemma 3.7.1**: _Let \(p_{\theta}(x)\) satisfy the assumptions of Lemma 3.4.2(iv)._

(i) _If \(\phi\) and \(\phi^{*}\) are two tests satisfying (3.33) and \(E_{\theta_{1}}\phi(T)=E_{\theta_{1}}\phi^{*}(T)\), and if \(\phi^{*}\) is to the right of \(\phi\), then \(\beta(\theta)<\) or \(>\beta^{*}(\theta)\) as \(\theta>\theta_{1}\) or \(<\theta_{1}\)._

(ii) _If \(\phi\) and \(\phi^{*}\) satisfy (3.33) and (3.34), then \(\phi=\phi^{*}\) with probability one._

Proof. (i): The result follows from Lemma 3.4.2(iv) with \(\psi=\phi^{*}-\phi\). (ii): Since \(E_{\theta_{1}}\phi(T)=E_{\theta_{1}}\phi^{*}(T)\), \(\phi^{*}\) lies either to the left or the right of \(\phi\), and application of (i) completes the proof.

Although a UMP test exists for testing that \(\theta\leq\theta_{1}\) or \(\geq\theta_{2}\) in an exponential family, the same is not true for the dual hypothesis \(H:\theta_{1}\leq\theta\leq\theta_{2}\) or for testing \(\theta=\theta_{0}\) (Problem 3.59). There do, however, exist UMP unbiased tests of these hypotheses, as will be shown in Chapter 4.

### Least Favorable Distributions

It is a consequence of Theorem 3.2.1 that there always exists a most powerful test for testing a simple hypothesis against a simple alternative. More generally, consider the case of a Euclidean sample space; probability densities \(f_{\theta},\theta\in\omega\), and \(g\) with respect to a measure \(\mu\); and the problem of testing \(H:f_{\theta},\theta\in\omega\), against the simple alternative \(K:g\). The existence of a most powerful level \(\alpha\) test then follows from the weak compactness theorem for critical functions (Theorem A.5.1 of the Appendix) as in Theorem 3.6.1(i).

Theorem 3.2.1 also provides an explicit construction for the most powerful test in the case of a simple hypothesis. We shall now extend this theorem to composite hypotheses in the direction of Theorem 3.6.1 by the method of undetermined multipliers. However, in the process of extension the result becomes much less explicit. Essentially it leaves open the determination of the multipliers, which now take the form of an arbitrary distribution. In specific problems this usually still involves considerable difficulty. From another point of view the method of attack, as throughout the theory of hypothesis testing, is to reduce the composite hypothesis to a simple one. This is achieved by considering weighted averages of the distributions of \(H\). The composite hypothesis \(H\) is replaced by the simple hypothesis \(H_{\Lambda}\) that the probability density of \(X\) is given by

\[h_{\Lambda}(x)=\int_{\omega}f_{\theta}(x)\,d\,\Lambda(\theta),\]

where \(\Lambda\) is a probability distribution over \(\omega\). The problem of finding a suitable \(\Lambda\) is frequently made easier by the following consideration. Since \(H\) provides no information concerning \(\theta\) and since \(H_{\Lambda}\) is to be equivalent to \(H\) for the purpose of testing against \(g\), knowledge of the distribution \(\Lambda\) should provide as little help for this task as possible. To make this precise suppose that \(\theta\) is known to have a distribution \(\Lambda\). Then the maximum power \(\beta_{\Lambda}\) that can be attained against \(g\) is that ofthe most powerful test \(\phi_{\Lambda}\) for testing \(H_{\Lambda}\) against \(g\). The distribution \(\Lambda\) is said to be _least favorable_ (at level-\(\alpha\)) if for all \(\Lambda^{\prime}\) the inequality \(\beta_{\Lambda}\leq\beta_{\Lambda^{\prime}}\) holds.

**Theorem 3.8.1**: _Let a \(\sigma\)-field be defined over \(\omega\) such that the densities \(f_{\theta}(x)\) are jointly measurable in \(\theta\) and \(x\). Suppose that over this \(\sigma\)-field there exists a probability distribution \(\Lambda\) such that the most powerful level-\(\alpha\) test \(\phi_{\Lambda}\) for testing \(H_{\Lambda}\) against \(g\) is of size \(\leq\alpha\) also with respect to the original hypothesis \(H\)._

(i) _The test \(\phi_{\Lambda}\) is most powerful for testing \(H\) against \(g\)._

(ii) _If \(\phi_{\Lambda}\) is the unique most powerful level \(\alpha\) for testing \(H_{\Lambda}\) against \(g\), it is also the unique most powerful test of \(H\) against \(g\)._

(iii) _The distribution \(\Lambda\) is least favorable._

Proof. We note first that \(h_{\Lambda}\) is again a density with respect to \(\mu\), since by Fubini's theorem (Theorem 2.2.4)

\[\int h_{\Lambda}(x)\,d\mu(x)=\int_{\omega}d\,\Lambda(\theta)\int f_{\theta}(x) \,d\mu(x)=\int_{\omega}d\,\Lambda(\theta)=1.\]

Suppose that \(\phi_{\Lambda}\) is a level-\(\alpha\) test for testing \(H\), and let \(\phi^{*}\) be any other level-\(\alpha\) test. Then since \(E_{\theta}\phi^{*}(X)\leq\alpha\) for all \(\theta\in\omega\), we have

\[\int\phi^{*}(x)h_{\Lambda}(x)\,d\mu(x)=\int_{\omega}E_{\theta}\phi^{*}(X)d\, \Lambda(\theta)\leq\alpha.\]

Therefore \(\phi*\) is a level-\(\alpha\) test also for testing \(H_{\Lambda}\) and its power cannot exceed that of \(\phi_{\Lambda}\). This proves (i) and (ii). If \(\Lambda^{\prime}\) is any distribution, it follows further that \(\phi_{\Lambda}\) is a level-\(\alpha\) test also for testing \(H_{\Lambda^{\prime}}\), and hence that its power against \(g\) cannot exceed that of the most powerful test, which by definition is \(\beta_{\Lambda^{\prime}}\).

The conditions of this theorem can be given a somewhat different form by noting that \(\phi_{\Lambda}\) can satisfy \(\int_{\omega}E_{\theta}\phi_{\Lambda}(X)\,d\,\Lambda(\theta)=\alpha\) and \(E_{\theta}\phi_{\Lambda}(X)\leq\alpha\) for all \(\theta\in\omega\) only if the set of \(\theta^{\prime}\)'s with \(E_{\theta}\phi_{\Lambda}(X)=\alpha\) has \(\Lambda\)-measure one.

**Corollary 3.8.1**: _Suppose that \(\Lambda\) is a probability distribution over \(\omega\) and that \(\omega^{\prime}\) is a subset of \(\omega\) with \(\Lambda(\omega^{\prime})=1\). Let \(\phi_{\Lambda}\) be a test such that_

\[\phi_{\Lambda}(x)=\left\{\begin{array}{ll}1&\mbox{if}\;\;g(x)>k\int f_{ \theta}(x)\,d\,\Lambda(\theta),\\ 0&\mbox{if}\;\;g(x)<k\int f_{\theta}(x)\,d\,\Lambda(\theta).\end{array}\right. \tag{3.35}\]

_Then \(\phi_{\Lambda}\) is a most powerful level-\(\alpha\) for testing \(H\) against \(g\) provided_

\[E_{\theta^{\prime}}\phi_{\Lambda}(X)=\sup_{\theta\in\omega}E_{\theta}\phi_{ \Lambda}(X)=\alpha\quad\mbox{ for }\;\;\theta^{\prime}\in\omega^{\prime}. \tag{3.36}\]

Theorems 3.4.1 and 3.7.1 constitute two simple applications of Theorem 3.8.1. The set \(\omega^{\prime}\) over which the least favorable distribution \(\Lambda\) is concentrated consists of the single point \(\theta_{0}\) in the first of these examples and of the two points \(\theta_{1}\) and \(\theta_{2}\) in the second. This is what one might expect, since in both cases these are the distributionsof \(H\) that appear to be "closest" to \(K\). Another example in which the least favorable distribution is concentrated at a single point is the following.

**Example 3.8.1**: **(Sign test)** The quality of items produced by a manufacturing process is measured by a characteristic \(X\) such as the tensile strength of a piece of material, or the length of life or brightness of a light bulb. For an item to be satisfactory \(X\) must exceed a given constant \(u\), and one wishes to test the hypothesis \(H:p\geq p_{0}\), where

\[p=P\{X\leq u\}\]

is the probability of an item being defective. Let \(X_{1}\),..., \(X_{n}\) be the measurements of \(n\) sample items, so that the \(X\)'s are independently distributed with common distribution about which no knowledge is assumed. Any distribution on the real line can be characterized by the probability \(p\) together with the conditional probability distributions \(P_{-}\) and \(P_{+}\) of \(X\) given \(X\leq u\) and \(X>u\) respectively. If the distributions \(P_{-}\) and \(P_{+}\) have probability densities \(p_{-}\) and \(p+\), for example with respect to \(\mu=P_{-}+P_{+}\), then the joint density of \(X_{1}\),..., \(X_{n}\) at a sample point \(x_{1}\),..., \(x_{n}\) satisfying

\[x_{i_{1}},\ldots,x_{i_{m}}\leq u<x_{j_{1}},\ldots,x_{j_{n-m}}\]

is

\[p^{m}(1-p)^{n-m}\,p_{-}(x_{i_{1}})\cdots p_{-}(x_{i_{m}})p_{+}(x_{j_{1}}) \cdots p_{+}(x_{j_{n-m}}).\]

Consider now a fixed alternative to \(H\), say (\(p_{1}\), \(P_{-}\), \(P_{+}\)), with \(p_{1}<p_{0}\). One would then expect the least favorable distribution \(\Lambda\) over \(H\) to assign probability 1 to the distribution (\(p_{0}\), \(P_{-}\), \(P_{+}\)) since this appears to be closest to the selected alternative. With this choice of \(\Lambda\), the test (3.35) becomes

\[\phi_{\Lambda}(x)=1\mbox{ or }0\qquad\mbox{as}\quad\left(\frac{p_{1}}{p_{0}} \right)^{m}\left(\frac{q_{1}}{q_{0}}\right)^{n-m}>\mbox{ or }<C,\]

and hence as \(m<\mbox{ or }>C\). The test therefore rejects when the number \(M\) of defectives is sufficiently small, or more precisely, when \(M<C\) and with probability \(\gamma\) when \(M=C\), where

\[P\{M<C\}+\gamma P\{M=C\}=\alpha\qquad\mbox{for}\quad p=p_{0}. \tag{3.37}\]

The distribution of \(M\) is the binomial distribution \(b(p,n)\), and does not depend on \(P_{+}\) and \(P_{-}\). As a consequence, the power function of the test depends only on \(p\) and is a decreasing function of \(p\), so that under \(H\) it takes on its maximum for \(p=p_{0}\). This proves \(\Lambda\) to be least favorable and \(\phi_{\Lambda}\) to be most powerful. Since the test is independent of the particular alternative chosen, it is UMP.

Expressed in terms of the variables \(Z_{i}=X_{i}-u\), the test statistic \(M\) is the number of variables \(\leq 0\), and the test is the so-called _sign test_ (cf. Section 4.9). It is an example of a _nonparametric_ test, since it is derived without assuming a given functional form for the distribution of the \(X\)'s such as the normal, uniform, or Poisson, in which only certain parameters are unknown.

The above argument applies, with only the obvious modifications, to the case that an item is satisfactory if \(X\) lies within certain limits: \(u<X<v\). This occurs, for example, if \(X\) is the length of a metal part or the proportion of an ingredient in a chemical compound, for which certain tolerances have been specified. More generally the argument applies also to the situation in which \(X\) is vector-valued. Suppose that an item is satisfactory only when \(X\) lies in a certain set \(S\), for example, if all the dimensions of a metal part or the proportions of several ingredients lie within specified limits. The probability of a defective is then

\[p=P\{X\in S^{c}\},\]

and \(P_{-}\) and \(P_{+}\) denote the conditional distributions of \(X\) given \(X\in S\) and \(X\in S^{c}\) respectively. As before, there exists a UMP test of \(H:p\geq p_{0}\), and it rejects \(H\) when the number \(M\) of defectives is sufficiently small, with the boundary of the test being determined by (3.37).

A distribution \(\Lambda\) satisfying the conditions of Theorem 3.8.1 exists in most of the usual statistical problems, and in particular under the following assumptions. Let the sample space be Euclidean, let \(\omega\) be a closed Borel set in \(s\)-dimensional Euclidean space, and suppose that \(f_{\theta}(x)\) is a continuous function of \(\theta\) for almost all \(x\). Then given any \(g\) there exists a distribution \(\Lambda\) satisfying the conditions of Theorem 3.8.1 provided

\[\lim_{n\to\infty}\int_{S}f_{\theta_{n}}(x)\,d\mu(x)=0\]

for every bounded set \(S\) in the sample space and for every sequence of vectors \(\theta_{n}\) whose distance from the origin tends to infinity.

From this it follows as did Corollaries 1 and 4 from Theorems 3.2.1 and 3.6.1, that if the above conditions hold and if \(0<\alpha<1\), there exists a test of power \(\beta>\alpha\) for testing \(H:f_{\theta}\), \(\theta\in\omega\), against \(g\) unless \(g=\int f_{\theta}\,d\,\Lambda(\theta)\) for some \(\Lambda\). An example of the latter possibility is obtained by letting \(f_{\theta}\) and \(g\) be the normal densities \(N(\theta,\sigma_{0}^{2})\) and \(N(0,\sigma_{1}^{2})\) respectively with \(\sigma_{0}^{2}<\sigma_{1}^{2}\). (See the following section.)

The above and related results concerning the existence and structure of least favorable distributions are given in Lehmann (1952b) (with the requirement that \(\omega\) be closed mistakenly omitted), in Reinhardt (1961), and in Krafft and Witting (1967), where the relation to linear programming is explored.

### 3.9 Applications to Normal Distributions

#### Univariate Normal Models

Because of their wide applicability, the problems of testing the mean \(\xi\) and variance \(\sigma^{2}\) of a normal distribution are of particular importance. Here and in similar problems later, the parameter not being tested is assumed to be unknown, but will not be shown explicitly in a statement of the hypothesis. We shall write, for example, \(\sigma\leq\sigma_{0}\) instead of the more complete statement \(\sigma\leq\sigma_{0}\), \(-\infty<\xi<\infty\). The standard (likelihood ratio) tests of the two hypotheses \(\sigma\leq\sigma_{0}\) and \(\xi\leq\xi_{0}\) are given by the rejection regions

\[\sum(x_{i}-\bar{x})^{2}\geq C \tag{3.38}\]

and

\[\frac{\sqrt{n}(\bar{x}-\xi_{0})}{\sqrt{\frac{1}{n-1}\sum(x_{i}-\bar{x})^{2}}} \geq C. \tag{3.39}\]

The corresponding tests for the hypotheses \(\sigma\geq\sigma_{0}\) and \(\xi\geq\xi_{o}\) are obtained from the rejection regions (3.38) and (3.39) by reversing the inequalities. As will be shown in later chapters, these four tests are UMP both within the class of unbiased and within the class of invariant test (but see Section 13.2 for problems arising when the assumption of normality does not hold exactly). However, at the usual significance levels only the first of them is actually UMP.

**Example 3.9.1**: **(One-sided tests of variance.)** Let \(X_{1},\ldots,X_{n}\) be a sample from \(N(\xi,\sigma^{2})\), and consider first the hypotheses \(H_{1}:\sigma\geq\sigma_{0}\) and \(H_{2}:\sigma\leq\sigma_{0}\), and a simple alternative \(K:\xi=\xi_{1}\), \(\sigma=\sigma_{1}\). It seems reasonable to suppose that the least favorable distribution \(\Lambda\) in the \((\xi,\sigma)\)-plane is concentrated on the line \(\sigma=\sigma_{0}\). Since \(Y=\sum X_{i}/n=\bar{X}\) and \(U=\sum(X_{i}-\bar{X})^{2}\) are sufficient statistics for the parameters \((\xi,\sigma)\), attention can be restricted to these variables. Their joint density under \(H_{\Lambda}\) is

\[C_{o}u^{(n-3)/2}\exp\left(-\frac{u}{2\sigma_{0}^{2}}\right)\int\exp\left[- \frac{n}{2\sigma_{o}^{2}}\left(y-\xi\right)^{2}\right]\,d\Lambda\left(\xi\right),\]

while under \(K\) it is

\[C_{1}u^{(n-3)/2}\exp\left(-\frac{u}{2\sigma_{1}^{2}}\right)\exp\left[-\frac{n }{2\sigma_{1}^{2}}\left(y-\xi_{1}\right)^{2}\right].\]

The choice of \(\Lambda\) is seen to affect only the distribution of \(Y\). A least favorable \(\Lambda\) should therefore have the property that the density of \(Y\) under \(H_{\Lambda}\),

[MISSING_PAGE_EMPTY:1716]

with that of the distribution, that is, when \(\xi=\xi_{1}\). (This follows for example from Problem 7.15.) The probability then becomes

\[P\left\{\sum\left(\frac{x_{i}-\xi_{1}}{\sigma}\right)^{2}\leq\frac{C}{\sigma^{2}} \,\Big{|}\,\xi_{1},\sigma\right\}=P\left\{\sum V_{i}^{2}\leq\frac{C}{\sigma^{2 }}\right\},\]

where \(V_{1},\ldots,V_{n}\) are independently distributed as \(N(0,1)\). This is a decreasing function of \(\sigma\) and therefore takes on its maximum when \(\sigma=\sigma_{0}\).

In the case of \(H_{2}\), application of Corollary 3.8.1 to the sufficient statistics (\(Y\), \(U\)) gives \(\phi(y,u)=1\) when

\[\frac{C_{1}u^{(n-3)/2}\exp\left(-\frac{u}{2\sigma_{1}^{2}}\right) \exp\left[-\frac{n}{2\sigma_{1}^{2}}(y-\xi_{1})^{2}\right]}{C_{0}u^{(n-3)/2} \exp\left(-\frac{u}{2\sigma_{0}^{2}}\right)\int\exp\left[-\frac{n}{2\sigma_{0 }^{2}}(y-\xi)^{2}\right]\Lambda^{\prime}(\xi)\,d\xi}\] \[\qquad=C^{\prime}\exp\left[-\frac{u}{2}\left(\frac{1}{\sigma_{1}^ {2}}-\frac{1}{\sigma_{0}^{2}}\right)\right]\geq C,\]

that is, when

\[u=\sum(x_{i}-\bar{x})^{2}\geq C. \tag{3.41}\]

Since the distribution of \(\sum(X_{i}-\bar{X})^{2}/\sigma^{2}\) does not depend on \(\xi\) or \(\sigma\), the probability \(P\{\sum(X_{i}-\bar{X})^{2}\geq C\mid\xi,\sigma\}\) is independent of \(\xi\) and increases with \(\sigma\) so that the conditions of Corollary 3.8.1 are satisfied. The test (3.41), being independent of \(\xi_{1}\) and \(\sigma_{1}\), is UMP for testing \(\sigma\leq\sigma_{0}\) against \(\sigma>\sigma_{0}\). It is also seen to coincide with the likelihood ratio test (3.38). On the other hand, the most powerful test (3.40) for testing \(\sigma\geq\sigma_{0}\) against \(\sigma<\sigma_{0}\) does depend on the value \(\xi_{1}\) of \(\xi\) under the alternative.

It has been tacitly assumed so far that \(n>1\). If \(n=1\), the argument applies without change with respect to \(H_{1}\), leading to (3.40) with \(n=1\). However, in the discussion of \(H_{2}\) the statistic \(U\) now drops out, and \(Y\) coincides with the single observation \(X\). Using the same \(\Lambda\) as before, one sees that \(X\) has the same distribution under \(H_{\Lambda}\) as under \(K\), and the test \(\phi_{\Lambda}\) therefore becomes \(\phi_{\Lambda}(x)\equiv\alpha\). This satisfies the conditions of Corollary 3.8.1 and is therefore the most powerful test for the given problem. It follows that a single observation is of no value for testing the hypothesis \(H_{2}\), as seems intuitively obvious, but that it could be used to test \(H_{1}\) if the class of alternatives were sufficiently restricted.

The corresponding derivation for the hypothesis \(\xi\leq\xi_{0}\) is less straightforward. It turns out11 that Student's test given by (3.39) is most powerful if the level of significance \(\alpha\) is \(\geq\frac{1}{2}\), regardless of the alternative \(\xi_{1}>\xi_{0}\), \(\sigma_{1}\). This test is therefore UMP for \(\alpha\geq\frac{1}{2}\). On the other hand, when \(\alpha<\frac{1}{2}\) the most powerful test of \(H\) rejects when \(\sum(x_{i}-a)^{2}\leq b\), where the constants \(a\) and \(b\) depend on the alternative (\(\xi_{1}\), \(\sigma_{1}\)) and on \(\alpha\). Thus for the significance levels that are of interest, a UMP test of \(H\) does not exist. No new problem arises for the hypothesis \(\xi\geq\xi_{0}\), since this reduces to the case just considered through the transformation \(Y_{i}=\xi_{0}-(X_{i}-\xi_{0})\).

#### Multivariate Normal Models

Let \(X\) denote a \(k\times 1\) random vector whose \(i\)th component, \(X_{i}\), is a real-valued random variable. The mean of \(X\), denoted \(E(X)\), is a vector with \(i\)th component \(E(X_{i})\) (assuming it exists). The covariance matrix of \(X\), denoted \(\Sigma\), is the \(k\times k\) matrix with \((i,\,j)\) entry \(Cov(X_{i},\,X_{j})\). \(\Sigma\) is well-defined iff \(E(|X|^{2})<\infty\), where \(|\cdot|\) denotes the Euclidean norm. Note that if \(A\) is an \(m\times k\) matrix, then the \(m\times 1\) vector \(Y=AX\) has mean (vector) \(AE(X)\) and covariance matrix \(A\Sigma\,A^{\top}\), where \(A^{\top}\) is the transpose of \(A\) (Problem 3.68).

The multivariate generalization of a real-valued normally distributed random variable is a random vector \(X=(X_{1},\,\ldots,\,X_{k})^{\top}\) with the _multivariate normal probability density_

\[\frac{\sqrt{|A|}}{(2\pi)^{\frac{1}{2}k}}\exp\left[-\tfrac{1}{2}\sum\sum a_{ij} (x_{i}-\xi_{i})(x_{j}-\xi_{j})\right], \tag{3.42}\]

where the matrix \(A=(a_{ij})\) is positive definite, and \(|A|\) denotes its determinant. The means and covariance matrix of the \(X\)'s are given by

\[E(X_{i})=\xi_{i},\qquad E(X_{i}-\xi_{i})(X_{j}-\xi_{j})=\sigma_{ij},\quad( \sigma_{ij})=A^{-1}. \tag{3.43}\]

The column vector \(\xi=(\xi_{1},\,\ldots,\,\xi_{k})^{\top}\) is the mean vector and \(\Sigma=A^{-1}\) is the covariance matrix of \(X\).

Such a definition only applies when \(A\) is nonsingular, in which case we say that \(X\) has a nonsingular multivariate normal distribution. More generally, we say that \(Y\) has a multivariate normal distribution if \(Y=BX+\mu\) for some \(m\times k\) matrix of constants \(B\) and \(m\times 1\) constant vector \(\mu\), where \(X\) has some nonsingular multivariate normal distribution. Then, \(Y\) is multivariate normal if and only if \(\sum_{i=1}^{m}c_{i}Y_{i}\) is univariate normal for all \(c\), where \(N(\xi,\,\sigma^{2})\) with \(\sigma=0\) is interpreted to be the distribution that is point mass at \(\xi\). Basic properties of the multivariate normal distribution are given in Anderson (2003).

**Example 3.9.2**: **(One-sided tests of a combination of means.)** _Assume \(X\) is multivariate normal with unknown mean \(\xi=(\xi_{1},\,\ldots,\,\xi_{k})^{\top}\) and known covariance matrix \(\Sigma\). Assume \(a=(a_{1},\,\ldots,\,a_{k})^{\top}\) is a fixed vector with \(a^{\top}\Sigma a>0\). The problem is to test_

\[H:\ \sum_{i=1}^{k}a_{i}\xi_{i}\leq\delta\quad\text{vs.}\quad K:\ \sum_{i=1}^{k}a _{k}\xi_{i}>\delta\.\]We will show that a UMP level \(\alpha\) test exists, which rejects when \(\sum_{i}a_{i}X_{i}>\sigma z_{1-\alpha}\), where \(\sigma^{2}=a^{\top}\Sigma a\). To see why,12 we will consider four cases of increasing generality.

Footnote 12: Proposition 15.2 of van der Vaart (1998) provides an alternative proof in the case \(\Sigma\) is invertible.

_Case 1._ If \(k=1\) and the problem is to test the mean of \(X_{1}\), the result follows by Problem 3.1.

_Case 2._ Consider now general \(k\), so that \((X_{1},\ldots,X_{k})\) has mean \((\xi_{1},\ldots,\xi_{k})\) and covariance matrix \(\Sigma\). However, consider the special case \((a_{1},\ldots,a_{k})=(1,0,\ldots,0)\). Also, assume \(X_{1}\) and \((X_{2},\ldots,X_{k})\) are independent. Then, for any fixed alternative \((\xi_{1}^{\prime},\ldots,\xi_{k}^{\prime})\) with \(\xi_{1}^{\prime}>\delta\), the least favorable distribution concentrates on the single point \((\delta,\xi_{2}^{\prime},\ldots,\xi_{k}^{\prime})\) (Problem 3.70).

_Case 3._ As in case 2, consider \(a_{1}=1\) and \(a_{i}=0\) if \(i>1\), but now allow \(\Sigma\) to be an arbitrary covariance matrix. We can reduce the problem to case 2 by an appropriate linear transformation. Simply let \(Y_{1}=X_{1}\) and, for \(i>1\), let

\[Y_{i}=X_{i}-\frac{Cov(X_{1},\,X_{i})}{Var(X_{1})}X_{1}\.\]

Then, it is easily checked that \(Cov(Y_{1},\,Y_{i})=0\) if \(i>1\). Moreover, \(Y\) is just a 1:1 transformation of \(X\). But, the problem of testing \(E(Y_{1})=E(X_{1})\) based on \(Y=(Y_{1},\ldots,Y_{k})\) is in the form already studied in case 2, and the UMP test rejects for large values of \(Y_{1}=X_{1}\).

_Case 4._ Now, consider arbitrary \((a_{1},\ldots,a_{k})\) satisfying \(a^{\top}\Sigma a>0\). Let \(Z=OX\), where \(O\) is any orthogonal matrix with first row \((a_{1},\ldots,a_{k})\). Then, \(E(Z_{1})=\sum_{i=1}^{k}a_{i}\xi_{i}\), and the problem of testing \(E(Z_{1})\leq\delta\) versus \(E(Z_{1})>\delta\) reduces to case 3. Hence, the UMP test rejects for large values of \(Z_{1}=\sum_{i=1}^{k}a_{i}X_{i}\).

**Example 3.9.3** (Equivalence tests of a combination of means.): As in Example 3.9.2, assume \(X\) is multivariate normal \(N(\xi,\,\Sigma)\) with unknown mean vector \(\xi\) and known covariance matrix \(\Sigma\). Fix \(\delta>0\) and any vector \(a=(a_{1},\ldots,a_{k})^{\top}\) satisfying \(a^{\top}\Sigma a>0\). Consider testing

\[H:\ |\sum_{i=1}^{k}a_{i}\xi_{i}|\geq\delta\ \ \ \ \ vs\ \ \ \ K:\ |\sum_{i=1}^{k}a_{i}\xi_{i}|<\delta\ \.\]

Then, a UMP level \(\alpha\) test also exists and it rejects \(H\) if

\[|\sum_{i=1}^{k}a_{i}X_{i}|\ <C\,\]

where \(C=C(\alpha,\,\delta,\sigma)\) satisfies\[\Phi\left(\frac{C-\delta}{\sigma}\right)-\Phi\left(\frac{-C-\delta}{\sigma}\right)=\alpha \tag{3.44}\]

and \(\sigma^{2}=a^{\top}\Sigma a\). Hence, the power of this test against an alternative \((\xi_{1},\ldots,\xi_{k})\) with \(|\sum_{i}a_{i}\xi_{i}|=\delta^{\prime}<\delta\) is

\[\Phi\left(\frac{C-\delta^{\prime}}{\sigma}\right)-\Phi\left(\frac{-C-\delta^{ \prime}}{\sigma}\right)\,.\]

To see why, we again consider four cases of increasing generality. _Case 1_. Suppose \(k=1\), so that \(X_{1}=X\) is \(N(\xi,\,\sigma^{2})\) and we are testing \(|\xi|\geq\delta\) versus \(|\xi|<\delta\). (This case follows by Theorem 3.7.1, but we argue independently so that the argument applies to the other cases as well.) Fix an alternative \(\xi=m\) with \(|m|<\delta\). Reduce the composite null hypothesis to a simple one via a least favorable distribution that places mass \(p\) on \(N(\delta,\,\sigma^{2})\) and mass \(1-p\) on \(N(-\delta,\,\sigma^{2})\). The value of \(p\) will be chosen shortly so that such a distribution is least favorable (and will be seen to depend on \(m\), \(\alpha\), \(\sigma\) and \(\delta\)). By the Neyman-Pearson Lemma, the MP test of

\[pN(\delta,\sigma^{2})+(1-p)N(-\delta,\sigma^{2})\quad\ vs\quad\ N(m,\,\sigma^ {2})\]

rejects for small values of

\[\frac{p\exp\left[-\frac{1}{2\sigma^{2}}(X-\delta)^{2}\right]+(1-p)\exp\left[- \frac{1}{2\sigma^{2}}(X+\delta)^{2}\right]}{\exp\left[-\frac{1}{2\sigma^{2}}(X -m)^{2}\right]}\,, \tag{3.45}\]

or equivalently for small values of \(f(X)\), where

\[f(x)=p\exp[(\delta-m)X/\sigma^{2}]+(1-p)\exp[-(\delta+m)X/\sigma^{2}]\,.\]

We can now choose \(p\) so that \(f(C)=f(-C)\), so that \(p\) must satisfy

\[\frac{p}{1-p}=\frac{\exp[(\delta+m)C/\sigma^{2}]-\exp[-(\delta+m)C/\sigma^{2}] }{\exp[(\delta-m)C/\sigma^{2}]-\exp[-(\delta-m)C/\sigma^{2}]}. \tag{3.46}\]

Since \(\delta-m>0\) and \(\delta+m>0\), both the numerator and denominator of the right side of (3.46) are positive, so the right side is a positive number. On the other hand, \(p/(1-p)\) is a nondecreasing function of \(p\) with range \([0,\,\infty)\) as \(p\) varies from \(0\) to \(1\). Thus, \(p\) is well-defined. Also, observe \(f^{\prime\prime}(x)\geq 0\) for all \(x\). It follows that (for this special choice of \(C\))

\[\{X:\ f(X)\leq f(C)\}=\{X:\ |X|\leq C\}\]

is the rejection region of the MP test. Such a test is easily seen to be level \(\alpha\) for the original composite null hypothesis because its power function is symmetric and decreases away from zero. Thus, the result follows by Theorem 3.8.1.

_Case 2._ Consider now general \(k\), so that \((X_{1},\ldots,X_{k})\) has mean \((\xi_{1},\ldots,\xi_{k})\) and covariance matrix \(\Sigma\). However, consider the special case \((a_{1},\ldots,a_{k})=(1,0,\ldots,0)\), so we are testing \(|\xi_{1}|\geq\delta\) versus \(|\xi_{1}|<\delta\). Also, assume \(X_{1}\) and \((X_{2},\ldots,X_{k})\) are independent, so that the first row and first column of \(\Sigma\) are zero except the first entry, which is \(\sigma^{2}\) (assumed positive). Using the same reasoning as case 1, fix an alternative \(m=(m_{1},\ldots,m_{k})\) with \(|m_{1}|<\delta\) and consider testing

\[p\,N\,\left((\delta,m_{2},\ldots,m_{k}),\,\Sigma\right)+(1-p)N\,\left((-\delta, m_{2},\ldots,m_{k}),\,\Sigma\right)\]

versus \(N\,\left((m_{1},\ldots,m_{k}),\,\Sigma\right)\). The likelihood ratio is in fact the same as (3.45) because each term is now multiplied by the density of \((X_{2},\ldots,X_{k})\) (by independence), and these densities cancel. The UMP test from case 1, which rejects when \(|X_{1}|\leq C\), is UMP in this situation as well.

_Case 3._ As in case 2, consider \(a_{1}=1\) and \(a_{i}=0\) if \(i>1\), but now allow \(\Sigma\) to be an arbitrary covariance matrix. By transforming \(X\) to \(Y\) as in Case 3 of Example 3.9.2, the result follows (Problem 3.71).

_Case 4._ Now, consider arbitrary \((a_{1},\ldots,a_{k})\) satisfying \(a^{\top}\Sigma a>0\). As in case 4 of Example 3.9.2, transform \(X\) to \(Z\) and the result follows (Problem 3.71).

### Problems

_Section 3.2_

**Problem 3.1**: Let \(X_{1},\ldots,X_{n}\) be a sample from the normal distribution \(N(\xi,\sigma^{2})\).

1. If \(\sigma=\sigma_{0}\) (known), there exists a UMP test for testing \(H:\xi\leq\xi_{0}\) against \(\xi>\xi_{0}\), which rejects when \(\sum(X_{i}-\xi_{0})\) is too large.
2. If \(\xi=\xi_{0}\) (known), there exists a UMP test for testing \(H:\sigma\leq\sigma_{0}\) against \(K:\sigma>\sigma_{0}\), which rejects when \(\sum(X_{i}-\xi_{0})^{2}\) is too large.

**Problem 3.2**: _UMP test for \(U(0,\theta)\)._ Let \(X=(X_{1},\ldots,X_{n})\) be a sample from the uniform distribution on \((0,\theta)\).

1. For testing \(H:\theta\leq\theta_{0}\) against \(K:\theta>\theta_{0}\) any test is UMP at level \(\alpha\) for which \(E_{\theta_{0}}\phi(X)=\alpha\), \(E_{\theta}\phi(X)\leq\alpha\) for \(\theta\leq\theta_{0}\), and \(\phi(x)=1\) when \(\max(x_{1},\ldots,x_{n})>\theta_{0}\).
2. For testing \(H:\theta=\theta_{0}\) against \(K:\theta\neq\theta_{0}\) a unique UMP test exists, and is given by \(\phi(x)=1\) when \(\max(x_{1},\ldots,x_{n})>\theta_{0}\) or \(\max(x_{1},\ldots,x_{n})\leq\theta_{0}\sqrt[n]{\alpha}\), and \(\phi(x)=0\) otherwise.

[(i): For each \(\theta>\theta_{0}\) determine the ordering established by \(r(x)=p_{\theta}(x)/p_{\theta_{0}}(x)\) and use the fact that many points are equivalent under this ordering.

(ii): Determine the UMP tests for testing \(\theta=\theta_{0}\) against \(\theta<\theta_{0}\) and combine this result with that of part (i).]

**Problem 3.3**: Suppose \(N\) i.i.d. random variables are generated from the same known strictly increasing absolutely continuous cdf \(F(\cdot)\). We are told only \(X\), the maximum of these random variables. Is there a UMP size \(\alpha\) test of

\[H_{0}:N\leq 5\ \ \mbox{versus}\ \ H_{1}:N>5?\]

If so, find it.

**Problem 3.4**: _UMP test for exponential densities_. Let \(X_{1}\),..., \(X_{n}\) be a sample from the exponential distribution \(E(a,b)\) of Problem 1.18, and let \(X_{(1)}=\min(X_{1}\),..., \(X_{n})\).

1. Determine the UMP test for testing \(H:a=a_{0}\) against \(K:a\neq a_{0}\) when \(b\) is assumed known.
2. The power of any MP level-\(\alpha\) test of \(H:a=a_{0}\) against \(K:a=a_{1}<a_{0}\) is given by \[\beta^{*}(a_{1})=1-(1-\alpha)e^{-n(a_{0}-a_{1})/b}.\]
3. For the problem of part (i), when \(b\) is unknown, the power of any level \(\alpha\) test which rejects when \[\frac{X_{(1)}-a_{0}}{\sum[X_{i}-X_{(1)}]}\leq C_{1}\ \mbox{or}\ \geq C_{2}\] against any alternative (\(a_{1}\), \(b\)) with \(a_{1}<a_{0}\) is equal to \(\beta^{*}(a_{1})\) of part (ii) (independent of the particular choice of \(C_{1}\) and \(C_{2}\)).
4. The test of part (iii) is a UMP level-\(\alpha\) test of \(H:a=a_{0}\) against \(K:a\neq a_{0}\) (\(b\) unknown).
5. Determine the UMP test for testing \(H:a=a_{0}\), \(b=b_{0}\) against the alternatives \(a<a_{0}\), \(b<b_{0}\).
6. Explain the (very unusual) existence in this case of a UMP test in the presence of a nuisance parameter [part(iv)] and for a hypothesis specifying two parameters [part(v)].

[(i) The variables \(Y_{i}=e^{-X_{i}/b}\) are a sample from the uniform distribution on \((0,e^{-\alpha/b})\).]

_Note_. For more general versions of parts (ii)-(iv), see Takeuchi (1969) and Kabe and Laurent (1981).

**Problem 3.5**: In the proof of Theorem 3.2.1(i), consider the set of \(c\) satisfying \(\alpha(c)\leq\alpha\leq\alpha(c-0)\). If there is only one such \(c\), \(c\) is unique; otherwise, there is an interval of such values \([c_{1},c_{2}]\). Argue that, in this case, if \(\alpha(c)\) is continuous at \(c_{2}\), then \(P_{i}(C)=0\) for \(i=0\), \(1\), where \[C=\left\{x:p_{0}(x)>0\text{ and }c_{1}<\frac{p_{1}(x)}{p_{0}(x)}\leq c_{2}\right\}\.\]

If \(\alpha(c)\) is not continuous at \(c_{2}\), then the result is false.

**Problem 3.6**: Let \(P_{0},\,P_{1},\,P_{2}\) be the probability distributions assigning to the integers \(1,\,\ldots,\,6\) the following probabilities:

\[\begin{array}{c|cccccc}&1&2&3&4&5&6\\ \hline P_{0}&0.03&0.02&0.02&0.01&0&0.92\\ P_{1}&0.06&0.05&0.08&0.02&0.01&0.78\\ P_{2}&0.09&0.05&0.12&0&0.02&0.72\\ \end{array}\]

Determine whether there exists a level-\(\alpha\) test of \(H:P=P_{0}\) which is UMP against the alternatives \(P_{1}\) and \(P_{2}\) when (i) \(\alpha=0.01\); (ii) \(\alpha=0.05\); (iii) \(\alpha=0.07\).

**Problem 3.7**: Let the distribution of \(X\) be given by

\[\begin{array}{c|cccc}\text{x}&0&1&2&3\\ \hline P_{\theta}(X=x)&\theta&2\theta&0.9-2\theta&0.1-\theta\\ \end{array}\]

where \(0<\theta<0.1\). For testing \(H:\theta=0.05\) against \(\theta>0.05\) at level \(\alpha=0.05\), determine which of the following tests (if any) is UMP:

1. \(\phi(0)=1\), \(\phi(1)=\phi(2)=\phi(3)=0\);
2. \(\phi(1)=0.5\), \(\phi(0)=\phi(2)=\phi(3)=0\);
3. \(\phi(3)=1\), \(\phi(0)=\phi(1)=\phi(2)=0\).

**Problem 3.8**: A random variable \(X\) has the _Pareto distribution_\(P(c,\,\tau)\) if its density is \(c\tau^{c}/x^{c+1}\), \(0<\tau<x\), \(0<C\).

1. Show that this defines a probability density.
2. If \(X\) has distribution \(P(c,\,\tau)\), then \(Y=\log X\) has exponential distribution \(E(\xi,b)\) with \(\xi=\log\tau\), \(b=1/c\).
3. If \(X_{1},\,\ldots,\,X_{n}\) is a sample from \(P(c,\,\tau)\), use (ii) and Problem 3.4 to obtain UMP tests of (a) \(H:\tau=\tau_{0}\) against \(\tau\neq\tau_{0}\) when \(b\) is known; (b) \(H:c=c_{0}\), \(\tau=\tau\) against \(c>c_{0}\), \(\tau<\tau_{0}\).

**Problem 3.9**: Let \(X\) be distributed according to \(P_{\theta},\,\theta\in\Omega\), and let \(T\) be sufficient for \(\theta\). If \(\varphi(X)\) is any test of a hypothesis concerning \(\theta\), then \(\psi(T)\) given by \(\psi(t)=E[\varphi(X)\mid t]\) is a test depending on \(T\) only, and its power function is identical with that of \(\varphi(X)\).

**Problem 3.10**: In the notation of Section 3.2, consider the problem of testing \(H_{0}:P=P_{0}\) against \(H_{1}:P=P_{1}\), and suppose that known probabilities \(\pi_{0}=\pi\) and \(\pi_{1}=1-\pi\) can be assigned to \(H_{0}\) and \(H_{1}\) prior to the experiment.

1. The overall probability of an error resulting from the use of a test \(\varphi\) is \[\pi E_{0}\varphi(X)+(1-\pi)E_{1}[1-\varphi(X)].\]
2. The _Bayes test_ minimizing this probability is given by (3.8) with \(k=\pi_{0}/\pi_{1}\).
3. The conditional probability of \(H_{i}\) given \(X=x\), the _posterior probability_ of \(H_{i}\) is \[\frac{\pi_{i}\,p_{i}\,(x)}{\pi_{0}p_{0}(x)+\pi_{1}p_{1}(x)},\] and the Bayes test therefore decides in favor of the hypothesis with the larger posterior probability.

**Problem 3.11**:
1. For testing \(H_{0}:\theta=0\) against \(H_{1}:\theta=\theta_{1}\) when \(X\) is \(N(\theta,1)\), given any \(0<\alpha<1\) and any \(0<\pi<1\) (in the notation of the preceding problem), there exists \(\theta_{1}\) and \(x\) such that (a) \(H_{0}\) is rejected when \(X=x\) but (b) \(P(H_{0}\mid x)\) is arbitrarily close to 1.
2. The paradox of part (i) is due to the fact that \(\alpha\) is held constant while the power against \(\theta_{1}\) is permitted to get arbitrarily close to 1. The paradox disappears if \(\alpha\) is determined so that the probabilities of type I and type II error are equal [but see Berger and Sellke (1987)].

[For a discussion of such paradoxes, see Lindley (1957), Bartlett (1957), Schafer (1982, 1988) and Robert (1993).]

**Problem 3.12**: Let \(X_{1},\ldots,\)\(X_{n}\) be independently distributed, each uniformly over the integers \(1,2,\ldots,\theta\). Determine whether there exists a UMP test for testing \(H:\theta=\theta_{0}\), at level \(1/\theta_{0}^{n}\) against the alternatives (i) \(\theta>\theta_{0}\); (ii) \(\theta<\theta_{0}\); (iii) \(\theta\neq\theta_{0}\).

**Problem 3.13**: The following example shows that the power of a test can sometimes be increased by selecting a random rather than a fixed sample size even when the randomization does not depend on the observations. Let \(X_{1},\ldots,\)\(X_{n}\) be independently distributed as \(N(\theta,1)\), and consider the problem of testing \(H:\theta=0\) against \(K:\theta=\theta_{1}>0\).

1. The power of the most powerful test as a function of the sample size \(n\) is not necessarily concave.
2. In particular for \(\alpha=0.005\), \(\theta_{1}=\frac{1}{2}\), better power is obtained by taking 2 or 16 observations with probability \(\frac{1}{2}\) each than by taking a fixed sample of 9 observations.
3. The power can be increased further if the test is permitted to have different significance levels \(\alpha_{1}\) and \(\alpha_{2}\) for the two sample sizes and it is required only that the expected significance level be equal to \(\alpha=0.005\). Examples are: (a) with probability \(\frac{1}{2}\) take \(n_{1}=2\) observations and perform the test of significance at level \(\alpha_{1}=0.001\), or take \(n_{2}=16\) observations and perform the test at level \(\alpha_{2}=0.009\); (b) with probability \(\frac{1}{2}\) take \(n_{1}=0\) or \(n_{2}=18\) observations and let the respective significance levels be \(\alpha_{1}=0\), \(\alpha_{2}=0.01\).

_Note_. This and related examples were discussed by Kruskal in a seminar held at Columbia University in 1954. A more detailed investigation of the phenomenon has been undertaken by Cohen (1958).

**Problem 3.14**: If the sample space \({\cal X}\) is Euclidean and \(P_{0}\), \(P_{1}\) have densities with respect to Lebesgue measure, there exists a nonrandomized most powerful test for testing \(P_{0}\) against \(P_{1}\) at every significance level \(\alpha\).13 [This is a consequence of Theorem 3.2.1 and the following lemma.14 Let \(f\geq 0\) and \(\int_{A}f(x)\,dx=a\). Given any \(0\leq b\leq a\), there exists a subset \(B\) of \(A\) such that \(\int_{B}f(x)\,dx=b\).]

Footnote 13: For more general results concerning the possibility of dispensing with randomized procedures, see Dvoretzky et al. (1951).

Footnote 14: For a proof of this lemma see Halmos (1974, p. 174). The lemma is a special case of a theorem of Lyapounov (1940); see Blackwell (1951a).

**Problem 3.15**: _Fully informative statistics._ A statistic \(T\) is _fully informative_ if for every decision problem the decision procedures based only on \(T\) form an essentially complete class. If \({\cal P}\) is dominated and \(T\) is fully informative, then \(T\) is sufficient. [Consider any pair of distributions \(P_{0}\), \(P_{1}\in{\cal P}\) with densities \(p_{0}\), \(p_{1}\), and let \(g_{i}=p_{i}/(p_{0}+p_{1})\). Suppose that \(T\) is fully informative, and let \({\cal A}_{t}\) be the subfield induced by \(T\). Then \({\cal A}_{t}\) contains the subfield induced by \((g_{0},g_{1})\) since it contains every rejection which is unique most powerful for testing \(P_{0}\) against \(P_{1}\) (or \(P_{1}\) against \(P_{0}\)) at some level \(\alpha\). Therefore, \(T\) is sufficient for every pair of distributions \((P_{0}\), \(P_{1})\), and hence by Problem 2.11 it is sufficient for \({\cal P}\).]

**Problem 3.16**: Based on \(X\) with distribution indexed by \(\theta\in\Omega\), the problem is to test \(\theta\in\omega\) versus \(\theta\in\omega^{\prime}\). Suppose there exists a test \(\phi\) such that \(E_{\theta}[\phi(X)]\leq\beta\) for all \(\theta\) in \(\omega\), where \(\beta<\alpha\). Show there exists a level \(\alpha\) test \(\phi^{*}(X)\) such that

\[E_{\theta}[\phi(X)]\leq E_{\theta}[\phi^{*}(X)]\,\]

for all \(\theta\) in \(\omega^{\prime}\) and this inequality is strict if \(E_{\theta}[\phi(X)]<1\).

**Problem 3.17**: _A counterexample._ Typically, as \(\alpha\) varies the most powerful level \(\alpha\) tests for testing a hypothesis \(H\) against a simple alternative are nested in the sense that the associated rejection regions, say \(R_{\alpha}\), satisfy \(R_{\alpha}\subseteq R_{\alpha^{\prime}}\), for any \(\alpha<\alpha^{\prime}\). Even if the most powerful tests are nonrandomized, this may be false. Suppose \(X\) takes values 1, 2, and 3 with probabilities 0.85, 0.1, and 0.05 under \(H\) and probabilities 0.7, 0.2, and 0.1 under \(K\) respectively.

(i) At any level \(<\) 0.15, the MP test is not unique.

(ii) At \(\alpha=\) 0.05 and \(\alpha^{\prime}=\) 0.1, there exist unique nonrandomized MP tests and they are not nested.

(iii) At these levels there exist MP tests \(\phi\) and \(\phi^{\prime}\) that are nested in the sense that \(\phi(x)\leq\phi^{\prime}(x)\) for all \(x\). [This example appears as Example 10.16 in Romano and Siegel (1986).]

**Problem 3.18**: Under the setup of Theorem 3.2.1, show that there always exist MP tests that are nested in the sense of Problem 3.17(iii).

**Problem 3.19**: Suppose \(X_{1},\ldots,\)\(X_{n}\) are i.i.d. \(N(\xi,\sigma^{2})\) with \(\sigma\) known. For testing \(\xi=0\) versus \(\xi\neq 0\), the average power of a test \(\phi=\phi(X_{1},\ldots,\)\(X_{n})\) is given by

\[\int_{-\infty}^{\infty}E_{\xi}(\phi)d\,\Lambda(\xi)\,\]

where \(\Lambda\) is a probability distribution on the real line. Suppose that \(\Lambda\) is symmetric about \(0\); that is, \(\Lambda\{E\}=\Lambda\{-E\}\) for all Borel sets \(E\). Show that, among \(\alpha\) level tests, the one maximizing average power rejects for large values of \(|\sum_{i}X_{i}|\). Show that this test need not maximize average power if \(\Lambda\) is not symmetric.

**Problem 3.20**: Let \(f_{\theta},\)\(\theta\in\Omega\), denote a family of densities with respect to a measure \(\mu\). (We assume \(\Omega\) is endowed with a \(\sigma\)-field so that the densities \(f_{\theta}(x)\) are jointly measurable in \(\theta\) and \(x\).) Consider the problem of testing a simple null hypothesis \(\theta=\theta_{0}\) against the composite alternatives \(\Omega_{K}=\{\theta:\ \theta\neq\theta_{0}\}\). Let \(\Lambda\) be a probability distribution on \(\Omega_{K}\).

(i) As explicitly as possible, find a test \(\phi\) that maximizes \(\int_{\Omega_{K}}E_{\theta}(\phi)d\,\Lambda(\theta)\), subject to it being level \(\alpha\).

(ii) Let \(h(x)=\int f_{\theta}(x)d\,\Lambda(\theta)\). Consider the nonrandomized \(\phi\) test that rejects if and only if \(h(x)/f_{\theta_{0}}(x)>k\), and suppose \(\mu\{x:\ h(x)=kf_{\theta}(x)\}=0\). Then, \(\phi\) is admissible at level \(\alpha=E_{\theta_{0}}(\phi)\) in the sense that it is impossible that there exists another level \(\alpha\) test \(\phi^{\prime}\) such that \(E_{\theta}(\phi^{\prime})\geq E_{\theta}(\phi)\) for all \(\theta\).

(iii) Show that the test of Problem 3.19 is admissible.

### _Section 3.3_

**Problem 3.21**: In Example 3.21, show that \(p\)-value is indeed given by \(\hat{p}=\hat{p}(X)=(11-X)/10\). Also, graph the c.d.f. of \(\hat{p}\) under \(H\) and show that the last inequality in (3.15) is an equality if and only if \(u\) is of the form \(0\),..., \(10\).

**Problem 3.22**: Suppose \(X\) has a continuous distribution function \(F\). Show that \(F(X)\) is uniformly distributed on (\(0\), \(1\)). [The transformation from \(X\) to \(F(X)\) is known as the _probability integral transformation_.]

**Problem 3.23**: (i) Show that if \(Y\) is any random variable with c.d.f. \(G(\cdot)\), then

\[P\{G(Y)\leq u\}\leq u\ \ \ \mbox{for all $0\leq u\leq 1$}\.\]

If \(G^{-}(t)=P\{Y<t\}\), then show

\[P\{1-G^{-}(Y)\leq u\}\leq u\ \ \ \mbox{for all $0\leq u\leq 1$}\.\]

(ii) In Example 3.3.3, show that \(F_{\theta_{0}}(T)\) and \(1-F_{\theta_{0}}^{-}(T)\) are both valid \(p\)-values, in the sense that (3.13) holds.

**Problem 3.24**: Under the setup of Lemma 3.3.1, suppose the rejection regions are defined by

\[R_{\alpha}=\{X:T(X)\geq k(\alpha)\} \tag{3.47}\]

for some real-valued statistic \(T(X)\) and \(k(\alpha)\) satisfying

\[\sup_{\theta\in\Omega_{H}}P_{\theta}\{T(X)\geq k(\alpha)\}=\alpha\.\]

Then, show

\[\hat{p}=\sup_{\theta\in\Omega_{H}}P\{T(X)\geq t\}\,\]

where \(t\) is the observed value of \(T(X)\).

**Problem 3.25**: Under the setup of Lemma 3.3.1, show that there exists a real-valued statistic \(T(X)\) so that the rejection region is necessarily of the form (3.47). [_Hint_: Let \(T(X)=-\hat{p}\).]

**Problem 3.26**: (i) If \(\hat{p}\) is uniform on (0, 1), show that \(-2\log(\hat{p})\) has the Chi-squared distribution with 2 degrees of freedom.

(ii) Suppose \(\hat{p}_{1}\),..., \(\hat{p}_{s}\) are i.i.d. uniform on (0, 1). Let \(F=-2\log(\hat{p}_{1}\cdots\hat{p}_{s})\). Argue that \(F\) has the Chi-squared distribution with \(2s\) degrees of freedom. What can you say about \(F\) if the \(\hat{p}_{i}\) are independent and satisfy \(P\{\hat{p}_{i}\leq u\}\leq u\) for all \(0\leq u\leq 1\)? [Fisher (1934a) proposed \(F\) as a means of combining \(p\)-values from independent experiments.]

**Problem 3.27**: Let \(X\) be the number of successes in \(n\) independent trials with probability \(p\) of success, and let \(\phi(x)\) be the UMP test (3.16) for testing \(p\leq p_{0}\) against \(p>p_{0}\) at the level of significance \(\alpha\).

1. For \(n=6\), \(p_{0}=0.25\) and the levels \(\alpha=0.05\), 0.1, 0.2 determine \(C\) and \(\gamma\), and the power of the test against \(p_{1}=0.3\), 0.4, 0.5, 0.6, 0.7.
2. If \(p_{0}=0.2\) and \(\alpha=0.05\), and it is desired to have power \(\beta\geq 0.9\) against \(p_{1}=0.4\), determine the necessary sample size (a) by using tables of the binomial distribution, (b) by using the normal approximation.15 Footnote 15: Tables and approximations are discussed, for example, in Chapter 3 of Johnson and Kotz (1969).
3. Use the normal approximation to determine the sample size required when \(\alpha=0.05\), \(\beta=0.9\), \(p_{0}=0.01\), \(p_{1}=0.02\).

**Problem 3.28**: (i) A necessary and sufficient condition for densities \(p_{\theta}(x)\) to have monotone likelihood ratio in \(x\), if the mixed second derivative \(\partial^{2}\log p_{\theta}(x)/\partial\theta\ \partial x\) exists, is that this derivative is \(\geq 0\) for all \(\theta\) and \(x\).

2. An equivalent condition is that \[p_{\theta}(x)\frac{\partial^{2}p_{\theta}(x)}{\partial\theta\,\partial x}\geq \frac{\partial p_{\theta}(x)}{\partial\theta}\,\,\frac{\partial p_{\theta}(x)}{ \partial x}\qquad\mbox{for all $\theta$ and $x$}.\]

**Problem 3.29**: Let the probability density \(p_{\theta}\) of \(X\) have monotone likelihood ratio in \(T(x)\), and consider the problem of testing \(H:\theta\leq\theta_{0}\) against \(\theta>\theta_{0}\). If the distribution of \(T\) is continuous, the \(p\)-value \(\hat{p}\) of the UMP test is given by \(\hat{p}=P_{\theta_{0}}[T\geq t]\), where \(t\) is the observed value of \(T\). This holds also without the assumption of continuity if for randomized tests \(\hat{p}\) is defined as the smallest significance level at which the hypothesis is rejected with probability 1. Show that, for any \(\theta\leq\theta_{0}\), \(P_{\theta}\{\hat{p}\leq u\}\leq u\) for any \(0\leq u\leq 1\).

**Problem 3.30**: Let \(X_{1}\),..., \(X_{n}\) be independently distributed with density \((2\theta)^{-1}\)\(e^{-x/2\theta}\), \(x\geq 0\), and let \(Y_{1}\leq\cdots\leq Y_{n}\) be the ordered \(X\)'s. Assume that \(Y_{1}\) becomes available first, then \(Y_{2}\), and so on, and that observation is continued until \(Y_{r}\) has been observed. On the basis of \(Y_{1}\),..., \(Y_{r}\) it is desired to test \(H:\theta\geq\theta_{0}=1000\) at level \(\alpha=0.05\) against \(\theta<\theta_{0}\).

1. Determine the rejection region when \(r=4\), and find the power of the test against \(\theta_{1}=500\).
2. Find the value of \(r\) required to get power \(\beta\geq 0.95\) against the alternative.

[In Problem 2.15 the distribution of \([\sum_{i=1}^{r}Y_{i}+(n-r)Y_{r}]/\theta\) was found to be \(\chi^{2}\) with \(2r\) degrees of freedom.]

**Problem 3.31**: When a Poisson process with rate \(\lambda\) is observed for a time interval of length \(\tau\), the number \(X\) of events occurring has the Poisson distribution \(P(\lambda\tau)\). Under an alternative scheme, the process is observed until \(r\) events have occurred, and the time \(T\) of observation is then a random variable such that \(2\lambda T\) has a \(\chi^{2}\)-distribution with \(2r\) degrees of freedom. For testing \(H:\lambda\leq\lambda_{0}\) at level \(\alpha\) one can, under either design, obtain a specified power \(\beta\) against an alternative \(\lambda_{1}\) by choosing \(\tau\) and \(r\) sufficiently large.

1. The ratio of the time of observation required for this purpose under the first design to the expected time required under the second is \(\lambda\tau/r\).
2. Determine for which values of \(\lambda\) each of the two designs is preferable when \(\lambda_{0}=1\), \(\lambda_{1}=2\), \(\alpha=0.05\), \(\beta=9\).

**Problem 3.32**: Let \(X=(X_{1},\ldots,X_{n})\) be a sample from the uniform distribution \(U(\theta,\theta+1)\).

1. For testing \(H:\theta\leq\theta_{0}\) against \(K:\theta>\theta_{0}\) at level \(\alpha\), there exists a UMP test which rejects when \(\min(X_{1}\),..., \(X_{n})>\theta_{0}+C(\alpha)\) or \(\max(X_{1}\),..., \(X_{n})>\theta_{0}+1\) for suitable \(C(\alpha)\).
2. The family \(U(\theta,\theta+1)\) does not have monotone likelihood ratio. [Additional results for this family are given in Birnbaum (1954b) and Pratt (1958).][(ii) By Theorem 3.4.1, monotone likelihood ratio implies that the family of UMP test of \(H:\theta\leq\theta_{0}\) against \(K:\theta>\theta_{0}\) generated as \(\alpha\) varies from 0 to 1 is independent of \(\theta_{0}\).]

**Problem 3.33**: Let \(X\) be a single observation from the Cauchy density given at the end of Section 3.4.

1. Show that no UMP test exists for testing \(\theta=0\) against \(\theta>0\).
2. Determine the totality of different shapes the MP level-\(\alpha\) rejection region for testing \(\theta=\theta_{0}\) against \(\theta=\theta_{1}\) can take on for varying \(\alpha\) and \(\theta_{1}-\theta_{0}\).

**Problem 3.34**: Let \(X_{i}\) be independently distributed as \(N(i\Delta,1),i=1,\ldots,n\). Show that there exists a UMP test of \(H:\Delta\leq 0\) against \(K:\Delta>0\), and determine it as explicitly as possible.

**Problem 3.35**: Suppose a time series \(X_{0}\), \(X_{1}\), \(X_{2}\), \(\ldots\) evolves in the following way. The process starts at 0, so \(X_{0}=0\). For any \(i\geq 1\), conditional on \(X_{0}\), \(\ldots\), \(X_{i-1}\), \(X_{i}=\rho X_{i-1}+\epsilon_{i}\), where the \(\epsilon_{i}\) are i.i.d. standard normal. You observe \(X_{0}\), \(X_{1}\), \(X_{2}\), \(\ldots\), \(X_{n}\). For testing the null hypothesis \(\rho=0\) versus a fixed alternative \(\rho=\rho^{\prime}>0\), determine a MP level \(\alpha\) test. Determine whether or not there exists a uniformly most powerful test against all \(\rho>0\).

_Note_. The following problems (and some in later chapters) refer to the gamma, Pareto, Weibull, and inverse Gaussian distributions. For more information about these distributions, see Chapters 17, 19, 20, and 25 respectively of Johnson and Kotz (1970).

**Problem 3.36**: Let \(X_{1}\), \(\ldots\), \(X_{n}\) be a sample from the _gamma distribution_\(\Gamma(g,b)\) with density

\[\frac{1}{\Gamma(g)b^{g}}x^{g-1}e^{-x/b},\ \ \ \ \ 0<x,\ \ \ 0<b,g.\]

Show that there exists a UMP test for testing

1. \(H:b\leq b_{0}\) against \(b>b_{0}\) when \(g\) is known;
2. \(H:g\leq g_{0}\) against \(g>g_{0}\) when \(b\) is known.

In each case give the form of the rejection region.

**Problem 3.37**: A random variable \(X\) has the _Weibull distribution_\(W(b,c)\) if its density is

\[\frac{c}{b}\left(\frac{x}{b}\right)^{c-1}e^{-(x/b)^{c}},\ \ \ \ \ x>0,b,c>0.\]

Show that this defines a probability density. If \(X_{1}\), \(\ldots\), \(X_{n}\) is a sample from \(W(b,c)\), with the shape parameter \(c\) known, show that there exists a UMP test of \(H:b\leq b_{0}\) against \(b>b_{0}\) and give its form.

**Problem 3.38**: Consider a single observation \(X\) from \(W(1,c)\).

1. The family of distributions does not have a monotone likelihood ratio in \(x\).
2. The most powerful test of \(H:c=1\) against \(c=2\) rejects when \(X<k_{1}\) and when \(X>k_{2}\). Show how to determine \(k_{1}\) and \(k_{2}\).
3. Generalize (ii) to arbitrary alternatives \(c_{1}>1\), and show that a UMP test of \(H:c=1\) against \(c>1\) does not exist.
4. For any \(c_{1}>1\), the power function of the MP test of \(H:c=1\) against \(c=c_{1}\) is an increasing function of \(c\).

**Problem 3.39**: Let \(X_{1},\ldots,X_{n}\) be a sample from the _inverse Gaussian_ distribution \(I(\mu,\tau)\) with density

\[\sqrt{\frac{\tau}{2\pi x^{3}}}\exp\left(-\frac{\tau}{2x\mu^{2}}(x-\mu)^{2} \right),\ \ \ \ \ \ x>0,\ \ \ \tau,\mu>0.\]

Show that there exists a UMP test for testing

1. \(H:\mu\leq\mu_{0}\) against \(\mu>\mu_{0}\) when \(\tau\) is known;
2. \(H:\tau\leq\tau_{0}\) against \(\tau>\tau_{0}\) when \(\mu\) is known. In each case give the form of the rejection region.
3. The distribution of \(V=r(X_{i}-\mu)^{2}/X_{i}\mu^{2}\) is \(\chi_{1}^{2}\) and hence that of \(\tau\sum[(X_{i}-\mu)^{2}/X_{i}\mu^{2}]\) is \(\chi_{n}^{2}\).

[Let \(Y=\min(X_{i},\,\mu^{2}/X_{i}),\,Z=\tau(Y-\mu)^{2}/\mu^{2}Y\). Then \(Z=V\) and \(Z\) is \(\chi_{1}^{2}\) [Shuster (1968)].] _Note_. The UMP test for (ii) is discussed in Chhikara and Folks (1976).

**Problem 3.40**: Let \(X_{1},\cdots,X_{n}\) be a sample from a location family with common density \(f(x-\theta)\), where the location parameter \(\theta\in\mathbf{R}\) and \(f(\cdot)\) is known. Consider testing the null hypothesis that \(\theta=\theta_{0}\) versus an alternative \(\theta=\theta_{1}\) for some \(\theta_{1}>\theta_{0}\). Suppose there exists a most powerful level \(\alpha\) test of the form: reject the null hypothesis iff \(T=T(X_{1},\cdots,X_{n})>C\), where \(C\) is a constant and \(T(X_{1},\ldots,X_{n})\) is location equivariant, i.e., \(T(X_{1}+c,\ldots,X_{n}+c)=T(X_{1},\ldots,X_{n})+c\) for all constants \(c\). Is the test also most powerful level \(\alpha\) for testing the null hypothesis \(\theta\leq\theta_{0}\) against the alternative \(\theta=\theta_{1}\). Prove or give a counterexample.

**Problem 3.41**: _Extension of Lemma 3.4.2._ Let \(P_{0}\) and \(P_{1}\) be two distributions with densities \(p_{0}\), \(p_{1}\) such that \(p_{1}(x)/p_{0}(x)\) is a nondecreasing function of a real-valued statistic \(T(x)\).

1. If \(T=T(X)\) has probability density \(p_{i}^{\prime}\) when the original distribution of \(X\) is \(P_{i}\), then \(p_{1}^{\prime}(t)/p_{0}^{\prime}(t)\) is nondecreasing in \(t\).
2. \(E_{0}\psi(T)\leq E_{1}\psi(T)\) for any nondecreasing function \(\psi\).
3. If \(p_{1}(x)/p_{0}(x)\) is a strictly increasing function of \(t=T(x)\), so is \(p_{1}^{\prime}(t)/p_{0}^{\prime}(t)\), and \(E_{0}\psi(T)<E_{1}\psi(T)\) unless \(\psi[T(x)]\) is constant a.e. (\(P_{0}+P_{1}\)) or \(E_{0}\psi(T)=E_{1}\psi(T)=\pm\infty\).

* For any distinct distributions with densities \(p_{0}\), \(p_{1}\), \[-\infty\leq E_{0}\log\left[\frac{p_{1}(X)}{p_{0}(X)}\right]<E_{1}\log\left[\frac{ p_{1}(X)}{p_{0}(X)}\right]\leq\infty.\] [(i): Without loss of generality suppose that \(p_{1}(x)/p_{0}(x)=T(x)\). Then for any integrable \(\phi\), \[\int\phi(t)p_{1}^{{}^{\prime}}(t)\,dv(t)=\int\phi[T(x)]T(x)p_{0}(x)\,d\mu(x)= \int\phi(t)tp_{0}^{{}^{\prime}}(t)\,dv(t),\] and hence \(p_{1}^{{}^{\prime}}(t)/p_{0}^{{}^{\prime}}(t)=t\) a.e. (iv): The possibility \(E_{0}\log[p_{1}(X)/p_{0}(X)]=\infty\) is excluded, since by the convexity of the function \(\log\), \[E_{0}\log\left[\frac{p_{1}(X)}{p_{0}(X)}\right]<\log E_{0}\left[\frac{p_{1}(X )}{p_{0}(X)}\right]=0.\] Similarly for \(E_{1}\). The strict inequality now follows from (iii) with \(T(x)=p_{1}(x)/p_{0}(x)\).]

**Problem 3.42**: \(F_{0}\), \(F_{1}\) are two cumulative distribution functions on the real line, then \(F_{i}(x)\leq F_{0}(x)\) for all \(x\) if and only if \(E_{0}\psi(X)\leq E_{1}\psi(X)\) for any nondecreasing function \(\psi\).

**Problem 3.43**: Let \(F\) and \(G\) be two continuous, strictly increasing c.d.f.s, and let \(k(u)=G[F^{-1}(u)]\), \(0<u<1\).

(i) Show \(F\) and \(G\) are stochastically ordered, say \(F(x)\leq G(x)\) for all \(x\), if and only if \(k(u)\leq u\) for all \(0<u<1\).

(ii) If \(F\) and \(G\) have densities \(f\) and \(g\), then show they are monotone likelihood ratio ordered, say \(g/f\) nondecreasing, if and only if \(k\) is convex.

(iii) Use (i) and (ii) to give an alternative proof of the fact that MLR implies stochastic ordering.

**Problem 3.44**: Let \(f(x)/[1-F(x)]\) be the "mortality" of a subject at time \(x\) given that it has survived to this time. A c.d.f. \(F\) is said to be smaller than \(G\) in the hazard ordering if

\[\frac{g(x)}{1-G(x)}\leq\frac{f(x)}{1-F(x)}\quad\mbox{for all }x. \tag{3.48}\]

(i) Show that (3.48) is equivalent to

\[\frac{1-F(x)}{1-G(x)}\ \ \mbox{is nonincreasing}. \tag{3.49}\](ii) Show that (3.48) holds if and only if \(k\) is starshaped. [A function \(k\) defined on an interval \(I\subseteq[0,\infty)\) is starshaped on \(I\) if \(k(\lambda x)\leq\lambda k(x)\) whenever \(x\in I\), \(\lambda x\in I\), \(0\leq\lambda\leq 1\). Problems 3.43 and 3.44 are based on Lehmann and Rojo (1992).]

#### 3.5.2 Section 3.5

**Problem 3.45**: Typically, lower confidence bounds \(\underline{\theta}(X)\) satisfying (3.21) also satisfy

\[P_{\theta}\{\underline{\theta}(X)<\theta\}\geq 1-\alpha\ \ \ \ \ \mbox{ for all }\theta\]

so that \(\theta\) is strictly greater than \(\underline{\theta}(X)\) with probability \(\geq 1-\alpha\). A similar issue of course also applies to upper confidence bounds. Investigate conditions where one can claim the endpoints are open endpoints. What happens in Example 3.5.2 for both the uniformly most accurate upper confidence bound, as well as the Clopper-Pearson solution?

**Problem 3.46**: In Example 3.5.2, what is an explicit formula for the uniformly most accurate upper bound at level \(1-\alpha\) when \(X=0\) and \(U=u\)? Compare it to the Clopper-Pearson bound in the same situation.

**Problem 3.47**:
1. For \(n=5,10\) and \(1-\alpha=0.95\), graph the upper confidence limits \(\bar{p}\) and \(\bar{p}^{*}\) of Example 3.5.2 as functions of \(t=x+u\).
2. For the same values of \(n\) and \(\alpha_{1}=\alpha_{2}=0.05\), graph the lower and upper confidence limits \(\underline{p}\) and \(\bar{p}\).

**Problem 3.48**: (i) Suppose \(U_{1}\),..., \(U_{n}\) are i.i.d. \(U(0,1)\) and let \(U_{(k)}\) denote the \(k\)th largest value (or \(k\)th order statistic). Find the density of \(U_{(k)}\) and show that

\[P\{U_{(k)}\leq p\}=\int_{0}^{p}\frac{n!}{(k-1)!(n-k)!}u^{k-1}(1-u)^{n-k}du\,\]

which in turn is equal to

\[\sum_{j=k}^{n}\binom{n}{j}p^{j}(1-p)^{n-j}\.\]

(ii) Use (i) to show that, in Example 3.5.2 with \(0<x<n\), the Clopper-Pearson solution \(\hat{p}_{U}\) for an upper \(1-\alpha\) confidence bound for \(p\) can be expressed as the \(1-\alpha\) quantile of the Beta distribution with parameters \(x+1\) and \(n-x\).

**Problem 3.49**: _Confidence bounds with minimum risk._ Let \(L(\theta,\underline{\theta})\) be nonnegative and nonincreasing in its second argument for \(\underline{\theta}<\theta\), and equal to \(0\) for \(\underline{\theta}\geq\theta\). If \(\underline{\theta}\) and \(\underline{\theta}^{*}\) are two lower confidence bounds for \(\theta\) such that

\[P_{0}\{\underline{\theta}\leq\theta^{\prime}\}\leq P_{\theta}\{\underline{ \theta}^{*}\leq\theta^{\prime}\}\ \ \ \ \mbox{ for all }\ \ \theta^{\prime}\leq\theta,\]then

\[E_{\theta}L(\theta,\underline{\theta})\leq E_{\theta}L(\theta,\underline{\theta}^{*}).\]

[Define two cumulative distribution functions \(F\) and \(F^{*}\) by \(F(u)=P_{\theta}\{\theta\leq u\}/P_{\theta}\{\underline{\theta}^{*}\leq\theta\}\), \(F^{*}(u)=P_{\theta}\{\underline{\theta}^{*}\leq u\}/P_{\theta}\{\underline{ \theta}^{*}\leq\theta\}\) for \(u<\theta\), \(F(u)=F^{*}(u)=1\) for \(u\geq\theta\). Then \(F(u)\leq F^{*}(u)\) for all \(u\), and it follows from Problem 3.42 that

\[E_{\theta}[L(\theta,\underline{\theta})] = P_{\theta}\{\theta^{*}\leq\theta\}\int L(\theta,u)dF(u)\] \[\leq P_{\theta}\{\underline{\theta}^{*}\leq\theta\}\int L(\theta,u)dF^ {*}(u)=E_{\theta}[L(\theta,\underline{\theta}^{*})].]\]

_Section 3.6_

**Problem 3.50**: If \(\beta(\theta)\) denotes the power function of the UMP test of Corollary 3.4.1, and if the function \(Q\) of (3.19) is differentiable, then \(\beta^{\prime}(\theta)>0\) for all \(\theta\) for which \(Q^{\prime}(\theta)>0\).

[To show that \(\beta^{\prime}(\theta_{0})>0\), consider the problem of maximizing, subject to \(E_{\theta_{0}}\phi(X)=\alpha\), the derivative \(\beta^{\prime}(\theta_{0})\) or equivalently the quantity \(E_{\theta_{0}}[T(X)\ \phi(X)]\).]

**Problem 3.51**: _Optimum selection procedures_. On each member of a population \(n\) measurements \((X_{1},\ldots,X_{n})=X\) are taken, for example the scores of \(n\) aptitude tests which are administered to judge the qualifications of candidates for a certain training program. A future measurement \(Y\) such as the score in a final test at the end of the program is of interest but unavailable. The joint distribution of \(X\) and \(Y\) is assumed known.

1. One wishes to select a given proportion \(\alpha\) of the candidates in such a way as to maximize the expectation of \(Y\) for the selected group. This is achieved by selecting the candidates for which \(E(Y|x)\geq C\), where \(C\) is determined by the condition that the probability of a member being selected is \(\alpha\). When \(E(Y|x)=C\), it may be necessary to randomize in order to get the exact value \(\alpha\).
2. If instead the problem is to maximize the probability with which in the selected population \(Y\) is greater than or equal to some preassigned score \(y_{0}\), one selects the candidates for which the conditional probability \(P\{Y\geq y_{0}|x\}\) is sufficiently large.

[(i): Let \(\phi(x)\) denote the probability with which a candidate with measurements \(x\) is to be selected. Then the problem is that of maximizing

\[\int\left[\int\,yp^{Y|x}(y)\ \phi(x)dy\right]p^{x}(x)dx\]subject to

\[\int\phi(x)\,p^{x}(x)dx=\alpha.]\]

**Problem 3.52**: The following example shows that Corollary 3.6.1 does not extend to a countably infinite family of distributions. Let \(p_{n}\) be the uniform probability density on \([0,\,1+1/n]\), and \(p_{0}\) the uniform density on \((0,\,1)\).

1. Then \(p_{0}\) is linearly independent of \((p_{1},\,p_{2},\,\ldots)\), that is, there do not exist constants \(c_{1},\,c_{2},\,\ldots\) such that \(p_{0}=\sum c_{n}p_{n}\).
2. There does not exist a test \(\phi\) such that \(\int\phi p_{n}=\alpha\) for \(n=1,\,2,\,\ldots\) but \(\int\phi p_{0}>\alpha\).

**Problem 3.53**: Let \(F_{1},\,\ldots,\,F_{m+1}\) be real-valued functions defined over a space \(U\). A sufficient condition for \(u_{0}\) to maximize \(F_{m+1}\) subject to \(F_{i}(u)\leq c_{i}(i=1,\,\ldots,\,m)\) is that it satisfies these side conditions, that it maximizes \(F_{m+1}(u)-\sum k_{i}\,F_{i}(u)\) for some constants \(k_{i}\geq 0\), and that \(F_{i}(u_{o})=c_{i}\) for those values \(i\) for which \(k_{i}>0\).

### _Section 3.7_

**Problem 3.54**: For a random variable \(X\) with binomial distribution \(b(p,\,n)\), determine the constants \(C_{i}\), \(\gamma\) (\(i=1,\,2\)) in the UMP test (3.33) for testing \(H:p\leq 0.2\) or \(\leq 0.7\) when \(\alpha=0.1\) and \(n=15\). Find the power of the test against the alternative \(p=0.4\).

**Problem 3.55**: _Totally positive families._ A family of distributions with probability densities \(p_{\theta}(x),\,\theta\) and \(x\) real-valued and varying over \(\Omega\) and \(\mathcal{X}\), respectively, is said to be totally positive of order \(r(\mathrm{TP}_{r})\) if for all \(x_{1}<\cdots<x_{n}\) and \(\theta_{1}<\cdots<\theta_{n}\)

\[\triangle_{n}=\left|\begin{array}{cc}p_{\theta_{1}}(x_{1})&\cdots&p_{\theta_ {1}}(x_{n})\\ p_{\theta_{n}}(x_{1})&\cdots&p_{\theta_{n}}(x_{n})\end{array}\right|\geq 0\quad \mbox{ for all }\;\;n=1,\,2,\,\ldots,\,r. \tag{3.50}\]

It is said to be strictly totally positive of order \(r\) (\(ST\,P_{r}\)) if strict inequality holds in (3.50). The family is said to be (strictly) totally positive of infinity if (3.50) holds for all \(n=1,\,2,\,\ldots\). These definitions apply not only to probability densities but to any real-valued functions \(p_{\theta}(x)\) of two real variables.

1. For \(r=1\), (3.50) states that \(p_{\theta}(x)\geq 0\); for \(r=2\), that \(p_{\theta}(x)\) has monotone likelihood ratio in \(x\).
2. If \(a(\theta)>0\), \(b(x)>0\), and \(p_{\theta}(x)\) is \(\mathrm{STP}_{r}\) then so is \(a(\theta)b(x)\,p_{\theta}(x)\).
3. If \(a\) and \(b\) are real-valued functions mapping \(\Omega\) and \(\mathcal{X}\) onto \(\Omega^{\prime}\) and \(\mathcal{X}^{\prime}\) and are strictly monotone in the same direction, and if \(p_{\theta}(x)\) is \(\mathrm{STP}_{r}\), then \(p_{\theta^{\prime}}(x^{\prime})\) with \(\theta^{\prime}=a^{-1}(\theta)\) and \(x^{\prime}=b^{-1}(x)\) is \((ST\,P)_{r}\) over \((\Omega^{\prime},\,\mathcal{X}^{\prime})\).

**Problem 3.56**: _Exponential families._ The exponential family (3.19) with \(T(x)=x\) and \(Q(\theta)=\theta\) is STP\({}_{\infty}\), with \(\Omega\) the natural parameter space and \(\mathcal{X}=(-\infty,\infty)\).

[That the determinant \(|e^{\theta_{i}x_{j}}|\), \(i\), \(j=1,\ldots,n\), is positive can be proved by induction. Divide the \(i\)th column by \(e^{\theta_{1}x_{i}}\), \(i=1,\ldots,n\); subtract in the resulting determinant the \((n-1)\)st column from the \(n\)th, the \((n-2)\)nd from the \((n-1)\)st,..., the 1st from the 2nd; and expand the determinant obtained in this way by the first row. Then \(\triangle_{n}\) is seen to have the same sign as

\[\triangle_{n}^{\prime}=|e^{\eta_{i}x_{j}}-e^{\eta_{i}x_{j}-1}|,\ \ \ \ \ \ i,\,j=2,\ldots,n,\]

where \(\eta_{i}=\theta_{i}-\theta_{1}\). If this determinant is expanded by the first column, one obtains a sum of the form

\[a_{2}(e^{\eta_{2}x_{2}}-e^{\eta_{2}x_{1}})+\cdots+a_{n}(e^{\eta_ {k}x_{2}}-e^{\eta_{k}x_{1}}) = h(x_{2})-h(x_{1})\] \[= (x_{2}-x_{1})h^{\prime}(y_{2}),\]

where \(x_{1}<y_{2}<x_{2}\). Rewriting \(h^{\prime}(y_{2})\) as a determinant of which all columns but the first coincide with those of \(\triangle_{n}^{\prime}\) and proceeding in the same manner with the columns, one reduces the determinant to \(|e^{\eta_{1}y_{j}}|\), \(i\), \(j=2,\ldots,n\), which is positive by the induction hypothesis.]

**Problem 3.57**: STP\({}_{3}\). Let \(\theta\) and \(x\) be real-valued, and suppose that the probability densities \(p_{\theta}(x)\) are such that \(p_{\theta^{\prime}}(x)/p_{\theta}(x)\) is strictly increasing in \(x\) for \(\theta<\theta^{\prime}\). Then the following two conditions are equivalent: (a) For \(\theta_{1}<\theta_{2}<\theta_{3}\) and \(k_{1}\), \(k_{2}\), \(k_{3}>0\), let

\[g(x)=k_{1}p_{\theta_{1}}(x)-k_{2}p_{\theta_{2}}(x)+k_{3}p_{\theta_{3}}(x).\]

If \(g(x_{1})-g(x_{3})=0\), then the function \(g\) is positive outside the interval \((x_{1},x_{3})\) and negative inside. (b) The determinant \(\triangle_{3}\) given by (3.50) is positive for all \(\theta_{1}<\theta_{2}<\theta_{3}\), \(x_{1}<x_{2}<x_{3}\). [It follows from (a) that the equation \(g(x)=0\) has at most two solutions.]

[That (b) implies (a) can be seen for \(x_{1}\), \(<x_{2}<x_{3}\) by considering the determinant

\[\left|\begin{array}{ccc}g(x_{1})&g(x_{2})&g(x_{3})\\ p_{\theta_{2}}(x_{1})&p_{\theta_{2}}(x_{2})&p_{\theta_{2}}(x_{3})\\ p_{\theta_{3}}(x_{1})&p_{\theta_{3}}(x_{2})&p_{\theta_{3}}(x_{3})\end{array} \right|\]

Suppose conversely that (a) holds. Monotonicity of the likelihood ratios implies that the rank of \(\triangle_{3}\) is at least two, so that there exist constants \(k_{1}\), \(k_{2}\), \(k_{3}\) such that \(g(x_{1})=g(x_{3})=0\). That the \(k\)'s are positive follows again from the monotonicity of the likelihood ratios.]

**Problem 3.58**: _Extension of Theorem 3.7.1._ The conclusions of Theorem 3.7.1 remain valid if the density of a sufficient statistic \(T\) (which without loss of generality will be taken to be \(X\)), say \(p_{\theta}(x)\), is STP\({}_{3}\) and is continuous in \(x\) for each \(\theta\).

[The two properties of exponential families that are used in the proof of Theorem 3.7.1 are continuity in \(x\) and (a) of the preceding problem.]

**Problem 3.59**: For testing the hypothesis \(H^{\prime}:\theta_{1}\leq\theta\leq\theta_{2}(\theta_{1}\leq\theta_{2})\) against the alternatives \(\theta<\theta_{1}\) or \(\theta>\theta_{2}\), or the hypothesis \(\theta=\theta_{0}\) against the alternatives \(\theta\neq\theta_{0}\), in an exponential family or more generally in a family of distributions satisfying the assumptions of Problem 3.58, a UMP test does not exist.

[This follows from a consideration of the UMP tests for the one-sided hypotheses \(H_{1}:\theta\geq\theta_{1}\) and \(H_{2}:\theta\leq\theta_{2}\).]

**Problem 3.60**: Let \(f\), \(g\) be two probability densities with respect to \(\mu\). For testing the hypothesis \(H:\theta\leq\theta_{0}\) or \(\theta\geq\theta_{1}(0<\theta_{0}<\theta_{1}<1)\) against the alternatives \(\theta_{0}<\theta_{1}\), in the family \(\mathcal{P}=\{\theta f(x)+(1-\theta)g(x)\), \(0\leq\theta\leq 1\}\), the test \(\varphi(x)\equiv\alpha\) is UMP at level \(\alpha\).

**Problem 3.61**: Let the variables \(X_{i}(i=1,\ldots,s)\) be independently distributed with Poisson distribution \(P(\lambda_{i})\). For testing the hypothesis \(H:\sum\lambda_{j}\leq a\) (for example, that the combined radioactivity of a number of pieces of radioactive material does not exceed \(a\)), there exists a UMP test, which rejects when \(\sum X_{j}>C\).

[If the joint distribution of the \(X\)'s is factored into the marginal distribution of \(\sum X_{j}\) (Poisson with mean \(\sum\lambda_{j}\)) times the conditional distribution of the variables \(Y_{i}=X_{j}/\sum X_{j}\) given \(\sum X_{j}\) (multinomial with probabilities \(p_{i}=\lambda_{i}/\sum\lambda_{j}\)), the argument is analogous to that given in Example 3.8.1.]

**Problem 3.62**: _Confidence bounds for a median._ Let \(X_{1}\),..., \(X_{n}\) be a sample from a continuous cumulative distribution functions \(F\). Let \(\xi\) be the unique median of \(F\) if it exists, or more generally let \(\xi=\inf\{\xi^{\prime}:F(\xi^{\prime})\geq\frac{1}{2}\}\).

1. If the ordered \(X\)'s are \(X_{(1)}<\cdots<X_{(n)}\), a uniformly most accurate lower confidence bound for \(\xi\) is \(\underline{\xi}=X_{(k)}\) with probability \(\rho\), \(\underline{\xi}=X_{(k+1)}\) with probability \(1-\rho\), where \(k\) and \(\rho\) are determined by \[\rho\sum_{j=k}^{n}{n\choose j}\frac{1}{2^{n}}+(1-\rho)\sum_{j=k+1}^{n}{n \choose j}\frac{1}{2^{n}}=1-\alpha.\]
2. This bound has confidence coefficient \(1-\alpha\) for any median of \(F\).
3. Determine most accurate lower confidence bounds for the \(100\,p\)-percentile \(\xi\) of \(F\) defined by \(\xi=\inf\{\xi^{\prime}:F(\xi^{\prime})=p\}\).

[For fixed \(\xi_{0}\), the problem of testing \(H:\xi=\xi_{0}\) to against \(K:\xi>\xi_{0}\) is equivalent to testing \(H^{\prime}:p=\frac{1}{2}\) against \(K^{\prime}:p<\frac{1}{2}\).]

**Problem 3.63**: A _counterexample_. Typically, as \(\alpha\) varies, the most powerful level \(\alpha\) tests for testing a hypothesis \(H\) against a simple alternative are nested in the sense that the associated rejection regions, say \(R_{\alpha}\), satisfy \(R_{\alpha}\subseteq R_{\alpha^{\prime}}\), for any \(\alpha<\alpha^{\prime}\). The following example shows that this need not be satisfied for composite \(H\). Let \(X\) take on the values 1, 2, 3, 4 with probabilities under distributions \(P_{0}\), \(P_{1}\), \(Q\):

\[\begin{array}{c|cccc}&1&2&3&4\\ \hline P_{0}&\frac{2}{13}&\frac{4}{13}&\frac{3}{13}&\frac{4}{13}\\ P_{1}&\frac{4}{13}&\frac{2}{13}&\frac{1}{13}&\frac{6}{13}\\ Q&\frac{4}{13}&\frac{3}{13}&\frac{2}{13}&\frac{4}{13}\end{array}\]

Then the most powerful test for testing the hypothesis that the distribution of \(X\) is \(P_{0}\) or \(P_{1}\) against the alternative that it is \(Q\) rejects at level \(\alpha=\frac{5}{13}\) when \(X=1\) or 3, and at level \(\alpha=\frac{6}{13}\) when \(X=1\) or 2.

**Problem 3.64**: Let \(X\) and \(Y\) be the number of successes in two sets of \(n\) binomial trials with probabilities \(p_{1}\) and \(p_{2}\) of success.

1. The most powerful test of the hypothesis \(H:p_{2}\leq p_{1}\) against an alternative (\(p_{1}^{\prime}\), \(p_{2}^{\prime}\)) with \(p_{1}^{\prime}<p_{2}^{\prime}\) and \(p_{1}^{\prime}+p_{2}^{\prime}=1\) at level \(\alpha<\frac{1}{2}\) rejects when \(Y-X>C\) and with probability \(\gamma\) when \(Y-X=C\).
2. This test is not UMP against the alternatives \(p_{1}<p_{2}\).

[(i): Take the distribution \(\Lambda\) assigning probability 1 to the point \(p_{1}=p_{2}=\frac{1}{2}\) as an a priori distribution over \(H\). The most powerful test against (\(p_{1}^{\prime}\), \(p_{2}^{\prime}\)) is then the one proposed above. To see that \(\Lambda\) is least favorable, consider the probability of rejection \(\beta(p_{1}\), \(p_{2})\) for \(p_{1}=p_{2}=p\). By symmetry this is given by

\[2\beta(p,\,p)=P\{|Y-X|>C\}+\gamma P\{|Y-X|=C\}.\]

Let \(X_{i}\) be 1 or 0 as the \(i\)th trial in the first series is a success or failure, and let \(Y_{1}\), be defined analogously with respect to the second series. Then \(Y-X=\sum_{i-1}^{n}(Y_{i}-X_{i})\), and the fact that \(2\beta(p,\,p)\) attains its maximum for \(p=\frac{1}{2}\) can be proved by induction over \(n\).

(ii): Since \(\beta(p,\,p)<\alpha\) for \(p\neq 1\), the power \(\beta(p_{1},\,p_{2})\) is \(<\alpha\) for alternatives \(p_{1}<p_{2}\) sufficiently close to the line \(p_{1}=p_{2}\). That the test is not UMP now follows from a comparison with \(\phi(x,\,y)\equiv\alpha\).]

**Problem 3.65**: _Sufficient statistics with nuisance parameters_.

1. A statistic \(T\) is said to be _partially sufficient_ for \(\theta\) in the presence of a nuisance parameter \(\eta\) if the parameter space is the direct product of the set of possible \(\theta\)- and \(\eta\)-values, and if the following two conditions hold: (a) the conditional distribution given \(T=t\) depends only on \(\eta\); (b) the marginal distribution of \(T\) depends only on \(\theta\). If these conditions are satisfied, there exists a UMP test for testing the composite hypothesis \(H:\theta=\theta_{0}\) against the composite class of alternatives \(\theta=\theta_{1}\), which depends only on \(T\).

(ii) Part (i) provides an alternative proof that the test of Example 3.8.1 is UMP.

[Let \(\psi_{0}(t)\) be the most powerful level \(\alpha\) test for testing \(\theta_{0}\) against \(\theta_{1}\) that depends only on \(t\), let \(\phi(x)\) be any level-\(\alpha\) test, and let \(\psi(t)=E_{\eta_{1}}[\phi(X)\mid t]\). Since \(E_{\theta_{i}}\psi(T)=E_{\theta_{i},\eta_{1}}\phi(X)\), it follows that \(\psi\) is a level-\(\alpha\) test of \(H\) and its power, and therefore the power of \(\phi\), does not exceed the power of \(\psi_{0}\).]

_Note_. For further discussion of this and related concepts of partial sufficiency see Fraser (1956), Dawid (1975), Sprott (1975), Basu (1978), and Barndorff-Nielsen (1978).

_Section 3.9_

**Problem 3.66** Let \(X_{1}\),..., \(X_{m}\) and \(Y_{1}\),..., \(Y_{n}\) be independent samples from \(N(\xi\), 1) and \(N(\eta\), 1), and consider the hypothesis \(H:\eta\leq\xi\) against \(K:\eta>\xi\). There exists a UMP test, and it rejects the hypothesis when \(\bar{Y}-\bar{X}\) is too large.

[If \(\xi_{1}<\eta_{1}\) is a particular alternative, the distribution assigning probability 1 to the point \(\eta=\xi=(m\xi_{1}+n\eta_{1})/(m+n)\) is least favorable.]

**Problem 3.67** Let \(X_{1}\),..., \(X_{m}\); \(Y_{1}\),..., \(Y_{n}\) be independently, normally distributed with means \(\xi\) and \(\eta\), and variances a \(\sigma^{2}\) and \(\tau^{2}\) respectively, and consider the hypothesis \(H:\tau\leq\sigma\) a against \(K:\sigma<\tau\).

(i) If \(\xi\) and \(\eta\) are known, there exists a UMP test given by the rejection region \(\sum(Y_{j}-\eta)^{2}/\sum(X_{i}-\xi)^{2}\geq C\).

(ii) No UMP test exists when \(\xi\) and \(\eta\) are unknown.

**Problem 3.68** Suppose \(X\) is a \(k\times 1\) random vector with \(E(|X|^{2})<\infty\) and covariance matrix \(\Sigma\). Let \(A\) be an \(m\times k\) (nonrandom) matrix and let \(Y=AX\). Show \(Y\) has mean vector \(AE(X)\) and covariance matrix \(A\Sigma A^{\top}\).

**Problem 3.69** Suppose \((X_{1}\),..., \(X_{k})\) has the multivariate normal distribution with unknown mean vector \(\xi=(\xi_{1},\ldots,\xi_{k})\) and known covariance matrix \(\Sigma\). Suppose \(X_{1}\) is independent of \((X_{2}\),..., \(X_{k})\). Show that \(X_{1}\) is partially sufficient for \(\xi_{1}\) in the sense of Problem 3.65. Provide an alternative argument for Case 2 of Example 3.9.2.

**Problem 3.70** In Example 3.9.2, Case 2, verify the claim for the least favorable distribution.

**Problem 3.71** In Example 3.9.3, provide the details for Cases 3 and 4.

### Notes

Hypothesis testing developed gradually, with early instances frequently being rather vague statements of the significance or nonsignificance of a set of observations. Isolated applications are found in the eighteenth century [Arbuthnot (1710), Daniel Bernoulli (1734), and Laplace (1773), for example] and centuries earlier in the Royal Mint's Trial of the Pyx [discussed by Stigler (1977)]. They became more frequent in the nineteenth century in the writings of such authors as Gavarret (1840), Lexis (1875, 1877), and Edgeworth (1885). A new stage began with the work of Karl Pearson, particularly his \(\chi^{2}\) paper of 1900, followed in the decade 1915-1925 by Fisher's normal theory and \(\chi^{2}\) tests. Fisher presented this work systematically in his enormously influential book _Statistical Methods for Research Workers_ (1925b).

The first authors to recognize that the rational choice of a test must involve consideration not only of the hypothesis but also of the alternatives against which it is being tested were Neyman and Pearson (1928). They introduced the distinction between errors of the first and second kind, and thereby motivated their proposal of the likelihood ratio criterion as a general method of test construction. These considerations were carried to their logical conclusion by Neyman and Pearson in their paper of 1933, in which they developed the theory of UMP tests. Accounts of their collaboration can be found in Pearson's recollections (1966), and in the biography of Neyman by Reid (1982).

The Neyman-Pearson Lemma has been generalized in many directions, including the results in Sections 3.6, 3.8, and 3.9. Dantzig and Wald (1951) give necessary conditions including those of Theorem 3.6.1, for a critical function which maximizes an integral subject to a number of integral side conditions, to satisfy (3.30). The role of the Neyman-Pearson Lemma in hypothesis testing is surveyed in Lehmann (1985a).

An extension to a selection problem, proposed by Birnbaum and Chapman (1950), is sketched in Problem 3.51. Further developments in this area are reviewed in Gibbons (1986, 1988). Grenander (1981) applies the fundamental lemma to problems in stochastic processes.

Lemmas 3.4.1, 3.4.2, and 3.7.1 are due to Lehmann (1961).

Complete class results for simple null hypothesis testing problems are obtained in Brown and Marden (1989).

The earliest example of confidence intervals appears to occur in the work of Laplace (1812), who points out how an (approximate) probability statement concerning the difference between an observed frequency and a binomial probability \(p\) can be inverted to obtain an associated interval for \(p\). Other examples can be found in the works of Gauss (1816), Fourier (1826), and Lexis (1875). However, in all these cases, although the statements made are formally correct, the authors appear to consider the parameter as the variable which with the stated probability falls in the fixed confidence interval. The proper interpretation seems to have been pointed out for the first time by E. B. Wilson (1927). About the same time two examples of exact confidence statements were given by Working and Hotelling (1929) and Hotelling (1931).

A general method for obtaining exact confidence bounds for a real-valued parameter was proposed by Fisher (1930), who however later disavowed this interpretation of his work. For a discussion of Fisher's controversial concept of fiducial probability, see Section 5.7. At about the same time,16 a completely general theory of confidence statements was developed by Neyman and shown by him to be intimately related to the theory of hypothesis testing. A detailed account of this work, which underlies the treatment given here, was published by Neyman in his papers of 1937 and 1938.

Footnote 16: Cf. Neyman (1941b).

The calculation of \(p\)-values was the standard approach to hypothesis testing throughout the nineteenth century and continues to be widely used today. For various questions of interpretation, extensions, and critiques, see Cox (1977), Berger and Sellke (1987), Marden (1991), Hwang et al. (1992), Lehmann (1993), Robert (1994), Berger et al. (1994), Meng (1994), Blyth and Staudte (1995, 1997), Liu and Singh (1997), Sackrowitz and Samuel-Cahn (1999), Marden (2000), Sellke et al. (2001), and Berger (2003).

Extensions of \(p\)-values to hypotheses with nuisance parameters are discussed by Berger and Boos (1994) and Bayarri and Berger (2000), and the large-sample behavior of \(p\)-values in Lambert and Hall (1982) and Robins et al. (2000). An optimality theory in terms of \(p\)-values is sketched by Schweder (1988), and \(p\)-values for the simultaneous testing of several hypotheses is treated by Schweder and Spjotvoll (1982), Westfall and Young (1993), and by Dudoit et al. (2003).

An important use of \(p\)-values occurs in meta-analysis when one is dealing with the combination of results from independent experiments. The early literature on this topic is reviewed in Hedges and Olkin (1985, Chapter 3). Additional references are Marden (1982b, 1985), Scholz (1982), and a review article by Becker (1997). Associated confidence intervals are proposed by Littell and Louv (1981).

## Chapter 4 Unbiasedness: Theory and First Applications

### 4.1 Unbiasedness for Hypothesis Testing

A simple condition that one may wish to impose on tests of the hypothesis \(H:\theta\in\Omega_{H}\) against the composite class of alternatives \(K:\theta\in\Omega_{K}\) is that for no alternative in \(K\) should the probability of rejection be less than the size of the test. Unless this condition is satisfied, there will exist alternatives under which acceptance of the hypothesis is more likely than in some cases in which the hypothesis is true. A test \(\phi\) for which the above condition holds, that is, for which the power function \(\beta_{\phi}(\theta)=E_{\theta}\phi(X)\) satisfies

\[\begin{array}{l}\beta_{\phi}(\theta)\leq\alpha\ \text{if}\ \ \theta\in\Omega_{H},\\ \beta_{\phi}(\theta)\geq\alpha\ \text{if}\ \ \theta\in\Omega_{K}\end{array} \tag{4.1}\]

is said to be _unbiased_. For an appropriate loss function this was seen in Chapter 1 to be a particular case of the general definition of unbiasedness given there. Whenever a UMP test exists, it is unbiased, since its power cannot fall below that of the test \(\phi(x)\equiv\alpha\).

For a large class of problems for which a UMP test does not exist, there does exist a UMP unbiased test (which we sometimes abbreviate as UMPU). This is the case in particular for certain hypotheses of the form \(\theta\leq\theta_{0}\) or \(\theta=\theta_{0}\), where the distribution of the random observables depends on other parameters besides \(\theta\).

When \(\beta_{\phi}(\theta)\) is a continuous function of \(\theta\), unbiasedness implies

\[\beta_{\phi}(\theta)=\alpha\ \ \ \text{for all}\ \ \ \theta\ \text{in}\ \omega, \tag{4.2}\]

where \(\omega\) is the common boundary of \(\Omega_{H}\) and \(\Omega_{K}\), that is, the set of points \(\theta\) that are points or limit points of both \(\Omega_{H}\) and \(\Omega_{K}\). Tests satisfying this condition are said to be similar _on the boundary_ (of \(H\) and \(K\)). Since it is more convenient to work with (4.2) than with (4.1), the following lemma plays an important role in the determination of UMP unbiased tests.

**Lemma 4.1.1**: _If the distributions \(P_{\theta}\) are such that the power function of every test is continuous, and if \(\phi_{0}\) is UMP among all tests satisfying (4.2) and is a level-\(\alpha\) test of \(H\) then \(\phi_{0}\) is UMP unbiased._

Proof. The class of tests satisfying (4.2) contains the class of unbiased tests, and hence \(\phi_{0}\) is uniformly at least as powerful as any unbiased test. On the other hand, \(\phi_{0}\) is unbiased, since it is uniformly at least as powerful as \(\phi(x)\equiv\alpha\).

### One-Parameter Exponential Families

Let \(\theta\) be a real parameter, and \(X=(X_{1},\ldots,X_{n})\) a random vector with probability density (with respect to some measure \(\mu\))

\[p_{\theta}(x)=C(\theta)e^{\theta T(x)}h(x).\]

It was seen in Chapter 3 that a UMP test exists when the hypothesis \(H\) and the class K of alternatives are given by (i) \(H:\theta\leq\theta_{0}\), \(K:\theta>\theta_{0}\) (Corollary 3.4.1) and (ii) \(H:\theta\leq\theta_{1}\) or \(\theta\geq\theta_{2}\) (\(\theta_{1}<\theta_{2}\)), \(K:\theta_{1}<\theta<\theta_{2}\) (Theorem 3.7.1), but not for (iii) \(H:\theta_{1}\leq\theta\leq\theta_{2}\), \(K:\theta<\theta_{1}\) or \(\theta>\theta_{2}\). We shall now show that in case (iii) there does exist a UMP unbiased test given by

\[\phi(x)=\left\{\begin{array}{ll}1&\mbox{when}\quad T(x)<C_{1}\mbox{ or }>C_{2},\\ \gamma_{i}\mbox{ when}\quad T(x)=C_{i},\quad i=1,2,\\ 0&\mbox{when}\quad C_{1}<T(x)<C_{2},\end{array}\right. \tag{4.3}\]

where the \(C\)'s and \(\gamma\)'s are determined by

\[E_{\theta_{1}}\phi(X)=E_{\theta_{2}}\phi(X)=\alpha. \tag{4.4}\]

The power function \(E_{\theta}\phi(X)\) is continuous by Theorem 2.7.1, so that Lemma 4.1.1 is applicable. The set \(\omega\) consists of the two points \(\theta_{1}\) and \(\theta_{2}\), and we therefore consider first the problem of maximizing \(E_{\theta^{\prime}}\phi(X)\) for some \(\theta^{\prime}\) outside the interval [\(\theta_{1}\), \(\theta_{2}\)], subject to (4.4). If this problem is restated in terms of \(1-\phi(x)\), it follows from part (ii) of Theorem 3.7.1 that its solution is given by (4.3) and (4.4). This test is therefore UMP among those satisfying (4.4), and hence UMP unbiased by Lemma 4.1.1. It further follows from part (iii) of the theorem that the power function of the test has a minimum at a point between \(\theta_{1}\) and \(\theta_{2}\), and is strictly increasing as \(\theta\) tends away from this minimum in either direction.

A closely related problem is that of testing (iv) \(H:\theta=\theta_{0}\) against the alternatives \(\theta\neq\theta_{0}\). For this there also exists a UMP unbiased test given by (4.3), but the constants are now determined by

\[E_{\theta_{0}}[\phi(X)]=\alpha \tag{4.5}\]and

\[E_{\theta_{0}}[T(X)\phi(X)]=E_{\theta_{0}}[T(X)]\alpha. \tag{4.6}\]

To see this, let \(\theta^{\prime}\) be any particular alternative, and restrict attention to the sufficient statistic \(T\), the distribution of which by Lemma 2.7.2 is of the form

\[dP_{\theta}(t)=C(\theta)e^{\theta t}\,d\nu(t).\]

Unbiasedness of a test \(\psi(t)\) implies (4.5) with \(\phi(x)=\psi[T(x)]\); also that the power function \(\beta(\theta)=E_{\theta}[\psi(T)]\) must have a minimum at \(\theta=\theta_{0}\). By Theorem 2.7.1, the function \(\beta(\theta)\) is differentiable, and the derivative can be computed by differentiating \(E_{\theta}\psi(T)\) under the expectation sign, so that for all tests \(\psi(t)\)

\[\beta^{\prime}(\theta)=E_{\theta}[T\psi(T)]+\frac{C^{\prime}(\theta)}{C(\theta )}E_{\theta}[\psi(T)].\]

For \(\psi(t)\equiv\alpha\), this equation becomes

\[0=E_{\theta}(T)+\frac{C^{\prime}(\theta)}{C(\theta)}.\]

Substituting this in the expression for \(\beta^{\prime}(\theta)\) gives

\[\beta^{\prime}(\theta)=E_{\theta}[T\psi(T)]-E_{\theta}(T)E_{\theta}[\psi(T)],\]

and hence unbiasedness implies (4.6) in addition to (4.5).

Let \(M\) be the set of points (\(E_{\theta_{0}}[\psi(T)]\), \(E_{\theta_{0}}[T\psi(T)]\)) as \(\psi\) ranges over the totality of critical functions. Then \(M\) is convex and contains all points \((u,uE_{\theta_{0}}(T))\) with \(0<u<1\). It also contains points \((\alpha,u_{2})\) with \(u_{2}>\alpha E_{\theta_{0}}(T)\). This follows from the fact that there exist tests with \(E_{\theta_{0}}[\psi(T)]=\alpha\) and \(\beta^{\prime}(\theta_{0})>0\) (see Problem 3.50). Since similarly \(M\) contains points \((\alpha,u_{1})\) with \(u_{1}<\alpha E_{\theta_{0}}(T)\), the point \((\alpha,\alpha E_{\theta_{0}}(T))\) is an inner point of \(M\). Therefore, by Theorem 3.6.1(iv), there exist constants \(k_{1}\), \(k_{2}\) and a test \(\psi(t)\) satisfying (4.5) and (4.6) with \(\phi(x)=\psi[T(x)]\), such that \(\psi(t)=1\) when

\[C(\theta_{0})(k_{1}+k_{2}t)e^{\theta_{0}t}\,<C(\theta^{\prime})e^{\theta^{ \prime}t}\]

and therefore when

\[a_{1}+a_{2}t\,<e^{bt}.\]

This region is either one-sided or the outside of an interval. By Theorem 3.4.1, a one-sided test has a strictly monotone power function and therefore cannot satisfy (4.6). Thus \(\psi(t)\) is \(1\) when \(t<C_{1}\) or \(>C_{2}\), and the most powerful test subject to (4.5) and (4.6) is given by (4.3). This test is unbiased, as is seen by comparing it with \(\phi(x)\equiv\alpha\). It is then also UMP unbiased, since the class of tests satisfying (4.5) and (4.6) includes the class of unbiased tests.

A simplification of this test is possible if for \(\theta=\theta_{0}\) the distribution of \(T\) is symmetric about some point \(a\), that is, if \(P_{\theta_{0}}\{T<a-u\}=P_{\theta_{0}}\{T>a+u\}\) for all real \(u\). Any test which is symmetric about \(a\) and satisfies (4.5) must also satisfy (4.6), since \(E_{\theta_{0}}[T\psi(T)]=E_{\theta_{0}}[(T-a)\psi(T)]+aE_{\theta_{0}}\psi(T)=a \alpha=E_{\theta_{0}}(T)\alpha\). The \(C\)'s and \(\gamma\)'s are therefore determined by

\[P_{\theta_{0}}\{T<C_{1}\}+\gamma_{1}P_{\theta_{0}}\{T=C_{1}\}= \tfrac{\alpha}{2},\] \[C_{2}=2a-C_{1},\quad\gamma_{2}=\gamma_{1}.\]

The above tests of the hypotheses \(\theta_{1}\leq\theta\leq\theta_{2}\) and \(\theta=\theta_{0}\) are _strictly unbiased_ in the sense that the power is \(>\alpha\) for all alternatives \(\theta\). For the first of these tests, given by (4.3) and (4.4), strict unbiasedness is an immediate consequence of Theorem 3.7.1(iii). This states in fact that the power of the test has a minimum at a point \(\theta_{0}\) between \(\theta_{1}\) and \(\theta_{2}\) and increases strictly as \(\theta\) tends away from \(\theta_{0}\) in either direction. The second of the tests, determined by (4.3), (4.5), and (4.6), has a continuous power function with a minimum of \(\alpha\) at \(\theta=\theta_{0}\). Thus there exist \(\theta_{1}<\theta_{0}<\theta_{2}\) such that \(\beta(\theta_{1})=\beta(\theta_{2})=c\) where \(\alpha\leq c<1\). The test therefore coincides with the UMP unbiased level-\(c\) test of the hypothesis \(\theta_{1}\leq\theta\leq\theta_{2}\), and the power increases strictly as \(\theta\) moves away from \(\theta_{0}\) in either direction. This proves the desired result.

**Example 4.2.1**: **(Binomial)** Let \(X\) be the number of successes in \(n\) binomial trials with probability \(p\) of success. A theory to be tested assigns to \(p\) the value \(p_{0}\), so that one wishes to test the hypothesis \(H:p=p_{0}\). When rejecting \(H\) one will usually wish to state also whether \(p\) appears to be less or greater than \(p_{0}\). If, however, the conclusion that \(p\neq p_{0}\) in any case requires further investigation, the preliminary decision is essentially between the two possibilities that the data do or do not contradict the hypothesis \(p=p_{0}\). The formulation of the problem as one of hypothesis testing may then be appropriate.

The UMP unbiased test of \(H\) is given by (4.3) with \(T(X)=X\). Condition (4.5) becomes

\[\sum_{x=C_{1}+1}^{C_{2}-1}\binom{n}{x}p_{0}^{x}q_{0}^{n-x}+\sum_{i=1}^{2}(1- \gamma_{i})\binom{n}{C_{i}}p_{0}^{C_{i}}q_{0}^{n-C_{i}}=1-\alpha,\]

and the left-hand side of this can be obtained from tables of the individual probabilities and cumulative distribution function of \(X\). Condition (4.6) with the help of the identity

\[x\binom{n}{x}p_{0}^{x}q_{0}^{n-x}=np_{0}\binom{n-1}{x-1}p_{0}^{x-1}q_{0}^{(n-1 )-(x-1)}\]

reduces to \[\sum_{x=C_{1}+1}^{C_{2}-1}\binom{n-1}{x-1}p_{0}^{x-1}q_{0}^{(n-1)-(x-1)}\] \[\qquad+\sum_{i=1}^{2}(1-\gamma_{i})\binom{n-1}{C_{i}-1}p_{0}^{C_{i }-1}q_{0}^{(n-1)-(C_{i}-1)}=1-\alpha\]

the left-hand side of which can be computed from the binomial tables.

For sample sizes which are not too small, and values of \(p_{0}\) which are not too close to 0 or 1, the distribution of \(X\) is therefore approximately symmetric. In this case, the much simpler "equal-tails" test, for which the \(C\)'s and \(\gamma\)'s are determined by

\[\sum_{x=0}^{C_{1}-1}\binom{n}{x}p_{0}^{x}q_{0}^{(n-x)}+\gamma_{1} \binom{n}{C_{1}}p_{0}^{C_{1}}q_{0}^{n-C_{1}}\] \[\qquad=\gamma_{2}\binom{n}{C_{2}}p_{0}^{C_{2}}q_{0}^{n-C_{2}}+ \sum_{x=C_{2}+1}^{n}\binom{n}{x}p_{0}^{x}q_{0}^{n-x}=\frac{\alpha}{2}\]

is approximately unbiased, and constitutes a reasonable approximation to the unbiased test. Note, however, that this approximation requires large sample sizes when \(p_{0}\) is close to 0 or 1; in this connection, see Example 5.7.2 which discusses the corresponding problem of confidence intervals for \(p\). The literature on this and other approximations to the binomial distribution is reviewed in Johnson, Kotz, and Kemp (1992). See also the related discussion in Example 5.7.2.

**Example 4.2.2**: **(Normal variance)** Let \(X=(X_{1},\ldots,X_{n})\) be a sample from a normal distribution with mean 0 and variance \(\sigma^{2}\), so that the density of the \(X\)'s is

\[\left(\frac{1}{\sqrt{2\pi}\sigma}\right)\exp\left(-\frac{1}{2\pi\sigma^{2}} \sum x_{i}^{2}\right).\]

Then \(T(X)=\sum X_{i}^{2}\) is sufficient for \(\sigma^{2}\), and has probability density \((1/\sigma^{2})f_{n}(y/\sigma^{2})\), where

\[f_{n}(y)=\frac{1}{2^{n/2}\Gamma(n/2)}y^{(n/2)-1}e^{(y/2)},\qquad y>0,\]

is the density of a \(\chi^{2}\)-distribution with \(n\) degrees of freedom. For varying \(\sigma\), these distributions form an exponential family, which arises also in problems of life testing (see Problem 2.15), and concerning normally distributed variables with unknown mean and variance (Section 5.3). The acceptance region of the UMP unbiased test of the hypothesis \(H:\sigma=\sigma_{0}\) is

\[C_{1}\leq\sum\frac{x_{i}^{2}}{\sigma_{0}^{2}}\leq C_{2}\]with

\[\int_{C_{1}}^{C_{2}}f_{n}(y)\,dy=1-\alpha\]

and

\[\int_{C_{1}}^{C_{2}}yf_{n}(y)\,dy=\frac{(1-\alpha)E_{\sigma_{0}}(\sum X_{i}^{2}) }{\sigma_{0}^{2}}=n(1-\alpha).\]

For the determination of the constants from tables of the \(\chi^{2}\)-distribution, it is convenient to use the identity

\[yf_{n}(y)=nf_{n+2}(y),\]

to rewrite the second condition as

\[\int_{C_{1}}^{C_{2}}f_{n+2}(y)\,dy=1-\alpha.\]

Alternatively, one can integrate \(\int_{C_{1}}^{C_{2}}f_{n}(y)\,dy\) by parts to reduce the second condition to

\[C_{1}^{n/2}e^{-C_{1}/2}=C_{2}^{n/2}e^{-C_{2}/2}.\]

[For tables giving \(C_{1}\) and \(C_{2}\) see Pachares (1961).] Actually, unless \(n\) is very small or \(\sigma_{0}\) very close to 0 or \(\infty\), the equal-tails test given by

\[\int_{0}^{C_{1}}f_{n}(y)\,dy=\int_{C_{2}}^{\infty}f_{n}(y)\,dy=\frac{\alpha}{2}\]

is a good approximation to the unbiased test. This follows from the fact that \(T\), suitably normalized, tends to be normally and hence symmetrically distributed for large \(n\).

UMP unbiased tests of the hypotheses (iii) \(H:\theta_{1}\leq\theta\leq\theta_{2}\) and (iv) \(H:\theta=\theta_{0}\) against two-sided alternatives exist not only when the family \(p_{\theta}(x)\) is exponential but also more generally when it is strictly totally positive (STP\({}_{\infty}\)). A proof of (iv) in this case is given in Brown, Johnstone, and MacGibbon (1981); the proof of (iii) follows from Problem 3.58.

### Similarity and Completeness

In many important testing problems, the hypothesis concerns a single real-valued parameter, but the distribution of the observable random variables depends in addition on certain nuisance parameters. For a large class of such problems a UMP unbiased test exists and can be found through the method indicated by Lemma 4.1.1. This requires the characterization of the tests \(\phi\) which satisfy

\[E_{\theta}\phi(X)=\alpha\]

for all distributions of \(X\) belonging to a given family \(\mathcal{P}^{X}=\{P_{\theta},\,\theta\in\omega\}\). Such tests are called _similar_ with respect to \(\mathcal{P}^{X}\) or \(\omega\), since if \(\phi\) is nonrandomized with critical region \(S\), the latter is "similar to the sample space" \(\mathcal{X}\) in that both probabilities \(P_{\theta}\{X\in S\}\) and \(P_{\theta}\{X\in\mathcal{X}\}\) are independent of \(\theta\in\omega\).

Let \(T\) be a sufficient statistic for \(\mathcal{P}^{X}\), and let \(\mathcal{P}^{T}\) denote the family \(\{P_{\theta}^{T},\,\theta\in\omega\}\) of distributions of \(T\) as \(\theta\) ranges over \(\omega\). Then any test satisfying1

Footnote 1: A statement is said to hold a.e. \(\mathcal{P}\) if it holds except on a set \(N\) with \(P(N)=0\) for all \(P\in\mathcal{P}\).

\[E[\phi(X)|t]=\alpha\quad\quad\text{a.e.}\,\,\mathcal{P}^{T} \tag{4.7}\]

is similar with respect to \(\mathcal{P}^{X}\), since then

\[E_{\theta}[\phi(X)]=E_{\theta}\{E[\phi(X)|T]\}=\alpha\quad\quad\text{for all} \quad\theta\in\omega.\]

A test satisfying (4.7) is said to have _Neyman structure_ with respect to \(T\). It is characterized by the fact that the conditional probability of rejection is \(\alpha\) on each of the surfaces \(T=t\). Since the distribution on each such surface is independent of \(\theta\) for \(\theta\in\omega\), condition (4.7) essentially reduces the problem to that of testing a simple hypothesis for each value of \(t\). It is frequently easy to obtain a most powerful test among those having Neyman structure, by solving the optimum problem on each surface separately. The resulting test is then most powerful among all similar tests provided every similar test has Neyman structure. A condition for this to be the case can be given in terms of the following definition.

A family \(\mathcal{P}\) of probability distributions \(P\) is _complete_ if

\[E_{P}[f(X)]=0\quad\quad\text{for all}\quad P\in\mathcal{P} \tag{4.8}\]

implies

\[f(x)=0\quad\quad\text{a.e.}\,\,\mathcal{P}. \tag{4.9}\]

In applications, \(\mathcal{P}\) will be the family of distributions of a sufficient statistic.

**Example 4.3.1**: Consider \(n\) independent trials with probability \(p\) of success, and let \(X_{i}\) be 1 or 0 as the \(i\)th trial is a success or failure. Then \(T=X_{1}+\cdots+X_{n}\) is a sufficient statistic for \(p\), and the family of its possible distributions is \(\mathcal{P}=\{b(p,n),\,0<p\leq 1\}\). For this family (4.8) implies that

\[\sum_{t=0}^{n}f(t)\binom{n}{t}\rho^{t}=0\quad\quad\text{for all}\quad 0< \rho<\infty,\]where \(\rho=p/(1-p)\). The left-hand side is a polynomial in \(\rho\), all the coefficients of which must be zero. Hence \(f(t)=0\) for \(t=0\),..., \(n\) and the binomial family of distributions of \(T\) is complete.

**Example 4.3.2**: Let \(X_{1},\ldots,X_{n}\) be a sample from the uniform distribution \(U(0,\theta)\), \(0<\theta<\infty\). Then \(T=\max(X_{1},\ldots,X_{n})\) is a sufficient statistic for \(\theta\), and (4.8) becomes

\[\int f(t)\,dP_{\theta}^{T}(t)=n\theta^{-n}\int_{0}^{\theta}f(t)\cdot t^{n-1}\, dt=0\quad\quad\mbox{for all}\quad\theta.\]

Let \(f(t)=f^{+}(t)-f^{-}(t)\) where \(f^{+}\) and \(f^{-}\) denote the positive and negative parts of \(f\) respectively. Then

\[v^{+}(A)=\int_{A}f^{+}(t)t^{n-1}\,dt\mbox{ and }v^{-}(A)=\int_{A}f^{-}(t)t^{n-1 }\,dt\]

are two measures over the Borel sets on \((0,\infty)\), which agree for all intervals and hence for all \(A\). This implies \(f^{+}(t)=f^{-}(t)\) except possibly on a set of Lebesgue measure zero, and hence \(f(t)=0\) a.e. \(\mathcal{P}^{T}\).

**Example 4.3.3**: Let \(X_{1},\ldots,X_{m}\); \(Y_{1},\ldots,Y_{n}\) be independently normally distributed as \(N(\xi,\sigma^{2})\) and \(N(\xi,\tau^{2})\), respectively. Then the joint density of the variables is

\[C(\xi,\sigma,\tau)\exp\left(-\frac{1}{2\sigma^{2}}\sum x_{i}^{2}+\frac{\xi}{ \sigma^{2}}\sum x_{i}-\frac{1}{2\tau^{2}}\sum y_{j}^{2}+\frac{\xi}{\tau^{2}} \sum y_{j}\right).\]

The statistic

\[T=\left(\sum X_{i},\sum X_{i}^{2},\sum Y_{j},\sum Y_{j}^{2}\right)\]

is sufficient; it is, however, not complete, since \(E(\sum Y_{j}/n-\sum X_{i}/m)\) is identically zero. If the \(Y\)'s are instead distributed with a mean \(E(Y)=\eta\) which varies independently of \(\xi\), the set of possible values of the parameters \(\theta_{1}=-1/2\sigma^{2}\), \(\theta_{2}=\xi/\sigma^{2}\), \(\theta_{3}=-1/2\tau^{2}\), \(\theta_{4}=\eta/\tau^{2}\) contains a four-dimensional rectangle, and it follows from Theorem 4.3.1 that \(\mathcal{P}^{T}\) is complete.

Completeness of a large class of families of distributions including that of Example 4.3.1 is covered by the following theorem.

**Theorem 4.3.1**: _Let \(X\) be a random vector with probability distribution_

\[d\,P_{\theta}(x)=C(\theta)\exp\left[\,\sum_{j=1}^{k}\theta_{j}\,T_{j}(x)\, \right]\,d\mu(x),\]

_and let \(\mathcal{P}^{T}\) be the family of distributions of \(T=(T_{1}(X),\ldots,T_{k}(X))\) as \(\theta\) ranges over the set \(\omega\). Then \(\mathcal{P}^{T}\) is complete provided \(\omega\) contains a \(k\)-dimensional rectangle._Proof. By making a translation of the parameter space one can assume without loss of generality that \(\omega\) contains the rectangle

\[I=\left\{(\theta_{1},\ldots,\theta_{k}):-a\leq\theta_{j}\leq a,\,j=1,\ldots,k \right\}.\]

Let \(f(t)=f^{+}(t)-f^{-}(t)\) be such that

\[E_{\theta}f(T)=0\quad\mbox{ for all }\ \ \theta\in\omega.\]

Then for all \(\theta\in I\), if \(\nu\) denotes the measure induced in \(T\)-space by the measure \(\mu\),

\[\int e^{\sum\theta_{j}t_{j}}f^{+}(t)\,d\nu(t)=\int e^{\sum\theta_{j}t_{j}}f^{-} (t)\,d\nu(t)\]

and hence in particular

\[\int f^{+}(t)\,d\nu(t)=\int f^{-}(t)\,d\nu(t).\]

Dividing \(f\) by a constant, one can take the common value of these two integrals to be 1, so that

\[dP^{+}(t)=f^{+}(t)\,d\nu(t)\ \ \mbox{ and }\ \ d\,P^{-}(t)=f^{-}(t)\,d\nu(t)\]

are probability measures, and

\[\int e^{\sum\theta_{j}t_{j}}\,dP^{+}(t)=\int e^{\sum\theta_{j}t_{j}}\,d\,P^{-} (t)\]

for all \(\theta\) in \(I\). Changing the point of view, consider these integrals now as functions of the complex variables \(\theta_{j}=\xi_{j}+i\eta_{j}\), \(j=1,\ldots,k\). For any fixed \(\theta_{1},\ldots,\theta_{j-1}\), \(\theta_{j+1}\), \(\ldots\), \(\theta_{k}\) with real parts strictly between \(-a\) and \(+a\), they are by Theorem 2.7.1 analytic functions of \(\theta_{j}\) in the strip \(R_{j}:-a<\xi_{j}<a\), \(-\infty<\eta_{j}<\infty\) of the complex plane. For \(\theta_{2}\), \(\ldots\), \(\theta_{k}\) fixed, real, and between \(-a\) and \(a\), equality of the integrals holds on the line segment \(\{(\xi_{1},\eta_{1}):-a<\xi_{1}<a\), \(\eta_{1}=0\}\) and can therefore be extended to the strip \(R_{1}\), in which the integrals are analytic. By induction the equality can be extended to the complex region \(\{(\theta_{1},\ldots,\theta_{k}):(\xi_{j},\eta_{j})\in R_{j}\) for \(j=1,\ldots,k\}\). It follows in particular that for all real \((\eta_{1},\ldots,\eta_{k})\)

\[\int e^{i\sum\eta_{j}t_{j}}\,dP^{+}(t)=\int e^{i\sum\eta_{j}t_{j}}\,dP^{-}(t).\]

These integrals are the characteristic functions of the distributions \(P^{+}\) and \(P^{-}\), respectively, and by the uniqueness theorem for characteristic functions,2 the two distributions \(P^{+}\) and \(P^{-}\) coincide. From the definition of these distributions it then follows that \(f^{+}(t)=f^{-}(t)\) a.e. \(\nu\), and hence that \(f(t)=0\) a.e. \(\mathcal{P}^{T}\), as was to be proved.

**Example 4.3.4**: **(Nonparametric completeness.)** Let \(X_{1},\ldots,X_{N}\) be independently and identically distributed with cumulative distribution function \(F\in\mathcal{F}\), where \(\mathcal{F}\) is the family of all absolutely continuous distributions. Then the set of order statistics \(T(X)=(X_{(1)},\ldots,X_{(N)})\) was shown to be sufficient for \(\mathcal{F}\) in Section 2.6. We shall now prove it to be complete. Since, by Example 2.4.1, \(T^{\prime}(X)=(\sum X_{i},\sum X_{i}^{2},\ldots,\sum X_{i}^{N})\) is equivalent to \(T(X)\) in the sense that both induce the same subfield of the sample space, \(T^{\prime}(X)\) is also sufficient and is complete if and only if \(T(X)\) is complete. To prove the completeness of \(T^{\prime}(X)\) and thereby that of \(T(X)\), consider the family of densities

\[f(X)=C(\theta_{1},\ldots,\theta_{N})\exp(-x^{2N}+\theta_{1}x+\cdots+\theta_{N }x^{N}),\]

where \(C\) is a normalizing constant. These densities are defined for all values of the \(\theta\)'s since the integral of the exponential is finite, and their distributions belong to \(\mathcal{F}\). The density of a sample of size \(N\) is

\[C^{N}\exp\left(-\sum x_{j}^{2N}+\theta_{1}\sum x_{j}+\ldots+\theta_{N}\sum x_{ j}^{N}\right)\]

and these densities constitute an exponential family \(\mathcal{F}_{0}\). By Theorem 4.3.1, \(T^{\prime}(X)\) is complete for \(\mathcal{F}_{0}\) and hence also for \(\mathcal{F}\), as was to be proved.

The same method of proof establishes also the following more general result. Let \(X_{ij}\), \(j=1,\ldots,N_{i}\), \(i=1,\ldots,c\), be independently distributed with absolutely continuous distributions \(F_{i}\), and let \(X_{i}^{(1)}<\cdots<X_{i}^{(N_{i})}\) denote the \(N_{i}\) observations \(X_{i1},\ldots,X_{iN_{i}}\) arranged in increasing order. Then the set of order statistics

\[(X_{1}^{(1)},\ldots,X_{1}^{(N_{1})},\ldots,X_{c}^{(1)},\ldots,X_{c}^{(N_{c})})\]

is sufficient and complete for the family of distributions obtained by letting \(F_{1},\ldots,F_{c}\) range over all distributions of \(\mathcal{F}\). Here completeness is proved by considering the subfamily \(\mathcal{F}_{0}\) of \(\mathcal{F}\) in which the distributions \(F_{i}\) have densities of the form

\[f_{i}(x)=C_{i}\left(\theta_{i1},\ldots,\theta_{iN_{i}}\right)\exp\left(-x^{2N _{i}}+\theta_{i1}x+\ldots+\theta_{iN_{i}}x^{N_{i}}\right).\]

The result remains true if \(\mathcal{F}\) is replaced by the family \(F_{1}\) of continuous distributions. For a proof see Problem 4.13 or Bell, Blackwell, and Breiman (1960). For related results, see Mandelbaum and Ruschendorf (1987) and Mattner (1996).

For the present purpose the slightly weaker property of bounded completeness is appropriate, a family \(\mathcal{P}\) of probability distributions being _boundedly complete_ if for all bounded functions \(f\), (4.8) implies (4.9). If \(\mathcal{P}\) is complete it is a fortiori boundedly complete. An example in which \(\mathcal{P}\) is boundedly complete but not complete is given in Problem 4.12. For additional examples, see Hoeffding (1977), Bar-Lev and Plachky (1989) and Mattner (1993).

**Theorem 4.3.2**: _Let \(X\) be a random variable with distribution \(P\in{\cal P}\), and let \(T\) be a sufficient statistic for \({\cal P}\). Then a necessary and sufficient condition for all similar tests to have Neyman structure with respect to \(T\) is that the family \({\cal P}^{T}\) of distributions of \(T\) is boundedly complete._

Proof. Suppose first that \({\cal P}^{T}\) is boundedly complete, and let \(\phi(X)\) be similar with respect to \({\cal P}\). Then

\[E[\phi(X)-\alpha]=0\quad\mbox{ for all }\quad P\in{\cal P}\]

and hence, if \(\psi(t)\) denotes the conditional expectation of \(\phi(X)-\alpha\) given \(t\),

\[E\psi(T)=0\quad\mbox{ for all }\quad P^{T}\in{\cal P}^{T}.\]

Since \(\psi(t)\) can be taken to be bounded by Lemma 2.4.1, it follows from the bounded completeness of \({\cal P}^{T}\) that \(\psi(t)=0\) and hence \(E[\phi(X)|t]=\alpha\) a.e. \({\cal P}^{T}\), as was to be proved.

Conversely suppose that \({\cal P}^{T}\) is not boundedly complete. Then there exists a function \(f\) such that \(|f(t)|\leq M\) for some \(M\), that \(Ef(T)=0\) for all \(P^{T}\in{\cal P}^{T}\) and \(f(T)\neq 0\) with positive probability for some \(P^{T}\in{\cal P}^{T}\). Let \(\phi(t)=cf(t)+\alpha\), where \(c=\min(\alpha,1-\alpha)/M\). Then \(\phi\) is a critical function, since \(0\leq\phi(t)\leq 1\), and it is a similar test, since \(E\phi(T)=\alpha\) for all \(P^{T}\in{\cal P}^{T}\). But \(\phi\) does not have Neyman structure, since \(\phi(T)\neq\alpha\) with positive probability for at least some distribution in \({\cal P}^{T}\).

### UMP Unbiased Tests for Multiparameter Exponential Families

An important class of hypotheses concerns a real-valued parameter in an exponential family, with the remaining parameters occurring as unspecified nuisance parameters. In many of these cases, UMP unbiased tests exist and can be constructed by means of the theory of the preceding section.

Let \(X\) be distributed according to

\[d\,P^{X}_{\theta,\vartheta}(x)=C(\theta,\vartheta)\exp\left[\theta U(X)+\sum_{ i=1}^{k}\vartheta_{i}\,T_{i}(x)\right]d\mu(x),\quad\quad(\theta,\vartheta)\in \Omega, \tag{4.10}\]

and let \(\vartheta=(\vartheta_{1},\ldots,\vartheta_{k})\) and \(T=(T_{1},\ldots,T_{k})\). We shall consider the problems3 of testing the following hypotheses \(H_{j}\) against the alternatives \(K_{j}\), \(j=1,\ldots,4\):\[\begin{array}{ll}H_{1}:\theta\leq\theta_{0}&K_{1}:\theta>\theta_{0}\\ H_{2}:\theta\leq\theta_{1}\text{ or }\theta\geq\theta_{2}&K_{2}:\theta_{1}<\theta< \theta_{2}\\ H_{3}:\theta_{1}\leq\theta\leq\theta_{2}&K_{3}:\theta<\theta_{1}\text{ or } \theta>\theta_{2}\\ H_{4}:\theta=\theta_{0}&K_{4}:\theta\neq\theta_{0}.\end{array}\]

We shall assume that the parameter space \(\Omega\) is convex, and that it is not contained in a linear space of dimension \(<k+1\). This is the case in particular when \(\Omega\) is the natural parameter space of the exponential family. We shall also assume that there are points in \(\Omega\) with \(\theta\) both \(<\) and \(>\theta_{0}\), \(\theta_{1}\), and \(\theta_{2}\), respectively.

Attention can be restricted to the sufficient statistics (\(U\), \(T\)) which have the joint distribution

\[d\,P_{\theta,\vartheta}^{U,T}(u,t)=C(\theta,\vartheta)\exp\left(\theta U+\sum _{i=1}^{k}\vartheta_{i}t_{i}\right)\,d\nu(u,t),\qquad(\theta,\vartheta)\in \Omega. \tag{4.11}\]

When \(T=t\) is given, \(U\) is the only remaining variable and, by Lemma 2.7.2, the conditional distribution of \(U\) given \(t\) constitutes an exponential family

\[d\,P_{\theta}^{U|t}(u)=C_{t}(\theta)e^{\theta u}\,d\nu_{t}(u).\]

In this conditional situation there exists by Corollary 3.4.1 a UMP test for testing \(H_{1}\), with critical function \(\phi_{1}\), satisfying

\[\phi(u,t)=\left\{\begin{array}{ll}1&\text{ when }u>C_{0}(t),\\ \gamma_{0}(t)&\text{ when }u=C_{0}(t),\\ 0&\text{ when }u<C_{0}(t),\end{array}\right. \tag{4.12}\]

where the functions \(C_{0}\) and \(\gamma_{0}\) are determined by

\[E_{\theta_{0}}[\phi_{1}(U,T)|t]=\alpha\qquad\text{for all }t. \tag{4.13}\]

For testing \(H_{2}\) in the conditional family there exists by Theorem 3.7.1 a UMP test with critical function

\[\phi(u,t)=\left\{\begin{array}{ll}1&\text{ when }C_{1}(t)<u<C_{2}(t),\\ \gamma_{i}(t)&\text{ when }u=C_{i}(t),\quad i=1,2,\\ 0&\text{ when }u<C_{1}(t)\text{ or }>C_{2}(t),\end{array}\right. \tag{4.14}\]

where the \(C\)'s and \(\gamma\)'s are determined by

\[E_{\theta_{1}}[\phi_{2}(U,T)|t]=E_{\theta_{2}}[\phi_{2}(U,T)|t]=\alpha. \tag{4.15}\]

Consider next the test \(\phi_{3}\) satisfying \[\phi(u,t)=\left\{\begin{array}{ll}1&\mbox{ when }u<C_{1}(t)\mbox{ or }>C_{2}(t),\\ \gamma_{i}(t)&\mbox{ when }u=C_{i}(t),\quad i=1,2,\\ 0&\mbox{ when }C_{1}(t)<u<C_{2}(t),\end{array}\right. \tag{4.16}\]

with the \(C\)'s and \(\gamma\)'s determined by

\[E_{\theta_{1}}[\phi_{3}(U,T)|t]=E_{\theta_{2}}[\phi_{3}(U,T)|t]=\alpha. \tag{4.17}\]

When \(T=t\) is given, this is (by Section 4.2 of the present chapter) UMP unbiased for testing \(H_{3}\) and UMP among all tests satisfying (4.17).

Finally, let \(\phi_{4}\) be a critical function satisfying (4.16) with the \(C\)'s and \(\gamma\)'s determined by

\[E_{\theta_{0}}[\phi_{4}(U,T)|t]=\alpha \tag{4.18}\]

and

\[E_{\theta_{0}}[U\phi_{4}(U,T)|t]=\alpha E_{\theta_{0}}[U|t]. \tag{4.19}\]

Then given \(T=t\), it follows again from the results of Section 4.2 that \(\phi_{4}\) is UMP unbiased for testing \(H_{4}\) and UMP among all tests satisfying (4.18) and (4.19).

So far, the critical functions \(\phi_{j}\) have been considered as conditional tests given \(T=t\). Reinterpreting them now as tests depending on \(U\) and \(T\) for the hypotheses concerning the distribution of \(X\) (or the joint distribution of \(U\) and \(T\)) as originally stated, we have the following main theorem.4

Footnote 4: A somewhat different asymptotic optimality property of these tests is established by Michel (1979).

**Theorem 4.4.1**: _Define the critical functions \(\phi_{1}\) by (4.12) and (4.13); \(\phi_{2}\) by (4.14) and (4.15); \(\phi_{3}\) by (4.16) and (4.17); \(\phi_{4}\) by (4.16), (4.18), and (4.19). These constitute UMP unbiased level-\(\alpha\) tests for testing the hypotheses \(H_{1},\ldots,H_{4}\) respectively when the joint distribution of \(U\) and \(T\) is given by (4.11)._

Proof. The statistic \(T\) is sufficient for \(\vartheta\) if \(\theta\) has any fixed value, and hence \(T\) is sufficient for each

\[\omega_{j}=\{(\theta,\vartheta):(\theta,\vartheta)\in\Omega,\theta=\theta_{j} \},\qquad j=0,1,2.\]

By Lemma 2.7.2, the associated family of distributions of \(T\) is given by

\[dP^{T}_{\theta_{j},\vartheta}(t)=C(\theta_{j},\vartheta)\exp\left(\sum_{i=1}^{ k}\vartheta_{i}t_{i}\right)\,d\nu_{\theta_{j}}(t),\quad(\theta_{j},\vartheta) \in\omega_{j}\qquad j=0,1,2.\]

Since by assumption \(\Omega\) is convex and of dimension \(k+1\) and contains points on both sides of \(\theta=\theta_{j}\), it follows that \(\omega_{j}\) is convex and of dimension \(k\). Thus \(\omega_{j}\) contains a \(k\)-dimensional rectangle; by Theorem 4.3.1 the family \[\mathcal{P}_{j}^{T}=\left\{P_{\theta_{j},\vartheta}^{T}:(\theta,\vartheta)\in\omega_ {j}\right\}\]

is complete, and similarity of a test \(\phi\) on \(\omega_{j}\) implies

\[E_{\theta_{j}}[\phi(U,T)|t]=\alpha.\]

1. Consider first \(H_{1}\). By Theorem 2.7.1, the power function of all tests is continuous for an exponential family. It is therefore enough to prove \(\phi_{1}\) to be UMP among all tests that are similar on \(\omega_{0}\) (Lemma 4.1.1), and hence among those satisfying (4.13). On the other hand, the overall power of a test \(\phi\) against an alternative (\(\theta\), \(\vartheta\)) is \[E_{\theta,\vartheta}[\phi(U,T)]=\int\left[\int\phi(u,t)\,dP_{\theta}^{U|t}(u) \right]\,dP_{\theta,\vartheta}^{T}(t).\] (4.20)

One therefore maximizes the overall power by maximizing the power of the conditional test, given by the expression in brackets, separately for each \(t\). Since \(\phi_{1}\) has the property of maximizing the conditional power against any \(\theta>\theta_{0}\) subject to (4.13), this establishes the desired result.
2. The proof for \(H_{2}\) and \(H_{3}\) is completely analogous. By Lemma 4.1.1, it is enough to prove \(\phi_{2}\) and \(\phi_{3}\) to be UMP among all tests that are similar on both \(\omega_{1}\) and \(\omega_{2}\), and hence among all tests satisfying (4.15). For each \(t\), \(\phi_{2}\) and \(\phi_{3}\) maximize the conditional power for their respective problems subject to this condition and therefore also the unconditional power.
3. Unbiasedness of a test of \(H_{4}\) implies similarity on \(\omega_{0}\) and \[\frac{\partial}{\partial\theta}\left[E_{\theta,\vartheta}\phi(U,T)\right]=0 \quad\text{ on }\omega_{0}.\]

The differentiation on the left-hand side of this equation can be carried out under the expectation sign, and by the computation which earlier led to (4.6), the equation is seen to be equivalent to

\[E_{\theta,\vartheta}[U\phi(U,T)-\alpha U]=0\quad\text{ on }\omega_{0}.\]

Therefore, since \(\mathcal{P}_{0}^{T}\) is complete, unbiasedness implies (4.18) and (4.19). As in the preceding cases, the test, which in addition satisfies (4.16), is UMP among all tests satisfying these two conditions. That it is UMP unbiased now follows, as in the proof of Lemma 4.1.1, by comparison with the test \(\phi(u,t)\equiv\alpha\).
4. The functions \(\phi_{1},\ldots,\phi_{4}\) were obtained above for each fixed \(t\) as a function of \(u\). To complete the proof it is necessary to show that they are jointly measurable in \(u\) and \(t\), so that expectation (4.20) exists. We shall prove this here for the case of \(\phi_{1}\); the proof for the other cases is sketched in Problems 4.21 and 4.22. Toestablish the measurability of \(\phi_{1}\), one needs to show that the functions \(C_{0}(t)\) and \(\gamma_{0}(t)\) defined by (4.12) and (4.13) are \(t\)-measurable. Omitting the subscript \(0\), and denoting the conditional distribution function of \(U\) given \(T=t\) and for \(\theta=\theta_{0}\) by

\[F_{t}(u)=P_{\theta_{0}}\{U\leq u|t\},\]

one can rewrite (4.13) as

\[F_{t}(C)-\gamma[F_{t}(C)-F_{t}(C-0)]=1-\alpha.\]

Here \(C=C(t)\) is such that \(F_{t}(C-0)\leq 1-\alpha\leq F_{t}(C)\), and hence

\[C(t)=F_{t}^{-1}(1-\alpha),\]

where \(F_{t}^{-1}(y)=\inf\{u:F_{t}(u)\geq y\}\). It follows that \(C(t)\) and \(\gamma(t)\) will both be measurable provided \(F_{t}(u)\) and \(F_{t}(u-0)\) are jointly measurable in \(u\) and \(t\) and \(F_{t}^{-1}(1-\alpha)\) is measurable in \(t\).

For each fixed \(u\) the function \(F_{t}(u)\) is a measurable function of \(t\), and for each fixed \(t\) it is a cumulative distribution function and therefore in particular nondecreasing and continuous on the right. From the second property it follows that \(F_{t}(u)\geq c\) if and only if for each \(n\) there exists a rational number \(r\) such that \(u\leq r<u+1/n\) and \(F_{t}(r)\geq c\). Therefore, if the rationals are denoted by \(r_{1}\), \(r_{2}\),...,

\[\{(u,t):F_{t}(u)\geq c\}=\bigcap_{n}\bigcup_{i}\left\{(u,t):0\leq r_{i}-u<\frac {1}{n},\,F_{t}(r_{i})\geq c\right\}.\]

This shows that \(F_{t}(u)\) is jointly measurable in \(u\) and \(t\). The proof for \(F_{t}(u-0)\) is completely analogous. Since \(F_{t}^{-1}(y)\leq u\) if and only if \(F_{t}(u)\geq y\), \(F_{t}^{-1}(y)\) is \(t\)-measurable for any fixed \(y\) and this completes the proof.

The test \(\phi_{1}\) of the above theorem is also UMP unbiased if \(\Omega\) is replaced by the set \(\Omega^{\prime}=\Omega\cap\{(\theta,\,\vartheta):\theta\geq\theta_{0}\}\), and hence for testing \(H^{\prime}:\theta=\theta_{0}\) against \(\theta>\theta_{0}\). The assumption that \(\Omega\) should contain points with \(\theta<\theta_{0}\) was in fact used only to prove that the boundary set \(\omega_{0}\) contains a \(k\)-dimensional rectangle, and this remains valid if \(\Omega\) is replaced by \(\Omega^{\prime}\).

The remainder of this chapter as well as the next chapter will be concerned mainly with applications of the preceding theorem to various statistical problems. While this provides the most expeditious proof that the tests in all these cases are UMP unbiased, there is available also a variation of the approach, which is more elementary. The proof of Theorem 4.4.1 is quite elementary except for the following points: (i) the fact that the conditional distributions of \(U\) given \(T=t\) constitute an exponential family, (ii) that the family of distributions of \(T\) is complete, (iii) that the derivative of \(E_{\theta,\,\vartheta}\phi(U,\,T)\) exists and can be computed by differentiating under the expectation sign, and (iv) that the functions \(\phi_{1},\,\ldots,\phi_{4}\) are measurable. Instead of verifying (i) through (iv) in general, as was done in the above proof, it is possible in applicationsof the theorem to check these conditions directly for each specific problem, which in some cases is quite easy.

Through a transformation of parameters, Theorem 4.4.1 can be extended to cover hypotheses concerning parameters of the form

\[\theta^{*}=a_{0}\theta+\sum_{i=1}^{k}a_{i}\vartheta_{i},\ \ \ \ \ \ a_{0}\neq 0.\]

This transformation is formally given by the following lemma, the proof of which is immediate.

**Lemma 4.4.1**: _The exponential family of distributions (4.10) can also be written as_

\[dP^{X}_{\theta,\vartheta}=K\left(\theta^{*},\vartheta\right)\exp\left[\theta^{ *}U^{*}(x)+\sum\vartheta_{i}T^{*}_{i}(x)\right]d\mu(x),\]

_where_

\[U^{*}=\frac{U}{a_{0}},\ \ \ \ \ T^{*}_{i}=T_{i}-\frac{a_{i}}{a_{0}}U.\]

Application of Theorem 4.4.1 to the form of the distributions given in the lemma leads to UMP unbiased tests of the hypothesis \(H^{*}_{1}:\theta^{*}\leq\theta_{0}\) and the analogously defined hypotheses \(H^{*}_{2}\), \(H^{*}_{3}\), \(H^{*}_{4}\).

When testing one of the hypotheses \(H_{j}\) one is frequently interested in the power \(\beta(\theta^{\prime},\vartheta)\) of \(\phi_{j}\) against some alternative \(\theta^{\prime}\). As is indicated by the notation and is seen from (4.20), this power will usually depend on the unknown nuisance parameters \(\vartheta\). On the other hand, the power of the conditional test given \(T=t\),

\[\beta(\theta^{\prime}|t)=E_{\theta^{\prime}}[\phi(U,T)|t],\]

is independent of \(\vartheta\) and therefore has a known value.

The quantity \(\beta(\theta^{\prime}|t)\) can be interpreted in two ways: (i) It is the probability of rejecting \(H\) when \(T=t\). Once \(T\) has been observed to have the value \(t\), it may be felt, at least in certain problems, that this is a more appropriate expression of the power in the given situation than \(\beta(\theta^{\prime},\vartheta)\), which is obtained by averaging \(\beta(\theta^{\prime}|t)\) with respect to other values of \(t\) not relevant to the situation at hand. This argument leads to difficulties, since in many cases the conditioning could be carried even further and it is not clear where the process should stop. (ii) A more clear-cut interpretation is obtained by considering \(\beta(\theta^{\prime}|t)\) as an estimate of \(\beta(\theta^{\prime},\vartheta)\). Since

\[E_{\theta^{\prime},\vartheta}[\beta(\theta^{\prime}|T)]=\beta(\theta^{\prime}, \vartheta),\]

this estimate is unbiased in the sense of equation (1.11). It follows further from the theory of unbiased estimation and the completeness of the exponential family that among all unbiased estimates of \(\beta(\theta^{\prime},\vartheta)\) the present one has the smallest variance (see Lehmann and Casella (1998), Chapter 2).

Regardless of the interpretation, \(\beta(\theta^{\prime}|t)\) has the disadvantage compared with an unconditional power that it becomes available only after the observations have been taken. It therefore cannot be used to plan the experiment and in particular to determine the sample size, if this must be done prior to the experiment. On the other hand, a simple sequential procedure guaranteeing a specified power \(\beta\) against the alternatives \(\theta=\theta^{\prime}\) is obtained by continuing taking observations until the conditional power \(\beta(\theta^{\prime}|t)\) is \(\geq\beta\).

### Comparing Two Poisson or Binomial Populations

A problem arising in many different contexts is the comparison of two treatments or of one treatment with a control situation in which no treatment is applied. If the observations consist of the number of successes in a sequence of trials for each treatment for example the number of cures of a certain disease, the problem becomes that of testing the equality of two binomial probabilities. If the basic distributions are Poisson, for example in a comparison of the radioactivity of two substances, one will be testing the equality of two Poisson distributions.

When testing whether a treatment has a beneficial effect by comparing it with the control situation of no treatment, the problem is of the one-sided type. If \(\xi_{2}\) and \(\xi_{1}\) denote the parameter values when the treatment is or is not applied, the class of alternatives is \(K:\xi_{2}>\xi_{1}\). The hypothesis is \(\xi_{2}=\xi_{1}\) if it is known a priori that there is either no effect or a beneficial one; it is \(\xi_{2}\leq\xi_{1}\) if the possibility is admitted that the treatment may actually be harmful. Since the test is the same for the two hypotheses, the second somewhat safer hypothesis would seem preferable in most cases.

A one-sided formulation is sometimes appropriate also when a new treatment or process is being compared with a standard one, where the new treatment is of interest only if it presents an improvement. On the other hand, if the two treatments are on an equal footing, the hypothesis \(\xi_{2}=\xi_{1}\) of equality of two treatments is tested against the two-sided alternatives \(\xi_{2}\neq\xi_{1}\). The formulation of this problem as one of hypothesis testing is usually quite artificial, since in case of rejection of the hypothesis one will obviously wish to know which of the treatments is better.5 Such two-sided tests do, however, have important applications to the problem of obtaining confidence limits for the extent by which one treatment is better than the other. They also arise when the parameter \(\xi\) does not measure a treatment effect but refers to an auxiliary variable which one hopes can be ignored. For example, \(\xi_{1}\) and \(\xi_{2}\) may refer to the effect of two different hospitals in a medical investigation in which one would like to combine the patients into a single study group. (In this connection, see also Section 7.3.)

Footnote 5: The comparison of two treatments as a three-decision problem or as the simultaneous testing of two one-sided hypotheses is discussed and the literature reviewed in Shaffer (2002).

To apply Theorem 4.4.1 to this comparison problem it is necessary to express the distributions in an exponential form with \(\theta=f(\xi_{1},\xi_{2})\), for example \(\theta=\theta\)\(\xi_{2}-\xi_{1}\) or \(\xi_{2}/\xi_{1}\), such that the hypotheses of interest become equivalent to those of Theorem 4.4.1. In the present section the problem will be considered for Poisson and binomial distributions; the case of normal distributions will be taken up in Chapter 5.

We consider first the Poisson problem in which \(X\) and \(Y\) are independently distributed according to \(P(\lambda)\) and \(P(\mu)\), so that their joint distribution can be written as

\[P\{X=x,\,Y=y\}=\frac{e^{-(\lambda+\mu)}}{x!y!}\exp\left[y\log\frac{\mu}{ \lambda}+(x+y)\log\lambda\right].\]

By Theorem 4.4.1 there exist UMP unbiased tests of the four hypotheses \(H_{1},\,\ldots,\,H_{4}\) concerning the parameter \(\theta=\log(\mu/\lambda)\) or equivalently concerning the ratio \(\rho=\mu/\lambda\). This includes, in particular, the hypotheses \(\mu\leq\lambda\) (or \(\mu=\lambda\)) against the alternatives \(\mu>\lambda\), and \(\mu=\lambda\) against \(\mu\neq\lambda\). Comparing the distribution of \((X,\,Y)\) with (4.10), one has \(U=Y\) and \(T=X+Y\), and by Theorem 4.4.1 the tests are performed conditionally on the integer points of the line segment \(X+Y=t\) in the positive quadrant of the \((x,\,y)\)-plane. The conditional distribution of \(Y\) given \(X+Y=t\) is (Problem 2.14)

\[P\{Y=y|X+Y=t\}={t\choose y}\left(\frac{\mu}{\lambda+\mu}\right)^{y}\left( \frac{\lambda}{\lambda+\mu}\right)^{t-y},\qquad y=0,\,1,\ldots,\,t,\]

the binomial distribution corresponding to \(t\) trials and probability \(p=\mu/(\lambda+\mu)\) of success. The original hypotheses therefore reduce to the corresponding ones about the parameter \(p\) of a binomial distribution. The hypothesis \(H:\mu\leq a\lambda\), for example, becomes \(H:p\leq a/(a+1)\), which is rejected when \(Y\) is too large. The cutoff point depends of course, in addition to \(a\), also on \(t\). It can be determined from tables of the binomial, and for large \(t\) approximately from tables of the normal distribution.

In many applications the ratio \(\rho=\mu/\lambda\) is a reasonable measure of the extent to which the two Poisson populations differ, since the parameters \(\lambda\) and \(\mu\) measure the rates (in time or space) at which two Poisson processes produce the events in question. One might therefore hope that the power of the above tests depends only on this ratio, but this is not the case. On the contrary, for each fixed value of \(\rho\) corresponding to an alternative to the hypothesis being tested, the power \(\beta(\lambda,\,\mu)=\beta(\lambda,\,\rho\lambda)\) is an increasing function of \(\lambda\), which tends to 1 as \(\lambda\to\infty\) and to \(\alpha\) as \(\lambda\to 0\). To see this consider the power \(\beta(\rho|t)\) of the conditional test given \(t\). This is an increasing function of \(t\), since it is the power of the optimum test based on \(t\) binomial trials. The conditioning variable \(T\) has a Poisson distribution with parameter \(\lambda(1+\rho)\), and its distribution for varying \(\lambda\) forms an exponential family. It follows Lemma 3.4.2 that the overall power \(E[\beta(\rho|T)]\) is an increasing function of \(\lambda\). As \(\lambda\to 0\) or \(\infty\), \(T\) tends in probability to 0 or \(\infty\), and the power against a fixed alternative \(\rho\) tends to \(\alpha\) or 1.

The above test is also applicable to samples \(X_{1},\,\ldots,\,X_{m}\) and \(Y_{1},\,\ldots,\,Y_{n}\) from two Poisson distributions. The statistics \(X=\sum_{i=1}^{m}X_{i}\) and \(Y=\sum_{j=1}^{n}Y_{j}\) are then sufficient for \(\lambda\) and \(\mu\), and have Poisson distributions with parameters \(m\lambda\) and \(n\mu\) respectively. In planning an experiment one might wish to determine \(m=n\) so large that the test of, say, \(H:\rho\leq\rho_{0}\) has power against a specified alternative \(\rho_{1}\) greater than or equal to some preassigned \(\beta\). However, it follows from the discussion of the power function for \(n=1\), which applies equally to any other \(n\), that this cannot be achieved for any fixed \(n\), no matter how large. This is seen more directly by noting that as \(\lambda\to 0\), for both \(\rho=\rho_{0}\) and \(\rho=\rho_{1}\), the probability of the event \(X=Y=0\) tends to 1. Therefore, the power of any level-\(\alpha\) test against \(\rho=\rho_{1}\) and for varying \(\lambda\) cannot be bounded away from \(\alpha\). This difficulty can be overcome only by permitting observations to be taken sequentially. One can for example determine \(t_{0}\) so large that the test of the hypothesis \(p_{1}\leq\rho_{0}/(1+\rho_{0})\) on the basis of \(t_{0}\) binomial trials has power \(\geq\beta\) against the alternative \(p_{1}=\rho_{1}/(1+\rho_{1})\). By observing (\(X_{1}\), \(Y_{1}\)), (\(X_{2}\), \(Y_{2}\)),... and continuing until \(\sum(X_{i}+Y_{i})\geq t_{0}\), one obtains a test with power \(\geq\beta\) against all alternatives with \(\rho\geq\rho_{1}\).6

Footnote 6: A discussion of this and alternative procedures for achieving the same aim is given by Birnbaum (1954a).

The corresponding comparison of two binomial probabilities is quite similar. Let \(X\) and \(Y\) be independent binomial variables with joint distribution

\[P\{X=x,\,Y=y\} = {m\choose x}p_{1}^{x}q_{1}^{m-x}{n\choose y}p_{2}^{y}q_{2}^{n-y}\] \[= {m\choose x}{n\choose y}q_{1}^{m}q_{2}^{n}\exp\left[y\left(\log \frac{p_{2}}{q_{2}}-\log\frac{p_{1}}{q_{1}}\right)\right.\] \[\left.+(x+y)\log\frac{p_{1}}{q_{1}}\right].\]

The four hypotheses \(H_{1},\ldots,\,H_{4}\) can then be tested concerning the parameter

\[\theta=\log\left(\frac{p_{2}}{q_{2}}\bigg{/}\frac{p_{1}}{q_{1}}\right),\]

or equivalently concerning the _odds ratio_ (also called _cross-product ratio_)

\[\rho=\frac{p_{2}}{q_{2}}\bigg{/}\frac{p_{1}}{q_{1}}.\]

This includes in particular the problems of testing \(H_{1}^{\prime}:p_{2}\leq p_{1}\) against \(p_{2}>p_{1}\) and \(H_{4}^{\prime}:p_{2}=p_{1}\) against \(p_{2}\neq p_{1}\). As in the Poisson case, \(U=Y\) and \(T=X+Y\), and the test is carried out in terms of the conditional distribution of \(Y\) on the line segment \(X+Y=t\). This distribution is given by

\[P\{Y=y|X+Y=t\}=C_{t}(\rho){m\choose t-y}{n\choose y}\rho^{y},\ \ \ \ \ \ y=0,1,\ldots,t, \tag{4.21}\]

where

\[C_{t}(\rho)=\frac{1}{\sum_{y^{\prime}=0}^{t}{m\choose t-y^{\prime}}{n\choose y ^{\prime}}\rho^{y^{\prime}}}.\]In the particular case of the hypotheses \(H_{1}^{\prime}\) and \(H_{4}^{\prime}\), the boundary value \(\theta_{0}\) of (4.13), (4.18), and (4.19) is 0, and the corresponding value of \(\rho\) is \(\rho_{0}=1\). The conditional distribution then reduces to

\[P\{Y=y|X+Y=t\}=\frac{{m\choose t-y}{n\choose y}}{{m+n\choose t}},\]

which is the hypergeometric distribution.

Tables of critical values by Finney (1948) are reprinted in _Biometrika Tables for Statisticians_, Vol. 1, Table 38 and are extended in Finney, Latscha, Bennett, Hsu, and Horst (1963,1966). Somewhat different ranges are covered in Armsen (1955), and related charts are provided by Bross and Kasten (1957). Extensive tables of the hypergeometric distributions have been computed by Lieberman and Owen (1961). Various approximations are discussed in Johnson, Kotz and Kemp (1992, Section 6.5). Critical values can also be easily computed with built-in functions of statistical packages such as R.7

Footnote 7: This package can be downloaded for free from [http://cran.r-project.org/](http://cran.r-project.org/).

The UMP unbiased test of \(\rho_{1}=\rho_{2}\), which is based on the (conditional) hypergeometric distribution, requires randomization to obtain an exact conditional level \(\alpha\) for each \(t\) of the sufficient statistic \(T\). Since in practice randomization is usually unacceptable, the one-sided test is frequently performed by rejecting when \(Y\geq C(T)\), where \(C(t)\) is the smallest integer for which \(P\{Y\geq C(T)|T=t\}\leq\alpha\). This conservative test is called _Fisher's exact test_ [after the treatment given in Fisher (1934a)], since the probabilities are calculated from the exact hypergeometric rather than an approximate normal distribution. The resulting conditional levels (and hence the unconditional level) are often considerably smaller than \(\alpha\), and this results in a substantial loss of power. An approximate test whose overall level tends to be closer to \(\alpha\) is obtained by using the normal approximation to the hypergeometric distribution _without_ continuity correction. [For a comparison of this test with some competitors, see e.g., Garside and Mack (1976).] A nonrandomized test that provides a conservative overall level, but that is less conservative than the "exact" test, is described by Boschloo (1970) and by McDonald, Davis, and Milliken (1977). For surveys of the extensive literature on these and related aspects of \(2\times 2\) and more generally \(r\times c\) tables, see Agresti (1992, 2002), Sahai and Khurshid (1995) and Martin and Tapia (1998).

### Testing for Independence in a 2 \(\times\) 2 Table

Two characteristics \(A\) and \(B\), which each member of a population may or may not possess, are to be tested for independence. The probabilities or proportion of individuals possessing properties \(A\) and \(B\) are denoted by \(P(A)\) and \(P(B)\).

If \(P(A)\) and \(P(B)\) are unknown, a sample from one of the categories such as \(A\) does not provide a basis for distinguishing between the hypothesis and the alternatives. This follows from the fact that the number in the sample possessing characteristic \(B\) then constitutes a binomial variable with probability \(p(B|A)\), which is completely unknown both when the hypothesis is true and when it is false. The hypothesis can, however, be tested if samples are taken both from categories \(A\) and \(A^{c}\), the complement of \(A\), or both from \(B\) and \(B^{c}\). In the latter case, for example, if the sample sizes are \(m\) and \(n\), the numbers of cases possessing characteristic \(A\) in the two samples constitute independent variables with binomial distributions \(b(p_{1},m)\) and \(b(p_{2},n)\), respectively, where \(p_{1}=P(A|B)\) and \(p_{2}=P(A|B^{c})\). The hypothesis of independence of the two characteristics, \(P(A|B)=p(A)\), is then equivalent to the hypothesis \(p_{1}=p_{2}\) and the problem reduces to that treated in the preceding section.

Instead of selecting samples from two of the categories, it is frequently more convenient to take the sample at random from the population as a whole. The results of such a sample can be summarized in the following \(2\times 2\) contingency table, the entries of which give the numbers in the various categories:

\begin{tabular}{c|c c|c}  & \(A\) & \(A^{c}\) & \\ \hline \(B\) & \(X\) & \(X^{\prime}\) & \(M\) \\ \(B^{c}\) & \(Y\) & \(Y^{\prime}\) & \(N\) \\ \hline  & \(T\) & \(T^{\prime}\) & \(s\) \\ \end{tabular}

The joint distribution of the variables \(X\), \(X^{\prime}\), \(Y\), and \(Y^{\prime}\) is multinomial, and is given by

\[P\{X = x,\,X^{\prime}=x^{\prime},\,Y=y,\,Y^{\prime}=y^{\prime}\}\] \[=\frac{s!}{x!x^{\prime}!y!y^{\prime}!}p_{AB}^{x}p_{A^{c}B}^{x^{ \prime}}p_{AB^{c}}^{y}p_{AB^{c}}^{y^{\prime}}\] \[=\frac{s!}{x!x^{\prime}!y!y^{\prime}!}p_{A^{c}B^{c}}^{s}\exp\left( x\log\,\frac{p_{AB}}{p_{A^{c}B^{c}}}+x^{\prime}\log\,\frac{p_{A^{c}B}}{p_{A^{c} B^{c}}}+y\log\,\frac{p_{AB^{c}}}{p_{A^{c}B^{c}}}\right).\]

Lemma 4.4.1 and Theorem 4.4.1 are therefore applicable to any parameter of the form

\[\theta^{*}=a_{0}\log\,\frac{p_{AB}}{p_{A^{c}B^{c}}}+a_{1}\log\,\frac{p_{A^{c} B}}{p_{A^{c}B^{c}}}+a_{2}\log\,\frac{p_{AB^{c}}}{p_{A^{c}B^{c}}}.\]

Putting \(a_{1}=a_{2}=1\), \(a_{0}=-1\), \(\Delta=e^{\theta^{*}}=(p_{A^{c}B}p_{AB^{c}})/(p_{AB}p_{A^{c}B^{c}})\), and denoting the probabilities of \(A\) and \(B\) in the population by \(p_{A}=p_{AB}+p_{AB^{c}}\), \(p_{B}=p_{AB}+p_{A^{c}B}\), one finds\[p_{AB} = p_{A}p_{B}+\frac{1-\Delta}{\Delta}p_{A^{c}B}p_{AB^{c}},\] \[p_{A^{c}B} = p_{A^{c}}p_{B}+\frac{1-\Delta}{\Delta}p_{A^{c}B}p_{AB^{c}},\] \[p_{AB^{c}} = p_{A}p_{B^{c}}+\frac{1-\Delta}{\Delta}p_{A^{c}B}p_{AB^{c}},\] \[p_{A^{c}B^{c}} = p_{A^{c}}p_{B^{c}}+\frac{1-\Delta}{\Delta}p_{A^{c}B}p_{AB^{c}}.\]

Independence of \(A\) and \(B\) is therefore equivalent to \(\Delta=1\), and \(\Delta<1\) and \(\Delta>1\) correspond to positive and negative dependence respectively.8

Footnote 8: \(\Delta\) is equivalent to Yule’s measure of association which is \(Q=(1-\Delta)/(1+\Delta)\). For a discussion of this and related measures see Goodman and Kruskal (1954, 1959), Edwards (1963), Haberman (1982) and Agresti (2002).

The test of the hypothesis of independence, or any of the four hypotheses concerning \(\Delta\), is carried out in terms of the conditional distribution of \(X\) given \(X+X^{\prime}=m\), \(X+Y=t\). Instead of computing this distribution directly, consider first the conditional distribution subject only to the condition \(X+X^{\prime}=m\), and hence \(Y+Y^{\prime}=s-m=n\). This is seen to be

\[P\{X = x\,,\,Y=y|X+X^{\prime}=m\}\] \[=\binom{m}{x}\binom{n}{y}\left(\frac{p_{AB}}{p_{B}}\right)^{x} \left(\frac{p_{A^{c}B}}{p_{B}}\right)^{m-x}\left(\frac{p_{AB^{c}}}{p_{B^{c}}} \right)^{y}\left(\frac{p_{A^{c}B^{c}}}{p_{B^{c}}}\right)^{n-y},\]

which is the distribution of two independent binomial variables, the number of successes in \(m\) and \(n\) trials with probabilities \(p_{1}=p_{AB}/p_{B}\) and \(p_{2}=p_{AB^{c}}/p_{B^{c}}\). Actually, this is clear without computation, since we are now dealing with samples of fixed sizes \(m\) and \(n\) from the subpopulations \(B\) and \(B^{c}\) and the probabilities of \(A\) in these subpopulations is \(p_{1}\) and \(p_{2}\). If now the additional restriction \(X+Y=t\) is imposed, the conditional distribution of \(X\) subject to the two conditions \(X+X^{\prime}=m\) and \(X+Y=t\) is the same as that of \(X\) given \(X+Y=t\) in the case of two independent binomials considered in the previous section. It is therefore given by

\[P\{X=x|X+X^{\prime}=m\,,\,X+Y=t\}=C_{t}(\rho)\binom{m}{x}\binom{n}{t-x}\rho^{ t-x},\] \[x=0,\ldots,t,\]

that is, by (4.21) expressed in terms of \(x\) instead of \(y\). (Here the choice of \(X\) as testing variable is quite arbitrary; we could equally well again have chosen \(Y\).) For the parameter \(\rho\) one finds

\[\rho=\frac{p_{2}}{q_{2}}\Big{/}\frac{p_{1}}{q_{1}}=\frac{p_{A^{c}B}p_{AB^{c}}} {p_{AB}p_{A^{c}B^{c}}}=\Delta\,.\]From these considerations it follows that the conditional test given \(X\,+\,X^{\prime}=m\), \(X\,+\,Y=t\), for testing any of the hypotheses concerning \(\Delta\) is identical with the conditional test given \(X\,+\,Y=t\) of the same hypothesis concerning \(\rho=\Delta\) in the preceding section, in which \(X\,+\,X^{\prime}=m\) was given a priori. In particular, the conditional test for testing the hypothesis of independence \(\Delta=1\), Fisher's exact test, is the same as that of testing the equality of two binomial \(p\)'s and is therefore given in terms of the hypergeometric distribution.

At the beginning of the section it was pointed out that the hypothesis of independence can be tested on the basis of samples obtained in a number of different ways. Either samples of fixed size can be taken from \(A\) and \(A^{c}\) or from \(B\) and \(B^{c}\), or the sample can be selected at random from the population at large. Which of these designs is most efficient depends on the cost of sampling from the various categories and from the population at large, and also on the cost of performing the necessary classification of a selected individual with respect to the characteristics in question. Suppose, however, for a moment that these considerations are neglected and that the designs are compared solely in terms of the power that the resulting tests achieve against a common alternative. Then the following results9 can be shown to hold asymptotically as the total sample size \(s\) tends to infinity:

Footnote 9: These results were conjectured by Berkson and proved by Neyman in a course on \(\chi^{2}\).

1. If samples of size \(m\) and \(n\) (\(m\,+\,n=s\)) are taken from \(B\) and \(B^{c}\) or from \(A\) and \(A^{c}\), the best choice of \(m\) and \(n\) is \(m\,=\,n=s/2\).
2. It is better to select samples of equal size \(s/2\) from \(B\) and \(B^{c}\) than from \(A\) and \(A^{c}\) provided \(|\,p_{B}\,-\,\frac{1}{2}|\,>\,|\,p_{A}\,-\,\frac{1}{2}|\).
3. Selecting the sample at random from the population at large is worse than taking equal samples either from \(A\) and \(A^{c}\) or from \(B\) and \(B^{c}\).

These statements, which we shall not prove here, can be established by using the normal approximation for the distribution of the binomial variables \(X\) and \(Y\) when \(m\) and \(n\) are fixed, and by noting that under random sampling from the population at large, \(M/s\) and \(N/s\) tend in probability to \(p_{B}\) and \(p_{B^{c}}\), respectively.

### Alternative Models for 2 \(\times\) 2 Tables

Conditioning of the multinomial model for the 2 \(\times\) 2 table on the row (or column) totals was seen in the last section to lead to the two binomial model of Section 4.5. Similarly, the multinomial model itself can be obtained as a conditional model in some situations in which not only the marginal totals \(M\), \(N\), \(T\), and \(T^{\prime}\) are random but the total sample size \(s\) is also a random variable. Suppose that the occurrence of events (e.g., patients presenting themselves for treatment) is observed over a given period of time, and that the events belonging to each of the categories \(AB\), \(A^{c}\,B\), \(AB^{c}\), and \(A^{c}\,B^{c}\) are governed by independent Poisson processes, so that by (1.2) the numbers \(X\), \(X^{\prime}\)\(Y\), \(Y^{\prime}\) are independent Poisson variables with expectations \(\lambda_{AB}\), \(\lambda_{A^{c}B}\), \(\lambda_{AB^{c}}\), \(\lambda_{A^{c}B^{c}}\), and hence \(s\) is a Poisson variable with expectation \(\lambda=\lambda_{AB}+\lambda_{A^{c}B}+\lambda_{AB^{c}}+\lambda_{A^{c}B^{c}}\).

It may then be of interest to compare the ratio \(\lambda_{AB}/\lambda_{A^{c}B}\) with \(\lambda_{AB^{c}}/\lambda_{A^{c}B^{c}}\) and in particular to test the hypothesis \(H:\lambda_{AB}/\lambda_{A^{c}B}\leq\lambda_{AB^{c}}/\lambda_{A^{c}B^{c}}\). The joint distribution of \(X\),\(X^{\prime}\),\(Y\),\(Y^{\prime}\) constitutes a four-parameter exponential family, which can be written as

\[P(X = x,\,X^{\prime}=x^{\prime},\,Y=y,\,Y^{\prime}=y^{\prime})\] \[= \frac{1}{x!x^{\prime}!y!y^{\prime}!}\exp\left\{x\log\left(\frac{ \lambda_{AB}\lambda_{A^{c}B^{c}}}{\lambda_{AB^{c}}\lambda_{A^{c}B}}\right)+(x ^{\prime}+x)\log\lambda_{A^{c}B}\right.\] \[\left.+(y+x)\log\lambda_{AB^{c}}+(y^{\prime}-x)\log\lambda_{A^{c} B^{c}}\right\}\.\]

Thus, UMP unbiased tests exist of the usual one- and two-sided hypotheses concerning the parameter \(\theta=\lambda_{AB}\lambda_{A^{c}B^{c}}/\lambda_{A^{c}B}\lambda_{AB^{c}}\). These are carried out in terms of the conditional distribution of \(X\) given

\[X^{\prime}+X=m,\ \ \ \ \ Y+X=t,\ \ \ \ \ \ X+X^{\prime}+Y+Y^{\prime}=s,\]

where the last condition follows from the fact that given the first two it is equivalent to \(Y^{\prime}-X=s-t-m\). By Problem 2.14, the conditional distribution of \(X\), \(X^{\prime}\), \(Y\) given \(X+X^{\prime}+Y+Y^{\prime}=s\) is the multinomial distribution of Section 4.6 with

\[p_{AB}=\frac{\lambda_{AB}}{\lambda},\ \ \ p_{A^{c}B}=\frac{\lambda_{A^{c}B}}{ \lambda},\ \ \ p_{AB^{c}}=\frac{\lambda_{AB^{c}}}{\lambda},\ \ \ p_{A^{c}B^{c}}= \frac{\lambda_{A^{c}B^{c}}}{\lambda}.\]

The tests therefore reduce to those derived in Section 4.6.

The three models discussed so far involve different sampling schemes. However, frequently the subjects for study are not obtained by any sampling but are the only ones readily available to the experimenter. To create a probabilistic basis for a test in such situations, suppose that \(B\) and \(B^{c}\) are two treatments, either of which can be assigned to each subject, and that \(A\) and \(A^{c}\) denote success or failure (e.g., survival, relief of pain, etc.). The hypothesis of no difference in the effectiveness of the two treatments (i.e., independence of \(A\) and \(B\)) can then be tested by assigning the subjects to the treatments, say \(m\) to \(B\) and \(n\) to \(B^{c}\), _at random_, i.e., in such a way that all possible \(\binom{s}{m}\) assignments are equally likely. It is now this random assignment which takes the place of the sampling process in creating a probability model, thus making it possible to calculate significance.

Under the hypothesis \(H\) of no treatment difference, the success or failure of a subject is independent of the treatment to which it is assigned. If the numbers of subjects in categories \(A\) and \(A^{c}\) are \(t\) and \(t^{\prime}\) respectively (\(t+t^{\prime}=s\)), the values of \(t\) and \(t^{\prime}\) are therefore fixed, so that we are now dealing with a \(2\times 2\) table in which all four margins \(t\), \(t^{\prime}\), \(m\), \(n\) are fixed.

Then any one of the four cell counts \(X\), \(X^{\prime}\), \(Y\), \(Y^{\prime}\) determines the other three. Under \(H\), the distribution of \(Y\) is the hypergeometric distribution derived as the conditional null distribution of \(Y\) given \(X+Y=t\) at the end of Section 4.5. The hypothesis is rejected in favor of the alternative that treatment \(B^{c}\) enhances success if \(Y\) is sufficiently large. Although this is the natural test under the given circumstances, no optimum property can be claimed for it, since no clear alternative model to \(H\) has been formulated.10

Footnote 10: The one-sided test is of course UMP against the class of alternatives defined by the right side of (4.21), but no reasonable assumptions have been proposed that would lead to this class. For suggestions of a different kind of alternative see Gokhale and Johnson (1978).

Consider finally the situation in which the subjects are again given rather than sampled, but \(B\) and \(B^{c}\) are attributes (for example, male or female, smoker or nonsmoker) which cannot be assigned to the subjects at will. Then there exists no stochastic basis for answering the question whether observed differences in the rates \(X/M\) and \(Y/N\) correspond to differences between \(B\) and \(B^{c}\), or whether they are accidental. An approach to the testing of such hypotheses in a nonstochastic setting has been proposed by Freedman and Lane (1982).

The various models for the \(2\times 2\) table discussed in Sections 4.6 and 4.7 may be characterized by indicating which elements are random and which fixed:

1. All margins and \(s\) random (Poisson).
2. All margins are random, \(s\) fixed (multinomial sampling).
3. One set of margins random, the other (and then a fortiori \(s\)) fixed (binomial sampling).
4. All margins fixed. Sampling replaced by random assignment of subjects to treatments.
5. All aspects fixed; no element of randomness.

In the first three cases there exist UMP unbiased one- and two-sided tests of the hypothesis of independence of \(A\) and \(B\). These tests are carried out by conditioning on the values of all elements in (i)-(iii) that are random, so that in the conditional model all margins are fixed. The remaining randomness in the table can be described by any one of the four cell entries; once it is known, the others are determined by the margins. The distribution of such an entry under \(H\) has the hypergeometric distribution given at the end of Section 4.5.

The models (i)-(iii) have a common feature. The subjects under observation have been obtained by sampling from a population, and the inference corresponding to acceptance or rejection of \(H\) refers to that population. This is not true in cases (iv) and (v).

In (iv) the subjects are given, and a probabilistic basis is created by assigning them at random, \(m\) to \(B\) and \(n\) to \(\vec{B}\). Under the hypothesis \(H\) of no treatment difference, the four margins are fixed without any conditioning, and the four cell entries are again determined by any one of them, which under \(H\) has the same hypergeometric distribution as before. The present situation differs from the earlier three in that the inference cannot be extended beyond the subjects at hand.11

Footnote 11: For a more detailed treatment of the distinction between population models [such as (i)–(iii)] and randomization models [such as (iv)], see Lehmann (1998).

The situation (v) is outside the scope of this book, since it contains no basis for the type of probability calculations considered here. Problems of this kind are however of great importance, since they arise in many observational (as opposed to experimental) studies. For a related discussion, see Finch (1979).

### Some Three-Factor Contingency Tables

When an association between \(A\) and \(B\) exists in a \(2\times 2\) table, it does not follow that one of the factors has a causal influence on the other. Instead, the explanation may, for example, be in the fact that both factors are causally affected by a third factor \(C\). If \(C\) has \(K\) possible outcomes \(C_{1},\ldots,C_{K}\), one may then be faced with the apparently paradoxical situation (known as Simpson's paradox) that \(A\) and \(B\) are independent under each of the conditions \(C_{k}\) (\(k=1,\ldots,K\)) but exhibit positive (or negative) association when the tables are aggregated over \(C\), that is, when the \(K\) separate \(2\times 2\) tables are combined into a single one showing the total counts of the four categories. [An interesting example is discussed in Agresti (2002).] In order to determine whether the association of \(A\) and \(B\) in the aggregated table is indeed "spurious", one would test the hypothesis, (which arises also in other contexts) that \(A\) and \(B\) are conditionally independent given \(C_{k}\) for all \(k=1,\ldots,K\), against the alternative that there is an association for at least some \(k\).

Let \(X_{k}\), \(X^{\prime}_{k}\), \(Y_{k}\), \(Y^{\prime}_{k}\) denote the counts in the \(4K\) cells of the \(2\times 2\times K\) table which extends the \(2\times 2\) table of Section 4.6 to the present case.

Again, several sampling schemes are possible. Consider first a random sample of size \(s\) from the population at large. The joint distribution of the \(4K\) cell counts then is multinomial with probabilities \(p_{ABC_{k}}\), \(p_{\tilde{A}BC_{k}}\), \(p_{\tilde{A}\tilde{B}C_{k}}\), \(p_{\tilde{A}\tilde{B}C_{k}}\) for the outcomes indicated by the subscripts. If \(\Delta_{k}\) denotes the \(AB\) odds ratio for \(C_{k}\) defined by

\[\Delta_{k}=\frac{p_{\tilde{A}\tilde{B}C_{k}}p_{\tilde{A}BC_{k}}}{p_{ABC_{k}}p_ {\tilde{A}\tilde{B}C_{k}}}=\frac{p_{A\tilde{B}|C_{k}}p_{\tilde{A}\tilde{B}|C_ {k}}}{p_{AB|C_{k}}p_{\tilde{A}\tilde{B}|C_{k}}},\]

where \(p_{AB|C_{k}}\ldots\) denotes the conditional probability of the indicated event given \(C_{k}\), then the hypothesis to be tested is \(\Delta_{k}=1\) for all \(k\).

A second scheme takes samples of size \(s_{k}\) from \(C_{k}\) and classifies the subjects as \(AB\), \(\tilde{A}B\), \(A\tilde{B}\), or \(\tilde{A}\tilde{B}\). This is the case of \(K\) independent \(2\times 2\) tables, in which one is dealing with \(K\) quadrinomial distributions of the kind considered in the preceding sections. Since the \(k\)th of these distributions is also that of the same four outcomes in the first model conditionally given \(C_{k}\), we shall denote the probabilities of these outcomes in the present model again by \(p_{AB|C_{k}},\ldots\).

To motivate the next sampling scheme, suppose that \(A\) and \(\widetilde{A}\) represent success or failure of a medical treatment, \(\widetilde{B}\) and \(B\) that the treatment is applied or the subject is used as a control, and \(C_{k}\) the \(k\)th hospital taking part in this study. If samples of size \(n_{k}\) and \(m_{k}\) are obtained and are assigned to treatment and control respectively, we are dealing with \(K\) pairs of binomial distributions. Letting \(Y_{k}\) and \(X_{k}\) denote the number of successes obtained by the treatment subjects and controls in the \(k\)th hospital, the joint distribution of these variables by Section 4.5 is

\[\left[\prod\binom{m_{k}}{x_{k}}\binom{n_{k}}{y_{k}}q_{1k}^{m_{k}}q_{2k}^{n_{k} }\right]\exp\left(\sum y_{k}\log\Delta_{k}+\sum(x_{k}+y_{k})\log\frac{p_{1k}}{ q_{1k}}\right)\,\]

where \(p_{1k}\) and \(q_{1k}\), (\(p_{2k}\) and \(q_{2k}\)) denote the probabilities of success and failure under \(B\) (under \(\widetilde{B}\)).

The above three sampling schemes lead to \(2\times 2\times K\) tables in which respectively none, one, or two of the margins are fixed. Alternatively, in some situations a model may be appropriate in which the \(4K\) variables \(X_{k}\), \(X_{k}^{\prime}\), \(Y_{k}\), \(Y_{k}^{\prime}\) are independent Poisson with expectations \(\lambda_{ABC_{k}}\), \(\ldots\). In this case, the total sample size \(s\) is also random.

For a test of the hypothesis of conditional independence of \(A\) and \(B\) given \(C_{k}\) for all \(k\) (i.e., that \(\Delta_{1}=\cdots=\Delta_{k}=1\)), see Problem 14.69. Here we shall consider the problem under the simplifying assumption that the \(\Delta_{k}\) have a common value \(\Delta\), so that the hypothesis reduces to \(H:\Delta=1\). Applying Theorem 4.4.1 to the third model (\(K\) pairs of binomials) and assuming the alternatives to be \(\Delta>1\), we see that a UMP unbiased test exists and rejects \(H\) when \(\sum Y_{k}>C(X_{1}+Y_{1},\ldots,X_{K}+Y_{K})\), where \(C\) is determined so that the conditional probability of rejection, given that \(X_{k}+Y_{k}=t_{k}\), is \(\alpha\) for all \(k=1,\ldots,K\). It follows from Section 4.5 that the conditional joint distribution of the \(Y_{k}\) under \(H\) is

\[P_{H}[Y_{1} = y_{1},\ldots,Y_{K}=y_{K}|X_{k}+Y_{k}=t_{k},k=1,\ldots,K]\] \[= \prod\binom{\binom{m_{k}}{t_{k}-y_{k}}\binom{n_{k}}{y_{k}}}{ \binom{m_{k}+n_{k}}{t_{k}}}.\]

The conditional distribution of \(\sum Y_{k}\) can now be obtained by adding the probabilities over all \((y_{1},\ldots,y_{K})\) whose sum has a given value. Unless the numbers are very small, this is impractical and approximations must be used [see Cox (1966) and Gart (1970)].

The assumption \(H^{\prime}:\Delta_{1}=\cdots=\Delta_{K}=\Delta\) has a simple interpretation when the successes and failures of the binomial trials are obtained by dichotomizing underlying unobservable continuous response variables. In a single such trial, suppose the underlying variable is \(Z\) and that success occurs when \(Z>0\) and failure when \(Z\leq 0\). If \(Z\) is distributed as \(F(Z-\zeta)\) with location parameter \(\zeta\), we have \(p=1-F(-\zeta)\) and \(q=F(-\zeta)\). Of particular interest is the logistic distribution, for which \(F(x)=1/(1+e^{-x})\). In this case \(p=e^{\zeta}/(1+e^{\zeta})\), \(q=1/(1+e^{\zeta})\), and hence \(\log(p/q)=\zeta\). Applying this fact to the success probabilities\[p_{1k}=1-F(-\zeta_{1k}),\qquad p_{2k}=1-F(-\zeta_{2k}),\]

we find that

\[\theta_{k}=\log\Delta_{k}=\log\left(\frac{p_{2k}}{q_{2k}}\left/\frac{p_{1k}}{q_{ 1k}}\right.\right)=\zeta_{2k}-\zeta_{1k},\]

so that \(\zeta_{2k}=\zeta_{1k}+\theta_{k}\). In this model, \(H^{\prime}\) thus reduces to the assumption that \(\zeta_{2k}=\zeta_{1k}+\theta\), that is, that the treatment shifts the distribution of the underlying response by a constant amount \(\theta\).

If it is assumed that \(F\) is normal rather than logistic, \(F(x)=\Phi(x)\) say, then \(\zeta=\Phi^{-1}(p)\), and constancy of \(\zeta_{2k}-\zeta_{1k}\) requires the much more cumbersome condition \(\Phi^{-1}(p_{2k})-\Phi^{-1}(p_{1k})=\) constant. However, the functions \(\log(p/q)\) and \(\Phi^{-1}(p)\) agree quite well in the range \(.1\leq p\leq.9\) [see Cox (1970, p. 28)], and the assumption of constant \(\Delta_{k}\) in the logistic response model is therefore close to the corresponding assumption for an underlying normal response.12 [The so-called loglinear models, which for contingency tables correspond to the linear models to be considered in Chapter 7 but with a logistic rather than a normal response variable, provide the most widely used approach to contingency tables. See, for example, the books by Cox (1970), Haberman (1974), Bishop, Fienberg, Holland (1975), Fienberg (1980), Plackett (1981), and Agresti (2002).]

Footnote 12: The problem of discriminating between a logistic and normal response model is discussed by Chambers and Cox (1967).

The UMP unbiased test, derived above for the case that the \(B\)- and \(C\)-margins are fixed, applies equally when any two margins, any one margin, or no margins are fixed, with the understanding that in all cases the test is carried out conditionally, given the values of all random margins.

The test is also used (but no longer UMP unbiased) for testing \(H:\Delta_{1}=\cdots=\Delta_{K}=1\) when the \(\Delta\)'s are not assumed to be equal but when the \(\Delta_{k}-1\) can be assumed to have the same sign, so that the departure from independence is in the same direction for all the \(2\times 2\) tables. A one- or two-sided version is appropriate as the alternatives do or do not specify the direction. For a discussion of this test, the Cochran-Mantel-Haenszel test, and some of its extensions see Agresti (2002, Section 7.4).

Consider now the case \(K=2\), with \(m_{k}\) and \(n_{k}\) fixed, and the problem of testing \(H^{\prime}:\Delta_{2}=\Delta_{1}\) rather than assuming it. The joint distribution of the \(X\)'s and \(Y\)'s given earlier can then be written as

\[\left[\prod_{k=1}^{2}\binom{m_{k}}{x_{k}}\binom{n_{k}}{y_{k}}q_{1 k}^{m_{k}}q_{2k}^{n_{k}}\right]\] \[\qquad\times\exp\left(y_{2}\log\frac{\Delta_{2}}{\Delta_{1}}+(y_{ 1}+y_{2})\log\Delta_{1}+\sum(x_{i}+y_{i})\log\frac{p_{1i}}{q_{1i}}\right),\]and \(H^{\prime}\) is rejected in favor of \(\Delta_{2}>\Delta_{1}\) if \(Y_{2}>C\), where \(C\) depends on \(Y_{1}+Y_{2}\), \(X_{1}+Y_{1}\) and \(X_{2}+Y_{2}\), and is determined so that the conditional probability of rejection given \(Y_{1}+Y_{2}=w\), \(X_{1}+Y_{1}=t_{1}\), \(X_{2}+Y_{2}=t_{2}\) is \(\alpha\). The conditional null distribution of \(Y_{1}\) and \(Y_{2}\), given \(X_{k}+Y_{k}=t_{k}\) (\(k=1,2\)), by (4.21) with \(\Delta\) in place of \(\rho\) is

\[C_{t_{1}}(\Delta)C_{t_{2}}(\Delta){m_{1}\choose t_{1}-y_{1}}{n_{1}\choose y_{1} }{m_{2}\choose t_{2}-y_{2}}{n_{2}\choose y_{2}}\Delta^{y_{1}+y_{2}},\]

and hence the conditional distribution of \(Y_{2}\), given in addition that \(Y_{1}+Y_{2}=w\), is of the form

\[k(t_{1},t_{2},w){m_{1}\choose y+t_{1}-w}{n_{1}\choose w-y}{m_{2}\choose t_{2} -y}{n_{2}\choose y}\,.\]

Some approximations to the critical value of this test are discussed by Birch (1964); see also Venable and Bhapkar (1978). [Optimum large-sample tests of some other hypotheses in \(2\times 2\times 2\) tables are obtained by Cohen, Gatsonis, and Marden (1983).]

### The Sign Test

To test consumer preferences between two products, a sample of \(n\) subjects are asked to state their preferences. Each subject is recorded as plus or minus as it favors product \(B\) or \(A\). The total number \(Y\) of plus signs is then a binomial variable with distribution \(b(p,n)\). Consider the problem of testing the hypothesis \(p=\frac{1}{2}\) of no difference against the alternatives \(p\neq\frac{1}{2}\). (As in previous such problems, we disregard here that in case of rejection it will be necessary to decide which of the two products is preferred.) The appropriate test is the two-sided _sign test_, which rejects when \(|Y-\frac{1}{2}n|\) is too large. This is UMP unbiased (Section 4.2).

Sometimes the subjects are also given the possibility of declaring themselves as undecided. If \(p_{-}\), \(p_{+}\), and \(p_{0}\) denote the probabilities of preference for product \(A\), product \(B\), and of no preference respectively, the numbers \(X\), \(Y\), and \(Z\) of decisions in favor of these three possibilities are distributed according to the multinomial distribution

\[\frac{n!}{x!y!z!}p_{-}^{x}p_{+}^{y}p_{0}^{z}\ \ \ (x+y+z=n), \tag{4.22}\]

and the hypothesis to be tested is \(H:p_{+}=p_{-}\). The distribution (4.22) can also be written as

\[\frac{n!}{x!y!z!}\left(\frac{p_{+}}{1-p_{0}-p_{+}}\right)^{y}\left(\frac{p_{0} }{1-p_{0}-p_{+}}\right)^{z}(1-p_{0}-p_{+})^{n}, \tag{4.23}\]

and is then seen to constitute an exponential family with \(U=Y\), \(T=Z\), \(\theta=\log[p_{+}/(1-p_{0}-p_{+})]\), \(\vartheta=\log[p_{0}/(1-p_{0}-p_{+})]\). Rewriting the hypothesis as \(p_{+}=1-p_{0}-p_{+}\) it is seen to be equivalent to \(\theta=0\). There exists therefore a UMP unbiased test of \(H\), which is obtained by considering \(z\) as fixed and determining the best unbiased conditional test of \(H\) given \(Z=z\). Since the conditional distribution of \(Y\) given \(z\) is a binomial distribution \(b(p,\,n-z)\) with \(p=p_{+}/(p_{+}+p_{-})\), the problem reduces to that of testing the hypothesis \(p=\frac{1}{2}\) in a binomial distribution with \(n-z\) trials, for which the rejection region is \(|Y-\frac{1}{2}(n-z)|>C(z)\). The UMP unbiased test is therefore obtained by disregarding the number of cases in which no preference is expressed (the number of _ties_), and applying the sign test to the remaining data.

The power of the test depends strongly on \(p_{0}\), which governs the distribution of \(Z\). For large \(p_{0}\), the number \(n-z\) of trials in the conditional binomial distribution can be expected to be small, and the test will thus have little power. This may be an advantage in the present case, since a sufficiently high value of \(p_{0}\), regardless of the value of \(p_{+}/p_{-}\), implies that the population as a whole is largely indifferent with respect to the products.

The above conditional sign test applies to any situation in which the observations are the result of \(n\) independent trials, each of which is either a success (\(+\)), a failure (\(-\)), or a tie. As an alternative treatment of ties, it is sometimes proposed to assign each tie at random (with probability \(\frac{1}{2}\) each) to either plus or minus. The total number \(Y^{\prime}\) of plus signs after the ties have been broken is then a binomial variable with distribution \(b(\pi,n)\), where \(\pi=p_{+}+\frac{1}{2}p_{0}\). The hypothesis \(H\) becomes \(\pi=\frac{1}{2}\), and is rejected when \(|Y^{\prime}-\frac{1}{2}n|>C\), where the probability of rejection is \(\alpha\) when \(\pi=\frac{1}{2}\). This test can be viewed also as a randomized test based on \(X\), \(Y\), and \(Z\), and it is unbiased for testing \(H\) in its original form, since \(p_{+}\) is = or \(\neq p_{-}\) as \(\pi\) is = or \(\neq 1\). Since the test involves randomization other than on the boundaries of the rejection region, it is less powerful than the UMP unbiased test for this situation, so that the random breaking of ties results in a loss of power.

This remark might be thought to throw some light on the question of whether in the determination of consumer preferences it is better to permit the subject to remain undecided or to force an expression of preference. However, here the assumption of a completely random assignment in case of a tie does not apply. Even when the subject is not conscious of a definite preference, there will usually be a slight inclination toward one of the two possibilities, which in a majority of the cases will be brought out by a forced decision. This will be balanced in part by the fact that such forced decisions are more variable than those reached voluntarily. Which of these two factors dominates depends on the strength of the preference.

Frequently, the question of preference arises between a standard product and a possible modification or a new product. If each subject is required to express a definite preference, the hypothesis of interest is usually the one-sided hypothesis \(p_{+}\leq p_{-}\), where \(+\) denotes a preference for the modification. However, if an expression of indifference is permitted the hypothesis to be tested is not \(p_{+}\leq p_{-}\) but rather \(p_{+}\leq p_{0}+p_{-}\), since typically the modification is of interest only if it is actually preferred. As was shown in Example 3.8.1, the one-sided sign test which rejects when the number of plus signs is too large is UMP for this problem.

In some investigations, the subject is asked not only to express a preference but to give a more detailed evaluation, such as a score on some numerical scale. Depending on the situation, the hypothesis can then take on one of two forms. One may be interested in the hypothesis that there is no difference in the consumer's reaction to the two products. Formally, this states that the distribution of the scores \(X_{1}\),..., \(X_{n}\) expressing the degree of preference of the \(n\) subjects for the modified product is symmetric about the origin. This problem, for which a UMP unbiased test does not exist without further assumptions, will be considered in Section 6.10.

Alternatively, the hypothesis of interest may continue to be \(H:p_{+}=p_{-}\). Since \(p_{-}=P\{X<0\}\) and \(p_{+}=P\{X>0\}\), this now becomes

\[H:P\{X>0\}=P\{X<0\}.\]

Here symmetry of \(X\) is no longer assumed even when \(P\{X<0\}=P\{X>0\}\). If no assumptions are made concerning the distribution of \(X\) beyond the fact that the set of its possible values is given, the sign test based on the number of \(X\)'s that are positive and negative continues to be UMP unbiased.

To see this, note that any distribution of \(X\) can be specified by the probabilities

\[p_{-}=P\{X<0\},\qquad p_{+}=P\{X>0\},\qquad p_{0}=P\{X=0\},\]

and the conditional distributions \(F_{-}\) and \(F_{+}\) of \(X\) given \(X<0\) and \(X>0\) respectively. Consider any fixed distributions \(F^{\prime}_{-}\), \(F^{\prime}_{+}\), and denote by \(\mathcal{F}_{0}\) the family of all distributions with \(F_{-}=F^{\prime}_{-}\), \(F_{+}=F^{\prime}_{+}\) and arbitrary \(p_{-}\), \(p_{+}\), \(p_{0}\). Any test that is unbiased for testing \(H\) in the original family of distributions \(\mathcal{F}\) in which \(F_{-}\) and \(F_{+}\) are unknown is also unbiased for testing \(H\) in the smaller family \(\mathcal{F}_{0}\). We shall show below that there exists a UMP unbiased test \(\phi_{0}\) of \(H\) in \(\mathcal{F}_{0}\). It turns out that \(\phi_{0}\) is also unbiased for testing \(H\) in \(\mathcal{F}\) and is independent of \(F^{\prime}_{-}\), \(F^{\prime}_{+}\). Let \(\phi\) be any other unbiased test of \(H\) in \(\mathcal{F}\), and consider any fixed alternative, which without loss of generality can be assumed to be in \(\mathcal{F}_{0}\). Since \(\phi\) is unbiased for \(\mathcal{F}\), it is unbiased for testing \(p_{+}=p_{-}\) in \(\mathcal{F}_{0}\); the power of \(\phi_{0}\) against the particular alternative is therefore at least as good as that of \(\phi\). Hence \(\phi_{0}\) is UMP unbiased.

To determine the UMP unbiased test of \(H\) in \(\mathcal{F}_{0}\), let the densities of \(F^{\prime}_{-}\) and \(F^{\prime}_{+}\) with respect to some measure \(\mu\) be \(f^{\prime}_{-}\) and \(f^{\prime}_{+}\). The joint density of the \(X\)'s at a point \((x_{1},\ldots,x_{n})\) with

\[x_{i_{1}},\ldots,x_{i_{r}}<0=x_{j_{1}}=\cdots=x_{j_{s}}<x_{k_{1}},\ldots,x_{k_ {m}}\]

is

\[p^{r}_{-}p^{s}_{0}p^{m}_{+}f^{\prime}_{-}(x_{i_{1}})\ldots f^{\prime}_{-}(x_{i _{r}})f^{\prime}_{+}(x_{k_{1}})\ldots f^{\prime}_{+}(x_{k_{m}}).\]

The set of statistics (\(r\), \(s\), \(m\)) is sufficient for (\(p_{-}\), \(p_{0}\), \(p_{+}\)), and its distribution is given by (4.22) with \(x=r\), \(y=m\), \(z=s\). The sign test is therefore seen to be UMP unbiased as before.

A different application of the sign test arises in the context of a \(2\times 2\) table for matched pairs. In Section 4.5 success probabilities for two treatments were compared on the basis of two independent random samples. Unless the population of subjects from which these samples are drawn is fairly homogeneous, a more powerful test can often be obtained by using a sample of matched pairs (for example, twins or the same subject given the treatments at different times). For each pair there are then four possible outcomes: (0, 0), (0, 1), (1, 0), and (1, 1), where 1 and 0 stand for success and failure, and the first and second numbers in each pair of responses refer to the subject receiving treatment 1 or 2, respectively.

The results of such a study are sometimes displayed in a \(2\times 2\) table,

\[\begin{array}{c|ccc}&&&&&1\text{st}\\ &&0&1\\ \hline 0&&X&X^{\prime}\\ 1&&Y&Y^{\prime}\end{array}\]

which despite the formal similarity differs from that considered in Section 4.6. If a sample of \(s\) pairs is drawn, the joint distribution of \(X\), \(Y\), \(X^{\prime}\), \(Y^{\prime}\) as before is multinomial, with probabilities \(p_{00}\), \(p_{01}\), \(p_{10}\),\(p_{11}\). The success probabilities of the two treatments are \(\pi_{1}=p_{10}+p_{11}\) for the first and \(\pi_{2}=p_{01}+p_{11}\) for the second treatment, and the hypothesis to be tested is \(H:\pi_{1}=\pi_{2}\) or equivalently \(p_{10}=p_{01}\) rather than \(p_{10}p_{01}=p_{00}p_{11}\) as it was earlier.

In exponential form, the joint distribution can be written as

\[\frac{s!p_{11}^{s}}{x!x^{\prime}!y!y^{\prime}!}\exp\left(y\log\frac{p_{01}}{p_ {10}}+(x^{\prime}+y)\log\frac{p_{10}}{p_{11}}+x\log\frac{p_{00}}{p_{11}}\right). \tag{4.24}\]

There exists a UMP unbiased test, _McNemar's test_, which rejects \(H\) in favor of the alternatives \(p_{10}<p_{01}\) when \(Y>C(X^{\prime}+Y,X)\), where the conditional probability of rejection given \(X^{\prime}+Y=d\) and \(X=x\) is \(\alpha\) for all \(d\) and \(x\). Under this condition, the numbers of pairs (0, 0) and (1, 1) are fixed, and the only remaining variables are \(Y\) and \(X^{\prime}=d-Y\) which specify the division of the \(d\) cases with mixed response between the outcomes (0, 1) and (1, 0). Conditionally, one is dealing with \(d\) binomial trials with success probability \(p=p_{01}/(p_{01}+p_{10})\), \(H\) becomes \(p=\frac{1}{2}\), and the UMP unbiased test reduces to the sign test. [The issue of conditional versus unconditional power for this test is discussed by Frisen (1980).]

The situation is completely analogous to that of the sign test in the presence of undecided opinions, with the only difference that there are now two types of ties, (0, 0) and (1, 1), both of which are disregarded in performing the test.

### Problems

#### Section 4.1

**Problem 4.1**: _Admissibility._ Any UMP unbiased test \(\phi_{0}\) is admissible in the sense that there cannot exist another test \(\phi_{1}\) which is at least as powerful as \(\phi_{0}\) against all alternatives and more powerful against some.

[If \(\phi\) is unbiased and \(\phi^{\prime}\) is uniformly at least as powerful as \(\phi\), then \(\phi^{\prime}\) is also unbiased.]

**Problem 4.2**: \(p\)_-values._ Consider a family of tests of \(H:\theta=\theta_{0}\) (or \(\theta\leq\theta_{0}\)), with level-\(\alpha\) rejection regions \(S_{\alpha}\), such that (a) \(P_{\theta_{0}}\{X\in S_{\alpha}\}=\alpha\) for all \(0<\alpha<1\), and (b) \(S_{\alpha}\subseteq S_{\alpha^{\prime}}\) for \(\alpha<\alpha^{\prime}\). If the tests \(S_{\alpha}\) are unbiased, the distribution of \(\hat{p}\) under any alternative \(\theta\) satisfies

\[P_{\theta}\{\hat{p}\leq\alpha\}\geq P_{\theta_{0}}\{\hat{p}\leq\alpha\}=\alpha\]

so that it is shifted toward the origin.

#### Section 4.2

**Problem 4.3**: Let \(X\) have the binomial distribution \(b(p,n)\), and consider the hypothesis \(H:p=p_{0}\) at level of significance \(\alpha\). Determine the boundary values of the UMP unbiased test for \(n=10\) with \(\alpha=0.1\), \(p_{0}=0.2\) and with \(\alpha=0.05\), \(p_{0}=0.4\), and in each case graph the power functions of both the unbiased and the equal-tails test.

**Problem 4.4**: Let \(X\) have the Poisson distribution \(P(\tau)\), and consider the hypothesis \(H:\tau=\tau_{0}\). Then Condition (4.6) reduces to

\[\sum_{x=C_{1}+1}^{C_{2}-1}\frac{\tau_{0}^{x-1}}{(x-1)!}e^{-\tau_{0}}+\sum_{i=1 }^{2}(1-\gamma_{i})\frac{\tau_{0}^{C_{i}-1}}{(C_{i}-1)!}e^{-\tau_{0}}=1-\alpha,\]

provided \(C_{1}>1\).

**Problem 4.5**: Let \(T_{n}/\theta\) have a \(\chi^{2}\)-distribution with \(n\) degrees of freedom. For testing \(H:\theta=1\) at level of significance \(\alpha=.05\), find \(n\) so large that the power of the UMP unbiased test is \(\geq.9\) against both \(\theta\geq 2\) and \(\theta\leq\frac{1}{2}\). How large does \(n\) have to be if the test is not required to be unbiased?

**Problem 4.6**: Suppose \(X\) has density (with respect to some measure \(\mu\))

\[p_{\theta}(x)=C(\theta)\exp[\theta T(x)]h(x)\,\]

for some real-valued \(\theta\). Assume the distribution of \(T(X)\) is continuous under \(\theta\) (for any \(\theta\)). Consider the problem of testing \(\theta=\theta_{0}\) versus \(\theta\neq\theta_{0}\). If the null hypothesisis rejected, then a decision is to be made as to whether \(\theta>\theta_{0}\) or \(\theta<\theta_{0}\). We say that a Type 3 (or directional) error is made when it is declared that \(\theta>\theta_{0}\) when in fact \(\theta<\theta_{0}\) (or vice versa). Consider a level \(\alpha\) test that rejects the null hypothesis if \(T<C_{1}\) or \(T>C_{2}\) for constants \(C_{1}<C_{2}\). Further suppose that it is declared that \(\theta<\theta_{0}\) if \(T<C_{1}\) and \(\theta>\theta_{0}\) if \(T>C_{2}\).

(i) If the constants are chosen so that the test is UMPU, show that the Type 3 error is controlled in the sense that

\[\sup_{\theta\neq\theta_{0}}P_{\theta}\{\mbox{Type 3 error is made}\}\leq\alpha. \tag{4.25}\]

(ii) If the constants are chosen so that the test is equi-tailed in the sense

\[P_{\theta_{0}}\{T(X)<C_{1}\}=P_{\theta_{0}}\{T(X)>C_{2}\}=\alpha/2\,\]

then show (4.25) holds with \(\alpha\) replaced by \(\alpha/2\).

(iii) Give an example where the UMPU level \(\alpha\) test has the left side of (4.25) strictly \(>\alpha/2\). [Confidence intervals for \(\theta\) after rejection of a two-sided test are discussed in Finner (1994).]

**Problem 4.7**: Let \(X\) and \(Y\) be independently distributed according to one-parameter exponential families, so that their joint distribution is given by

\[dP_{\theta_{1},\theta_{2}}(x,\,y)=C(\theta_{1})e^{\theta_{1}T(x)}\,d\mu(x)\,K (\theta_{2})e^{\theta_{2}U(y)}\,d\nu(y).\]

Suppose that with probability 1 the statistics \(T\) and \(U\) each take on at least three values and that \((a,\,b)\) is an interior point of the natural parameter space. Then a UMP unbiased test does not exist for testing \(H:\theta_{1}=a,\,\theta_{2}=b\) against the alternatives \(\theta_{1}\neq a\) or \(\theta_{2}\neq b\).13

Footnote 13: For counterexamples when the conditions of the problem are not satisfied, see Kallenberg et al. (1984).

[The most powerful unbiased tests against the alternatives \(\theta_{1}\neq a,\theta_{2}\neq b\) have acceptance regions \(C_{1}<T(x)<C_{2}\) and \(K_{1}<U(y)<K_{2}\) respectively. These tests are also unbiased against the wider class of alternatives \(K:\theta_{1}\neq a\) or \(\theta_{2}\neq b\) or both.]

**Problem 4.8**: Let \((X,\,Y)\) be distributed according to the exponential family

\[dP_{\theta_{1},\theta_{2}}(x,\,y)=C(\theta_{1},\theta_{2})e^{\theta_{1}x+ \theta_{2}y}\,d\mu(x,\,y)\.\]

The only unbiased test for testing \(H:\theta_{1}\leq a,\theta_{2}\leq b\) against \(K:\theta_{1}>a\) or \(\theta_{2}>b\) or both is \(\phi(x,\,y)\equiv\alpha\).

[Take \(a=b=0\), and let \(\beta(\theta_{1},\,\theta_{2})\) be the power function of any level-\(\alpha\) test. Unbiasedness implies \(\beta(0,\,\theta_{2})=\alpha\) for \(\theta_{2}<0\) and hence for all \(\theta_{2}\), since \(\beta(0,\,\theta_{2})\) is an analytic function of \(\theta_{2}\). For fixed \(\theta_{2}>0\), \(\beta(\theta_{1},\,\theta_{2})\) considered as a function of \(\theta_{1}\) therefore has a minimum at \(\theta_{1}=0\), so that \(\partial\beta(\theta_{1},\,\theta_{2})/\partial\theta_{1}\) vanishes at \(\theta_{1}=0\) for all positive \(\theta_{2}\), and hence for all \(\theta_{2}\). By considering alternatively positive and negative values of \(\theta_{2}\) and using the fact that the partial derivatives of all orders of \(\beta(\theta_{1},\theta_{2})\) with respect to \(\theta_{1}\) are analytic, one finds that for each fixed \(\theta_{2}\) these derivatives all vanish at \(\theta_{1}=0\) and hence that the function \(\beta\) must be a constant. Because of the completeness of (\(X\), \(Y\)), \(\beta(\theta_{1},\theta_{2})\equiv\alpha\) implies \(\phi(x,y)\equiv\alpha\).]

**Problem 4.9**: For testing the hypothesis \(H:\theta=\theta_{0}\), (\(\theta_{0}\) an interior point of \(\Omega\)) in the one-parameter exponential family of Section 4.2, let \({\cal C}\) be the totality of tests satisfying (4.3) and (4.5) for some \(-\infty\leq C_{1}\leq C_{2}\leq\infty\) and \(0\leq\gamma_{1}\), \(\gamma_{2}\leq 1\).

* \({\cal C}\) is complete in the sense that given any level-\(\alpha\) test \(\phi_{0}\) of \(H\) there exists \(\phi\in{\cal C}\) such that \(\phi\) is uniformly at least as powerful as \(\phi_{0}\).
* If \(\phi_{1}\), \(\phi_{2}\in{\cal C}\), then neither of the two tests is uniformly more powerful than the other.
* Let the problem be considered as a two-decision problem, with decisions \(d_{0}\) and \(d_{1}\) corresponding to acceptance and rejection of \(H\) and with loss function \(L(\theta,d_{i})=L_{i}(\theta)\), \(i=0,1\). Then \({\cal C}\) is minimal essentially complete provided \(L_{1}(\theta)<L_{0}(\theta)\) for all \(\theta\neq\theta_{0}\).
* Extend the result of part (iii) to the hypothesis \(H^{\prime}:\theta_{1}\leq\theta\leq\theta_{2}\). (For more general complete class results for exponential families and beyond (see Brown and Marden (1989)).

[(i): Let the derivative of the power function of \(\phi_{0}\) at \(\theta_{0}\) be \(\beta^{\prime}_{\phi_{0}}(\theta_{0})=\rho\). Then there exists \(\phi\in{\cal C}\) such that \(\beta^{\prime}_{\phi}(\theta_{0})=\rho\) and \(\phi\) is UMP among all tests satisfying this condition.

(ii): See the end of Section 3.7.

(iii): See the proof of Theorem 3.4.2.]

**Section 4.3**

**Problem 4.10**: Let \(X_{1},\ldots,X_{n}\) be a sample from (i) the normal distribution \(N(a\sigma,\sigma^{2})\), with \(a\) fixed and \(0<\sigma<\infty\); (ii) the uniform distribution \(U(\theta-\frac{1}{2},\theta+\frac{1}{2})\), \(-\infty<\theta<\infty\); (iii) the uniform distribution \(U(\theta_{1},\theta_{2})\), \(\infty<\theta_{1}<\theta_{2}<\infty\). For these three families of distributions the following statistics are sufficient: (i), \(T=(\sum X_{i},\sum X_{i}^{2})\); (ii) and (iii), \(T=(\min(X_{1},\ldots,X_{n})\), \(\max(X_{1},\ldots,X_{n}))\). The family of distributions of \(T\) is complete for case (iii), but for (i) and (ii) it is not complete or even boundedly complete.

[(i): The distribution of \(\sum X_{i}/\sqrt{\sum X_{i}^{2}}\) does not depend on \(\sigma\).]

**Problem 4.11**: Let \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) be samples from \(N(\xi,\sigma^{2})\) and \(N(\xi,\tau^{2})\). Then \(T=(\sum X_{i},\sum Y_{j},\sum X_{i}^{2},\sum Y_{j}^{2})\), which in Example 4.3.3 was seen not to be complete, is also not boundedly complete.

[Let \(f(t)\) be \(1\) or \(-1\) as \(\vec{y}-\vec{x}\) is positive or not.]

**Problem 4.12**: _Counterexample._ Let \(X\) be a random variable taking on the values \(-1,0,1,2,\ldots\) with probabilities

\[P_{\theta}\{X=-1\}=\theta;\qquad P_{\theta}\{X=x\}=(1-\theta)^{2}\theta^{x}, \quad x=0,1,\ldots.\]

Then \({\cal P}=\{P_{\theta},0<\theta<1\}\) is boundedly complete but not complete. [Girschick et al. (1946)]

**Problem 4.13**: The completeness of the order statistics in Example 4.3.4 remains true if the family \({\cal F}\) is replaced by the family \({\cal F}_{1}\) of all continuous distributions.

[Due to Fraser (1956). To show that for any integrable symmetric function \(\phi\), \(\int\phi(x_{1},\ldots,x_{n})\,d\,F(x_{1})\ldots\)

\(dF(x_{n})=0\) for all continuous \(F\) implies \(\phi=0\) a.e., replace \(F\) by \(\alpha_{1}F_{1}+\cdots+\alpha_{n}F_{n}\), where \(0<\alpha_{i}<1\), \(\sum\alpha_{i}=1\). By considering the left side of the resulting identity as a polynomial in the \(\alpha\)'s one sees that \(\int\phi(x_{1},\ldots,x_{n})\,d\,F_{1}(x_{1})\ldots d\,F_{n}(x_{n})=0\) for all continuous \(F_{i}\). This last equation remains valid if the \(F_{i}\) are replaced by \(I_{a_{i}}(x)\,F(x)\), where \(I_{a_{i}}(x)=1\) if \(x\leq a_{i}\) and \(=0\) otherwise. This implies that \(\phi=0\) except on a set which has measure \(0\) under \(F\times\ldots\times F\) for all continuous \(F\).]

**Problem 4.14**: Determine whether \(T\) is complete for each of the following situations:

* \(X_{1},\ldots,X_{n}\) are independently distributed according to the uniform distribution over the integers \(1\), \(2\), \(\ldots,\theta\) and \(T=\max(X_{1},\ldots,X_{n})\).
* \(X\) takes on the values \(1\),\(2\),\(3\),\(4\) with probabilities \(pq\), \(p^{2}q\), \(pq^{2}\), \(1-2pq\), respectively, and \(T=X\).

**Problem 4.15**: Let \(X\), \(Y\) be independent binomial \(b(p,m)\) and \(b(p^{2},n)\), respectively. Determine whether \((X,Y)\) is complete when

* \(m=n=1\),
* \(m=2\), \(n=1\).

**Problem 4.16**: Let \(X_{1},\ldots,X_{n}\) be a sample from the uniform distribution over the integers \(1\), \(\ldots,\theta\) and let \(a\) be a positive integer.

* The sufficient statistic \(X_{(n)}\) is complete when the parameter space is \(\Omega=\{\theta:\theta\leq a\}\).
* Show that \(X_{(n)}\) is not complete when \(\Omega=\{\theta:\theta\geq a\},a\geq 2\), and find a complete sufficient statistic in this case.

**Section 4.4**:

**Problem 4.17**: Let \(X_{i}(i=1,2)\) be independently distributed according to distributions from the exponential families (3.19) with \(C\), \(Q\), \(T\), and \(h\) replaced by \(C_{i}\), \(Q_{i}\), \(T_{i}\), and \(h_{i}\). Then there exists a UMP unbiased test of

[MISSING_PAGE_FAIL:175]

(2) Let \(H(y,t)\) denote the left-hand side of (4.27), with \(v=v(y,t)\), etc. Then \(H(0,t)>\alpha\) and \(H(\alpha,t)<\alpha\). This follows by Theorem 3.4.1 from the fact that \(v(0,t)=-\infty\) and \(w(\alpha,t)=\infty\) (which shows the conditional tests corresponding to \(y=0\) and \(y=\alpha\) to be one-sided), and that the left-hand side of (4.27) for any \(y\) is the power of this conditional test.

(3) For fixed \(t\), the functions

\[H_{1}(y,t)=G_{t}(v-)+\gamma_{1}[G_{t}(v)-G_{t}(v-)]\]

and

\[H_{2}(y,t)=1-G_{t}(w)+\gamma_{2}[G_{t}(w)-G_{t}(w-)]\]

are continuous functions of \(y\). This is a consequence of the fact, which follows from (4.28), that a.e. \(\mathcal{P}^{T}\) the discontinuities and flat stretches of \(F_{t}\) and \(G_{t}\) coincide.

(4) The function \(H(y,t)\) is jointly measurable in \(y\) and \(t\). This follows from the continuity of \(H\) by an argument similar to the proof of measurability of \(F_{t}(u)\) in the text. Define

\[y(t)=\inf\{y:H(y,t)<\alpha\},\]

and let \(v(t)=v[y(t),t]\), etc. Then (4.26) and (4.27) are satisfied for all \(t\). The measurability of \(v(t)\), \(w(t)\), \(\gamma_{1}(t)\), and \(\gamma_{2}(t)\) defined in this manner will follow from measurability in \(t\) of \(y(t)\) and \(F_{t}^{-1}[y(t)]\). This is a consequence of the relations, which hold for all real \(c\),

\[\{t:y(t)<c\}=\bigcup_{r<c}\{t:H(r,t)<\alpha\},\]

where \(r\) indicates a rational, and

\[\{t:F_{t}^{-1}[y(t)]\leq c\}=\{t:y(t)-F_{t}(c)\leq 0\}.]

**Problem 4.22**: _Continuation_. The function \(\phi_{4}\) defined by (4.16), (4.18), and (4.19) is jointly measurable in \(u\) and \(t\).

[The proof, which otherwise is essentially like that outlined in the preceding problem, requires the measurability in \(z\) and \(t\) of the integral

\[g(z,t)=\int_{-\infty}^{z-}u\,dF_{t}(u).\]

This integral is absolutely convergent for all \(t\), since \(F_{t}\) is a distribution belonging to an exponential family. For any \(z<\infty\), \(g(z,t)=\lim g_{n}(z,t)\), where

\[g_{n}(z,t)=\sum_{j=1}^{\infty}\left(z-\frac{j}{2^{n}}\right)\left[F_{t}\left( z-\frac{j-1}{2^{n}}-0\right)-F_{t}\left(z-\frac{j}{2^{n}}-0\right)\right],\]and the measurability of \(g\) follows from that of the functions \(g_{n}\). The inequalities corresponding to those obtained in step (2) of the preceding problem result from the property of the conditional one-sided tests established in Problem 3.50.]

**Problem 4.23**: The UMP unbiased tests of the hypotheses \(H_{1},\ldots,H_{4}\) of Theorem 4.4.1 are unique if attention is restricted to tests depending on \(U\) and the \(T\)'s.

**Problem 4.24**: The singly truncated normal (STN) distribution, indexed by parameters \(\nu\) and \(\lambda\) has support the positive real line with density

\[p(x;\,\nu,\,\lambda)=C(\nu,\,\lambda)\,\exp(-\nu x-\lambda x^{2})\,\]

where \(C(\nu,\,\lambda)\) is a normalizing constant. Based on an i.i.d. sample, show there exists a UMPU test of the null hypothesis that the observations are exponential against the STN alternative, and describe the form of rejection region as explicitly as possible. [See Castillo and Puig (1999).]

### _Section 4.5_

**Problem 4.25**: _Negative binomial_. Let \(X\), \(Y\) be independently distributed according to negative binomial distributions \(Nb(p_{1},m)\) and \(Nb(p_{2},n)\) respectively, and let \(q_{i}=1-p_{i}\).

* There exists a UMP unbiased test for testing \(H:\theta=q_{2}/q_{1}\leq\theta_{0}\) and hence in particular \(H^{\prime}:p_{1}\leq p_{2}\).
* Determine the conditional distribution required for testing \(H^{\prime}\) when \(m=n=1\).

**Problem 4.26**: Let \(X\) and \(Y\) be independently distributed with Poisson distributions \(P(\lambda)\) and \(P(\mu)\). Find the power of the UMP unbiased test of \(H:\mu\leq\lambda\), against the alternatives \(\lambda=.1,\mu=.2\); \(\lambda=1,\mu=2\); \(\lambda=10\), \(\mu=20\); \(\lambda=.1,\mu=0.4\); at level of significance \(\alpha=0.1\).

[Since \(T=X+Y\) has the Poisson distribution \(P(\lambda+\mu)\), the power is

\[\beta=\sum_{t=0}^{\infty}\beta(t)\frac{(\lambda+\mu)^{t}}{t!}e^{-(\lambda+\mu)},\]

where \(\beta(t)\) is the power of the conditional test given \(t\) against the alternative in question.]

**Problem 4.27**: _Sequential comparison of two binomials_. Consider two sequences of binomial trials with probabilities of success \(p_{1}\) and \(p_{2}\), respectively, and let \(\rho=(p_{2}/q_{2})\div(p_{1}/q_{1})\).

1. If \(\alpha<\beta\), no test with fixed numbers of trials \(m\) and \(n\) for testing \(H:\rho=\rho_{0}\) can have power \(\geq\beta\) against all alternatives with \(\rho=\rho_{1}\).
2. The following is a simple sequential sampling scheme leading to the desired result. Let the trials be performed in pairs of one of each kind, and restrict attention to those pairs in which one of the trials is a success and the other a failure. If experimentation is continued until \(N\) such pairs have been observed, the number of pairs in which the successful trial belonged to the first series has the binomial distribution \(b(\pi,N)\) with \(\pi=p_{1}q_{2}/(p_{1}q_{2}+P_{2}q_{1})=1/(1+\rho)\). A test of arbitrarily high power against \(\rho_{1}\) is therefore obtained by taking \(N\) large enough.
3. If \(p_{1}/p_{2}=\lambda\), use inverse binomial sampling to devise a test of \(H:\lambda=\lambda_{0}\) against \(K:\lambda>\lambda_{0}\).

**Problem 4.28**: _Positive dependence._ Two random variables \((X,Y)\) with c.d.f. \(F(x,y)\) are said to be _positively quadrant dependent_ if \(F(x,y)\geq F(x,\infty)F(\infty,y)\) for all \(x,y\).14 For the case that \((X,Y)\) takes on the four pairs of values \((0,0)\), \((0,1)\), \((1,0)\), \((1,1)\) with probabilities \(p_{00}\), \(p_{01}\), \(p_{10}\), \(p_{11}\), \((X,Y)\) are positively quadrant dependent if and only if the odds ratio \(\Delta=p_{01}p_{10}/p_{00}p_{11}\leq 1\).

Footnote 14: For a systematic discussion of this and other concepts of dependence, see Tong (1980, Chapter 5), Kotz, Wang and Hung (1990), and Yanagimoto (1990).

**Problem 4.29**: _Runs._ Consider a sequence of \(N\) dependent trials, and let \(X_{i}\) be \(1\) or \(0\) as the \(i\)th trial is a success or failure. Suppose that the sequence has the _Markov_ property15

Footnote 15: Statistical inference in these and more general Markov chains is discussed, for example, in Bhat and Miller (2002); they provide references at the end of Chapter 5.

\[P\{X_{i}=1|x_{i},\ldots,x_{i-1}\}=P\{X_{i}=1|x_{i-1}\}\]

and the property of _stationarity_ according to which \(P\{X_{i}=1\}\) and \(P\{X_{i}=1|x_{i-1}\}\) are independent of \(i\). The distribution of the \(X\)'s is then specified by the probabilities

\[p_{1}=P\{X_{i}=1|x_{i-1}=1\}\ \ \mbox{and}\ \ \ p_{0}=P\{X_{i}=1|x_{i-1}=0\}\]

and by the initial probabilities

\[\pi_{1}=P\{X_{1}=1\}\ \ \mbox{and}\ \ \ \pi_{0}=1-\pi_{1}=P\{X_{1}=0\}.\]

1. Stationarity implies that \[\pi_{1}=\frac{p_{0}}{p_{0}+q_{1}},\ \ \ \ \ \pi_{0}=\frac{q_{1}}{p_{0}+q_{1}}.\]
2. A set of successive outcomes \(x_{i},x_{i+1},\ldots,x_{i+j}\) is said to form a _run_ of zeros if \(x_{i}=x_{i+1}=\cdots=x_{i+j}=0\), and \(x_{i-1}=1\) or \(i=1\), and \(x_{i+j+1}=1\) or \(i+j=N\). A run of ones is defined analogously. The probability of any particular sequence of outcomes \((x_{1},\ldots,x_{N})\) is \[\frac{1}{p_{0}+q_{1}}p_{0}^{v}p_{1}^{n-v}q_{1}^{u}q_{0}^{m-u},\]

where \(m\) and \(n\) denote the numbers of zeros and ones, and \(u\) and \(v\) the numbers of runs of zeros and ones in the sequence.

**Problem 4.30**: _Continuation_. For testing the hypothesis of independence of the \(X\)'s, \(H:p_{0}=p_{1}\), against the alternatives \(K:p_{0}<p_{1}\), consider the _run test_, which rejects \(H\) when the total number of runs \(R=U+V\) is less than a constant \(C(m)\) depending on the number \(m\) of zeros in the sequence. When \(R=C(m)\), the hypothesis is rejected with probability \(\gamma(m)\), where \(C\) and \(\gamma\) are determined by

\[P_{H}\{R<C(m)|m\}+\gamma(m)P_{H}\{R=C(m)|m\}=\alpha.\]

* Against any alternative of \(K\) the most powerful similar test (which is at least as powerful as the most powerful unbiased test) coincides with the run test in that it rejects \(H\) when \(R<C(m)\). Only the supplementary rule for bringing the conditional probability of rejection (given \(m\)) up to \(\alpha\) depends on the specific alternative under consideration.
* The run test is unbiased against the alternatives \(K\).
* The conditional distribution of \(R\) given \(m\), when \(H\) is true, is16 Footnote 16: This distribution is tabled by Swed and Eisenhart (1943) and Gibbons and Chakraborti (1992); it can be obtained from the hypergeometric distribution [Guenther (1978)]. For further discussion of the run test, see Lou (1996).

\[P\{R=2r\}=\frac{2{{m-1}\choose{r-1}}{{n-1}\choose{r-1}}}{{m+n} \choose{m}},\] \[P\{R=2r+1\}=\frac{{{m-1}\choose{r-1}}{{n-1}\choose{r}+{m-1} \choose{r-1}}}{{m+n}\choose{m}}.\]

[(i): Unbiasedness implies that the conditional probability of rejection given \(m\) is \(\alpha\) for all \(m\). The most powerful conditional level-\(\alpha\) test rejects \(H\) for those sample sequences for which \(\Delta(u,v)=(p_{0}/p_{1})^{v}(q_{1}/q_{0})^{u}\) is too large. Since \(p_{0}<p_{1}\) and \(q_{1}<q_{0}\) and since \(|v-u|\) can only take on the values 0 and 1, it follows that

\[\Delta(1,1)>\Delta(1,2),\quad\Delta(2,1)>\Delta(2,2)>\Delta(2,3),\quad\Delta( 3,2)>\cdots.\]

Thus only the relation between \(\Delta(i,i+1)\) and \(\Delta(i+1,i)\) depends on the specific alternative, and this establishes the desired result.

(ii): That the above conditional test is unbiased for each \(m\) is seen by writing its power as

\[\beta(p_{0},p_{1}|m)=(1-\gamma)P\{R<C(m)|m\}+\gamma P\{R\leq C(m)|m\},\]since by (i) the rejection regions \(R<C(m)\) and \(R<C(m)+1\) are both UMP at their respective conditional levels.

(iii): When \(H\) is true, the conditional probability given \(m\) of any set of \(m\) zeros and \(n\) ones is \(1/{m+n\choose m}\). The number of ways of dividing \(n\) ones into \(r\) groups is \({m-1\choose r-1}\), and that of dividing \(m\) zeros into \(r+1\) groups is \({m-1\choose r}\). The conditional probability of getting \(r+1\) runs of zeros and \(r\) runs of ones is therefore

\[{{m-1\choose r}{n-1\choose r-1}\over{m+n\choose m}}.\]

To complete the proof, note that the total number of runs is \(2r+1\) if and only if there are either \(r+1\) runs of zeros and \(r\) runs of ones or \(r\) runs of zeros and \(r+1\) runs of ones.]

**Problem 4.31**:
1. Based on the conditional distribution of \(X_{2}\),..., \(X_{n}\) given \(X_{1}=x_{1}\) in the model of Problem 4.29, there exists a UMP unbiased test of \(H:p_{0}=p_{1}\) against \(p_{0}>p_{1}\) for every \(\alpha\).
2. For the same testing problem, without conditioning on \(X_{1}\) there exists a UMP unbiased test if the initial probability \(\pi_{1}\) is assumed to be completely unknown instead of being given by the value stated in (i) of Problem 4.29.

[The conditional distribution of \(X_{2}\),..., \(X_{n}\) given \(x_{1}\) is of the form \[C(x_{1};\,p_{0},\,p_{1},\,q_{0},\,q_{1})p_{1}^{y_{1}}p_{0}^{y_{0}}q_{1}^{z_{1} }q_{0}^{z_{0}}(y_{1},\,y_{2},\,z_{1},\,z_{2}),\] where \(y_{1}\) is the number of times a 1 follows a 1, \(y_{0}\) the number of times a 1 follows a 0, and so on, in the sequence \(x_{1}\), \(X_{2}\),..., \(X_{n}\). [See Billingsley (1961, p. 14).]

**Problem 4.32**: _Rank-sum test._ Let \(Y_{1}\),..., \(Y_{N}\) be independently distributed according to the binomial distributions \(b(p_{i},\,n_{i})\), \(i=1\),..., \(N\) where

\[p_{i}={1\over 1+e^{-(\alpha+\beta x_{i})}}.\]

This is the model frequently assumed in bioassay, where \(x_{i}\) denotes the dose, or some function of the dose such as its logarithm, of a drug given to \(n_{i}\) experimental subjects, and where \(Y_{i}\) is the number among these subjects which respond to the drug at level \(x_{i}\). Here the \(x_{i}\) are known, and \(\alpha\) and \(\beta\) are unknown parameters.

1. The joint distribution of the \(Y\)'s constitutes an exponential family, and UMP unbiased tests exist for the four hypotheses of Theorem 4.4.1, concern both \(\alpha\) and \(\beta\).
2. Suppose in particular that \(x_{i}=\Delta i\), where \(\Delta\) is known, and that \(n_{i}=1\) for all \(i\). Let \(n\) be the number of successes in the \(N\) trials, and let these successes occur in the \(s_{1}\)st, \(s_{2}\)nd,..., \(s_{n}\)th trial, where \(s_{1}<s_{2}<\cdots<s_{n}\). Then the UMP unbiased test for testing \(H:\beta=0\) against the alternatives \(\beta>0\) is carried out conditionally, given \(n\), and rejects when the _rank sum_\(\sum_{i=1}^{n}s_{i}\) is too large.

* Let \(Y_{1},\ldots,Y_{M}\) and \(Z_{1},\ldots,Z_{N}\) be two independent sets of experiments of the type described at the beginning of the problem, corresponding, say, to two different drugs. If \(Y_{i}\) is distributed as \(b(p_{i},m_{i})\) and \(Z_{j}\) as \(b(\pi_{j},n_{j})\), with \[p_{i}=\frac{1}{1+e^{-(\alpha+\beta n_{i})}},\ \ \ \ \ \pi_{j}=\frac{1}{a+e^{-( \gamma+\beta v_{j})}},\] then UMP unbiased tests exist for the four hypotheses concerning \(\gamma-\alpha\) and \(\delta-\beta\).

**Problem 4.33**: In a \(2\times 2\times 2\) table with \(m_{1}=3\), \(n_{1}=4\); \(m_{2}=4\), \(n_{2}=4\); and \(t_{1}=3\), \(t_{1}^{{}^{\prime}}=4\), \(t_{2}=t_{2}^{{}^{\prime}}=4\), determine the probabilities that \(P(Y_{1}+Y_{2}\leq K|X_{i}+Y_{i}=t_{i}\), \(i=1\), \(2\)) for \(k=0\), \(1\), \(2\), \(3\).

**Problem 4.34**: In a \(2\times 2\times K\) table with \(\Delta_{k}=\Delta\), the test derived in the text as UMP unbiased for the case that the \(B\) and \(C\) margins are fixed has the same property when any two, one, or no margins are fixed.

**Problem 4.35**: The UMP unbiased test of \(H:\Delta=1\) derived in Section 4.8 for the case that the \(B\)- and \(C\)-margins are fixed (where the conditioning now extends to all random margins) is also UMP unbiased when

* only one of the margins is fixed;
* the entries in the \(4K\) cells are independent Poisson variables with means \(\lambda_{ABC}\), \(\ldots\), and \(\Delta\) is replaced by the corresponding cross-ratio of the \(\lambda\)'s.

**Problem 4.36**: Let \(X_{ijkl}\) (\(i,j,k=0,1\), \(l=1,\ldots,L\)) denote the entries in a \(2\times 2\times 2\times L\) table with factors \(A\), \(B\), \(C\), and \(D\), and let \[\Gamma_{l}=\frac{P_{AB^{c}CD_{l}}P_{\tilde{A}BC_{l}}P_{A\tilde{B}\tilde{C}D_{l }}P_{\tilde{A}\tilde{B}\tilde{C}D_{l}}}{P_{ABCD_{l}}P_{\tilde{A}\tilde{B}CD_{l }}P_{A\tilde{B}\tilde{C}D_{l}}P_{\tilde{A}\tilde{B}\tilde{C}D_{l}}}.\]

Then

* under the assumption \(\Gamma_{l}=\Gamma\) there exists a UMP unbiased test of the hypothesis \(\Gamma\leq\Gamma_{0}\) to for any fixed \(\Gamma_{0}\);
* When \(l=2\), there exists a UMP unbiased test of the hypothesis \(\Gamma_{1}=\Gamma_{2}\) --in both cases regardless of whether \(0\), \(1\), \(2\), or \(3\) of the sets of margins are fixed.

### Section 4.9

**Problem 4.37**: In the \(2\times 2\) table for matched pairs, show by formal computation that the conditional distribution of \(Y\) given \(X^{\prime}+Y=d\) and \(X=x\) is binomial with the indicated \(p\).

**Problem 4.38**: Consider the comparison of two success probabilities in (a) the two binomial situation of Section 4.5 with \(m=n\), and (b) the matched-pairs situation of Section 4.9. Suppose the matching is completely at random, that is, a random sample of \(2n\) subjects, obtained from a population of size \(N(2n\leq N)\), is divided at random into \(n\) pairs, and the two treatments \(B\) and \(B^{c}\) are assigned at random within each pair.

1. The UMP unbiased test for design (a) (Fisher's exact test) is always more powerful than the UMP unbiased test for design (b) (McNemar's test).
2. Let \(X_{i}\) (respectively \(Y_{i}\)) be 1 or 0 as the 1st (respectively 2nd) member of the \(i\)th pair is a success or failure. Then the correlation coefficient of \(X_{i}\) and \(Y_{i}\) can be positive or negative and tends to zero as \(N\rightarrow\infty\).

[(ii): Assume that the \(k\)th member of the population has probability of success \(P_{A}^{(k)}\) under treatment \(A\) and \(P_{\tilde{A}}^{(k)}\) under \(\tilde{A}\).]

**Problem 4.39**: In the \(2\times 2\) table for matched pairs, in the notation of Section 4.9, the correlation between the responses of the two members of a pair is

\[\rho=\frac{p_{11}-\pi_{1}\pi_{2}}{\sqrt{\pi_{1}(1-\pi_{1})\pi_{2}(1-\pi_{2})}}.\]

For any given values of \(\pi_{1}<\pi_{2}\), the power of the one-sided McNemar test of \(H:\pi_{1}=\pi_{2}\) is an increasing function of \(\rho\).

[The conditional power of the test given \(X+Y=d\), \(X=x\) is an increasing function \(p=p_{0l}/(p_{01}+p_{10})\).]

_Note_. The correlation \(\rho\) increases with the effectiveness of the matching, and McNemar's test under (b) of Problem 4.38 soon becomes more powerful than Fisher's test under (a). For detailed numerical comparisons see Wacholder and Weinberg (1982) and the references given there.

### Notes

The closely related properties of similarity (on the boundary) and unbiasedness are due to Neyman and Pearson (1933a; 1933b; 1936a; 1936b), who applied them to a variety of examples. It was pointed out by Neyman (1937) that similar testscould be obtained through the construction method now called Neyman structure. Theorem 4.3.1 is due to Ghosh (1948) and Hoel (1948). The concepts of completeness and bounded completeness, and the application of the latter to Theorem 4.4.1, were developed by Lehmann and Scheffe (1950).

The sign test, proposed by Arbuthnot (1710) to test that the probability of a male birth is \(1/2\), may be the first significance test in the literature. The exact test for independence in 2 by 2 table is due to Fisher (1934a).

## Chapter 5 Unbiasedness: Applications to Normal Distributions; Confidence Intervals

### 5.1 Statistics Independent of a Sufficient Statistic

A general expression for the UMP unbiased tests of the hypotheses \(H_{1}:\theta\leq\theta_{0}\) and \(H_{4}:\theta=\theta_{0}\) in the exponential family

\[d\,P_{\theta,\vartheta}(x)=C(\theta,\vartheta)\exp\left[\theta U(x)+\sum\vartheta _{i}T_{i}(x)\right]\,d\mu(x) \tag{5.1}\]

was given in Theorem 4.4.1 of the preceding chapter. However, this turns out to be inconvenient in the applications to normal and certain other families of continuous distributions, with which we shall be concerned in the present chapter. In these applications, the tests can be given a more convenient form, in which they no longer appear as conditional tests in terms of \(U\) given \(t\), but are expressed unconditionally in terms of a single test statistic. The following are three general methods of achieving this.

(i) In many of the problems to be considered below, the UMP unbiased test \(\phi_{0}\) is also UMP invariant, as will be shown in Chapter 6. From Theorem 6.5.3, it is then possible to conclude that \(\phi_{0}\) is UMP unbiased. This approach, in which the latter property must be taken on faith during the discussion of the test in the present chapter, is the most economical of the three, and has the additional advantage that it derives the test instead of verifying a guessed solution as is the case with methods (ii) and (iii).

(ii) The conditional descriptions (4.12), (4.14), and (4.16) can be replaced by equivalent unconditional ones, and it is then enough to find an unbiased test which has the indicated structure. This approach is discussed in Pratt (1962).

(iii) Finally, it is often possible to show the equivalence of the test given by Theorem 4.4.1 to a test suspected to be optimal, by means of Theorem 5.1.2. This is the course we shall follow here; the alternative derivation (i) will be discussed in Chapter 6.

[MISSING_PAGE_EMPTY:1787]

[MISSING_PAGE_EMPTY:1788]

[MISSING_PAGE_EMPTY:1789]

**Example 5.1.3**: Let \((X_{1},\ldots,X_{n})\) and \((Y_{1},\ldots,Y_{n})\) be samples from normal distributions \(N(\xi,\sigma^{2})\) and \(N(\eta,\tau^{2})\), respectively. Then \(T=(\bar{X},\sum X_{i}^{2},\bar{Y},\sum Y_{i}^{2})\) is sufficient for (\(\xi\), \(\sigma^{2}\), \(\eta\), \(\tau^{2}\)) and the family of distributions of \(T\) is complete. Since

\[V=\frac{\sum(X_{i}-\bar{X})(Y_{i}-\bar{Y})}{\sqrt{\sum(X_{i}-\bar{X})^{2}(Y_{i} -\bar{Y})^{2}}}\]

is unchanged when \(X_{i}\) and \(Y_{i}\) are replaced by \((X_{i}-\xi)/\sigma\) and \((Y_{i}-\eta)/\tau\), the distribution of \(V\) does not depend on any of the parameters, and Theorem 5.1.2 shows \(V\) to be independent of \(T\).

### Testing the Parameters of a Normal Distribution

The four hypotheses \(\sigma\leq\sigma_{0}\), \(\sigma\geq\sigma_{0}\), \(\xi\leq\xi_{0}\), \(\xi\geq\xi_{0}\) concerning the variance \(\sigma^{2}\) and mean \(\xi\) of a normal distribution were discussed in Section 3.9, and it was pointed out there that at the usual significance levels there exists a UMP test only for the first one. We shall now show that the standard (likelihood ratio) tests are UMP unbiased for the above four hypotheses as well as for some of the corresponding two-sided problems.

For varying \(\xi\) and \(\sigma\), the densities

\[(2\pi\sigma^{2})^{-n/2}\exp\left(-\frac{n\xi^{2}}{2\sigma^{2}}\right)\exp\left( -\frac{1}{2\sigma^{2}}\sum x_{i}^{2}+\frac{\xi}{\sigma^{2}}\sum x_{i}\right) \tag{5.8}\]

of a sample \(X_{1}\),..., \(X_{n}\) from \(N(\xi\), \(\sigma^{2})\) constitute a two-parameter exponential family, which coincides with (5.1) for

\[\theta=-\frac{1}{2\sigma^{2}},\quad\vartheta=\frac{n\xi}{\sigma^{2}},\quad U(X) =\sum x_{i}^{2},\quad T(x)=\bar{x}=\frac{\sum x_{i}}{n}.\]

By Theorem 4.4.1, there exists therefore a UMP unbiased test of the hypothesis \(\theta\geq\theta_{0}\), which for \(\theta_{0}=-1/2\sigma_{0}^{2}\) is equivalent to \(H:\sigma\geq\sigma_{0}\). The rejection region of this test can be obtained from (4.12), with the inequalities reversed because the hypothesis is now \(\theta\geq\theta_{0}\). In the present case this becomes

\[\sum x_{i}^{2}\leq C_{0}(\bar{x}),\]

where

\[p_{\sigma_{0}}\left\{\sum X_{i}^{2}\leq C_{0}(\bar{x})\mid\bar{x}\right\}=\alpha.\]If this is written as

\[\sum x_{i}^{2}-n\bar{x}^{2}<C_{0}^{\prime}(\bar{x})\]

it follows from the independence of \(\sum X_{1}^{2}-n\bar{X}^{2}=\sum(X_{i}-\bar{X})^{2}\) and \(\bar{X}\) (Example 5.1.1) that \(C_{0}^{\prime}(x)\) does not depend on \(\bar{x}\). The test therefore rejects when \(\sum(x_{i}-\bar{x})^{2}\leq C_{0}^{\prime}\), or equivalently when

\[\frac{\sum(x_{i}-\bar{x})^{2}}{\sigma_{0}^{2}}\leq C_{0}, \tag{5.9}\]

with \(C_{0}\) determined by \(P_{\sigma_{0}}\{\sum(X_{i}-\bar{X})^{2}/\sigma_{0}^{2}\leq C_{0}\}=\alpha\). Since \(\sum(X_{i}-\bar{X})^{2}/\sigma_{0}^{2}\) has a \(\chi^{2}\)-distribution with \(n-1\) degrees of freedom, the determining condition for \(C_{0}\) is

\[\int_{0}^{C_{0}}\chi_{n-1}^{2}(y)\,dy=\alpha\, \tag{5.10}\]

where \(\chi_{n-1}^{2}\) denotes the density of a \(\chi^{2}\) variable with \(n-1\) degrees of freedom.

The same result can be obtained through Theorem 5.1.1. A statistic \(V=h(U,\,T)\) of the kind required by the theorem--that is, independent of \(\bar{X}\) for \(\sigma=\sigma_{0}\), and all \(\xi\)--is

\[V=\sum(X_{i}-\bar{X})^{2}=U-nT^{2}.\]

This is in fact independent of \(\bar{X}\) for all \(\xi\) and \(\sigma^{2}\). Since \(h(u,\,t)\) is an increasing function of \(u\) for each \(t\), it follows that the UMP unbiased test has a rejection region of the form \(V\leq C_{0}^{\prime}\).

This derivation also shows that the UMP unbiased rejection region for \(H:\sigma\leq\sigma_{1}\) or \(\sigma\geq\sigma_{2}\) is

\[C_{1}<\sum(x_{i}-\bar{x})^{2}<C_{2}, \tag{5.11}\]

where the \(C\)'s are given by

\[\int_{C_{1}/\sigma_{1}^{2}}^{C_{2}/\sigma_{1}^{2}}\chi_{n-1}^{2}(y)\,dy=\int_{ C_{1}/\sigma_{2}^{2}}^{C_{2}/\sigma_{2}^{2}}\chi_{n-1}^{2}(y)\,dy=\alpha. \tag{5.12}\]

Since \(h(u,\,t)\) is linear in \(u\), it is further seen that the UMP unbiased test of \(H:\sigma=\sigma_{0}\) has the acceptance region

\[C_{1}^{\prime}<\frac{\sum(x_{i}-\bar{x})^{2}}{\sigma_{0}^{2}}<C_{2}^{\prime} \tag{5.13}\]

with the constants determined by \[\int_{C_{1}^{\prime}}^{C_{2}^{\prime}}\chi_{n-1}^{2}(y)\,dy=\frac{1}{n-1}\int_{C_{2 }^{\prime}}^{C_{1}^{\prime}}y\chi_{n-1}^{2}(y)\,dy=1-\alpha. \tag{5.14}\]

This is just the test obtained in Example 4.2.2 with \(\sum(x_{i}-\bar{x})^{2}\) in place of \(\sum x_{i}^{2}\) and \(n-1\) degrees of freedom instead of \(n\), as could have been foreseen. Theorem 5.1.1 shows for this and the other hypotheses considered that the UMP unbiased test depends only on \(V\). Since the distributions of \(V\) do not depend on \(\xi\), and constitute an exponential family in \(\sigma\), the problems are thereby reduced to the corresponding ones for a one-parameter exponential family, which were solved previously.

The power of the above tests can be obtained explicitly in terms of the \(\chi^{2}\)-distribution. In the case of the one-sided test (5.9), for example, it is given by

\[\beta(\sigma)=P_{\sigma}\left\{\frac{\sum(X_{i}-\bar{X})^{2}}{\sigma^{2}}\leq \frac{C_{0}\sigma_{0}^{2}}{\sigma^{2}}\right\}=\int_{0}^{C_{0}\sigma_{0}^{2}/ \sigma^{2}}\chi_{n-1}^{2}(y)\,dy.\]

The same method can be applied to the problems of testing the hypotheses \(\xi\leq\xi_{0}\) against \(\xi>\xi_{0}\) and \(\xi=\xi_{0}\) against \(\xi\neq\xi_{0}\). As is seen by transforming to the variables \(X_{i}-\xi_{0}\), there is no loss of generality in assuming that \(\xi_{0}=0\). It is convenient here to make the identification of (5.8) with (5.1) through the correspondence

\[\theta=\frac{n\xi}{\sigma^{2}},\quad\vartheta=-\frac{1}{2\sigma^{2}},\quad U(x )=\bar{x},\quad T(x)=\sum x_{i}^{2}.\]

Theorem 4.4.1 then shows that UMP unbiased tests exist for the hypotheses \(\theta\leq 0\) and \(\theta=0\), which are equivalent to \(\xi\leq 0\) and \(\xi=0\). Since

\[V=\frac{\bar{X}}{\sqrt{\sum(X_{i}-\bar{X})^{2}}}=\frac{U}{\sqrt{T-nU^{2}}}\]

is independent of \(T=\sum X_{i}^{2}\) when \(\xi=0\) (Example 5.1.1), it follows from Theorem 5.1.1 that the UMP unbiased rejection region for \(H:\xi\leq 0\) is \(V\geq C_{0}^{\prime}\) or equivalently

\[t(x)\geq C_{0}, \tag{5.15}\]

where

\[t(x)=\frac{\sqrt{n}\bar{x}}{\sqrt{\frac{1}{n-1}\sum(x_{i}-\bar{x})^{2}}}. \tag{5.16}\]

In order to apply the theorem to \(H^{\prime}:\xi=0\), let \(W=\bar{X}/\sqrt{\sum X_{i}^{2}}\). This is also independent of \(\sum X_{i}^{2}\) when \(\xi=0\), and in addition is linear in \(U=\bar{X}\). The distribution of \(W\) is symmetric about \(0\) when \(\xi=0\), and conditions (5.4), (5.5), (5.6) with \(W\) in place of \(V\) are therefore satisfied for the rejection region \(|w|\geq C^{\prime}\) with \(P_{\xi=0}\{|W|\geq C^{\prime}\}=\alpha\). Since

\[t(x)=\frac{\sqrt{(n-1)n}W(x)}{\sqrt{1-nW^{2}(x)}},\]

the absolute value of \(t(x)\) is an increasing function of \(|W(x)|\), and the rejection region is equivalent to

\[|t(x)|\geq C. \tag{5.17}\]

From (5.16) it is seen that \(t(X)\) is the ratio of the two independent random \(\sqrt{n}\tilde{X}/\sigma\) and \(\sqrt{\sum(X_{i}-\tilde{X})^{2}/(n-1)\sigma^{2}}\). The denominator is distributed as the square root of a \(\chi^{2}\)-variable with \(n-1\) degrees of freedom, divided by \(n-1\); the distribution of the numerator, when \(\xi=0\), is the normal distribution \(N(0,1)\). The distribution of such a ratio is _Student's_\(t\)-distribution with \(n-1\) degrees of freedom, which has probability density (Problem 5.3)

\[t_{n-1}(y)=\frac{1}{\sqrt{\pi(n-1)}}\frac{\Gamma(\frac{1}{2}n)}{\Gamma\Big{[} \frac{1}{2}(n-1)\Big{]}}\frac{1}{\Big{(}1+\frac{y^{2}}{n-1}\Big{)}^{\frac{1}{ 2}n}}. \tag{5.18}\]

The distribution is symmetric about 0, and the constants \(C_{0}\) and \(C\) of the one- and two-sided tests are determined by

\[\int_{C_{0}}^{\infty}t_{n-1}(y)\,dy=\alpha\ \ \ \text{and}\ \int_{C}^{\infty}t_{n-1}(y)\,dy=\frac{\alpha}{2}. \tag{5.19}\]

For \(\xi\neq 0\), the distribution of \(t(X)\) is the so-called _noncentral_\(t\)-distribution which is derived in Problem 5.3. Some properties of the power function of the one- and two-sided \(t\)-test are given in Problems 5.1, 5.2, and 5.4. We note here that the distribution of \(t(X)\), and therefore the power of the above tests, depends only on the noncentrality parameter \(\delta=\sqrt{n}\xi/\sigma\). This is seen from the expression of the probability density given in Problem 5.3, but can also be shown by the following direct argument. Suppose that \(\xi^{\prime}/\sigma^{\prime}=\xi/\sigma\neq 0\), and denote the common value of \(\xi^{\prime}/\xi\) and \(\sigma^{\prime}/\sigma\) by \(c\), which is then also different from zero. If \(X_{i}^{\prime}=cX_{i}\) and the \(X_{i}\) are distributed as \(N(\xi,\sigma^{2})\), the variables \(X_{i}^{\prime}\) have distribution \(N(\xi^{\prime},\sigma^{\prime 2})\). Also \(t(X)=t(X^{\prime})\), and hence \(t(X^{\prime})\) has the same distribution as \(t(X)\), as was to be proved. [Tables of the power of the \(t\)-test are discussed, for example, in Chapter 31, Section 7 of Johnson, Kotz and Balakrishnan (1995, Vol. 2).]

If \(\xi_{1}\) denotes any alternative value to \(\xi=0\), the power \(\beta(\xi,\sigma)=f(\delta)\) depends on \(\sigma\). As \(\sigma\to\infty\), \(\delta\to 0\), and

\[\beta(\xi_{1},\sigma)\to f(0)=\beta(0,\sigma)=\alpha,\]since \(f\) is continuous by Theorem 2.7.1. Therefore, regardless of the sample size, the probability of detecting the hypothesis to be false when \(\xi\geq\xi_{1}>0\) cannot be made \(\geq\beta>\alpha\) for all \(\sigma\). This is not surprising, since the distributions \(N(0,\sigma^{2})\) and \(N(\xi_{1},\sigma^{2})\) become practically indistinguishable when \(\sigma\) is sufficiently large. To obtain a procedure with guaranteed power for \(\xi\geq\xi_{1}\), the sample size must be made to depend on \(\sigma\). This can be achieved by a sequential procedure, with the stopping rule depending on an estimate of \(\sigma\), but not with a procedure of fixed sample size (see Problems 5.23 and 5.25).

The tests of the more general hypotheses \(\xi\leq\xi_{0}\) and \(\xi=\xi_{0}\) are reduced to those above by transforming to the variables \(X_{i}-\xi_{0}\). The rejection regions for these hypotheses are given as before by (5.15), (5.17), and (5.19), but now with

\[t(x)=\frac{\sqrt{n}(\bar{x}-\xi_{0})}{\sqrt{\frac{1}{n-1}\sum(x_{i}-\bar{x})^{ 2}}}.\]

It is seen from the representation of (5.8) as an exponential family with \(\theta=n\xi/\sigma^{2}\) that there exists a UMP unbiased test of the hypothesis \(a\leq\xi/\sigma^{2}\leq b\), but the method does not apply to the more interesting hypothesis \(a\leq\xi\leq b\);1 nor is it applicable to the corresponding hypothesis for the mean expressed in \(\sigma\)-units: \(a\leq\xi/\sigma\leq b\), which will be discussed in Chapter 6. The dual equivalence problem of testing \(\xi/\sigma\notin[a,b]\) is treated in Brown et al. (1995), Brown, Hwang, and Munk (1997) and Perlman and Wu (1999).

Footnote 1: This problem is discussed in Section 3 of Hodges and Lehmann (1954).

When testing the mean \(\xi\) of a normal distribution, one may from extensive past experience believe \(\sigma\) to be essentially known. If in fact \(\sigma\) is known to be equal to \(\sigma_{0}\), it follows from Problem 3.1 that there exists a UMP test \(\phi_{0}\) of \(H:\xi\leq\xi_{0}\), against \(K:\xi>\xi_{0}\), which rejects when \((\bar{X}-\xi_{0})/\sigma_{0}\) is sufficiently large, and this test is then uniformly more powerful than the \(t\)-test (5.15). On the other hand, if the assumption \(\sigma=\sigma_{0}\) is in error, the size of \(\phi_{0}\) will differ from \(\alpha\) and may greatly exceed it. Whether to take such a risk depends on one's confidence in the assumption and the gain resulting from the use of \(\phi_{0}\) when \(\sigma\) is equal to \(\sigma_{0}\). A measure of this gain is the _deficiency_\(d\) of the \(t\)-test with respect to \(\phi_{0}\), the number of additional observations required by the \(t\)-test to match the power of \(\phi_{0}\) when \(\sigma=\sigma_{0}\). Except for very small \(n\), \(d\) is essentially independent of sample size and for typical values of \(\alpha\) is of the order of 1 to 3 additional observations. [For details see Hodges and Lehmann (1970). Other approaches to such comparisons are reviewed, for example, in Rothenberg (1984).]

### Comparing the Means and Variances of Two Normal Distributions

The problem of comparing the parameters of two normal distributions arises in the comparison of two treatments, products, etc., under conditions similar to those discussed at the beginning of Section 4.5. We consider first the comparison of two variances \(\sigma^{2}\) and \(\tau^{2}\), which occurs for example when one is concerned with the variability of analyses made by two different laboratories or by two different methods, and specifically the hypotheses \(H:\tau^{2}/\sigma^{2}\leq\Delta_{0}\) and \(H^{\prime}:\tau^{2}/\sigma^{2}=\Delta_{0}\).

Let \(X=(X_{1},\ldots,X_{m})\) and \(Y=(Y_{1},\ldots,Y_{n})\) be samples from the normal distributions \(N(\xi,\sigma^{2})\) and \(N(\eta,\tau^{2})\) with joint density

\[C(\xi,\eta,\sigma,\tau)\exp\left(-\frac{1}{2\sigma^{2}}\sum x_{i}^{2}-\frac{1}{ 2\tau^{2}}\sum y_{j}^{2}+\frac{m\xi}{\sigma^{2}}\bar{x}+\frac{n\eta}{\tau^{2} }\bar{y}\right).\]

This is an exponential family with the four parameters

\[\theta=-\frac{1}{2\tau^{2}},\quad\vartheta_{1}=-\frac{1}{2\sigma^{2}},\quad \vartheta_{2}=\frac{n\eta}{\tau^{2}},\quad\vartheta_{3}=\frac{m\xi}{\sigma^{2}}\]

and the sufficient statistics

\[U=\sum Y_{j}^{2},\quad T_{1}=\sum X_{i}^{2},\quad T_{2}=\bar{Y},\quad T_{3}= \bar{X}.\]

It can be expressed equivalently (see Lemma 4.4.1) in terms of the parameters

\[\theta^{*}=-\frac{1}{2\tau^{2}}+\frac{1}{2\Delta_{0}\sigma^{2}},\quad\vartheta _{i}^{*}=\vartheta_{i}\quad(i=1,2,3)\]

and the statistics

\[U^{*}=\sum Y_{j}^{2},\quad T_{1}^{*}=\sum X_{i}^{2}+\frac{1}{\Delta_{0}}\sum Y _{j}^{2},\quad T_{2}^{*}=\bar{Y},\quad T_{3}^{*}=\bar{X}.\]

The hypotheses \(\theta^{*}\leq 0\) and \(\theta^{*}=0\), which are equivalent to \(H\) and \(H^{\prime}\), respectively, therefore possess UMP unbiased tests by Theorem 4.4.1.

When \(\tau^{2}=\Delta_{0}\sigma^{2}\), the distribution of the statistic

\[V=\frac{\sum(Y_{j}-\bar{Y})^{2}/\Delta_{0}}{\sum(X_{i}-\bar{X})^{2}}=\frac{ \sum(Y_{j}-\bar{Y})^{2}/\tau^{2}}{\sum(X_{i}-\bar{X})^{2}/\sigma^{2}}\]

does not depend on \(\sigma,\xi\), or \(\eta\), and it follows from Corollary 5.1.1 that \(V\) is independent of \((T_{1}^{*},\,T_{2}^{*},\,T_{3}^{*})\). The UMP unbiased test of \(H\) is therefore given by (5.2) and (5.3), so that the rejection region can be written as\[\frac{\sum(Y_{j}-\bar{Y})^{2}/\Delta_{0}(n-1)}{\sum(X_{i}-\bar{X})^{2}/(m-1)}\geq C _{0}. \tag{5.20}\]

When \(\tau^{2}=\Delta_{0}\sigma^{2}\), the statistic on the left-hand side of (5.20) is the ratio of the two independent \(\chi^{2}\) variables \(\sum(Y_{j}-\bar{Y})^{2}/\tau^{2}\) and \(\sum(X_{i}-\bar{X})^{2}/\sigma^{2}\), each divided by the number of its degrees of freedom. The distribution of such a ratio is the _F-distribution_ with \(n-1\) and \(m-1\) degrees of freedom, which has the density

\[F_{n-1,m-1}(y) = \frac{\Gamma\bigl{[}\frac{1}{2}(m+n-2)\bigr{]}}{\Gamma\bigl{[} \frac{1}{2}(m-1)\bigr{]}\Gamma\bigl{[}\frac{1}{2}(n-1)\bigr{]}}\left(\frac{n-1 }{m-1}\right)^{\frac{1}{2}(n-1)}\] \[\times\frac{y^{\frac{1}{2}(n-1)-1}}{\bigl{(}1+\frac{n-1}{m-1}y \bigr{)}^{\frac{1}{2}(m+n-2)}}.\]

The constant \(C_{0}\) of (5.20) is then determined by

\[\int_{C_{0}}^{\infty}F_{n-1,m-1}(y)\,dy=\alpha. \tag{5.22}\]

In order to apply Theorem 5.1.1 to \(H^{\prime}\) let

\[W=\frac{\sum(Y_{j}-\bar{Y})^{2}/\Delta_{0}}{\sum(X_{i}-\bar{X})^{2}+(1/\Delta_{ 0})\sum(Y_{j}-\bar{Y})^{2}}.\]

This is also independent of \(T^{*}=(T_{1}^{*},\,T_{2}^{*},\,T_{3}^{*})\) when \(\tau^{2}=\Delta_{0}\sigma^{2}\), and is linear in \(U^{*}\). The UMP unbiased acceptance region of \(H^{\prime}\) is therefore

\[C_{1}\leq W\leq C_{2} \tag{5.23}\]

with the constants determined by (5.5) and (5.6) where \(V\) is replaced by \(W\). On dividing numerator and denominator of \(W\) by \(\sigma^{2}\) it is seen that for \(\tau^{2}=\Delta_{0}\sigma^{2}\), the statistic \(W\) is a ratio of the form \(W_{1}/(W_{1}+W_{2})\), where \(W_{1}\) and \(W_{2}\) are independent \(\chi^{2}\) variables with \(n-1\) and \(m-1\) degrees of freedom respectively. Equivalently, \(W=Y/(1+Y)\), where \(Y=W_{1}/W_{2}\) and where \((m-1)Y/(n-1)\) has the distribution \(F_{n-1,m-1}\). The distribution of \(W\) is the _beta distribution2_ with density \[B_{\frac{1}{2}(n-1),\frac{1}{2}(m-1)}(w)=\frac{\Gamma\big{[}\frac{1}{2 }(m+n-2)\big{]}}{\Gamma\big{[}\frac{1}{2}(m-1)\big{]}\Gamma\big{[}\frac{1}{2}(n -1)\big{]}}w^{\frac{1}{2}(n-3)}(1-w)^{\frac{1}{2}(m-3)},\] \[0<w<1.\]

Conditions (5.5) and (5.6), by means of the relations

\[E(W)=\frac{n-1}{m+n-2}\]

and

\[w\,B_{\frac{1}{2}(n-1),\frac{1}{2}m-1)}(w)=\frac{n-1}{m+n-2}\,B_{\frac{1}{2}(n +1),\frac{1}{2}(m-1)}(w),\]

become

\[\int_{C_{1}}^{C_{2}}B_{\frac{1}{2}(n-1),\frac{1}{2}(m-1)}(w)\,dw=\int_{C_{1}}^ {C_{2}}B_{\frac{1}{2}(n+1),\frac{1}{2}(m-1)}(w)\,dw=1-\alpha. \tag{5.25}\]

The definition of \(V\) shows that its distribution depends only on the ratio \(\tau^{2}/\sigma^{2}\), and so does the distribution of \(W\). The power of the tests (5.20) and (5.23) is therefore also a function only of the variable \(\Delta=\tau^{2}/\sigma^{2}\); it can be expressed explicitly in terms of the \(F\)-distribution, for example in the first case by

\[\beta(\Delta) = P\left\{\frac{\sum(Y_{j}-\bar{Y})^{2}/\tau^{2}(n-1)}{\sum(X_{i} -\bar{X})^{2}/\sigma^{2}(m-1)}\geq\frac{C_{0}\Delta_{0}}{\Delta}\right\}\] \[= \int_{C_{0}\Delta_{0}/\Delta}^{\infty}F_{n-1,m-1}(y)\,dy.\]

The hypothesis of equality of the means \(\xi\), \(\eta\) of two normal distributions with unknown variances \(\sigma^{2}\) and \(\tau^{2}\), the so-called _Behrens-Fisher problem_, is not accessible by the present method. (See Example 4.3.3; for a discussion of this problem, Sections 6.6, 13.2.1 and Example 15.5.4.) We shall therefore consider only the simpler case in which the two variances are assumed to be equal. The joint density of the \(X\)'s and \(Y\)'s is then

\[C(\xi,\eta,\sigma)\exp\left[-\frac{1}{2\sigma^{2}}\left(\sum x_{i}^{2}+\sum y_ {j}^{2}\right)+\frac{\xi}{\sigma^{2}}\sum x_{i}+\frac{\eta}{\sigma^{2}}\sum y_ {j}\right], \tag{5.26}\]

which is an exponential family with parameters

\[\theta=\frac{\eta}{\sigma^{2}},\quad\vartheta_{1}=\frac{\xi}{\sigma^{2}},\quad \vartheta_{2}=-\frac{1}{2\sigma^{2}}\]and the sufficient statistics

\[U=\sum Y_{j},\quad T_{1}=\sum X_{i}\quad T_{2}=\sum X_{i}^{2}+\sum Y_{j}^{2}.\]

For testing the hypotheses

\[H:\eta-\xi\leq 0\quad\text{and}\quad H^{\prime}:\eta-\xi=0\]

it is more convenient to represent the densities as an exponential family with the parameters

\[\theta^{*}=\frac{\eta-\xi}{\left(\frac{1}{m}+\frac{1}{n}\right)\sigma^{2}}, \quad\vartheta_{1}^{*}=\frac{m\xi+n\eta}{(m+n)\sigma^{2}},\quad\vartheta_{2}^ {*}=\vartheta_{2}\]

and the sufficient statistics

\[U^{*}=\bar{Y}-\bar{X},\quad T_{1}^{*}=m\bar{X}+n\bar{Y},\quad T_{2}^{*}=\sum X _{i}^{2}+\sum Y_{j}^{2}.\]

That this is possible is seen from the identity

\[m\xi\bar{x}+n\eta\bar{y}=\frac{(\bar{y}-\bar{x})(\eta-\xi)}{\frac{1}{m}+\frac{ 1}{n}}+\frac{(m\bar{x}+n\bar{y})(m\xi+n\eta)}{m+n}.\]

It follows from Theorem 4.4.1 that UMP unbiased tests exist for the hypotheses \(\theta^{*}\leq 0\) and \(\theta^{*}=0\), and hence for \(H\) and \(H^{\prime}\).

When \(\eta=\xi\), the distribution of

\[V =\frac{\bar{Y}-\bar{X}}{\sqrt{\sum(X_{i}-\bar{X})^{2}+\sum(Y_{j}- \bar{Y})^{2}}}\] \[=\frac{U^{*}}{\sqrt{T_{2}^{*}-\frac{1}{m+n}T_{1}^{*2}-\frac{mn}{m+ n}U^{*2}}}\]

does not depend on the common mean \(\xi\) or on \(\sigma\), as is seen by replacing \(X_{i}\) with \((X_{i}-\xi)/\sigma\) and \(Y_{j}\) with \((Y_{j}-\xi)/\sigma\) in the expression for \(V\), and \(V\) is independent of \((T_{1}^{*},T_{2}^{*})\). The rejection region of the UMP unbiased test of \(H\) can therefore be written as \(V\geq C_{0}^{\prime}\) or

\[t(X,Y)\geq C_{0}, \tag{5.27}\]

where

\[t(X,Y)=\frac{(\bar{Y}-\bar{X})\Big{/}\sqrt{\frac{1}{m}+\frac{1}{n}}}{\sqrt{ \left[\sum(X_{i}-\bar{X})^{2}+\sum(Y_{j}-\bar{Y})^{2}\right]/(m+n-2)}}. \tag{5.28}\]The statistic \(t(X,\,Y)\) is the ratio of the two independent variables

\[\frac{\bar{Y}-\bar{X}}{\sqrt{\left(\frac{1}{m}+\frac{1}{n}\right)\sigma^{2}}}\;\; \;\text{and}\;\sqrt{\frac{\sum(X_{i}-\bar{X})^{2}+\sum(Y_{j}-\bar{Y})^{2}}{(m+n -2)\sigma^{2}}}.\]

The numerator is normally distributed with mean \((\eta-\xi)/\sqrt{m^{-1}+n^{-1}}\sigma\) and unit variance; the square of the denominator as a \(\chi^{2}\) variable with \(m+n-2\) degrees of freedom, divided by \(m+n-2\). Hence \(t(X,\,Y)\) has a noncentral \(t\)-distribution with \(m+n-2\) degrees of freedom and noncentrality parameter

\[\delta=\frac{\eta-\xi}{\sqrt{\frac{1}{m}+\frac{1}{n}}\sigma}.\]

When in particular \(\eta-\xi=0\), the distribution of \(t(X,\,Y)\) is Student's \(t\)-distribution, and the constant \(C_{0}\) is determined by

\[\int_{C_{0}}^{\infty}t_{m+n-2}(y)\,dy=\alpha. \tag{5.29}\]

As before, the assumptions required by Theorem 5.1.1 for \(H^{\prime}\) are not satisfied by \(V\) itself but by a function of \(V\),

\[W=\frac{\bar{Y}-\bar{X}}{\sqrt{\sum X_{i}^{2}+\sum Y_{j}^{2}-\frac{\left(\sum X _{i}+\sum Y_{j}\right)^{2}}{m+n}}}\]

which is related to \(V\) through

\[V=\frac{W}{\sqrt{1-\frac{mn}{m+n}\,W^{2}}}.\]

Since \(W\) is a function of \(V\), it is also independent of \((T_{1}^{*},\,T_{2}^{*})\) when \(\eta=\xi\); in addition it is a linear function of \(U^{*}\) with coefficients dependent only on \(T^{*}\). The distribution of \(W\) being symmetric about \(0\) when \(\eta=\xi\), it follows, as in the derivation of the corresponding rejection region (5.17) for the one-sample problem, that the UMP unbiased test of \(H^{\prime}\) rejects when \(|\,W|\) is too large, or equivalently when

\[|t(X,\,Y)|\,\succ\,C. \tag{5.30}\]

The constant \(C\) is determined by \[\int_{C}^{\infty}t_{m+n-2}(y)\,dy=\frac{\alpha}{2}.\]

The power of the tests (5.27) and (5.30) depends only on \((\eta-\xi)/\sigma\) and is given in terms of the noncentral \(t\)-distribution. Its properties are analogous to those of the one-sample \(t\)-test (Problems 5.1, 5.2, and 5.4).

### Confidence Intervals and Families of Tests

Confidence bounds for a parameter \(\theta\) corresponding to a confidence level \(1-\alpha\) were defined in Section 3.5, for the case that the distribution of the random variable \(X\) depends only on \(\theta\). When nuisance parameters \(\vartheta\) are present the defining condition for a lower confidence bound \(\underline{\theta}\) becomes

\[P_{\theta,\vartheta}\{\underline{\theta}(X)\leq\theta\}\geq 1-\alpha\quad\quad \text{for all $\theta$, $\vartheta$}. \tag{5.31}\]

Similarly, confidence intervals for \(\theta\) at confidence level \(1-\alpha\) are defined as a set of random intervals with end points \(\underline{\theta}(X)\), \(\bar{\theta}(X)\) such that

\[P_{\theta,\vartheta}\{\underline{\theta}(X)\leq\theta\leq\bar{\theta}(X)\} \geq 1-\alpha\quad\quad\text{for all $\theta$, $\vartheta$}. \tag{5.32}\]

The infimum over \((\theta,\vartheta)\) of the left-hand side of (5.31) and (5.32) is the _confidence coefficient_ associated with these statements.

As was already indicated in Chapter 3, confidence statements permit a dual interpretation. Directly, they provide bounds for the unknown parameter \(\theta\) and thereby a solution to the problem of estimating \(\theta\). The statement \(\underline{\theta}\leq\theta\leq\bar{\theta}\) is not as precise as a point estimate, but it has the advantage that the probability of it being correct can be guaranteed to be at least \(1-\alpha\). Similarly, a lower confidence bound can be thought of as an estimate \(\underline{\theta}\) which overestimates the true parameter value with probability \(\leq\alpha\). In particular for \(\alpha=\frac{1}{2}\), if \(\underline{\theta}\) satisfies

\[P_{\theta,\vartheta}\{\underline{\theta}\leq\theta\}=P_{\theta,\vartheta}\{ \underline{\theta}\geq\theta\}=\frac{1}{2},\]

the estimate is as likely to underestimate as to overestimate and is then said to be _median unbiased_. (See Problem 1.3, for the relation of this property to a more general concept of unbiasedness.) For an exponential family given by (4.10) there exists an estimator of \(\theta\) which among all median unbiased estimators uniformly minimizes the risk for any loss function \(L(\theta,\,d)\) that is monotone in the sense of the last paragraph of Section 3.5. A full treatment of this result including some probabilistic and measure-theoretic complications is given by Pfanzagl (1979).

Alternatively, as was shown in Chapter 3, confidence statements can be viewed as equivalent to a family of tests. The following is essentially a review of the discussion of this relationship in Chapter 3, made slightly more specific by restricting attention to the two-sided case. For each \(\theta_{0}\), let \(A(\theta_{0})\) denote the acceptance region of a level-\(\alpha\) test (assumed for the moment to be nonrandomized) of the hypothesis \(H(\theta_{0}):\theta=\theta_{0}\). If

\[S(x)=\{\theta:x\in A(\theta)\}\]

then

\[\theta\in S(x)\ \ \ \mbox{if and only if}\ \ \ x\in A(\theta), \tag{5.33}\]

and hence

\[P_{\theta,\vartheta}\{\theta\in S(X)\}\geq 1-\alpha\ \ \ \ \ \ \mbox{for all $\theta$,}\vartheta. \tag{5.34}\]

Thus any family of level-\(\alpha\) acceptance regions, through the correspondence (5.33), leads to a family of confidence sets at confidence level \(1-\alpha\).

Conversely, given any class of confidence sets \(S(x)\) satisfying (5.34), let

\[A(\theta)=\{x:\theta\in S(x)\}. \tag{5.35}\]

Then the sets \(A(\theta_{0})\) are level-\(\alpha\) acceptance regions for testing the hypotheses \(H(\theta_{0}):\theta=\theta_{0}\), and the confidence sets \(S(x)\) show for each \(\theta_{0}\) whether for the particular \(x\) observed the hypothesis \(\theta=\theta_{0}\) is accepted or rejected at level \(\alpha\).

Exactly the same arguments apply if the sets \(A(\theta_{0})\) are acceptance regions for the hypotheses \(\theta\leq\theta_{0}\). As will be seen below, one- and two-sided tests typically, although not always, lead to one-sided confidence bounds and to confidence intervals respectively.

**Example 5.4.1**: **(Normal mean)** Confidence intervals for the mean \(\xi\) of a normal distribution with unknown variance can be obtained from the acceptance regions \(A(\xi_{0})\) of the hypothesis \(H:\xi=\xi_{0}\). These are given by

\[\frac{|\sqrt{n}(\bar{x}-\xi_{0})|}{\sqrt{\sum(x_{i}-\bar{x})^{2}/(n-1)}}\leq C,\]

where \(C\) is determined from the \(t\)-distribution so that the probability of this inequality is \(1-\alpha\) when \(\xi=\xi_{0}\) [See (5.17) and (5.19) of Section 5.2]. The set \(S(x)\) is then the set of \(\xi\)'s satisfying this inequality with \(\xi=\xi_{0}\), that is, the interval

\[\bar{x}-\frac{C}{\sqrt{n}}\sqrt{\frac{1}{n-1}\sum(x_{i}-\bar{x})^{2}}\leq\xi \leq\bar{x}+\frac{C}{\sqrt{n}}\sqrt{\frac{1}{n-1}\sum(x_{i}-\bar{x})^{2}}. \tag{5.36}\]

The class of these intervals therefore constitutes confidence intervals for \(\xi\) with confidence coefficient \(1-\alpha\).

The length of the intervals (5.36) is proportional to \(\sqrt{\sum(x_{i}-\bar{x})^{2}}\) and their expected length to \(\sigma\). For large \(\sigma\), the intervals will therefore provide little information concerning the unknown \(\xi\). This is a consequence of the fact, which led to similar difficulties for the corresponding testing problem, that two normal distributions \(N(\xi_{0},\sigma^{2})\) and \(N(\xi_{1},\sigma^{2})\) with fixed difference of means become indistinguishable as a tends to infinity. In order to obtain confidence intervals for \(\xi\) whose length does not tend to infinity with \(\sigma\), it is necessary to determine the number of observations sequentially so that it can be adjusted to \(\sigma\). A sequential procedure leading to confidence intervals of prescribed length is given in Problems 5.23 and 5.24.

However, even such a sequential procedure does not really dispose of the difficulty, but only shifts the lack of control from the length of the interval to the number of observations. As \(\sigma\to\infty\), the number of observations required to obtain confidence intervals of bounded length also tends to infinity. Actually, in practice, one will frequently have an idea of the order of magnitude of \(\sigma\). With a sample either of fixed size or obtained sequentially, it is then necessary to establish a balance between the desired confidence \(1-\alpha\), the accuracy given by the length \(l\) of the interval, and the number of observations \(n\) one is willing to expend. In such an arrangement two of the three quantities \(1-\alpha,l\), and \(n\) will be fixed, while the third is a random variable whose distribution depends on \(\sigma\), so that it will be less well controlled than the others. If \(1-\alpha\) is taken as fixed, the choice between a sequential scheme and one of fixed sample size thus depends essentially on whether it is more important to control \(l\) or \(n\).

To obtain lower confidence limits for \(\xi\), consider the acceptance regions

\[\frac{\sqrt{n}(\bar{x}-\xi_{0})}{\sqrt{\sum(x_{i}-\bar{x})^{2}/(n-1)}}\leq C_{0}\]

for testing \(\xi\leq\xi_{0}\) to against \(\xi>\xi_{0}\). The sets \(S(x)\) are then the one-sided intervals

\[\bar{x}-\frac{C_{0}}{\sqrt{n}}\sqrt{\frac{1}{n-1}\sum(x_{i}-\bar{x})^{2}}\leq\xi,\]

the left-hand side of which therefore constitute the desired lower bound for \(\xi\). If \(\alpha=\frac{1}{2}\), the constant \(C_{0}\) is \(0\); the resulting confidence bound \(\xi=\bar{X}\) is a median unbiased estimate of \(\xi\), and among all such estimates it uniformly maximizes

\[P\{-\Delta_{1}\leq\xi-\underline{\xi}\leq\Delta_{2}\}\quad\mbox{ for all }\quad\Delta_{1},\,\Delta_{2}\geq 0.\]

(For a proof see Section 3.5.)

### Unbiased Confidence Sets

Confidence sets can be viewed as a family of tests of the hypotheses \(\theta\in H(\theta^{\prime})\) against alternatives \(\theta\in K(\theta^{\prime})\) for varying \(\theta^{\prime}\). A confidence level of \(1-\alpha\) then simply expresses the fact that all the tests are to be at level \(\alpha\), and the condition therefore becomes \[P_{\theta,\vartheta}\{\theta^{\prime}\in S(X)\}\geq 1-\alpha\quad\quad\text{for all $\theta\in H(\theta^{\prime})$ and all $\vartheta$.} \tag{5.37}\]

In the case that \(H(\theta^{\prime})\) is the hypothesis \(\theta=\theta^{\prime}\) and \(S(X)\) is the interval \([\underline{\theta}(X),\bar{\theta}(X)]\), this agrees with (5.32). In the one-sided case in which \(H(\theta^{\prime})\) is the hypothesis \(\theta\leq\theta^{\prime}\) and \(S(X)=\{\theta:\underline{\theta}(X)\leq\theta\}\), the condition reduces to \(P_{\theta,\vartheta}\{\underline{\theta}(X)\leq\theta^{\prime}\}\geq 1-\alpha\) for all \(\theta^{\prime}\geq\theta\), and this is seen to be equivalent to (5.31). With this interpretation of confidence sets, the probabilities

\[P_{\theta,\vartheta}\{\theta^{\prime}\in S(X)\},\quad\quad\theta\in K(\theta^ {\prime}) \tag{5.38}\]

are the probabilities of false acceptance of \(H(\theta^{\prime})\) (error of the second kind). The smaller these probabilities are, the more desirable are the tests.

From the point of view of estimation, on the other hand, (5.38) is the probability of covering the wrong value \(\theta^{\prime}\). With a controlled probability of covering the true value, the confidence sets will be more informative the less likely they are to cover false values of the parameter. In this sense the probabilities (5.38) provide a measure of the accuracy of the confidence sets. A justification of (5.38) in terms of loss functions was given for the one-sided case in Section 3.5.

In the presence of nuisance parameters, UMP tests usually do not exist, and this implies the nonexistence of confidence sets that are uniformly most accurate in the sense of minimizing (5.38) for all \(\theta^{\prime}\) such that \(\theta\in K(\theta^{\prime})\) and for all \(\vartheta\). This suggests restricting attention to confidence sets which in a suitable sense are unbiased. In analogy with the corresponding definition for tests, a family of confidence sets at confidence level \(1-\alpha\) is said to be _unbiased_ if

\[P_{\theta,\vartheta}\{\theta^{\prime}\in S(X)\}\leq 1-\alpha \tag{5.39}\]

\[\text{for all $\theta^{\prime}$ such that $\theta\in K(\theta^{\prime})$ and for all $\vartheta$ and $\theta$,}\]

so that the probability of covering these false values does not exceed the confidence level.

In the two- and one-sided cases mentioned above, condition (5.39) reduces to

\[P_{\theta,\vartheta}\{\underline{\theta}\leq\theta^{\prime}\leq\bar{\theta}\} \leq 1-\alpha\quad\quad\text{for all $\theta^{\prime}\neq\theta$ and all $\vartheta$}\]

and

\[P_{\theta,\vartheta}\{\underline{\theta}\leq\theta^{\prime}\}\leq 1-\alpha \quad\quad\text{for all $\theta^{\prime}<\theta$ and all $\vartheta$.}\]

With this definition of unbiasedness, unbiased families of tests lead to unbiased confidence sets and conversely. A family of confidence sets is uniformly most accurate unbiased at confidence level \(1-\alpha\) if it minimizes the probabilities

\[P_{\theta,\vartheta}\{\theta^{\prime}\in S(X)\}\text{ for all $\theta^{\prime}$ such that $\theta\in K(\theta^{\prime})$ and for all $\vartheta$ and $\theta$,}\]subject to (5.37) and (5.39). The confidence sets obtained on the basis of the UMP unbiased tests of the present and preceding chapter are therefore uniformly most accurate unbiased. This applies in particular to the confidence intervals obtained in the preceding sections. Some further examples are the following.

**Example 5.5.1**: **(Normal variance)** If \(X_{1},\ldots,\)\(X_{n}\) is a sample from \(N(\xi,\sigma^{2})\), the UMP unbiased test of the hypothesis \(\sigma=\sigma_{0}\) is given by the acceptance region (5.13)

\[C^{\prime}_{1}\leq\frac{\sum(x_{i}-\bar{x})^{2}}{\sigma_{0}^{2}}\leq C^{\prime }_{2},\]

where \(C^{\prime}_{1}\) and \(C^{\prime}_{2}\) are determined by (5.14). The most accurate unbiased confidence intervals for \(\sigma^{2}\) are therefore

\[\frac{1}{C^{\prime}_{2}}\sum(x_{i}-\bar{x})^{2}\leq\sigma^{2}\leq\frac{1}{C^{ \prime}_{1}}\sum(x_{i}-\bar{x})^{2}.\]

[Tables of \(C^{\prime}_{1}\) and \(C^{\prime}_{2}\) are provided by Tate and Klett (1959).] Similarly, from (5.9) and (5.10) the most accurate unbiased upper confidence limits for \(\sigma^{2}\) are

\[\sigma^{2}\leq\frac{1}{C_{0}}\sum(x_{i}-\bar{x})^{2},\]

where

\[\int_{C_{0}}^{\infty}\chi_{n-1}^{2}(y)\,dy=1-\alpha.\]

The corresponding lower confidence limits are uniformly most accurate (without the restriction of unbiasedness) by Section 3.9.

**Example 5.5.2**: **(Difference of means)** Confidence intervals for the difference \(\Delta=\eta-\xi\) of the means of two normal distributions with common variance are obtained from tests of the hypothesis \(\eta-\xi=\Delta_{0}\). If \(X_{1},\ldots,\)\(X_{m}\) and \(Y_{1},\ldots,\)\(Y_{n}\) are distributed as \(N(\xi,\sigma^{2})\) and \(N(\eta,\sigma^{2})\) respectively, and if \(Y^{\prime}_{j}=Y_{j}-\Delta_{0}\), \(\eta^{\prime}=\eta-\Delta_{0}\), the hypothesis can be expressed in terms of the variables \(X_{i}\) and \(Y^{\prime}_{j}\) as \(\eta^{\prime}-\xi=0\). From (5.28) and (5.30), the UMP unbiased acceptance region is then seen to be

\[\frac{|(\bar{y}-\bar{x}-\Delta_{0})|\Big{/}\sqrt{\frac{1}{m}+\frac{1}{n}}}{ \sqrt{\Big{[}\sum(x_{i}-\bar{x})^{2}+\sum(y_{j}-\bar{y})^{2}\Big{]}\Big{/}(m+n -2)}}\leq C,\]

where \(C\) is determined by Eq. (5.30). The most accurate unbiased confidence intervals for \(\eta-\xi\) are therefore

\[(\bar{y}-\bar{x})-CS\leq\eta-\xi\leq(\bar{y}-\bar{x})+CS, \tag{5.40}\]where

\[S^{2}=\left(\frac{1}{m}+\frac{1}{n}\right)\frac{\sum(x_{i}-\bar{x})^{2}+\sum(y_{j} -\bar{y})^{2}}{m+n-2}.\]

The one-sided intervals are obtained analogously.

**Example 5.5.3**: **(Ratio of variances)** If \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) are samples from \(N(\xi,\sigma^{2})\) and \(N(\eta,\tau^{2})\), most accurate unbiased confidence intervals for \(\Delta=\tau^{2}/\sigma^{2}\) are derived from the acceptance region (5.23) as

\[\frac{1-C_{2}}{C_{2}}\frac{\sum(y_{j}-\bar{y})^{2}}{\sum(x_{i}-\bar{x})^{2}} \leq\frac{\tau^{2}}{\sigma^{2}}\leq\frac{1-C_{1}}{C_{1}}\frac{\sum(y_{j}-\bar{ y})^{2}}{\sum(x_{i}-\bar{x})^{2}}, \tag{5.41}\]

where \(C_{1}\) and \(C_{2}\) are determined from (5.25).3 In the particular case that \(m=n\), the intervals take on the simpler form

Footnote 3: A comparison of these limits with those obtained from the equal-tails test is given by Scheffe (1942); some values of \(C_{1}\) and \(C_{2}\) are provided by Ramachandran (1958).

\[\frac{1}{k}\frac{\sum(y_{j}-\bar{y})^{2}}{\sum(x_{i}-\bar{x})^{2}}\leq\frac{ \tau^{2}}{\sigma^{2}}\leq k\frac{\sum(y_{j}-\bar{y})^{2}}{\sum(x_{i}-\bar{x})^ {2}}, \tag{5.42}\]

where \(k\) is determined from the \(F\)-distribution. Most accurate unbiased lower confidence limits for the variance ratio are

\[\underline{\Delta}=\frac{1}{C_{0}}\frac{\sum(y_{j}-\bar{y})^{2}/(n-1)}{\sum(x_ {i}-\bar{x})^{2}/(m-1)}\leq\frac{\tau^{2}}{\sigma^{2}} \tag{5.43}\]

with \(C_{0}\) given by (5.22). If in (5.22) \(\alpha\) is taken to be \(\frac{1}{2}\), this lower confidence limit \(\underline{\Delta}\) becomes a median unbiased estimate of \(\tau^{2}/\sigma^{2}\). Among all such estimates it uniformly minimizes

\[P\left\{-\Delta_{1}\leq\frac{\tau^{2}}{\sigma^{2}}-\underline{\Delta}\leq \Delta_{2}\right\}\qquad\mbox{for all}\quad\Delta_{1},\,\Delta_{2}\geq 0.\]

(for a proof see Section 3.5).

So far it has been assumed that the tests from which the confidence sets are obtained are nonrandomized. The modifications that are necessary when this assumption is not satisfied were discussed in Chapter 3. The randomized tests can then be interpreted as being nonrandomized in the space of \(X\) and an auxiliary variable \(V\) which is uniformly distributed on the unit interval. If, in particular, \(X\) is integer-valued as in the binomial or Poisson case, the tests can be represented in terms of the continuous variable \(X+V\). In this way, most accurate unbiased confidence intervals can be obtained, for example, for a binomial probability \(p\) from the UMP unbiased tests of \(H:p=p_{0}\) (Example 4.2.1). It is not clear a priori that the resulting confidence sets for \(p\) will necessarily be intervals. This is, however, a consequence of the following lemma.

**Lemma 5.5.1**: _Let \(X\) be a real-valued random variable with probability density \(p_{\theta}(x)\) which has monotone likelihood ratio in \(x\). Suppose that UMP unbiased tests of the hypotheses \(H(\theta_{0}):\theta=\theta_{0}\) exist and are given by the acceptance regions_

\[C_{1}(\theta_{0})\leq x\leq C_{2}(\theta_{0})\]

_and that they are strictly unbiased. Then the functions \(C_{i}(\theta)\) are strictly increasing in \(\theta\), and the most accurate unbiased confidence intervals for \(\theta\) are_

\[C_{2}^{-1}(x)\leq\theta\leq C_{1}^{-1}(x).\]

Proof. Let \(\theta_{0}<\theta_{1}\), and let \(\beta_{0}(\theta)\) and \(\beta_{1}(\theta)\) denote the power functions of the above tests \(\phi_{0}\) and \(\phi_{1}\), for testing \(\theta=\theta_{0}\) and \(\theta=\theta_{1}\). It follows from the strict unbiasedness of the tests that

\[E_{\theta_{0}}\left[\phi_{1}(X)-\phi_{0}(X)\right] =\beta_{1}(\theta_{0})-\alpha>0>\alpha-\beta_{0}(\theta_{1})\] \[=E_{\theta_{1}}\left[\phi_{1}(X)-\phi_{0}(X)\right].\]

Thus neither of the two intervals \(\left[C_{1}(\theta_{i}),\,C_{2}(\theta_{i})\right]\) (\(i=0\), \(1\)) contains the other, and it is seen from Lemma 3.4.2(iii) that \(C_{i}(\theta_{0})<C_{i}(\theta_{1})\) for \(i=1\), \(2\). The functions \(C_{i}\) therefore have inverses, and the inequalities defining the acceptance region for \(H(\theta)\) are equivalent to \(C_{2}^{-1}(x)\leq\theta\leq C_{1}^{-1}(x)\), as was to be proved.

The situation is indicated in Figure 5.1. For fixed \(\theta_{0}\), the acceptance region \(A(\theta_{0})\) corresponds to a horizontal line segment, whose endpoints trace out two curves as \(\theta_{0}\) varies. The top curve corresponds to \(C_{1}^{-1}(x)\) and the bottom curve corresponds to \(C_{2}^{-1}(x)\).

By Section 4.2, the conditions of the lemma are satisfied in particular for a one-parameter exponential family, provided the tests are nonrandomized. In cases such as that of binomial or Poisson distributions, where the family is exponential but \(X\) is integer-valued so that randomization is required, the intervals can be obtained by

Figure 5.1: Construction of confidence set \(S(x)\)applying the lemma to the variable \(X+V\) instead of \(X\), where \(V\) is independent of \(X\) and uniformly distributed over \((0,1)\).

**Example 5.5.4**: In the binomial case, a table of the (randomized) uniformly most accurate unbiased confidence intervals is given by Blyth and Hutchinson (1960). The best choice of nonrandomized intervals and some approximations are discussed (and tables provided) by Blyth and Still (1983) and Blyth (1986). The Clopper-Pearson interval was discussed in Example 3.5.2. Recent approximations and comparisons are provided by Agresti and Coull (1998) and Brown Cai and DasGupta (2001, 2002). A large-sample approach will be considered in Example 11.3.4.

In Lemma 5.5.1, the distribution of \(X\) was assumed to depend only on \(\theta\). Consider now the exponential family (5.1) in which nuisance parameters are present in addition to \(\theta\). The UMP unbiased tests of \(\theta=\theta_{0}\) are then performed as conditional tests given \(T=t\), and the confidence intervals for \(\theta\) will as a consequence also be obtained conditionally. If the conditional distributions are continuous, the acceptance regions will be of the form

\[C_{1}(\theta;t)\leq u\leq C_{2}(\theta;t),\]

where for each \(t\) the functions \(C_{i}\) are increasing by Lemma 5.5.1. The confidence intervals are then

\[C_{2}^{-1}(u;t)\leq\theta\leq C_{1}^{-1}(u;t).\]

If the conditional distributions are discrete, continuity can be obtained as before through addition of a uniform variable.

**Example 5.5.5**: **(Poisson ratio)** Let \(X\) and \(Y\) be independent Poisson variables with means \(\lambda\) and \(\mu\), and let \(\rho=\mu/\lambda\). The conditional distribution of \(Y\) given \(X+Y=t\) is the binomial distribution \(b(p,t)\) with

\[p=\frac{\rho}{1+\rho}.\]

The UMP unbiased test \(\phi(y,t)\) of the hypothesis \(\rho=\rho_{0}\) is defined for each \(t\) as the UMP unbiased conditional test of the hypothesis \(\rho=\rho_{0}/(1+\rho_{0})\). If

\[\underline{p}(t)\leq p\leq\bar{p}(t)\]

are the associated most accurate unbiased confidence intervals for \(p\) given \(t\), it follows that the most accurate unbiased confidence intervals for \(\mu/\lambda\) are

\[\frac{p(t)}{1-\underline{p}(t)}\leq\frac{\mu}{\lambda}\leq\frac{\bar{p}(t)}{1- \bar{p}(t)}.\]

The binomial tests which determine the functions \(\underline{p}(t)\) and \(\bar{p}(t)\) are discussed in Example 4.2.1.

### Regression

The relation between two variables \(X\) and \(Y\) can be studied by drawing an unrestricted sample and observing the two variables for each subject, obtaining \(n\) pairs of measurements \((X_{1},\,Y_{1}),\,\ldots,\,(X_{n},\,Y_{n})\) (see Section 5.13 and Problem 5.13). Alternatively, it is frequently possible to control one of the variables such as the age of a subject, the temperature at which an experiment is performed, or the strength of the treatment that is being applied. Observations \(Y_{1},\,\ldots,\,Y_{n}\) of \(Y\) can then be obtained at a number of predetermined levels \(x_{1},\,\ldots,\,x_{n}\) of \(x\). Suppose that for fixed \(x\) the distribution of \(Y\) is normal with constant variance \(\sigma^{2}\) and a mean which is a function of \(x\), _the regression of \(Y\) on \(x\)_, and which is assumed to be linear,4

Footnote 4: The literature on regression is enormous and we treat the simplest model. Some texts on the subject include Weisberg (1985), Atkinson and Riani (2000) and Chatterjee Hadi and Price (2000).

\[E[Y|x]=\alpha+\beta x.\]

If we put \(v_{i}=(x_{i}-\bar{x})/\sqrt{\sum(x_{j}-\bar{x})^{2}}\) and \(\gamma+\delta v_{i}=\alpha+\beta x_{i}\), so that \(\sum v_{i}=0\), \(\sum v_{i}^{2}=1\), and

\[\alpha=\gamma-\delta\frac{\bar{x}}{\sqrt{\sum(x_{j}-\bar{x})^{2}}},\ \ \ \ \ \beta=\frac{\delta}{\sqrt{\sum(x_{j}-\bar{x})^{2}}},\]

the joint density of \(Y_{1},\,\ldots,\,Y_{n}\) is

\[\frac{1}{(\sqrt{2\pi}\sigma)^{n}}\exp\left[-\frac{1}{2\sigma^{2}}\sum(y_{i}- \gamma-\delta v_{i})^{2}\right].\]

These densities constitute an exponential family (5.1) with

\[U=\sum v_{i}\,Y_{i},\ T_{1}=\sum Y_{i}^{2},\ T_{2}=\sum Y_{i}\] \[\theta=\tfrac{\delta}{\sigma^{2}},\ \ \ \ \ \ \vartheta_{1}=-\tfrac{1}{2\sigma^{2}},\ \vartheta_{2}=\tfrac{\gamma}{\sigma^{2}}.\]

This representation implies the existence of UMP unbiased tests of the hypotheses \(a\gamma+b\delta=c\) where \(a\), \(b\), and \(c\) are given constants, and therefore of most accurate unbiased confidence intervals for the parameter

\[\rho=a\gamma+b\delta.\]

To obtain these confidence intervals explicitly, one requires the UMP unbiased test of \(H:\rho=\rho_{0}\), which is given by the acceptance region\[\frac{|b\sum v_{i}Y_{i}+a\bar{Y}-\rho_{0}|\Big{/}\sqrt{(a^{2}/n)+b^{2}}}{\sqrt{ \left[\sum(Y_{i}-\bar{Y})^{2}-\left(\sum v_{i}Y_{i}\right)^{2}\right]\Big{/}(n- 2)}},\leq C \tag{5.44}\]

where

\[\int_{-C}^{C}t_{n-2}(y)\,dy=1-\alpha\,;\]

see Problem 5.33. The resulting confidence intervals for \(\rho\) are centered at \(b\sum v_{i}Y_{i}\,+\,a\bar{Y}\), and their length is

\[L=2C\sqrt{\left[\frac{a^{2}}{n}+b^{2}\right]\frac{\sum(Y_{i}-\bar{Y})^{2}- \left(\sum v_{i}Y_{i}\right)^{2}}{n-2}}.\]

It follows from the transformations given in Problem 5.33 that \(\left[\sum(Y_{i}-\bar{Y})^{2}-(\sum v_{i}Y_{i})^{2}\right]/\sigma^{2}\) has a \(\chi^{2}\)-distribution with \(n-2\) degrees of freedom and hence that the expected length of the intervals is

\[E(L)=2C_{n}\sigma\sqrt{\frac{a^{2}}{n}+b^{2}}.\]

In particular applications, \(a\) and \(b\) typically are functions of the \(x\)'s. If these are at the disposal of the experimenter and there is therefore some choice with respect to \(a\) and \(b\), the expected length of \(L\) is minimized by minimizing \((a^{2}/n)+b^{2}\). Actually, it is not clear that the expected length is a good criterion for the accuracy of confidence intervals, since short intervals are desirable when they cover the true parameter value but not necessarily otherwise. However, the same result holds for other criteria such as the expected value of \((\bar{\rho}-\rho)^{2}+(\rho-\rho)^{2}\) or more generally of \(f_{1}(|\bar{\rho}-\rho|)+f_{2}(|\rho-\rho|)\), where \(f_{1}\) and \(f_{2}\) are increasing functions of their arguments. (See Problem 5.33.) Furthermore, the same choice of \(a\) and \(b\) also minimizes the probability of the intervals covering any false value of the parameter. We shall therefore consider \((a^{2}/n)+b^{2}\) as an inverse measure of the accuracy of the intervals.

**Example 5.6.1**: **(Slope of regression line)**__Confidence levels for the slope \(\beta=\delta/\sqrt{\sum(x_{j}-\bar{x})^{2}}\) are obtained from the above intervals by letting \(a=0\) and \(b=1/\sqrt{\sum(x_{j}-\bar{x})^{2}}\). Here the accuracy increases with \(\sum(x_{j}-\bar{x})^{2}\), and if the \(x_{j}\) must be chosen from an interval [\(C_{0}\), \(C_{1}\)], it is maximized by putting half of the values at each end point. However, from a practical point of view, this is frequently not a good design, since it permits no check of the linearity of the regression. \(\blacksquare\)

**Example 5.6.2**: **(Ordinate of regression line)**__Another parameter of interest is the value \(\alpha+\beta x_{0}\) to be expected from an observation \(Y\) at \(x=x_{0}\). Since

\[\alpha+\beta x_{0}=\gamma+\frac{\delta(x_{0}-\bar{x})}{\sqrt{\sum(x_{j}-\bar{x })^{2}}},\]the constants \(a\) and \(b\) are \(a=1\), \(b=(x_{0}-\bar{x})/\sqrt{\sum(x_{j}-\bar{x})^{2}}\). The maximum accuracy is obtained by minimizing \(|\bar{x}-x_{0}|\) and, if \(\bar{x}=x_{0}\) cannot be achieved exactly, also maximizing \(\sum(x_{j}-\bar{x})^{2}\).

**Example 5.6.3**: **(Intercept of regression line)** Frequently it is of interest to estimate the point \(x\) at which \(\alpha+\beta x\) has a preassigned value. One may for example wish to find the dosage \(x=-\alpha/\beta\) at which \(E(Y\mid x)=0\), or equivalently the value \(v=(x-\bar{x})/\sqrt{\sum(x_{j}-\bar{x})^{2}}\) at which \(\gamma+\delta v=0\). Most accurate unbiased confidence sets for the solution \(-\gamma/\delta\) of this equation can be obtained from the UMP unbiased tests of the hypotheses \(-\gamma/\delta=v_{0}\). The acceptance regions of these tests are given by (5.44) with \(a=1\), \(b=v_{0}\), and \(\rho_{0}=0\), and the resulting confidence sets for \(v\) are the sets of values \(v\) satisfying

\[v^{2}\left[C^{2}S^{2}-\left(\sum v_{i}Y_{i}\right)^{2}\right]-2v\bar{Y}\left( \sum v_{i}Y_{i}\right)+\frac{1}{n}(C^{2}S^{2}-n\bar{Y}^{2})\geq 0,\]

where \(S^{2}=[\sum(Y_{i}-\bar{Y})^{2}(\sum v_{i}Y_{i})^{2}]/(n-2)\). If the associated quadratic equation in \(v\) has roots \(\underline{v}\), \(\bar{v}\), the confidence statement becomes

\[\underline{v}\leq v\leq\bar{v}\ \ \ \ \ \ \mbox{when}\ \ \ \frac{\left|\sum v _{i}Y_{i}\right|}{S}>C\]

and

\[v\leq\underline{v}\mbox{ or }v\geq\bar{v}\ \ \ \ \ \mbox{when}\ \ \ \frac{\left| \sum v_{i}Y_{i}\right|}{S}<C.\]

The somewhat surprising possibility that the confidence sets may be the outside of an interval actually is quite appropriate here. When the line \(y=\gamma+\delta v\) is nearly parallel to the \(v\)-axis, the intercept with the \(v\)-axis will be large in absolute value, but its sign can be changed by a very small change in angle. There is the further possibility that the discriminant of the quadratic polynomial is negative,

\[n\bar{Y}^{2}+\left(\sum v_{i}Y_{i}\right)^{2}<C^{2}S^{2},\]

in which case the associated quadratic equation has no solutions. This condition implies that the leading coefficient of the quadratic polynomial is positive, so that the confidence set in this case becomes the whole real axis. The fact that the confidence sets are not necessarily finite intervals has led to the suggestion that their use be restricted to the cases in which they do have this form. Such usage will however affect the probability with which the sets cover the true value and hence the validity of the reported confidence coefficient.5

### Bayesian Confidence Sets

The left side of the confidence statement (5.34) denotes the probability that the random set \(S(X)\) will contain the constant point \(\theta\). The interpretation of this probability statement, before \(X\) is observed, is clear: it refers to the frequency with which this random event will occur. Suppose for example that \(X\) is distributed as \(N\left(\theta,\,1\right)\), and consider the confidence interval

\[X-1.96<\theta<X+1.96\]

corresponding to confidence coefficient \(\gamma=0.95\). Then the random interval \((X-1.96,\,X+1.96)\) will contain \(\theta\) with probability \(0.95\). Suppose now that \(X\) is observed to be \(2.14\). At this point, the earlier statement reduces to the inequality \(0.18<\theta<4.10\), which no longer involves any random element. Since the only unknown quantity is \(\theta\), it is tempting (but not justified) to say that \(\theta\) lies between \(0.18\) and \(4.10\) with probability \(0.95\).

To attach a meaningful probability to the event \(\theta\in S(x)\) when \(x\) is fixed requires that \(\theta\) be random. Inferences made under the assumption that the parameter \(\theta\) is itself a random (though unobservable) quantity with a known distribution are called _Bayesian_, and the distribution \(\Lambda\) of \(\theta\) before any observations are taken its _prior distribution_. After \(X=x\) has been observed, inferences concerning \(\theta\) can be based on its conditional distribution given \(x\), the _posterior distribution_. In particular, any set \(S(x)\) with the property

\[P\left[\theta\in S(x)\mid X=x\right]\geq\gamma\quad\text{ for all }x\]

is a \(100\gamma\)% Bayesian confidence set or _credible region_ for \(\theta\). In the rest of this section, the random variable with prior distribution \(\Lambda\) will be denoted by \(\Theta\), with \(\theta\) being the value taken on by \(\Theta\) in the experiment at hand.

**Example 5.7.1**: **(Normal mean)** Suppose that \(\Theta\) has a normal prior distribution \(N(\mu,b^{2})\) and that given \(\Theta=\theta\), the variables \(X_{1}\),..., \(X_{n}\) are independent \(N(\theta,\,\sigma^{2})\), \(\sigma\) known. Then the posterior distribution of \(\Theta\) given \(x_{1}\),..., \(x_{n}\) is normal with mean (Problem 5.34)

\[\eta_{x}=E[\Theta\mid x]=\frac{n\bar{x}/\sigma^{2}+\mu/b^{2}}{n/\sigma^{2}+1/b ^{2}}\]

and variance

\[\tau_{x}^{2}=Var[\Theta\mid x]=\frac{1}{n/\sigma^{2}+1/b^{2}}.\]

Since \([\Theta-\eta_{x}]/\tau_{x}\) then has a standard normal distribution, the interval \(I\left(x\right)\) with endpoints\[\frac{n\bar{x}/\sigma^{2}+\mu/b^{2}}{n/\sigma^{2}+1/b^{2}}\pm\frac{1.96}{\sqrt{n/ \sigma^{2}+1/b^{2}}}\]

satisfies \(P[\Theta\in I(x)\mid X=x]=0.95\) and is thus a 95% credible region.

For \(n=1\), \(\mu=0\), \(\sigma=1\), the interval reduces to

\[\frac{x}{1+\frac{1}{b^{2}}}\pm\frac{1.96}{\sqrt{1+\frac{1}{b^{2}}}}\]

which for large \(b\) is very close to the confidence interval for \(\theta\) stated at the beginning of the section. But now the statement that \(\theta\) lies between these limits with probability 0.95 is justified, since it is a probability statement concerning the random variable \(\Theta\).

The distribution \(N(\mu,b^{2})\) assigns higher probability to \(\theta\)-values near \(\mu\) than to those further away. Suppose instead that no information whatever about \(\theta\) is available, so that one wishes to model a state of complete ignorance. This could be done by assigning a constant density to all values of \(\theta\), that is, by assigning to \(\Theta\) the density \(\pi(\theta)\equiv c\), \(-\infty<\theta<\infty\). Unfortunately, the resulting \(\pi\) is not a probability density, since \(\int_{-\infty}^{\infty}\pi(\theta)\;d\theta=\infty\). However, if this fact is ignored and the posterior distribution of \(\Theta\) given \(x\) is calculated in the usual way, it turns out (Problem 5.35) that \(\pi(\theta\mid x)\) is the density of a genuine probability distribution, namely \(N(\mu,\,\sigma^{2}/n)\), the limit of the earlier posterior distribution as \(b\to\infty\). The _improper_ (since it integrates to infinity), _noninformative_ prior density \(\pi(\theta)\equiv c\) thus leads approximately to the same results as the normal prior \(N(\mu,\,b^{2})\) for large \(b\), and can be viewed as an approximation to the latter.

Unlike confidence sets, Bayesian credible regions provide exactly the desired kind of probability statement even after the observations are known. They do so, however, at the cost of an additional assumption: that \(\theta\) is random and has a known prior distribution. Detailed accounts of the Bayesian approach, its application to credible regions, and comparison of the two approaches can be found in Berger (1985a) and Robert (1994). The following examples provide a few illustrations and additional comments.

**Example 5.7.2**: Let \(X\) be binomial \(b(p,n)\), and suppose that the prior distribution for \(p\) is the beta distribution6\(B(a,b)\) with density \(Cp^{a-1}(1-p)^{b-1}\), \(0<p<1\), \(0<a,b\). Then the posterior distribution of \(p\) given \(X=x\) is the beta distribution \(B(a+x,\,b+n-x)\) (Problem 5.36). There are of course many sets \(S(x)\) whose probability under this distribution is equal to the prescribed coefficient \(\gamma\). A choice that is frequently recommended is the HPD (highest probability density) region, defined by the requirement that the posterior density of \(p\) given \(x\) be \(\geq k\).

With a beta prior, only the following possibilities can occur: for fixed \(x\),

1. \(\pi(p\mid x)\) is decreasing,
2. \(\pi(p\mid x)\) is increasing,
3. \(\pi(p\mid x)\) is increasing in (0, \(p_{0}\)) and decreasing in (\(p_{0}\), 1) for some \(p_{0}\),
4. \(\pi(p\mid x)\) is U-shaped, i.e., decreasing in (0, \(p_{0}\)) and increasing in (\(p_{0},1\)) for some \(p_{0}\).

The HPD region then is of the form

1. \(p<K(-x)\),
2. \(p>K(x)\),
3. \(K_{1}(x)<p<K_{2}(x)\),
4. \(p<K_{1}(x)\) or \(p>K_{2}(x)\),

where the \(K\)'s are determined by the requirement that the posterior probability of the region, given \(x\), be \(\gamma\); in cases (c) and (d) this condition must be supplemented by

\[\pi[K_{1}(x)\mid x]=\pi[K_{2}(x)\mid x].\]

In general, if \(\pi(\theta\mid x)\) denotes the posterior density of \(\theta\), the HPD region is defined by

\[\pi(\theta\mid x)\geq k\]

with \(C\) determined by the size condition

\[P[\pi(\theta)\mid x)\geq k]=\gamma.\]

**Example 5.7.3**: **(Two-parameter normal mean)** Let \(X_{1},\ldots,X_{n}\) be independent \(N(\xi,\sigma^{2})\), and for the sake of simplicity suppose that \((\xi,\sigma)\) has the joint improper prior density given by

\[\pi(\xi,\sigma)\,d\xi\,d\sigma=d\xi\frac{1}{\sigma}\,d\sigma\quad\quad\mbox{ for all}\quad-\infty<\xi<\infty,\ 0<\sigma,\]

which is frequently used to model absence of information concerning the parameters. Then the joint posterior density of \((\xi,\sigma)\) given \(x=(x_{1},\ldots,x_{n})\) is of the form

\[\pi(\xi,\sigma\mid x)\,d\xi\,d\sigma=C(x)\frac{1}{\sigma^{n+1}}\exp\left(- \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(\xi-x_{i})^{2}\right)\,d\xi\,d\sigma.\]

Determination of a credible region for \(\xi\) requires the marginal posterior density of given \(x\), which is obtained by integrating the joint posterior density with respect to \(\sigma\). These densities depend only on the sufficient statistics \(\bar{x}\) and \(S^{2}=\sum(x_{i}-\bar{x})^{2}\), and the posterior density of \(\xi\) is of the form (Problem 5.37)\[A(x)\left[\frac{1}{1+\frac{n(\xi-\bar{x})^{2}}{S^{2}}}\right]^{n/2}.\]

Here \(\bar{x}\) and \(S\) enter only as location and scale parameters, and the linear function

\[t=\frac{\sqrt{n}(\xi-\bar{x})}{S/\sqrt{n-1}}\]

of \(\xi\) has the \(t\)-distribution with \(n-1\) degrees of freedom. Since this agrees with the distribution of \(t\) for fixed \(\xi\) and \(\sigma\) given in Section 5.2, the credible \(100(1-\alpha)\)% region

\[\left|\frac{\sqrt{n}(\xi-\bar{x})}{S/\sqrt{n-1}}\right|\leq C\]

is formally identical with the confidence intervals (5.36). However, they are derived under different assumptions, and their interpretation differs accordingly. The relationship between Bayesian intervals and classical intervals is further explored in Nicolaou (1993) and Severini (1993).

**Example 5.7.4**: **(Two-parameter normal: estimating \(\sigma\))** Under the assumptions of the preceding example, credible regions for \(\sigma\) are based on the posterior distribution of \(\sigma\) given \(x\), obtained by integrating the joint posterior density of \((\xi,\sigma)\) with respect to \(\xi\). Using the fact that \(\sum(\xi-x_{i})^{2}=n(\xi-\bar{x})^{2}+\sum(x_{i}-\bar{x})^{2}\), it is seen (Problem 5.38) that given \(x\), the conditional (posterior) distribution of \(\sum(x_{i}-\bar{x})^{2}/\sigma^{2}\) is \(\chi^{2}\) with \(n-1\) degrees of freedom. As in the case of the mean, this agrees with the sampling distribution of the same quantity when \(a\) is a (constant) parameter, given in Section 5.2. (The agreement in both cases of two distributions derived under such different assumptions is a consequence of the particular choice of the prior distribution and the fact that it is invariant in the sense of Lehmann and Casella (1998), Section 4.4.) A change of variables now gives the posterior density of \(\sigma\) and shows that \(\pi(\sigma\mid x)\) is of the form (c) of Example 5.7.2, so that the HPD region is of the form \(K_{1}(x)<\sigma<K_{2}(x)\) with \(0<K_{1}(x)<K_{2}(x)<\infty\).

Suppose that a credible region is required, not for \(\sigma\), but for \(\sigma^{r}\) for some \(r>0\). For consistency, this should then be given by \([K_{1}(x)]^{r}<\sigma^{r}<[K_{2}(x)]^{r}\), but this is not the case, since the relative height of the density of a random variable at two points is not invariant under monotone transformations of the variable. In fact, in the present case, the HPD region for \(\sigma^{r}\) will become one-sided for sufficiently large \(r\) although it is two-sided for \(r=1\) (Problem 5.38).

Such inconsistencies do not occur if the HPD region is replaced by the equal-tails interval \((C_{1}(x),C_{2}(x))\) for which \(P[\Theta<C_{1}(x)\mid X=x]=P[\Theta>C_{2}(x)\mid X=x]=(1-\gamma)/2\).7 More generally inconsistencies under transformations of \(\Theta\) are avoided when the posterior distribution of \(\Theta\) is summarized by a number of its percentiles corresponding to the standard confidence points mentioned in Section 3.5. Such a set is a compromise between providing the complete posterior distribution and providing a single interval corresponding to only two percentiles.

Both the confidence and the Bayes approach present difficulties: the first, the problem of postdata interpretation; the second, the choice of a prior distribution and the interpretation of the posterior coverage probabilities if there is no clear basis for this choice. It is therefore not surprising that efforts have been made to find an approach without these drawbacks. The first such attempt, from which most later ones derive, is due to Fisher [1930; for his final account see Fisher (1973)].

To discuss Fisher's concept of fiducial probability, consider once more the example at the beginning of the section, in which \(X\) is distributed as \(N(\theta,1)\). Since then \(X-\theta\) is distributed as \(N(0,1)\), so is \(\theta-X\), and hence

\[P(\theta-X\leq y)=\Phi(y)\quad\text{ for all }y.\]

For fixed \(X=x\), this is the formal statement that a random variable \(\theta\) has distribution \(N(x,1)\). Without assuming \(\theta\) to be random, Fisher calls \(N(x,1)\) the _fiducial distribution_ of \(\theta\). Since this distribution is to embody the information about \(\theta\) provided by the data, it should be unique, and Fisher imposes conditions which he hopes will ensure uniqueness. This leads to some technical difficulties, but more basic is the question of how to interpret fiducial probability. In a series of independent repetitions of the experiment with arbitrarily varying \(\theta_{i}\), the quantities \(\theta_{1}-X_{1}\), \(\theta_{2}-X_{2}\),... will constitute a sequence of independent standard normal variables. From this fact, Fisher attempts to derive the fiducial distribution \(N(x,1)\) of \(\theta\) as a frequency distribution with respect to an appropriate reference set. However, this argument is difficult to follow and unconvincing. For summaries of the fiducial literature and of later related developments by Dempster, Fraser, and others, see Buehler (1983), Edwards (1983), Seidenfeld (1992), Zabell (1992), Barnard (1995, 1996) and Fraser (1996).

Fisher's effort to define a suitable frame of reference led him to the important concept of _relevant subsets_, which will be discussed in Chapter 10.

To appreciate the differences between the frequentist, Bayesian and Fisherian points of view, see Lehmann (1993), Robert (1994), Berger et al. (1997), Berger (2003) and Bayarri and Berger (2004).

### Permutation Tests

For the comparison of a treatment with a control situation in which no treatment is given, it was shown in Section 5.3 that the one-sided \(t\)-test is UMP unbiased for testing \(H:\eta=\xi\) against \(\eta-\xi=\Delta>0\) when the measurements \(X_{1}\),..., \(X_{m}\) and \(Y_{1}\),..., \(Y_{n}\) are samples from normal populations \(N(\xi,\sigma^{2})\) and \(N(\eta,\sigma^{2})\). It will be shown in Section 17.3 that the level of this test is (asymptotically) robust against nonnormality--that is, that except for small \(m\) or \(n\) the level of the test is approximately equal to the nominal level \(\alpha\) when the \(X\)'s and \(Y\)'s are samples from any distributions with densities \(f(x)\) and \(f(y-\Delta)\) with finite variance. If such an approximate level is not satisfactory, one may prefer to try to obtain an exact level-\(\alpha\) unbiased test (valid for all \(f\)) by replacing the original normal model with the nonparametric model for which the joint density of the variables is

\[f(x_{1})\ldots f(x_{m})f(y_{1}-\Delta)\ldots f(y_{n}-\Delta),\qquad f\in{\cal F}, \tag{5.45}\]

where we shall take \({\cal F}\) to be the family of all probability densities that are continuous a.e.

If there is much variation in the population being sampled, the sensitivity of the experiment can frequently be increased by dividing the population into more homogeneous subgroups, defined for example by some characteristic such as age or sex. A sample of size \(N_{i}(i=1,\ldots,c)\) is then taken from the \(i\)th subpopulation: \(m_{i}\) to serve as controls, and the other \(n_{i}=N_{i}-m_{i}\), to receive the treatment. If the observations in the \(i\)th subgroup of such a _stratified sample_ are denoted by

\[(X_{i1},\ldots,X_{im_{i}};\,Y_{i1},\ldots,Y_{in_{i}})=(Z_{i1},\ldots,Z_{iN_{i} }),\]

the density of \(Z=(Z_{11},\ldots,Z_{cN_{c}})\) is

\[p_{\Delta}(z)=\prod_{i=1}^{c}\left[f_{i}(x_{i1})\ldots f_{i}(x_{im_{i}})f_{i}( y_{i1}-\Delta)\ldots f_{i}(y_{in_{i}}-\Delta)\right]. \tag{5.46}\]

Unbiasedness of a test \(\phi\) for testing \(\Delta=0\) against \(\Delta>0\) implies that for all \(f_{1},\ldots,f_{c}\),

\[\int\phi(z)p_{0}(z)\,dz=\alpha\qquad(dz=dz_{11}\ldots dz_{cN_{c}}). \tag{5.47}\]

**Theorem 5.8.1**: _If \({\cal F}\) is the family of all probability densities \(f\) that are continuous a.e., then (5.47) holds for all \(f_{1},\ldots,f_{c}\in{\cal F}\) if and only if_

\[\frac{1}{N_{1}!\ldots N_{c}!}\sum_{z^{\prime}\in S(z)}\phi(z^{\prime})=\alpha \qquad\mbox{a.e.}, \tag{5.48}\]

_where \(S(z)\) is the set of points obtained from \(z\) by permuting for each \(i=1,\ldots,c\) the coordinates \(z_{ij}(j=1,\ldots,N_{i})\) within the \(i\)th subgroup in all \(N_{1}!\ldots N_{c}!\) possible ways._

Proof. To prove the result for the case \(c=1\), note that the set of order statistics \(T(Z)=(Z_{(1)},\ldots,Z_{(N)})\) is a complete sufficient statistic for \({\cal F}\) (Example 4.3.4). A necessary and sufficient condition for (5.47) is therefore

\[E[\phi(Z)\mid T(z)]=\alpha\qquad\mbox{a.e.} \tag{5.49}\]The set \(S(z)\) in the present case (\(c=1\)) consists of the \(N!\) points obtained from \(z\) through permutation of coordinates, so that \(S(z)=\{z^{\prime}:T(z^{\prime})=T(z)\}\). It follows from Section 2.4 that the conditional distribution of \(Z\) given \(T(z)\) assigns probability \(1/N!\) to each of the \(N!\) points of \(S(z)\). Thus (5.49) is equivalent to

\[\frac{1}{N!}\sum_{z^{\prime}\in S(z)}\phi(z^{\prime})=\alpha\qquad\text{a.e.,} \tag{5.50}\]

as was to be proved. The proof for general \(c\) is completely analogous and is left as an exercise (Problem 5.44.)

The tests satisfying (5.48) are called permutation tests. An extension of this definition is given in Problem 5.54. The more general class of randomization tests will be studied in Chapter 17.

### Most Powerful Permutation Tests

For the problem of testing the hypothesis \(H:\Delta=0\) of no treatment effect on the basis of a stratified sample with density (5.46) it was shown in the preceding section that unbiasedness implies (5.48). We shall now determine the test which, subject to (5.48), maximizes the power against a fixed alternative (5.46) or more generally against an alternative with arbitrary fixed density \(h(z)\).

The power of a test \(\phi\) against an alternative \(h\) is

\[\int\phi(z)h(z)\,dz=\int\,E[\phi(Z)\mid t]\,dP^{T}(t).\]

Let \(t=T(z)=(z_{(1)},\ldots,z_{(N)})\), so that \(S(z)=S(t)\). As was seen in Example 2.4.1 and Problem 2.6, the conditional expectation of \(\phi(Z)\) given \(T(Z)=t\) is

\[\psi(t)=\frac{\sum\limits_{z\in S(t)}\phi(z)h(z)}{\sum\limits_{z\in S(t)}h(z)}.\]

To maximize the power of \(\phi\) subject to (5.48) it is therefore sufficient to maximize \(\psi(t)\) for each \(t\) subject to this condition. The problem thus reduces to the determination of a function \(\phi\) which subject to

\[\sum_{z\in S(t)}\phi(z)\,\frac{1}{N_{1}!\ldots N_{c}!}=\alpha\]

maximizes\[\sum_{z\in S(t)}\phi(z)\frac{h(z)}{\sum\limits_{z^{\prime}\in S(t)}h(z^{\prime})}.\]

By the Neyman-Pearson fundamental lemma, this is achieved by rejecting \(H\) for those points \(z\) of \(S(t)\) for which the ratio

\[\frac{h(z)N_{1}!\ldots N_{c}!}{\sum\limits_{z^{\prime}\in S(t)}h(z^{\prime})}\]

is too large. Thus the most powerful test is given by the critical function

\[\phi(z)=\left\{\begin{array}{ll}1&\mbox{when}\quad h(z)>C[T(z)],\\ \gamma&\mbox{when}\quad h(z)=C[T(z)],\\ 0&\mbox{when}\quad h(z)<C[T(z)].\end{array}\right. \tag{5.51}\]

To carry out the test, the \(N_{1}!\ldots N_{c}!\) points of each set \(S(z)\) are ordered according to the values of the density \(h\). The hypothesis is rejected for the \(k\) largest values and with probability \(\gamma\) for the (\(k+1\))st value, where \(k\) and \(\gamma\) are defined by

\[k+\gamma=\alpha N_{1}!\ldots N_{c}!.\]

Consider now in particular the alternatives (5.46). The most powerful permutation test is seen to depend on \(\Delta\) and the \(f_{i}\), and is therefore not UMP.

Of special interest is the class of normal alternatives with common variance:

\[f_{i}=N\,(\xi_{i},\,\sigma^{2}).\]

The most powerful test against these alternatives, which turns out to be independent of the \(\xi_{i},\,\sigma^{2}\), and \(\Delta\), is appropriate when approximate normality is suspected but the assumption is not felt to be reliable. It may then be desirable to control the size of the test at level \(\alpha\) regardless of the form of the densities \(f_{i}\) and to have the test unbiased against all alternatives (5.46). However, among the class of tests satisfying these broad restrictions it is natural to make the selection so as to maximize the power against the type of alternative one expects to encounter, that is, against the normal alternatives.

With the above choice of \(f_{i}\), (5.46) becomes

\[h(z)=\left(\sqrt{2\pi}\sigma\right)^{-N}\times\]

\[\exp\left[-\frac{1}{2\sigma^{2}}\sum_{i=1}^{c}\left(\sum_{j=1}^{m_{i}}\left(z _{ij}-\xi_{i}\right)^{2}+\sum_{j=m_{i}+1}^{N_{i}}\left(z_{ij}-\xi_{i}-\Delta \right)^{2}\right)\right]. \tag{5.52}\]

[MISSING_PAGE_EMPTY:1819]

\[E\phi[f(V_{1}),\ldots,f(V_{m});g(V_{m+1}),\ldots,g(V_{m+n})]=\beta.\]

Since for all \((v_{1},\ldots,v_{m+n})\),

\[\phi[f(v_{1}),\ldots,f(v_{m});\;\;f(v_{m+1}),\ldots,f(v_{m+n})]\] \[\qquad\leq\phi[f(v_{1}),\ldots,f(v_{m});\;\;g(v_{m+1}),\ldots,g(v_ {m+n})],\]

the same inequality holds for the expectations of both sides, and hence \(\alpha\leq\beta\).

The proof for the case that \(X\) is stochastically larger than \(Y\) is completely analogous.

The lemma also generalizes to the case of \(c\) vectors \((X_{i1},\ldots,X_{im_{i}};\,Y_{i1},\ldots,Y_{im_{i}})\) with distributions \((F_{i},G_{i})\). If the expectation of a function \(\phi\) is \(\alpha\) when \(F_{i}=G_{i}\) and \(\phi\) is nondecreasing in each \(y_{ij}\) when all other variables are held fixed, then it follows as before that the expectation of \(\phi\) is \(\geq\alpha\) when the random variables with distribution \(G_{i}\) are stochastically larger than those with distribution \(F_{i}\).

In applying the lemma to the permutation test (5.53) it is enough to consider the case \(c=1\), the argument in the more general case being completely analogous. Since the rejection probability of the test (5.53) is \(\alpha\) whenever \(F=G\), it is only necessary to show that the critical function \(\phi\) of the test satisfies (b). Now \(\phi=1\) if \(\sum_{i=m+1}^{m+n}z_{i}\) exceeds sufficiently many of the sums \(\sum_{i=m+1}^{m+n}z_{j}\), and hence if sufficiently many of the differences

\[\sum_{i=m+1}^{m+n}z_{i}-\sum_{i=m+1}^{m+n}z_{j_{i}}\]

are positive. For a particular permutation \((j_{1},\ldots,j_{m+n})\)

\[\sum_{i=m+1}^{m+n}z_{i}-\sum_{i=m+1}^{m+n}z_{j_{i}}=\sum_{i=1}^{p}z_{s_{i}}- \sum_{i=1}^{p}z_{r_{i}},\]

where \(r_{1}<\cdots<r_{p}\) denote those of the integers \(j_{m+1},\ldots,j_{m+n}\) that are \(\leq m\), and \(s_{1}<\cdots<s_{p}\) those of the integers \(m+1,\ldots,m+n\) not included in the set \((j_{m+1},\ldots,j_{m+n})\). If \(\sum z_{s_{i}}-\sum z_{r_{i}}\) is positive and \(y_{i}\leq y^{\prime}_{i}\), that is, \(z_{i}\leq z^{\prime}_{i}\) for \(i=m+1,\ldots,m+n\), then the difference \(\sum z^{\prime}_{s_{i}}-\sum z_{r_{i}}\) is also positive and hence \(\phi\) satisfies (b).

The same argument also shows that the rejection probability of the test is \(\leq\alpha\) when the density of the variables is given by (5.46) with \(\Delta\leq 0\). The test is therefore equally appropriate if the hypothesis \(\Delta=0\) is replaced by \(\Delta\leq 0\).

Except for small values of the sample sizes \(N_{i}\), the amount of computation required to carry out the permutation test (5.53) is large. Computational methods are discussed by Green (1977), John and Robinson (1983a), Diaconis and Holmes (1994), and Chapter 13 of Good (1994), who has an extensive bibliography.

One can relate the permutation test to the corresponding normal theory \(t\)-test as follows. On multiplying both sides of the inequality

\[\sum y_{j}>C[T(z)]\]

by \((1/m)+(1/n)\) and subtracting \((\sum x_{1},+\sum y_{j})/m\), the rejection region for \(c=1\) becomes \(\bar{y}-\bar{x}>C[T(z)]\) or \(W=(\bar{y}-\bar{x})/\sqrt{\sum_{i=1}^{n}(z_{i}-\bar{z})^{2}}>C[T(z)]\), since the denominator of \(W\) is constant over \(S(z)\) and hence depends only on \(T(z)\). As was seen at the end of Section 5.3, this is equivalent to

\[\frac{(\bar{y}-\bar{x})/\sqrt{\frac{1}{m}+\frac{1}{n}}}{\sqrt{[\sum(x_{i}-\bar{ x})^{2}+\sum(y_{j}-\bar{y})^{2}]/(m+n-2)}}>C[T(z)]. \tag{5.54}\]

The rejection region therefore has the form of a \(t\)-test in which the constant cutoff point \(C_{0}\) of (5.27) has been replaced by a random one. It turns out that when the hypothesis is true, so that the \(Z^{\prime}\)s are identically and independently distributed, and \(m/n\) is bounded away from zero and infinity as \(m\) and \(n\) tend to infinity, the difference between the random cutoff point \(C[T(Z)]\) and \(C_{0}\) is small in an appropriate asymptotic sense, and so the permutation test and the \(t\)-test given by (5.27)-(5.29) behave similarly in large samples. Such results will be developed in Section 17.3. It follows that the _permutation test can be approximated for large samples by the standard \(t\)-test_. Exactly analogous results hold for \(c>1\); the appropriate generalization of the two-sample \(t\)-test is provided in Problem 7.9.

### Randomization as a Basis For Inference

The problem of testing for the effect of a treatment was considered in Section 5.3 under the assumption that the treatment and control measurements \(X_{1}\),..., \(X_{m}\), and \(Y_{1}\),..., \(Y_{n}\) constitute samples from normal distributions, and in Sections 5.8 and 5.9 without relying on the assumption of normality. We shall now consider in somewhat more detail the structure of the experiment from which the data are obtained, resuming for the moment the assumption that the distributions involved are normal.

Suppose that the experimental material consists of \(m+n\) patients, plants, pieces of material, or the like, drawn at random from the population to which the treatment could be applied. The treatment is given to \(n\) of these while the other \(m\) serve as controls. The characteristic that is to be influenced by the treatment is then measured in each case, leading to observations \(X_{1}\),..., \(X_{m}\); \(Y_{1}\),..., \(Y_{n}\).

To be specific, suppose that the treatment is carried out by injecting a drug and that \(m+n\) ampules are assigned to the \(m+n\) patients. The \(i\)th measurement can be considered as the sum of two components. One, say \(U_{i}\), is associated with the \(i\)th patient; the other, \(V_{i}\), with the \(i\)th ampule and the circumstances under which it is administered and under which the measurements are taken. The variables \(U_{i}\) and \(V_{i}\) are assumed to be independently distributed, the \(V\)'s with normal distribution \(N(\eta,\sigma^{2})\) or \(N(\xi,\sigma^{2})\) as the ampule contains the drug or is one of those used for control. If in addition the \(U\)'s are assumed to constitute a random sample from \(N(\mu,\sigma_{1}^{2})\), it follows that the \(X\)'s and \(Y\)'s are independently normally distributed with common variance \(\sigma^{2}+\sigma_{1}^{2}\) and means

\[E(X)=\mu+\xi,\qquad E(Y)=\mu+\eta.\]

Except for a change of notation their joint distribution is then given by (5.26), and the hypothesis \(\eta=\xi\) can be tested by the standard \(t\)-test.

Unfortunately, under actual experimental conditions, it is frequently not possible to ensure that the patients or other experimental units constitute a random sample from the population of such units. They may be patients in a certain hospital at a given time, or volunteers for an experiment, and may constitute a haphazard rather than a random sample. In this case the \(U\)'s would have to be considered as unknown constants, since they are not obtained by any definite sampling procedure. This assumption is appropriate also in a different context. Suppose that the experimental units are all the machines in a shop or fields on a farm. If the experiment is performed only to determine the best method for this particular shop or farm, these experimental units are the only relevant ones; that is, a replication of the experiment would consist in comparing the two treatments again for the same machines or fields rather than for a new batch drawn at random from a large population. In this case the units themselves, and therefore the \(u\)'s, are constant. Under the above assumptions the joint density of the \(m+n\) measurements is

\[\frac{1}{(\sqrt{2\pi}\sigma)^{m+n}}\exp\left[-\frac{1}{2\sigma^{2}}\left(\sum _{i=1}^{m}(x_{i}-u_{i}-\xi)^{2}+\sum_{j=1}^{n}(y_{j}-u_{m+j}-\eta)^{2}\right) \right].\]

Since the \(u\)'s are completely arbitrary, it is clearly impossible to distinguish between \(H:\eta=\xi\) and the alternatives \(K:\eta>\xi\). In fact, every distribution of \(K\) also belongs to \(H\) and vice versa, and the most powerful level-\(\alpha\) test for testing \(H\) against any simple alternative specifying \(\xi\), \(\eta\), \(\sigma\), and the \(u\)'s rejects \(H\) with probability \(\alpha\) regardless of the observations.

Data which could serve as a basis for testing whether or not the treatment has an effect can be obtained through the fundamental device of _randomization_. Suppose that the \(N=m+n\) patients are assigned to the \(N\) ampules at random, that is, in such a way that each of the \(N!\) possible assignments has probability \(1/N!\) of being chosen. Then for a given assignment the \(N\) measurements are independently normally distributed with variance \(\sigma^{2}\) and means \(\xi+u_{j_{i}}(i=1,\ldots,m)\) and \(\eta+u_{j_{i}}(i=m+1,\ldots,m+n)\). The overall joint density of the variables

\[(Z_{1},\ldots,Z_{N})=(X_{1},\ldots,X_{m};Y_{1},\ldots,Y_{n})\]is therefore

\[\frac{1}{N!}\sum_{(j_{1},\ldots,j_{N})}\frac{1}{(\sqrt{2\pi}\sigma)^{N}} \tag{5.55}\] \[\times\exp\left[-\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{m}(x_{i}-u _{j_{i}}-\xi)^{2}+\sum_{i=1}^{n}(y_{i}-u_{j_{in+i}}-\eta)^{2}\right)\right],\]

where the outer summation extends over all \(N!\) permutations \((j_{1},\ldots,j_{N})\) of \((1,\ldots,N)\). Under the hypothesis \(\eta=\xi\) this density can be written as

\[\frac{1}{N!}\sum_{(j_{1},\ldots,j_{N})}\frac{1}{(\sqrt{2\pi}\sigma)^{N}}\exp \left[-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(z_{i}-\zeta_{j_{i}})^{2}\right], \tag{5.56}\]

where \(\zeta_{j_{i}}=u_{j_{i}}+\xi=u_{j_{i}}+\eta\).

Without randomization a set of \(y\)'s which is large relative to the \(x\)-values could be explained entirely in terms of the unit effects \(u_{i}\). However, if these are assigned to the \(y\)'s at random, they will on the average balance those assigned to the \(x\)'s. As a consequence, a marked superiority of the second sample becomes very unlikely under the hypothesis, and must therefore be attributed to the effectiveness of the treatment.

The method of assigning the treatments to the experimental units completely at random permits the construction of a level-\(\alpha\) test of the hypothesis \(\eta=\xi\), whose power exceeds \(\alpha\) against all alternatives \(\eta-\xi>0\). The actual power of such a test will however depend not only on the alternative value of \(\eta-\xi\), which measures the effect of the treatment, but also on the unit effects \(u_{i}\). In particular, if there is excessive variation among the \(u\)'s this will swamp the treatment effect (much in the same way as an increase in the variance \(\sigma^{2}\) would), and the test will accordingly have little power to detect any given alternative \(\eta-\xi\).

In such cases the sensitivity of the experiment can be increased by an approach exactly analogous to the method of stratified sampling discussed in Section 5.8. In the present case this means replacing the process of complete randomization described above by a more restricted randomization procedure. The experimental material is divided into subgroups, which are more homogeneous than the material as a whole, so that within each group the differences among the \(u\)'s are small. In animal experiments, for example, this can frequently be achieved by a division into litters. Randomization is then applied only within each group. If the \(i\)th group contains \(N_{i}\) units, \(n_{i}\) of these are selected at random to receive the treatment, and the remaining \(m_{i}=N_{i}-n_{i}\) serve as controls (\(\sum N_{i}=N\), \(\sum m_{i}=m\), \(\sum n_{i}=n\)).

An example of this approach is the method of _matched pairs_. Here the experimental units are divided into pairs, which are as like each other as possible with respect to all relevant properties, so that within each pair the difference of the \(u\)'s will be as small as possible. Suppose that the data consists of \(n\) such pairs, and denote the associated unit effects (the \(u\)'s of the previous discussion) by \(u_{1}\), \(u^{\prime}_{1}\); \(\ldots\) ; \(u_{n}\), \(u^{\prime}_{n}\). Let the first and second members of each pair receive the treatment or serve as control respectively, and let the observations for the \(i\)th pair be \(X_{i}\) and \(Y_{i}\). If the matching is completely successful, as may be the case, for example, when the same patient is used twice in the investigation of a sleeping drug, or when identical twins are used, then \(u_{i}^{\prime}=u_{i}\) for all \(i\), and the density of the \(X\)'s and \(Y\)'s is9

Footnote 9: The problem is simplified here, not just due to the assumption of normality, but also by the assumption that the distribution of \(X_{i}-u_{i}\) does not depend on \(\mu_{i}\), and similarly for the \(Y_{i}\). In general, when observations are paired approximately according to covariates, then pairs cannot be treated as if they were sampled from a population. A recent treatment is provided in Bai, Romano, and Shaikh (2019).

\[\frac{1}{(\sqrt{2\pi}\sigma)^{2}}\exp\left[-\frac{1}{2\sigma^{2}}\left[\sum(x_{ i}-\xi-u_{i})^{2}+\sum(y_{i}-\eta-u_{i})^{2}\right]\right]. \tag{5.57}\]

The UMP unbiased test for testing \(H:\eta=\xi\) against \(\eta>\xi\) is then given in terms of the differences \(W_{i}=Y_{i}-X_{i}\) by the rejection region

\[\sqrt{n}\tilde{w}\diagup\sqrt{\frac{1}{n-1}\sum(w_{i}-\tilde{w})^{2}}>C. \tag{5.58}\]

(See Problem 5.48.)

However, usually one is not willing to trust the assumption \(u_{i}^{\prime}=u_{i}\) even after matching, and it again becomes necessary to randomize. Since as a result of the matching the variability of the \(u\)'s within each pair is presumably considerably smaller than the overall variation, randomization is carried out only within each pair. For each pair, one of the units is selected with probability \(\frac{1}{2}\) to receive the treatment, while the other serves as control. The density of the \(X\)'s and \(Y\)'s is then

\[\frac{1}{2^{n}}\frac{1}{(\sqrt{2\pi}\sigma)^{2n}}\prod_{i=1}^{n} \left\{\exp\left[-\frac{1}{2\sigma^{2}}\left[(x_{i}-\xi-u_{i})^{2}+(y_{i}-\eta -u_{i}^{\prime})^{2}\right]\right]\right. \tag{5.59}\] \[\left.+\exp\left[-\frac{1}{2\sigma^{2}}\left[(x_{i}-\xi-u_{i}^{ \prime})^{2}+(y_{i}-\eta-u_{i})^{2}\right]\right]\right\}.\]

Under the hypothesis \(\eta=\xi\), and writing

\[z_{i1}=x_{i},\quad z_{i2}=y_{i},\quad\zeta_{i1}=\xi+u_{i},\quad\zeta_{i2}=\eta +u_{i}^{\prime}\quad\quad(i=1,\ldots,n),\]

this becomes

\[\frac{1}{2^{n}}\sum\frac{1}{(\sqrt{2\pi}\sigma)^{2n}}\exp\left[-\frac{1}{2 \sigma^{2}}\sum_{i=1}^{n}\sum_{j=1}^{2}(z_{ij}-\zeta_{ij}^{\prime})^{2}\right]. \tag{5.60}\]

Here the outer summation extends over the \(2^{n}\) points \(\zeta^{\prime}=(\zeta_{11}^{\prime},\ldots,\zeta_{n2}^{\prime})\) for which \((\zeta_{i1}^{\prime},\zeta_{i2}^{\prime})\) is either \((\zeta_{i1},\zeta_{i2})\) or \((\zeta_{i2},\zeta_{i1})\).

### Permutation Tests and Randomization

It was shown in the preceding section that randomization provides a basis for testing the hypothesis \(\eta=\xi\) of no treatment effect, without any assumptions concerning the experimental units. In the present section, a specific test will be derived for this problem. When the experimental units are treated as constants, the probability density of the observations is given by (5.55) in the case of complete randomization and by (5.59) in the case of matched pairs. More generally, let the experimental material be divided into \(c\) subgroups, let the randomization be applied within each subgroup, and let the observations in the \(i\)th subgroup be

\[(Z_{i1},\ldots,Z_{iN_{i}})=(X_{i1},\ldots,X_{im_{i}};Y_{i1},\ldots,Y_{in_{i}}).\]

For any point \(u=(u_{11},\ldots,u_{cN_{c}})\), let \(S(u)\) denote as before the set of \(N_{1}!\ldots N_{c}!\) points obtained from \(u\) by permuting the coordinates within each subgroup in all \(N_{1}!\ldots N_{c}!\) possible ways. Then the joint density of the \(Z\)'s given \(u\) is

\[\frac{1}{N_{1}!\ldots N_{c}!}\sum_{u^{\prime}\in S(u)}\frac{1}{( \sqrt{2\pi}\sigma)^{N}} \tag{5.61}\] \[\times\exp\left[-\frac{1}{2\sigma^{2}}\sum_{i=1}^{c}\left(\sum_{j =1}^{m_{i}}(z_{ij}-\xi-u^{\prime}_{ij})^{2}+\sum_{j=m_{i}+1}^{N_{i}}(z_{ij}- \eta-u^{\prime}_{ij})^{2}\right)\right],\]

and under the hypothesis of no treatment effect

\[p_{\sigma,\zeta}(z)=\frac{1}{N_{1}!\ldots N_{c}!}\sum_{\zeta^{\prime}\in S( \zeta)}\frac{1}{(\sqrt{2\pi}\sigma)^{N}}\exp\left[-\frac{1}{2\sigma^{2}}\sum_ {i=1}^{c}\sum_{j=1}^{N_{i}}(z_{ij}-\zeta^{\prime}_{ij})^{2}\right]. \tag{5.62}\]

It may happen that the coordinates of \(u\) or \(\zeta\) are not distinct. If then some of the points of \(S(u)\) or \(S(\zeta)\) also coincide, each should be counted with its proper multiplicity. More precisely, if the \(N_{1}!\ldots N_{c}!\) relevant permutations of \(N_{1}+\ldots+N_{c}\) coordinates are denoted by \(g_{k},k=1,\ldots,N_{1}!\ldots N_{c}!\), then \(S(\zeta)\) can be taken to be the ordered set of points \(g_{k}\zeta\), \(k=1,\ldots,N_{1}!\ldots N_{c}!\), and (5.62), for example, becomes

\[p_{\sigma,\zeta}(z)=\frac{1}{N_{1}!\ldots N_{c}!}\sum_{k=1}^{N_{1}!\ldots N_{c }!}\frac{1}{(\sqrt{2\pi}\sigma)^{N}}\exp\left(-\frac{1}{2\sigma^{2}}|z-g_{k} \zeta|^{2}\right),\]

where \(|u|^{2}\) stands for \(\sum_{i=1}^{c}\sum_{j=1}^{N}u_{ij}^{2}\).

**Theorem 5.11.1**: _A necessary and sufficient condition for a critical function \(\phi\) to satisfy_\[\int\phi(z)\,p_{\sigma,\zeta}(z)\,dz\leq\alpha\ \ \ \ \ \ (dz=dz_{11}\ldots dz_{cN_{ \epsilon}}) \tag{5.63}\]

_for all \(\sigma>0\) and all vectors \(\zeta\) is that_

\[\frac{1}{N_{1}!\ldots N_{c!}}\sum_{z^{\prime}\in S(z)}\phi(z^{\prime})\leq \alpha\ \ \ \ \ \mbox{a.e.} \tag{5.64}\]

The proof will be based on the following lemma.

**Lemma 5.11.1**: _Let \(A\) be a set in \(N\)-space with positive Lebesgue measure \(\mu(A)\). Then for any \(\epsilon>0\) there exist real numbers \(\sigma>0\) and \(\xi_{1},\ldots,\xi_{N}\), such that_

\[P\{(X_{1},\ldots,X_{N})\in A\}\geq 1-\epsilon,\]

_where the \(X\)'s are independently normally distributed with means \(E(X_{i})=\xi_{i}\) and variance \(\sigma_{X_{i}}^{2}=\sigma^{2}\)._

Proof of Lemma 5.11.1. Suppose without loss of generality that \(\mu(A)<\infty\). Given any \(\eta>0\), there exists a square \(Q\) such that

\[\mu(Q\cap A^{c})\leq\eta\mu(Q).\]

This follows from the fact that almost every point of \(A\) is a density point,10 or from the more elementary fact that a measurable set can be approximated in measure by unions of disjoint squares. Let \(a\) be such that

Footnote 10: See, for example, Billingsley (1995), p. 417.

\[\frac{1}{\sqrt{2\pi}}\int_{-a}^{a}\left(-\frac{t^{2}}{2}\right)dt=\left(1- \frac{\epsilon}{2}\right)^{1/N},\]

and let

\[\eta=\frac{\epsilon}{2}\left(\frac{\sqrt{2\pi}}{2a}\right)^{N}.\]

If \((\xi_{1},\ldots,\xi_{N})\) is the center of \(Q\), and if \(\sigma=b/a=(1/2a)[\mu(Q)]^{1/N}\), where \(2b\) is the length of the side of \(Q\), then

\[\frac{1}{(\sqrt{2\pi}\sigma)^{N}}\int_{A^{c}\cap Q^{c}}\exp\left[ -\frac{1}{2\sigma^{2}}\sum(x_{i}-\xi_{i})^{2}\right]dx_{1}\ldots dx_{N}\] \[\leq\frac{1}{(\sqrt{2\pi}\sigma)^{N}}\int_{Q^{c}}\exp\left[-\frac {1}{2\sigma^{2}}\sum(x_{i}-\xi_{i})^{2}\right]dx_{1}\ldots dx_{N}\] \[=1-\left[\frac{1}{\sqrt{2\pi}}\int_{-a}^{a}\exp\left(-\frac{t^{2 }}{2}\right)dt\right]^{N}=\frac{\epsilon}{2}.\]On the other hand,

\[\frac{1}{(\sqrt{2\pi}\sigma)^{N}}\int_{A^{c}\cap Q}\exp\left[-\frac{1} {2\sigma^{2}}\sum(x_{i}-\xi_{i})^{2}\right]dx_{1}\ldots dx_{N}\] \[\quad\leq\frac{1}{(\sqrt{2\pi}\sigma)^{N}}\mu(A^{c}\cap Q)<\frac{ \epsilon}{2},\]

and by adding the two inequalities one obtains the desired result.

Proof of Theorem 5.11.1. Let \(\phi\) be any critical function, and let

\[\psi(z)=\frac{1}{N_{1}!\ldots N_{c}!}\sum_{z^{\prime}\in S(z)}\phi(z^{\prime}).\]

If (5.64) does not hold, there exists \(\eta>0\) such that \(\phi(z)>\alpha+\eta\) on a set \(A\) of positive measure. By Lemma 5.11.1, there exists \(\sigma>0\) and \(\zeta=(\zeta_{11},\ldots,\zeta_{cN_{c}})\) such that \(P\{Z\in A\}>1-\eta\) when \(Z_{11},\ldots,Z_{cN_{c}}\) are independently normally distributed with common variance \(\sigma^{2}\) and means \(E(Z_{ij})=\zeta_{ij}\). It follows that

\[\int\phi(z)\,p_{\sigma,\zeta}(z)\,dz=\int\psi(z)\,p_{\sigma,\zeta} (z)\,dz \tag{5.65}\] \[\quad\geq\int_{A}\psi(z)\frac{1}{(\sqrt{2\pi}\sigma)^{N}}\exp \left[-\frac{1}{2\sigma^{2}}\sum\sum(z_{ij}-\zeta_{ij})^{2}\right]dz\] \[\quad>(\alpha+\eta)(1-\eta),\]

which is \(>\alpha\), since \(\alpha+\eta<1\). This proves that (5.63) implies (5.64). The converse follows from the first equality in (5.65).

**Corollary 5.11.1**: _Let \(H\) be the class of densities_

\[\{p_{\sigma,\zeta}(z):\sigma>0,-\infty<\zeta_{ij}<\infty\}.\]

_A complete family of tests for \(H\) at level of significance \(\alpha\) is the class of tests \(\mathcal{C}\) satisfying_

\[\frac{1}{N_{1}!\ldots N_{c}!}\sum_{z^{\prime}\in S(z)}\phi(z^{\prime})=\alpha \quad\quad\mathrm{a.e.} \tag{5.66}\]

Proof. The corollary states that for any given level-\(\alpha\) test \(\phi_{0}\) there exists an element \(\phi\) of \(\mathcal{C}\) which is uniformly at least as powerful as \(\phi_{0}\). By the preceding theorem the average value of \(\phi_{0}\) over each set \(S(z)\) is \(\leq\alpha\). On the sets for which this inequality is strict, one can increase \(\phi_{0}\) to obtain a critical function \(\phi\) satisfying (5.66), and such that \(\phi_{0}(z)\leq\phi(z)\) for all \(z\). Since against all alternatives the power of \(\phi\) is at least that of \(\phi_{0}\), this establishes the result. An explicit construction of \(\phi\), which shows that it can be chosen to be measurable, is given in Problem 5.51.

This corollary shows that the normal randomization model (5.61) leads exactly to the class of tests that was previously found to be relevant when the \(U\)'s constituted a sample but the assumption of normality was not imposed. It therefore follows from Section 5.9 that the most powerful level-\(\alpha\) test for testing (5.62) against a simple alternative (5.61) is given by (5.51) with \(h(z)\) equal to the probability density (5.61). If \(\eta-\xi=\Delta\), the rejection region of this test reduces to

\[\sum_{u^{\prime}\in S(u)}\exp\left[\frac{1}{\sigma^{2}}\sum_{i=1}^{c}\left( \sum_{j=1}^{N_{i}}z_{ij}u^{\prime}_{ij}+\Delta\sum_{j=m_{i}+1}^{N_{i}}(z_{ij}- u^{\prime}_{ij})\right)\right]>C[T(z)], \tag{5.67}\]

since both \(\sum\sum z_{ij}\) and \(\sum\sum z_{ij}^{2}\) are constant on \(S(z)\) and therefore functions only of \(T(z)\). It is seen that this test depends on \(\Delta\) and the unit effects \(u_{ij}\), so that a UMP test does not exist.

Among the alternatives (5.61) a subclass occupies a central position and is of particular interest. This is the class of alternatives specified by the assumption that the unit effects \(u_{i}\) constitute a sample from a normal distribution. Although this assumption cannot be expected to hold exactly--in fact, it was just as a safeguard against the possibility of its breakdown that randomization was introduced--it is in many cases reasonable to suppose that it holds at least approximately. The resulting subclass of alternatives is given by the probability densities

\[\frac{1}{(\sqrt{2\pi}\sigma)^{N}} \tag{5.68}\] \[\qquad\times\exp\left[-\frac{1}{2\sigma^{2}}\sum_{i=1}^{c}\left( \sum_{j=1}^{m_{i}}(z_{ij}-u_{i}-\xi)^{2}+\sum_{j=m_{i}+1}^{N_{i}}(z_{ij}-u_{i} -\eta)^{2}\right)\right].\]

These alternatives are suggestive also from a slightly different point of view. The procedure of assigning the experimental units to the treatments at random within each subgroup was seen to be appropriate when the variation of the \(u\)'s is small within these groups and is employed when this is believed to be the case. This suggests, at least as an approximation, the assumption of constant \(u_{ij}=u_{i}\), which is the limiting case of a normal distribution as the variance tends to zero, and for which the density is also given by (5.68).

Since the alternatives (5.68) are the same as the alternatives (5.52) of Section 5.9 with \(u_{i}-\xi=\xi_{i}\), \(u_{i}-\eta=\xi_{i}-\Delta\), _the permutation test (5.53) is seen to be most powerful for testing the hypothesis \(\eta=\xi\) in the normal randomization model (5.61) against the alternatives (5.68) with \(\eta-\xi>0\)._ The test retains this property in the still more general setting in which neither normality nor the sample property of the \(U\)'s is assumed to hold. Let the joint density of the variables be

\[\sum_{u^{\prime}\in S(u)}\prod_{i=1}^{c}\left[\prod_{j=1}^{m_{i}}f_{i}(z_{ij}- u^{\prime}_{ij}-\xi)\prod_{j=m_{i}+1}^{N_{i}}f_{i}(z_{ij}-u^{\prime}_{ij}-\eta) \right], \tag{5.69}\]with \(f_{i}\) continuous a.e. but otherwise unspecified.11 Under the hypothesis \(H:\eta=\xi\), this density is symmetric in the variables \((z_{i1},\ldots,z_{iN_{i}})\) of the \(i\)th subgroup for each \(i\), so that any permutation test (5.48) has rejection probability \(\alpha\) for all distributions of \(H\). By Corollary 5.11.1, these permutation tests therefore constitute a complete class, and the result follows.

Footnote 11: Actually, all that is needed is that \(f_{1},\ldots,f_{c}\in\mathcal{F}\), where \(\mathcal{F}\) is any family containing all normal distributions.

### Randomization Model and Confidence Intervals

In the preceding section, the unit responses \(u_{i}\) were unknown constants (parameters) which were observed with error, the latter represented by the random terms \(V_{i}\). A limiting case assumes that the variation of the \(V\)'s is so small compared with that of the \(u\)'s that these error variables can be taken to be constant, i.e., that \(V_{i}=v\). The constant \(v\) can then be absorbed into the \(u\)'s, and can therefore be assumed to be zero. This leads to the following two-sample _randomization model_:

\(N\) subjects would give "true" responses \(u_{1},\ldots,u_{N}\) if used as controls. The subjects are assigned at random, \(n\) to treatment and \(m\) to control. If the responses are denoted by \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) as before, then under the hypothesis \(H\) of no treatment effect, the \(X\)'s and \(Y\)'s are a random permutation of the \(u\)'s. Under this model, in which the random assignment of the subjects to treatment and control constitutes the only random element, the probability of the rejection region (5.54) is the same as the conditional power under the more elaborate models of the preceding sections.

The corresponding limiting model under the alternatives assumes that the treatment has the effect of adding a constant amount \(\Delta\) to the unit response, so that the \(X\)'s and \(Y\)'s are given by \((u_{i_{1}},\ldots;u_{i_{n}};u_{i_{n+1}}+\Delta,\ldots,u_{i_{n+n}}+\Delta)\) for some permutation \((i_{1},\ldots,i_{N})\) of \((1,\ldots,N)\).

These models generalize in the obvious way to stratified samples. In particular, for paired comparisons it is assumed under \(H\) that the unit effects \((u_{i},u^{\prime}_{i})\) are constants, of which one is assigned at random to treatment and the other to control. Thus the pair \((X_{i},Y_{i})\) is equal to \((u_{i},u^{\prime}_{i})\) or \((u^{\prime}_{i},u_{i})\) with probability \(\frac{1}{2}\) each, and the assignments in the \(n\) pairs are independent; the sample space consists of \(2^{n}\) points each of which has probability \((\frac{1}{2})^{n}\). Under the alternative, it is assumed as before that \(\Delta\) is added to each treated subject, so that \(P(X_{i}=u_{i},\ Y_{i}=u^{\prime}_{i}+\Delta)=P(X_{i}=u^{\prime}_{i},\ Y_{i}=u_ {i}+\Delta)=\frac{1}{2}\). The distribution generated for the observations by such a randomization model is exactly the conditional distribution given \(T(z)\) of the preceding sections. In the two-sample case, for example, this common distribution is specified by the fact that all permutations of \((X_{1},\ldots,X_{m};\ Y_{1}-\Delta,\ldots,Y_{n}-\Delta)\) are equally likely. As a consequence, the power of the test (5.54) in the randomization model is also the conditional power in the two-sample model (5.45). As was pointed out in Section 4.4, the conditional power \(\beta(\Delta\mid T(z))\) can be interpreted as an unbiased estimate of the unconditional power \(\beta_{F}(\Delta)\) in the two-sample model. The advantage of \(\beta(\Delta\mid T(z))\) is that it depends only on \(\Delta\), not on the unknown \(F\). Approximations to \(\beta(\Delta\mid T(z))\) are discussed by Robinson (1973), Robinson (1982), John and Robinson (1983a), and Gabriel and Hsu (1983).

The tests (5.53), which apply to all three models--the sampling model (5.46), the randomization model, and the intermediate model (5.69)--can be inverted in the usual way to produce confidence sets for \(\Delta\). We shall now determine these sets explicitly for the paired comparisons and the two-sample case. The derivations will be carried out in the randomization model. However, they apply equally in the other two models, since the tests, and therefore the associated confidence sets are identical for the three models.

Consider first the case of paired observations \((x_{i},\,y_{i}),i=1,\,\ldots,\,n\). The one-sided test rejects \(H:\Delta=0\) in favor of \(\Delta>0\) when \(\sum_{i=1}^{n}y_{i}\) is among the \(K\) largest of the \(2^{n}\) sums obtained by replacing \(y_{i}\) by \(x_{i}\) for all, some, or none of the values \(i=1,\ldots,n\). (It is assumed here for the sake of simplicity that \(\alpha=K/2^{n}\), so that the test requires no randomization to achieve the exact level \(\alpha\).) Let \(d_{i}=y_{i}-x_{i}=2y_{i}-t_{i}\), where \(t_{i}=x_{i}+y_{i}\) is fixed. Then the test is equivalent to rejecting when \(\sum d_{i}\) is one of the \(K\) largest of the \(2^{n}\) values \(\sum\pm d_{i}\), since an interchange of \(y_{i}\) with \(x_{i}\) is equivalent to replacing \(d_{i}\) by \(-d_{i}\). Consider now testing \(H:\Delta=\Delta_{0}\) against \(\Delta>\Delta_{0}\). The test then accepts when \(\sum(d_{i}-\Delta_{0})\) is one of the \(l=2^{n}-K\) smallest of the \(2^{n}\) sums \(\sum\pm(d_{i}-\Delta_{0})\), since it is now \(y_{i}-\Delta_{0}\) that is being interchanged with \(x_{i}\). We shall next invert this statement, replacing \(\Delta_{0}\) by \(\Delta\), and see that it is equivalent to a lower confidence bound for \(\Delta\).

In the inequality

\[\sum(d_{i}-\Delta)<\sum\left[\pm(d_{i}-\Delta)\right], \tag{5.70}\]

suppose that on the right side the minus sign attaches to the \((d_{i}-\Delta)\) with \(i=i_{1},\ldots,i_{r}\) and the plus sign to the remaining terms. Then (5.70) is equivalent to

\[d_{i_{1}}+\cdots+d_{i_{r}}-r\Delta<0,\quad\mbox{or}\quad\frac{d_{i_{1}}+\cdots +d_{i_{r}}}{r}<\Delta.\]

Thus, \(\sum(d_{i}-\Delta)\) is among the \(l\) smallest of the \(\sum\pm(d_{i}-\Delta)\) if and only if at least \(2^{n}-l\) of the \(M=2^{n}-1\) averages \((d_{i_{1}}+\cdots+d_{i_{r}})/r\) are \(<\Delta\), i.e., if and only if \(\delta_{(K)}<\Delta\), where \(\delta_{(1)}<\cdots<\delta_{(M)}\) is the ordered set of averages \((d_{i_{1}}+\cdots+d_{i_{r}})/r\), \(r=1,\ldots,\,M\). This establishes \(\delta_{(K)}\) as a lower confidence bound for \(\Delta\) at confidence level \(\gamma=K/2^{n}\). [Among all confidence sets that are unbiased in the model (5.46) with \(m_{i}=n_{i}=1\) and \(c=n\), these bounds minimize the probability of falling below any value \(\Delta^{\prime}<\Delta\) for the normal model (5.52).]By putting successively \(K=1\), \(2\),..., \(2^{n}\), it is seen that the \(M+1\) intervals

\[(-\infty,\,\delta_{(1)}),\,(\delta_{(1)},\,\delta_{(2)}),\,\ldots,\,(\delta_{(M-1 )},\,\delta_{(M)}),\,(\delta_{M},\,\infty) \tag{5.71}\]

each have probability \(1/(M+1)=1/2^{n}\) of containing the unknown \(\Delta\). The two-sided confidence intervals \((\delta_{(K)},\,\delta_{(2^{n}-K)})\) with \(\gamma=(2^{n-1}-K)/2^{n-1}\) correspond to the two-sided version of the test (5.53) with error probability \((1-\gamma)/2\) in each tail. A suitable subset of the points \(\delta_{(1)},\,\ldots,\,\delta_{(M)}\) constitutes a set of confidence points in the sense of Section 3.5.

The inversion procedure for the two-group case is quite analogous. Let \((x_{1},\ldots,\,x_{m},\,y_{1},\ldots,\,y_{n})\) denote the \(m\) control and \(n\) treatment observations, and suppose without loss of generality that \(m\leq n\). Then the hypothesis \(\Delta=\Delta_{0}\) is accepted against \(\Delta>\Delta_{0}\) if \(\sum_{j=1}^{n}(y_{j}-\Delta_{0})\) is among the \(l\) smallest of the \({m+n\choose n}\) sums obtained by replacing a subset of the \((y_{j}-\Delta_{0})\)'s with \(x\)'s. For \(1\leq r\leq m\), let \(i_{1}<\cdots<i_{r}\) be a combination of values taken from \(1,\,\ldots,\,m\) and let \(j_{r+1}<\cdots<j_{n}\) be a combination of values taken from \(1,\,\ldots,\,n\). The inequality

\[\sum_{j=1}^{n}(y_{j}-\Delta_{0})<(x_{i_{1}}+\cdots+x_{i_{r}})+[y_{j_{r+1}}+ \cdots+y_{j_{n}}-(n-r)\Delta_{0}],\]

is equivalent to \(y_{j_{1}}+\cdots+y_{j_{r}}-r\Delta_{0}<x_{i_{1}}+\cdots+x_{i_{r}}\), or

\[\bar{y}_{j_{1},\ldots,j_{r}}-\bar{x}_{i_{1},\ldots,i_{r}}<\Delta_{0}\;. \tag{5.72}\]

Note that the number of such averages with \(r\geq 1\) (i.e., omitting the empty set of subscripts) is equal to

\[\sum_{K=1}^{m}{m\choose K}{n\choose K}={m+n\choose n}-1=M\]

(Problem 5.57). Thus, \(H:\Delta=\Delta_{0}\) is accepted against \(\Delta>\Delta_{0}\) at level \(\alpha=1-l/(M+1)\) if and only if at least \(K\) of the \(M\) differences (5.72) are less than \(\Delta_{0}\), and hence if and only if \(\delta_{(K)}<\Delta_{0}\), where \(\delta_{(1)}<\cdots<\delta_{(M)}\) denote the ordered set of differences (5.72). This establishes \(\delta_{(K)}\) as a lower confidence bound for \(\Delta\) with confidence coefficient \(\gamma=1-\alpha\).

As in the paired comparisons case, it is seen that the intervals (5.71) each have probability \(1/(M+1)\) of containing \(\Delta\). Thus, two-sided confidence intervals and standard confidence bounds can be derived as before. For the generalization to stratified samples, see Problem 5.58.

Algorithms for computing the order statistics \(\delta_{(1)},\ldots,\delta_{(M)}\) in the paired-comparison and two-sample cases are discussed by Tritchler (1984); also see Garthwaite (1996). If \(M\) is too large for the computations to be practicable, reduced analyses based on either a fixed or random subset of the set of all \(M+1\) permutations are discussed, for example, by Gabriel and Hall (1983) and Vadiveloo(1983). [See also Problem 5.60(i).] Different such methods are compared by Forsythe and Hartigan (1970). For some generalizations, and relations to other subsampling plans, see Efron (1982, Chapter 9).

### Testing for Independence in a Bivariate Normal Distribution

So far, the methods of the present chapter have been illustrated mainly by the two-sample problem. As a further example, we shall now apply two of the formulations that have been discussed, the normal model of Section 5.3 and the nonparametric one of Section 5.8, to the hypothesis of independence in a bivariate distribution.

The probability density of a sample \((X_{1},\,Y_{1}),\,\ldots,\,(X_{n},\,Y_{n})\) from a bivariate normal distribution is

\[\frac{1}{(2\pi\sigma\tau\sqrt{1-\rho^{2}})^{n}}\exp\left[-\frac{1} {2(1-\rho^{2})}\left(\frac{1}{\sigma^{2}}\sum(x_{i}-\xi)^{2}\right.\right. \tag{5.73}\] \[\left.\left.-\frac{2\rho}{\sigma\tau}\sum(x_{i}-\xi)(y_{i}-\eta)+ \frac{1}{\tau^{2}}\sum(y_{i}-\eta)^{2}\right)\right].\]

Here \((\xi,\,\sigma^{2})\) and \((\eta,\,\tau^{2})\) are the mean and variance of \(X\) and \(Y\) respectively, and \(\rho\) is the correlation coefficient between \(X\) and \(Y\). The hypotheses \(\rho\leq\rho_{0}\) and \(\rho=\rho_{0}\) for arbitrary \(\rho_{0}\) cannot be treated by the methods of the present chapter, and will be taken up in Chapter 6. For the present, we shall consider only the hypothesis \(\rho=0\) that \(X\) and \(Y\) are independent, and the corresponding one-sided hypothesis \(\rho\leq 0\).

The family of densities (5.73) is of the exponential form (1) with

\[U=\sum X_{i}Y_{i},\quad T_{1}=\sum X_{i}^{2},\quad T_{2}=\sum Y_{i}^{2},\quad T _{3}=\sum X_{i},\quad T_{4}=\sum Y_{i}\]

and

\[\theta=\frac{\rho}{\sigma\tau(1-\rho^{2})},\qquad\vartheta_{1}= \frac{-1}{2\sigma^{2}(1-\rho^{2})},\qquad\vartheta_{2}=\frac{-1}{2\tau^{2}(1- \rho^{2})},\] \[\vartheta_{3}=\frac{1}{1-\rho^{2}}\left(\frac{\xi}{\sigma^{2}}- \frac{\eta\rho}{\sigma\tau}\right),\qquad\vartheta_{4}=\frac{1}{1-\rho^{2}} \left(\frac{\eta}{\tau^{2}}-\frac{\xi\rho}{\sigma\tau}\right).\]

The hypothesis \(H:\rho\leq 0\) is equivalent to \(\theta<0\). Since the sample correlation coefficient

\[R=\frac{\sum(X_{i}-\bar{X})(Y_{i}-\bar{Y})}{\sqrt{\sum(X_{i}-\bar{X})^{2}\sum( Y_{i}-\bar{Y})^{2}}}\]

is unchanged when the \(X_{i}\) and \(Y_{i}\) are replaced by \((X_{i}-\xi)/\sigma\) and \((Y_{i}-\eta)/\tau\), the distribution of \(R\) does not depend on \(\xi,\eta,\sigma,\) or \(\tau\), but only on \(\rho\). For \(\theta=0\) it therefore does not depend on \(\vartheta_{1},\ldots,\vartheta_{4}\), and hence by Theorem 5.1.2, \(R\) is independent of (\(T_{1},\ldots,T_{4}\)) when \(\theta=0\). It follows from Theorem 5.1.1 that the UMP unbiased test of \(H\) rejects when

\[R\geq C_{0}, \tag{5.74}\]

or equivalently when

\[\frac{R}{\sqrt{(1-R^{2})/(n-2)}}>K_{0}. \tag{5.75}\]

The statistic \(R\) is linear in \(U\), and its distribution for \(\rho=0\) is symmetric about \(0\). The UMP unbiased test of the hypothesis \(\rho=0\) against the alternative \(\rho\neq 0\) therefore rejects when

\[\frac{|R|}{\sqrt{(1-R^{2})/(n-2)}}>K_{1}. \tag{5.76}\]

Since \(\sqrt{n-2}R/\sqrt{1-R^{2}}\) has the \(t\)-distribution with \(n-2\) degrees of freedom when \(\rho=0\) (Problem 5.64), the constants \(K_{0}\) and \(K_{1}\) in the above tests are given by

\[\int_{K_{0}}^{\infty}t_{n-2}(y)\,dy=\alpha\ \ \ \text{and}\ \ \ \int_{K_{1}}^{ \infty}t_{n-2}(y)\,dy=\frac{\alpha}{2}. \tag{5.77}\]

Since the distribution of \(R\) depends only on the correlation coefficient \(\rho\), the same is true of the power of these tests.

Some large-sample properties of the above test will be examined in Problem (13.3). In particular, if (\(X_{i}\), \(Y_{i}\)) is not bivariate normal, the level of the above test is approximately \(\alpha\) in large samples under the hypothesis \(H_{1}\) that \(X_{i}\) and \(Y_{i}\) are independent, but not necessarily under the hypothesis \(H_{2}\) that the correlation between \(X_{i}\) and \(Y_{i}\) is \(0\). For the nonparametric model \(H_{1}\), one can obtain an exact level-\(\alpha\) unbiased test of independence in analogy to the permutation test of Section 5.8. For any bivariate distribution of (\(X\), \(Y\)), let \(Y_{x}\) denote a random variable whose distribution is the conditional distribution of \(Y\) given \(x\). We shall say that there is _positive regression dependence_ between \(X\) and \(Y\) if for any \(x<x^{\prime}\) the variable \(Y_{x^{\prime}}\) is stochastically larger than \(Y_{x}\). Generally speaking, larger values of \(Y\) will then correspond to larger values of \(X\); this is the intuitive meaning of positive dependence. An example is furnished by any normal bivariate distribution with \(\rho>0\). (See Problem 5.68.) Regression dependence is a stronger requirement than positive quadrant dependence, which was defined in Problem 4.28. However, both reflect the intuitive meaning that large (small) values of \(Y\) will tend to correspond to large (small) values of \(X\).

As alternatives to \(H_{1}\) consider positive regression dependence in a general bivariate distribution possessing a density. To see that unbiasedness implies similarity, let \(F_{1}\), \(F_{2}\) be any two univariate distributions with densities \(f_{1}\), \(f_{2}\) and consider the one-parameter family of distribution functions

\[F_{1}(x)F_{2}(y)\{1+\Delta[1-F_{1}(x)][1-F_{2}(y)]\},\ \ \ \ \ 0\leq\Delta \leq 1. \tag{5.78}\]This is positively regression dependent (Problem 5.69), and by letting \(\Delta\to 0\) one sees that unbiasedness of \(\phi\) against these distributions implies that the rejection probability is \(\alpha\) when \(X\) and \(Y\) are independent, and hence that

\[\int\phi(x_{1},\ldots,x_{n};\,y_{1},\ldots,y_{n})f_{1}(x_{1})\cdots f_{1}(x_{n}) f_{2}(y_{1})\cdots f_{2}(y_{n})\,dx\,dy=\alpha\]

for all probability densities \(f_{1}\) and \(f_{2}\). By Theorem 5.8.1 this in turn implies

\[\frac{1}{(n!)^{2}}\sum\phi(x_{i_{1}},\ldots,x_{i_{n}};\,y_{j_{1}},\ldots,y_{jn} )=\alpha.\]

Here the summation extends over the \((n!)^{2}\) points of the set \(S(x,\,y)\), which is obtained from a fixed point \((x,\,y)\) with \(x=(x_{1},\,\ldots,\,x_{n})\), \(y=(y_{1},\,\ldots,\,y_{n})\) by permuting the \(x\)-coordinates and the \(y\)-coordinates, each among themselves in all possible ways.

Among all tests satisfying this condition, the most powerful one against the normal alternatives (5.73) with \(\rho>0\) rejects for the \(k^{\prime}\) largest values of (5.73) in each set \(S(x,\,y)\), where \(k^{\prime}/(n!)^{2}=\alpha\). Since \(\sum x_{i}^{2}\), \(\sum y_{i}^{2}\), \(\sum x_{i}\), \(\sum y_{i}\) are all constant on \(S(x,\,y)\), the test equivalently rejects for the \(k^{\prime}\) largest values of \(\sum x_{i}y_{i}\) in each \(S(x,\,y)\).

Of the \((n!)^{2}\) values that the statistic \(\sum X_{i}Y_{i}\) takes on over \(S(x,\,y)\), only \(n!\) are distinct, since the statistic remains unchanged if the \(X\)'s and \(Y\)'s are subjected to the same permutation. A simpler form of the test is therefore obtained, for example by rejecting \(H_{1}\) for the \(k\) largest values of \(\sum x_{(i)}y_{j_{i}}\), of each set \(S(x,\,y)\), where \(x_{(i)}<\cdots<x_{(n)}\) and \(k/n!=\alpha\). The test can be shown to be unbiased against all alternatives with positive regression dependence. (See Problem 6.64.)

In order to obtain a comparison of the permutation test with the standard normal test based on the sample correlation coefficient \(R\), let \(T(X,\,Y)\) denote the set of ordered \(X\)'s and \(Y\)'s

\[T(X,\,Y)=(X_{(1)},\ldots,\,X_{(n)};\,Y_{(1)},\ldots,\,Y_{(n)}).\]

The rejection region of the permutation test can then be written as

\[\sum X_{i}Y_{i}\,>\,C[T(X,\,Y)].\]

or equivalently as \(R>K[T(X,\,Y)]\). It again turns out that the difference between \(K[T(X,\,Y)]\) and the cutoff point \(C_{0}\) of the corresponding normal test (5.74) tends to zero in an appropriate sense. Such results are developed in Chapter 17; also see Problem 17.16. For large \(n\), the standard normal test (5.74) therefore serves as an approximation for the permutation test.

### Problems

#### Section 5.2

**Problem 5.1**: Let \(X_{1},\ldots,X_{n}\) be a sample from \(N(\xi,\sigma^{2})\). The power of Student's \(t\)-test is an increasing function of \(\xi/\sigma\) in the one-sided case \(H:\xi\leq 0\), \(K:\xi>0\), and of \(|\xi|/\sigma\) in the two-sided case \(H:\xi=0\), \(K:\xi\neq 0\).

[If

\[S=\sqrt{\frac{1}{n-1}\sum(X_{i}-\tilde{X})^{2}},\]

the power in the two-sided case is given by

\[1-P\left\{-\frac{CS}{\sigma}-\frac{\sqrt{n}\xi}{\sigma}\leq\frac{\sqrt{n}( \tilde{X}-\xi)}{\sigma}\leq\frac{CS}{\sigma}-\frac{\sqrt{n}\xi}{\sigma}\right\}\]

and the result follows from the fact that it holds conditionally for each fixed value of \(S/\sigma\).]

**Problem 5.2**: In the situation of the previous problem there exists no test for testing \(H:\xi=0\) at level \(\alpha\), which for all \(\sigma\) has power \(\geq\beta>\alpha\) against the alternatives (\(\xi\), \(\sigma\)) with \(\xi=\xi_{1}>0\).

[Let \(\beta(\xi_{1}\), \(\sigma\)) be the power of any level \(\alpha\) test of \(H\), and let \(\beta(\sigma)\) denote the power of the most powerful test for testing \(\xi=0\) against \(\xi=\xi_{1}\) when \(\sigma\) is known. Then \(\inf_{\sigma}\beta(\xi_{1}\), \(\sigma)\leq\inf_{\sigma}\beta(\sigma)=\alpha\).]

**Problem 5.3**: (i) Let \(Z\) and \(V\) be independently distributed as \(N(\delta\), \(1)\) and \(\chi^{2}\) with \(f\) degrees of freedom respectively. Then the ratio \(Z\div\sqrt{V/f}\) has the noncentral \(t\)-distribution with \(f\) degrees of freedom and noncentrality parameter \(\delta\), the probability density of which is 12

Footnote 12: A systematic account of this distribution can be found in in Owen (1985) and Johnson Kotz and Balakrishnan (1995).

\[p_{\delta}(t) = \frac{1}{2^{\frac{1}{2}(f-1)}\Gamma(\frac{1}{2}f)\sqrt{\pi f}}\int _{0}^{\infty}y^{\frac{1}{2}(f-1)}\] \[\times\exp\left(-\frac{1}{2}y\right)\exp\left[-\frac{1}{2}\left(t \sqrt{\frac{y}{f}}-\delta\right)^{2}dy\right]dy\]

or equivalently\[p_{\delta}(t) = \frac{1}{2^{\frac{1}{2}(f-1)}\Gamma(\frac{1}{2}f)\sqrt{\pi f}}\exp \left(-\frac{1}{2}\frac{f\delta^{2}}{f+t^{2}}\right)\] \[\times\left(\frac{f}{f+t^{2}}\right)^{\frac{1}{2}(f+1)}\int_{0}^{ \infty}v^{f}\exp\left[-\frac{1}{2}\left(\upsilon-\frac{\delta t}{\sqrt{f+t^{2} }}\right)^{2}\right]dv.\]

Another form is obtained by making the substitution \(w=t\sqrt{y}/\sqrt{f}\) in (5.79).
2. If \(X_{1},\ldots,X_{n}\) are independently distributed as \(N(\xi,\sigma^{2})\), then \(\sqrt{n}\tilde{X}\doteqdot\sqrt{\sum(X_{1}-\tilde{X})^{2}/(n-1)}\) has the noncentral \(t\)-distribution with \(n-1\) degrees of freedom and noncentrality parameter \(\delta=\sqrt{n}\xi/\sigma\). In the case \(\delta=0\), show that the \(t\)-distribution with \(n-1\) degrees of freedom is given by (5.18). [(i): The first expression is obtained from the joint density of \(Z\) and \(V\) by transforming to \(t=z\doteqdot\sqrt{\upsilon/f}\) and \(\upsilon\).]

**Problem 5.4**: Let \(X_{1},\ldots,X_{n}\) be a sample from \(N(\xi,\sigma^{2})\). Denote the power of the one-sided \(t\)-test of \(H:\xi\leq 0\) against the alternative \(\xi/\sigma\) by \(\beta(\xi/\sigma)\), and by \(\beta^{*}(\xi/\sigma)\) the power of the test appropriate when \(\sigma\) is known. Determine \(\beta(\xi/\sigma)\) for \(n=5\), \(10\), \(15\), \(\alpha=.05\), \(\xi/\sigma=.07\), \(0.8\), \(0.9\), \(1.0\), \(1.1\), \(1.2\), and in each case compare it with \(\beta^{*}(\xi/\sigma)\). Do the same for the two-sided case.

**Problem 5.5**: Let \(Z_{1},\ldots,Z_{n}\) be independently normally distributed with common variance \(\sigma^{2}\) and means \(E(Z_{i})=\zeta_{i}(i=1,\ldots,s)\), \(E(Z_{i})=0\) (\(i=s+1,\ldots,n\)). There exist UMP unbiased tests for testing \(\zeta_{1}\leq\zeta_{1}^{0}\) and \(\zeta_{1}=\zeta_{1}^{0}\) given by the rejection regions

\[\frac{Z_{1}-\zeta_{1}^{0}}{\sqrt{\sum\limits_{i=s+1}^{n}Z_{i}^{2}/(n-s)}}>C_{0} \quad\mbox{and}\quad\frac{|Z_{1}-\zeta_{1}^{0}|}{\sqrt{\sum\limits_{i=s+1}^{n }Z_{i}^{2}/(n-s)}}>C.\]

When \(\zeta_{1}=\zeta_{1}^{0}\), the test statistic has the \(t\)-distribution with \(n-s\) degrees of freedom.

**Problem 5.6**: Let \(X_{1},\ldots,X_{n}\) be independently normally distributed with common variance \(\sigma^{2}\) and means \(\zeta_{1},\ldots,\zeta_{n}\), and let \(Z_{i}=\sum_{j=1}^{n}a_{ij}X_{j}\) be an orthogonal transformation (that is, \(\sum_{i=1}^{n}a_{ij}a_{ik}=1\) or \(0\) as \(j=k\) or \(j\neq k\)). The \(Z\)'s are normally distributed with common variance \(\sigma^{2}\) and means \(\zeta_{i}=\sum a_{ij}\zeta_{j}\).

[The density of the \(Z\)'s is obtained from that of the \(X\)'s by substituting \(x_{i}=\sum b_{ij}z_{j}\), where \((b_{ij})\) is the inverse of the matrix \((a_{ij})\), and multiplying by the Jacobian, which is \(1\).]

**Problem 5.7**: If \(X_{1},\ldots,X_{n}\) is a sample from \(N(\xi,\sigma^{2})\), the UMP unbiased tests of \(\xi\leq 0\) and \(\xi=0\) can be obtained from Problems 5.5 and 5.6 by making an orthogonal transformation to variables \(Z_{1},\ldots,Z_{n}\) such that \(Z_{1}=\sqrt{n}\tilde{X}\).

[Then

\[\sum_{i=2}^{n}Z_{i}^{2}=\sum_{i=1}^{n}Z_{i}^{2}-Z_{1}^{2}=\sum_{i=1}^{n}X_{i}^ {2}-n\tilde{X}^{2}=\sum_{i=1}^{n}(X_{i}-\tilde{X})^{2}.]\]

**Problem 5.8**: Let \(X_{1}\), \(X_{2},\ldots\) be a sequence of independent variables distributed as \(N(\xi\), \(\sigma^{2})\), and let \(Y_{n}=[nX_{n+1}-(X_{1}+\cdots+X_{n})]/\sqrt{n(n+1)}\). Then the variables \(Y_{1}\), \(Y_{2}\), \(\ldots\) are independently distributed as \(N(0\), \(\sigma^{2})\).

**Problem 5.9**: Let \(N\) have the binomial distribution based on 10 trials with success probability \(p\). Given \(N=n\), let \(X_{1}\), \(\cdots\), \(X_{n}\) be i.i.d. normal with mean \(\theta\) and variance one. The data consists of \((N\), \(X_{1}\), \(\cdots\), \(X_{N})\).

(i). If \(p\) has a known value \(p_{0}\), show there does not exist a UMP test of \(\theta=0\) versus \(\theta>0\). [In fact, a UMPU test does not exist either.]

(ii). If \(p\) is unknown (taking values in (0,1)), find a UMPU test of \(\theta=0\) versus \(\theta>0\).

**Problem 5.10**: As in Example 3.9.2, suppose \(X\) is multivariate normal with unknown mean \(\xi=(\xi_{1}\), \(\ldots\), \(\xi_{k})^{\top}\) and known positive definite covariance matrix \(\Sigma\). Assume \(a=(a_{1},\ldots,a_{k})^{\top}\) is a fixed vector. The problem is to test

\[H:\ \sum_{i=1}^{k}a_{i}\xi_{i}=\delta\quad\mbox{vs.}\quad K:\ \sum_{i=1}^{k}a_{ k}\xi_{i}\neq\delta\.\]

Find a UMPU level \(\alpha\) test. _Hint:_ First consider \(\Sigma=I_{k}\), the identity matrix.

**Problem 5.11**: Let \(X_{i}=\xi+U_{i}\), and suppose that the joint density \(f\) of the \(U\)'s is _spherically symmetric_, that is, a function of \(\sum U_{i}^{2}\) only,

\[f(u_{1},\ldots,u_{n})=q(\sum u_{i}^{2})\.\]

Show that the null distribution of the one-sample \(t\)-statistic is independent of \(q\) and hence is the same as in the normal case, namely Student's \(t\) with \(n-1\) degrees of freedom. _Hint:_ Write \(t_{n}\) as

\[\frac{n^{1/2}\bar{X}_{n}/\sqrt{\sum X_{j}^{2}}}{\sqrt{\sum(X_{i}-\bar{X}_{n})^ {2}/(n-1)\sum X_{j}^{2}}}\,\]

and use the fact that when \(\xi=0\), the density of \(X_{1}\), \(\ldots\), \(X_{n}\) is constant over the spheres \(\sum x_{j}^{2}=c\) and hence the conditional distribution of the variables \(X_{i}/\sqrt{\sum X_{j}^{2}}\) given \(\sum X_{j}^{2}=c\) is uniform over the conditioning sphere and hence independent of \(q\). _Note._ This model represents one departure from the normal-theory assumption, which does not affect the level of the test. The effect of a much weaker symmetry condition more likely to arise in practice is investigated by Efron (1969).

**Problem 5.12**: Let \(X_{1},\ldots,X_{n}\) and \(Y_{1},\ldots,Y_{n}\) be independent samples from \(N(\xi,\sigma^{2})\) and \(N(\eta,\tau^{2})\), respectively. Determine the sample size necessary to obtain power \(\geq\beta\) against the alternatives \(\tau/\sigma>\Delta\) when \(\alpha=.05,\beta=.9\), \(\Delta=1.5\), \(2\), \(3\), and the hypothesis being tested is \(H:\tau/\sigma\leq 1\).

**Problem 5.13**: If \(m=n\), the acceptance region (5.23) can be written as

\[\max\left(\frac{S_{Y}^{2}}{\Delta_{0}S_{X}^{2}},\frac{\Delta_{0}S_{X}^{2}}{S_{ Y}^{2}}\right)\leq\frac{1-C}{C},\]

where \(S_{X}^{2}=\sum(X_{i}-\bar{X})^{2}\), \(S_{Y}^{2}=\sum(Y_{i}-\bar{Y})^{2}\) and where \(C\) is determined by

\[\int_{0}^{C}B_{n-1,n-1}(w)\,dw=\frac{\alpha}{2}.\]

**Problem 5.14**: Let \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) be samples from \(N(\xi,\sigma^{2})\) and \(N(\eta,\sigma^{2})\). The UMP unbiased test for testing \(\eta-\xi=0\) can be obtained through Problems 5.5 and 5.6 by making an orthogonal transformation from \((X_{1},\ldots X_{m},\)\(Y_{1},\ldots Y_{n})\) to \((Z_{1},\ldots,Z_{m+n})\) such that \(Z_{1}=(\bar{Y}-\bar{X})/\sqrt{1/m+(1/n)}\), \(Z_{2}=(\sum X_{i}+\sum Y_{i})/\sqrt{m+n}\).

**Problem 5.15**: Exponential densities. Let \(X_{1},\ldots,X_{n}\) be a sample from a distribution with exponential density \(a^{-1}e^{-(x-b)/a}\) for \(x\geq b\).

1. For testing \(a=1\) there exists a UMP unbiased test given by the acceptance region \[C_{1}\leq 2\sum[x_{i}-\min(x_{1},\ldots,x_{n})]\leq C_{2},\] where the test statistic has a \(\chi^{2}\)-distribution with \(2n-2\) degrees of freedom when \(\alpha=1\), and \(C_{1}\), \(C_{2}\) are determined by \[\int_{C_{1}}^{C_{2}}\chi^{2}_{2n-2}(y)\,dy=\int_{C_{1}}^{C_{2}}\chi^{2}_{2n}( y)\,dy=1-\alpha.\]
2. For testing \(b=0\) there exists a UMP unbiased test given by the acceptance region \[0\leq\frac{n\min(x_{1},\ldots,x_{n})}{\sum[x_{i}-\min(x_{i},\ldots,x_{n})]} \leq C.\] When \(b=0\), the test statistic has probability density \[p(u)=\frac{n-1}{(1+u)^{n}},\qquad u\geq 0.\][These distributions for varying \(b\) do not constitute an exponential family, and Theorem 4.4.1 is therefore not directly applicable. For (i), one can restrict attention to the ordered variables \(X_{(1)}<\cdots<X_{(n)}\), since these are sufficient for \(a\) and \(b\), and transform to new variables \(Z_{1}=nX_{(1)}\), \(Z_{i}=(n-i+1)[X_{(i)}-X_{(i-1)}]\) for \(i=2\),..., \(n\), as in Problem 2.15. When \(a=1\), \(Z_{1}\) is a complete sufficient statistic for \(b\), and the test is therefore obtained by considering the conditional problem given \(z_{1}\). Since \(\sum_{i=2}^{n}Z_{i}\) is independent of \(Z_{1}\), the conditional UMP unbiased test has the acceptance region \(C_{1}\leq\sum_{i=2}^{n}Z_{i}\leq C_{2}\) for each \(z_{1}\), and the result follows.

For (ii), when \(b=0\), \(\sum_{i=1}^{n}Z_{i}\), is a complete sufficient statistic for \(a\), and the test is therefore obtained by considering the conditional problem given \(\sum_{i=1}^{n}z_{i}\). The remainder of the argument uses the fact that \(Z_{1}/\sum_{i=1}^{n}Z_{i}\) is independent of \(\sum_{i=1}^{n}Z_{i}\), when \(b=0\), and otherwise is similar to that used to prove Theorem 5.1.1.]

**Problem 5.16**: Let \(X_{1},\ldots,X_{n}\) be a sample from the Pareto distribution \(P(c,\tau)\), both parameters unknown. Obtain UMP unbiased tests for the parameters \(c\) and \(\tau\). [Problems 5.15 and 3.8.]

**Problem 5.17**: Extend the results of the preceding problem to the case, considered in Problem 3.30, that observation is continued only until \(X_{(1)},\ldots,X_{(r)}\) have been observed.

**Problem 5.18**: _Gamma two-sample problem._ Let \(X_{1},\ldots X_{m}\); \(Y_{1},\ldots,Y_{n}\) be independent samples from gamma distributions \(\Gamma(g_{1},b_{1})\), \(\Gamma(g_{2},b_{2})\), respectively.

1. If \(g_{1},g_{2}\) are known, there exists a UMP unbiased test of \(H:b_{2}=b_{1}\) against one- and two-sided alternatives, which can be based on a beta distribution. [Some applications and generalizations are discussed in Lentner and Buehler (1963).]
2. If \(g_{1},g_{2}\) are unknown, show that a UMP unbiased test of \(H\) continues to exist, and describe its general form.
3. If \(b_{2}=b_{1}=b\) (unknown), there exists a UMP unbiased test of \(g_{2}=g_{1}\) against one- and two-sided alternatives; describe its general form.

[(i): If \(Y_{i}(i=1,2)\) are independent \(\Gamma(g_{i},b)\), then \(Y_{1}+Y_{2}\) is \(\Gamma(g_{1}+g_{2},b)\) and \(Y_{1}/(Y_{1}+Y_{2})\) has a beta distribution.]

**Problem 5.19**: _Inverse Gaussian distribution.13_ Let \(X_{1},\ldots,X_{n}\) be a sample from the inverse Gaussian distribution \(I(\mu,\tau)\), both parameters unknown.

Footnote 13: For additional information concerning inference in inverse Gaussian distributions, see Folks and Chhikara (1978) and Johnson, Kotz and Balakrishnan (1994, volume 1).

1. There exists a UMP unbiased test of \(\mu\leq\mu_{0}\) against \(\mu>\mu_{0}\), which rejects when \(\bar{X}>C[\sum(X_{i}+1/X_{i})]\), and a corresponding UMP unbiased test of \(\mu=\mu_{0}\) against \(\mu_{0}\not\equiv\mu_{0}\). [The conditional distribution needed to carry out this test is given by Chhikara and Folks (1976).]2. There exist UMP unbiased tests of \(H:\tau=\tau_{0}\) against both one- and two-sided hypotheses based on the statistic \(V=\sum(1/X_{i}-1/\bar{X})\).
3. When \(\tau=\tau_{0}\), the distribution of \(\tau_{0}V\) is \(\chi_{n-1}^{2}\).

[Tweedie (1957).]

**Problem 5.20**: Let \(X_{1}\),..., \(X_{m}\) and \(Y_{1}\),..., \(Y_{n}\) be independent samples from \(I\) (\(\mu\), \(\sigma\)) and \(I\) (\(\nu\), \(\tau\)), respectively.

1. There exist UMP unbiased tests of \(\tau_{2}/\tau_{1}\) against one- and two-sided alternatives.
2. If \(\tau=\sigma\), there exist UMP unbiased tests of \(\nu/\mu\) against one- and two-sided alternatives.

[Chhikara (1975).]

**Problem 5.21**: Suppose \(X\) and \(Y\) are independent, normally distributed with variance 1, and means \(\xi\) and \(\eta\), respectively. Consider testing the simple null hypothesis \(\xi=\eta=0\) against the composite alternative hypothesis \(\xi>0\), \(\eta>0\). Show that a UMPU test does not exist.

**Problem 5.22**: On the basis of a sample \(X=(X_{1},\ldots,X_{n})\) of fixed size from \(N(\xi\), \(\sigma^{2})\) there do not exist confidence intervals for \(\xi\) with positive confidence coefficient and of bounded length.14

Footnote 14: A similar conclusion holds in the problem of constructing a confidence interval for the ratio of normal means (Fieller’s problem), as discussed in Koschat (1987). For problems where it is impossible to construct confidence intervals with finite expected length, see Gleser and Hwang (1987).

[Consider any family of confidence intervals \(\delta(X)\pm L/2\) of constant length \(L\). Let \(\xi_{1}\),...\(\xi_{2n}\) be such that \(|\xi_{i}-\xi_{j}|>L\) whenever \(i\neq j\). Then the sets \(S_{i}\{x:|\delta(x)-\xi_{i}|\leq L/2\}\) (\(i=1,\ldots,2N\)) are mutually exclusive. Also, there exists \(\sigma_{0}>0\) such that

\[\left|P_{\xi_{i},\sigma}\{X\in S_{i}\}-P_{\xi_{1},\sigma}\{X\in S_{i}\}\right| \leq\frac{1}{2N}\ \ \ \ \ {\rm for}\ \ \ \sigma>\sigma_{0},\]

as is seen by transforming to new variables \(Y_{j}=(X_{j}-\xi_{1})/\sigma\) and applying Lemmas 5.5.1 and 5.11.1 of the appendix. Since \(\min_{i}P_{\xi_{1},\sigma}\{X\in S_{i}\}\leq 1/(2N)\), it follows for \(\sigma>\sigma_{0}\) that \(\min_{i}P_{\xi_{1},\sigma}\{X\in S_{i}\}\leq 1/N\), and hence that

\[\inf_{\xi,\sigma}P_{\xi,\sigma}\left\{|\delta(X)-\xi|\leq\frac{L}{2}\right\} \leq\frac{1}{N}.\]

The confidence coefficient associated with the intervals \(\delta(X)\pm L/2\) is therefore zero, and the same must be true a fortiori of any set of confidence intervals of length \(\leq L\).]

**Problem 5.23**: _Stein's two-stage procedure._

1. If \(mS^{2}/\sigma^{2}\) has a \(\chi^{2}\)-distribution with \(m\) degrees of freedom, and if the conditional distribution of \(Y\) given \(S=s\) is \(N(0,\sigma^{2}/S^{2})\), then \(Y\) has Student's \(t\)-distribution with \(m\) degrees of freedom.
2. Let \(X_{1},X_{2},\ldots\) be independently distributed as \(N(\xi,\sigma^{2})\). Let \(\bar{X}_{0}=\sum_{i=1}^{n_{0}}X_{i}/n_{0}\), \(S^{2}=\sum_{i=1}^{n_{0}}(X_{i}-\bar{X}_{0})^{2}/(n_{0}-1)\), and let \(a_{1}=\cdots=a_{n_{0}}=a\), \(a_{n_{0}+1}=\cdots=a_{n}=b\) and \(n\geq n_{0}\) be measurable functions of \(S\). Then \[Y=\frac{\sum\limits_{i=1}^{n}a_{i}(X_{i}-\xi)}{\sqrt{S^{2}\sum_{i=1}^{n}a_{i}^ {2}}}\] has Student's distribution with \(n_{0}-1\) degrees of freedom.
3. Consider a two-stage sampling scheme \(\prod_{1}\), in which \(S^{2}\) is computed from an initial sample of size \(n_{0}\), and then \(n-n_{0}\) additional observations are taken. The size of the second sample is such that \[n=\max\left\{n_{0}+1,\left[\frac{S^{2}}{c}\right]+1\right\},\] where \(c\) is any given constant and where \([y]\) denotes the largest integer \(\geq y\). There then exist numbers \(a_{1}\), \(\ldots,a_{n}\) such that \(a_{1}=\cdots=a_{n_{0}}\), \(a_{n_{0}+1}=\cdots a_{n}\), \(\sum_{i=1}^{n}a_{i}=1\), \(\sum_{i=1}^{n}a_{i}^{2}=c/S^{2}\). It follows from (ii) that \(\sum_{i=1}^{n}a_{i}(X_{i}-\xi)/\sqrt{c}\) has Student's \(t\)-distribution with \(n_{0}-1\) degrees of freedom.
4. The following sampling scheme \(\prod_{2}\), which does not require that the second sample contain at least one observation, is slightly more efficient than \(\prod_{1}\), for the applications to be made in Problems 5.24 and 5.25. Let \(n_{0}\), \(S^{2}\), and \(c\) be defined as before; let \[n=\max\left\{n_{0},\left[\frac{S^{2}}{c}\right]+1\right\}\] \(a_{i}=1/n\) (\(i=1,\ldots,n\)), and \(\bar{X}=\sum_{i=1}^{n}a_{i}X_{i}\). Then \(\sqrt{n}(\bar{X}-\xi)/S\) has again the \(t\)-distribution with \(n_{0}-1\) degrees of freedom.

[(ii): Given \(S=s\), the quantities \(a\), \(b\), and \(n\) are constants, \(\sum_{i=1}^{n}a_{i}(X_{i}-\xi)=n_{0}a(\bar{X}_{0}-\xi)\) is distributed as \(N(0,n_{0}a^{2}\sigma^{2})\), and the numerator of \(Y\) is therefore normally distributed with zero mean and variance \(\sigma^{2}\sum_{i=1}^{n}a_{i}^{2}\). The result now follows from (i).]

**Problem 5.24**: Confidence intervals of fixed length for a normal mean.

1. In the two-stage procedure \(\prod_{1}\), defined in part (iii) of the preceding problem, let the number \(c\) be determined for any given \(L>0\) and \(0<\gamma<1\) by\[\int_{-L/2\sqrt{c}}^{L/2\sqrt{c}}t_{n_{0}-1}(y)\,dy=\gamma,\] where \(t_{n_{0}-1}\) denotes the density of the \(t\)-distribution with \(n_{0}-1\) degrees of freedom. Then the intervals \(\sum_{i=1}^{n}a_{i}X_{i}\pm L/2\) are confidence intervals for \(\xi\) of length \(L\) and with confidence coefficient \(\gamma\).
2. Let \(c\) be defined as in (i), and let the sampling procedure be \(\prod_{2}\) as defined in part (iv) of Problem 5.23. The intervals \(\bar{X}\pm L/2\) are then confidence intervals of length \(L\) for \(\xi\) with confidence coefficient \(\geq\gamma\), while the expected number of observations required is slightly lower than under \(\prod_{1}\).

[(i): The probability that the intervals cover \(\xi\) equals

\[P_{\xi,\sigma}\left\{-\frac{L}{2\sqrt{c}}\leq\frac{\sum\limits_{i=1}^{n}a_{i}( X_{i}-\xi)}{\sqrt{c}}\leq\frac{L}{2\sqrt{c}}\right\}=\gamma.\]

(ii): The probability that the intervals cover \(\xi\) equals

\[P_{\xi,\sigma}\left\{\frac{\sqrt{n}|\bar{X}-\xi|}{S}\leq\frac{\sqrt{n}L}{2S} \right\}\geq\left\{\frac{\sqrt{n}|\bar{X}-\xi|}{S}\leq\frac{L}{2\sqrt{c}} \right\}=\gamma.]

**Problem 5.25**: _Two-stage \(t\)-tests with power independent of \(\sigma\)._

1. For the procedure \(\prod_{1}\) with any given \(c\), let \(C\) be defined by \[\int_{C}^{\infty}t_{n_{0}-1}(y)\,dy=\alpha.\] Then the rejection region \((\sum_{i=1}^{n}a_{i}X_{i}-\xi_{0})/\sqrt{c}>C\) defines a level-\(\alpha\) test of \(H:\xi\leq\xi_{0}\) with strictly increasing power function \(\beta_{c}(\xi)\) depending only on \(\xi\).
2. Given any alternative \(\xi_{1}\) and any \(\alpha<\beta<1\), the number \(c\) can be chosen so that \(\beta_{c}(\xi_{1})=\beta\).
3. The test with rejection region \(\sqrt{n}(\bar{X}-\xi_{0})/S>C\) based on \(\prod_{2}\) and the same \(c\) as in (i) is a level-\(\alpha\) test of \(H\) which is uniformly more powerful than the test given in (i).
4. Extend parts (i)-(iii) to the problem of testing \(\xi=\xi_{0}\) against \(\xi\neq\xi_{0}\).

[(i) and (ii): The power of the test is

\[\beta_{c}(\xi)=\int_{C-(\xi-\xi_{0})/\sqrt{c}}t_{n_{0}-1}(y)\,dy.\]

(iii): This follows from the inequality \(\sqrt{n}|\xi-\xi_{0}|/S\geq|\xi-\xi_{0}|/\sqrt{c}\).]

**Problem 5.26**: Let \(S(x)\) be a family of confidence sets for a real-valued parameter \(\theta\), and let \(\mu[S(x)]\) denote its Lebesgue measure. Then for every fixed distribution \(Q\) of \(X\) (and hence in particular for \(Q=P_{\theta_{0}}\) where \(\theta_{0}\) is the true value of \(\theta\))

\[E_{Q}\{\mu[S(X)]\}=\int_{\theta\neq\theta_{0}}Q\{\theta\in S(X)\}\,d\theta\]

provided the necessary measurability conditions hold.

[The identity is known as the Ghosh-Pratt identity; see Ghosh (1961) and Pratt (1961a). To prove it, write the expectation on the left side as a double integral, apply Fubini's Theorem, and note that the integral on the right side is unchanged if the point \(\theta=\theta_{0}\) is added to the region of integration.]

**Problem 5.27**: Use the preceding problem to show that uniformly most accurate confidence sets also uniformly minimize the expected Lebesgue measure (length in the case of intervals) of the confidence sets.15

Footnote 15: For the corresponding result concerning one-sided confidence bounds, see Madansky (1962).

### _Section 5.5_

**Problem 5.28**: Let \(X_{1},\,\ldots,\,X_{n}\) be distributed as in Problem 5.15. Then the most accurate unbiased confidence intervals for the scale parameter \(a\) are

\[\frac{2}{C_{2}}\sum[x_{i}-\min(x_{1},\,\ldots,\,x_{n})]\leq a\leq\frac{2}{C_{ 1}}\sum[x_{i}-\min(x_{1},\,\ldots,\,x_{n})].\]

**Problem 5.29**: Most accurate unbiased confidence intervals exist in the following situations:

1. If \(X\), \(Y\) are independent with binomial distributions \(b(p_{1},\,m)\) and \(b(p_{2},\,m)\), for the parameter \(p_{1}q_{2}/p_{2}q_{1}\).
2. In a 2 \(\times\) 2 table, for the parameter \(\Delta\) of Section 4.6.

**Problem 5.30**: _Shape parameter of a gamma distribution._ Let \(X_{1},\,\ldots,\,X_{n}\) be a sample from the gamma distribution \(\Gamma(g,\,b)\) defined in Problem 3.36.

1. There exist UMP unbiased tests of \(H:g\leq g_{0}\) against \(g>g_{0}\) and of \(H^{\prime}:g=g_{0}\) against \(g\neq g_{0}\), and their rejection regions are based on \(W=\prod(X_{i}/\bar{X})\).
2. There exist uniformly most accurate confidence intervals for \(g\) based on \(W\).

[Shorack (1972).]

_Notes_.

1. The null distribution of \(W\) is discussed in Bain and Engelhardt (1975), Glaser (1976), and Engelhardt and Bain (1978).

2. For \(g=1\), \(\Gamma(g,b)\) reduces to an exponential distribution, and (i) becomes the UMP unbiased test for testing that a distribution is exponential against the alternative that it is gamma with \(g>1\) or with \(g\neq 1\).
3. An alternative treatment of this and some of the following problems is given by Bar-Lev and Reiser (1982).

**Problem 5.31**: _Scale parameter of a gamma distribution_. Under the assumptions of the preceding problem, there exists

1. A UMP unbiased test of \(H:b\leq b_{0}\) against \(b>b_{0}\) which rejects when \(\sum X_{i}>C(\prod,X_{i})\).
2. Most accurate unbiased confidence intervals for \(b\).

[The conditional distribution of \(\sum X_{i}\) given \(\prod X_{i}\), which is required for carrying out this test, is discussed by Engelhardt and Bain (1977).]

**Problem 5.32**: In Example 5.5.1, consider a confidence interval for \(\sigma^{2}\) of the form \(I=[d_{n}^{-1}S_{n}^{2},\)\(c_{n}^{-1}S_{n}^{2}]\), where \(S_{n}^{2}=\sum_{i}(X_{i}-\bar{X})^{2}\) and \(c_{n}<d_{n}\) are constants. Subject to the level constraint, choose \(c_{n}\) and \(d_{n}\) to minimize the length of \(I\). Argue that the solution has shorter length that the uniformly most accurate one; however, it is biased and so does not uniformly improve the probability of covering false values. [The solution, given in Tate and Klett (1959), satisfies \(\chi_{n+3}^{2}(c_{n})=\chi_{n+3}^{2}(d_{n})\) and \(\int_{c_{n}}^{d_{n}}\chi_{n-1}^{2}(y)dy=1-\alpha\), where \(\chi_{n}^{2}(y)\) denotes the Chi-squared density with \(n\) degrees of freedom. Improvements of this interval which incorporate \(\bar{X}\) into their construction are discussed in Cohen (1972) and Shorrock (1990); also see Goutis and Casella (1991).]

**Problem 5.33**:
1. Under the assumptions made at the beginning of Section 5.6, the UMP unbiased test of \(H:\rho=\rho_{0}\) is given by (5.44).
2. Let \((\rho,\bar{\rho})\) be the associated most accurate unbiased confidence intervals for \(\rho=a\gamma+b\delta\), where \(\underline{\rho}=\underline{\rho}(a,b)\), \(\bar{\rho}=\bar{\rho}(a,b)\). Then if \(f_{1}\) and \(f_{2}\) are increasing functions, the expected value of \(f_{1}(|\bar{\rho}-\rho|)+f_{2}(|\rho-\underline{\rho}|)\) is an increasing function of \(a^{2}/n+b^{2}\).

[(i): Make an orthogonal transformation from \(y_{1},\ldots,y_{n}\) to new variables \(z_{1},\ldots,z_{n}\), such that \(z_{1}=\sum_{i}[bv_{i}+(a/n)]y_{i}/\sqrt{(a^{2}/n)+b^{2}},z_{2}=\sum_{i}(av_{i }-b)y_{i}/\sqrt{a^{2}+nb^{2}}\), and apply Problems 5.5 and 5.6.

(ii): If \(a_{1}^{2}/n+b_{1}^{2}<a_{2}^{2}/n+b_{2}^{2}\), the random variable \(|\tilde{\rho}(a_{2},\)\(b_{2})-\rho|\) is stochastically larger than \(|\tilde{\rho}(a_{1},\)\(b_{1})-\rho|\), and analogously for \(\rho\).]

### Section 5.7

**Problem 5.34**: Verify the posterior distribution of \(\Theta\) given \(x\) in Example 5.7.1.

**Problem 5.35**: If \(X_{1},\,\ldots,\,X_{n}\), are independent \(N(\theta,\,1)\) and \(\theta\) has the improper prior \(\pi(\theta)\equiv 1\), determine the posterior distribution of \(\theta\) given the \(X\)'s.

**Problem 5.36**: Verify the posterior distribution of \(p\) given \(x\) in Example 5.7.2.

**Problem 5.37**: In Example 5.7.3, verify the marginal posterior distribution of \(\xi\) given \(x\).

**Problem 5.38**: In Example 5.7.4, show that

1. the posterior density \(\pi(\sigma\mid x)\) is of type (c) of Example 5.7.2;
2. for sufficiently large \(r\), the posterior density of \(\sigma^{r}\) given \(x\) is no longer of type (c).

**Problem 5.39**: If \(X\) is normal \(N(\theta,\,1)\) and \(\theta\) has a Cauchy density \(b/\{\pi[b^{2}+(\theta-\mu)^{2}]\}\), determine the possible shapes of the HPD regions for varying \(\mu\) and \(b\).

**Problem 5.40**: Let \(\theta=(\theta_{1},\,\ldots,\,\theta_{s})\) with \(\theta_{i}\) real-valued, \(X\) have density \(p_{\theta}(x)\), and \(\Theta\) a prior density \(\pi(\theta)\). Then the 100\(\gamma\)% HPD region is the 100\(\gamma\)% credible region \(R\) that has minimum volume.

[Apply the Neyman-Pearson fundamental lemma to the problem of minimizing the volume of \(R\).]

**Problem 5.41**: Let \(X_{1},\,\ldots,\,X_{m}\) and \(Y_{1},\,\ldots,\,Y_{n}\) be independently distributed as \(N(\xi,\,\sigma^{2})\) and \(N(\eta,\,\sigma^{2})\), respectively, and let \((\xi,\,\eta,\,\sigma)\) have the joint improper prior density given by

\[\pi(\xi,\,\eta,\,\sigma)\,d\xi\,d\eta\,d\sigma=d\xi\,\,d\eta\cdot\frac{1}{ \sigma}\,d\sigma\qquad\mbox{for all}\quad-\infty<\xi,\,\eta<\infty,\quad 0<\sigma.\]

Under these assumptions, extend the results of Examples 5.7.3 and 5.7.4 to inferences concerning (i) \(\eta-\xi\) and (ii) \(\sigma\).

**Problem 5.42**: Let \(X_{1},\,\ldots,\,X_{m}\) and \(Y_{1},\,\ldots,\,Y_{n}\) be independently distributed as \(N(\xi,\,\sigma^{2})\) and \(N(\eta,\,\tau^{2})\), respectively and let \((\xi,\,\eta,\,\sigma,\,\tau)\) have the joint improper prior density \(\pi(\xi,\,\eta,\,\sigma,\,\tau)\,d\xi\,d\eta\,d\sigma\,d\tau=d\xi\,d\eta(1/ \sigma)\,d\sigma(1/\tau)\,d\tau\). Extend the result of Example 5.7.4 to inferences concerning \(\tau^{2}/\sigma^{2}\).

_Note_. The posterior distribution of \(\eta-\xi\) in this case is the so-called Behrens-Fisher distribution. The credible regions for \(\eta-\xi\) obtained from this distribution do not correspond to confidence intervals with fixed coverage probability, and the associated tests of \(H:\eta=\xi\) thus do not have fixed size (which instead depends on \(\tau/\sigma\)). From numerical evidence [see Robinson (1976) for a summary of his and earlier results] it appears that the confidence intervals are conservative, that is, the actual coverage probability always exceeds the nominal one.

**Problem 5.43**: Let \(T_{1}\),..., \(T_{s-1}\) have the multinomial distribution (2.34), and suppose that \((p_{1},\ldots,p_{s-1})\) has the Dirichlet prior density \(D(a_{1},\ldots,a_{s})\) with density proportional to \(p_{1}^{a_{1}-1}\ldots p_{s}^{a_{s}-1}\), where \(p_{s}=1-(p_{1}+\cdots+p_{s-1})\). Determine the posterior distribution of \((p_{1},\ldots,p_{s-1})\) given the \(T\)'s.

### _Section 5.8_

**Problem 5.44**: Prove Theorem 5.8.1 for arbitrary values of \(c\).

### _Section 5.9_

**Problem 5.45**: If \(c=1\), \(m=n=4\), \(\alpha=.1\) and the ordered coordinates \(z_{(1)}\),..., \(z_{(N)}\) of a point \(z\) are 1.97, 2.19, 2.61, 2.79, 2.88, 3.02, 3.28, 3.41, determine the points of \(S(z)\) belonging to the rejection region (5.53).

**Problem 5.46**: _Confidence intervals for a shift._ [Maritz (1979)]

1. Let \(X_{1}\),..., \(X_{m}\); \(Y_{1}\),..., \(Y_{n}\) be independently distributed according to continuous distributions \(F(x)\) and \(G(y)=F(y-\Delta)\) respectively. Without any further assumptions concerning \(F\), confidence intervals for \(\Delta\) can be obtained from permutation tests of the hypotheses \(H(\Delta_{0}):\Delta=\Delta_{0}\). Specifically, consider the point \((z_{1},\ldots,z_{m+n})=(x_{1},\ldots,x_{m},y_{1}-\Delta,\ldots,y_{n}-\Delta)\) and the \({m+n\choose m}\) permutations \(i_{1}<\cdots<i_{m}\); \(i_{m+1}<\cdots<i_{m+n}\) of the integers \(1,\ldots,m+n\). Suppose that the hypothesis \(H(\Delta)\) is accepted for the \(k\) of these permutations which lead to the smallest values of \[\left|\sum_{j=m+1}^{m+n}z_{i_{j}}/n-\sum_{j=1}^{m}z_{i_{j}}/m\right|,\] where \[k=(1-\alpha){m+n\choose m}.\] Then the totality of values \(\Delta\) for which \(H(\Delta)\) is accepted constitute an interval, and these intervals are confidence intervals for \(\Delta\) at confidence level \(1-\alpha\).
2. Let \(Z_{1}\),..., \(Z_{N}\) be independently distributed, symmetric about \(\theta\), with distribution \(F(z-\theta)\), where \(F(z)\) is continuous and symmetric about \(0\). Without any further assumptions about \(F\), confidence intervals for \(\theta\) can be obtained by considering the \(2^{N}\) points \(Z_{1}^{\prime}\),..., \(Z_{N}^{\prime}\) where \(Z_{i}^{\prime}=\pm(Z_{i}-\theta_{0})\), and accepting \(H(\theta_{0}):\theta=\theta_{0}\) for the \(k\) of these points which lead to the smallest values of \(|\sum Z_{i}^{\prime}|\), where \(k=(1-\alpha)2^{N}\).

[(i): A point is in the acceptance region for \(H(\Delta)\) if

\[\left|\frac{\sum(y_{j}-\Delta)}{n}-\frac{\sum x_{i}}{m}\right|=|\bar{y}-\bar{x}-\Delta|\]

is exceeded by at least \({m+n\choose n}-k\) of the quantities \(|\bar{y}^{\prime}-\bar{x}^{\prime}-\gamma\Delta|\), where \((x_{1}^{\prime},\ldots,x_{m}^{\prime},y_{1}^{\prime},\ldots,y_{n}^{\prime})\) is a permutation of \((x_{1},\ldots,x_{m},y_{1},\ldots,y_{n})\), the quantity \(\gamma\) is determined by this permutation, and \(|\gamma|\leq 1\). The desired result now follows from the following facts (for an alternative proof, see Section 14): (a) The set of \(\Delta\)'s for which \((\bar{y}-\bar{x}-\Delta)^{2}\leq(\bar{y}^{\prime}-\bar{x}^{\prime}-\gamma \Delta)^{2}\) is, with probability one, an interval containing \(\bar{y}-\bar{x}\). (b) The set of \(\Delta\)'s for which \((\bar{y}-\bar{x}-\Delta)^{2}\) is exceeded by a particular set of at least \({m+n\choose m}-k\) of the quantities \((\bar{y}^{\prime}-\bar{x}^{\prime}-\gamma\Delta)^{2}\) is the intersection of the corresponding intervals (a) and hence is an interval containing \(\bar{y}-\bar{x}\). (c) The set of \(\Delta\)'s of interest is the union of the intervals (b) and, since they have a nonempty intersection, also an interval.]

### Section 5.10

**Problem 5.47**: In the matched-pairs experiment for testing the effect of a treatment, suppose that only the differences \(Z_{i}=Y_{i}-X_{i}\) are observable. The \(Z\)'s are assumed to be a sample from an unknown continuous distribution, which under the hypothesis of no treatment effect is symmetric with respect to the origin. Under the alternatives it is symmetric with respect to a point \(\zeta>0\). Determine the test which among all unbiased tests maximizes the power against the alternatives that the \(Z\)'s are a sample from \(N(\zeta,\sigma^{2})\) with \(\zeta>0\).

[Under the hypothesis, the set of statistics \((\sum_{i=1}^{n}Z_{i}^{2},\ldots,\sum_{i=1}^{n}Z_{i}^{2n})\) is sufficient; that it is complete is shown as the corresponding result in Theorem 5.8.1. The remainder of the argument follows the lines of Section 5.9.]

**Problem 5.48**:
1. If \(X_{1}\),..., \(X_{n}\); \(Y_{1}\),..., \(Y_{n}\) are independent normal variables with common variance \(\sigma^{2}\) and means \(E(X_{i})=\xi_{i}\), \(E(Y_{i})=\xi_{i}+\Delta\), the UMP unbiased test of \(\Delta=0\) against \(\Delta>0\) is given by (5.58).
2. Determine the most accurate unbiased confidence intervals for \(\Delta\).

[(i): The structure of the problem becomes clear if one makes the orthogonal transformation \(X_{i}^{\prime}=(Y_{i}-X_{i})/\sqrt{2}\), \(Y_{i}^{\prime}=(X_{i}+Y_{i})/\sqrt{2}\).]

**Problem 5.49**: _Comparison of two designs._ Under the assumptions made at the beginning of Section 5.10, one has the following comparison of the methods of complete randomization and matched pairs. The unit effects and experimental effects \(U_{i}\) and \(V_{i}\) are independently normally distributed with variances \(\sigma_{1}^{2}\), \(\sigma^{2}\) and means \(E(U_{i})=\mu\) and \(E(V_{i})=\xi\) or \(\eta\) as \(V_{i}\) corresponds to a control or treatment. With complete randomization, the observations are \(X_{i}=U_{i}+V_{i}\) (\(i=1,\ldots,n\)) for the controls and \(Y_{i}=U_{n+i}+V_{n+i}\) (\(i=1,\ldots,n\)) for the treated cases, with \(E(X_{i})=\mu+\xi\)\(E(Y_{i})=\mu+\eta\). For the matched pairs, if the matching is assumed to be perfect, the \(X\)'s are as before, but \(Y_{i}=U_{i}+V_{m+i}\). UMP unbiased tests are given by (5.27) for complete randomization and by (5.58) for matched pairs. The distribution of the test statistic under an alternative \(\Delta=\eta-\xi\) is the noncentral \(t\)-distribution with noncentrality parameter \(\sqrt{n}\Delta/\sqrt{2(\sigma^{2}+\sigma_{1}^{2})}\) and \(2n-2\) degrees of freedom in the first case, and with noncentrality parameter \(\sqrt{n}\Delta/\sqrt{2}\sigma\) and \(n-1\) degrees of freedom in the second. Thus the method of matched pairs has the disadvantage of a smaller number of degrees of freedom and the advantage of a larger noncentrality parameter. For \(\alpha=.05\) and \(\Delta=4\), compare the power of the two methods as a function of \(n\) when \(\sigma_{1}\), \(\sigma=2\) and when \(\sigma_{1}=2\), \(\sigma=1\).

**Problem 5.50**: _Continuation_. An alternative comparison of the two designs is obtained by considering the expected length of the most accurate unbiased confidence intervals for \(\Delta=\eta-\xi\) in each case. Carry this out for varying \(n\) and confidence coefficient \(1-\alpha=.95\) when \(\sigma_{1}=1\), \(\sigma=2\) and when \(\sigma_{1}=2\), \(\sigma=1\).

### _Section 5.11_

**Problem 5.51**: Suppose that a critical function \(\phi_{0}\) satisfies (5.64) but not (5.66), and let \(\alpha<\frac{1}{2}\). Then the following construction provides a measurable critical function \(\phi\) satisfying (5.66) and such that \(\phi_{0}(z)\leq\phi(z)\) for all \(z\). Inductively, sequences of functions \(\phi_{1}\), \(\phi_{2}\),... and \(\psi_{0}\), \(\psi_{1}\),... are defined through the relations

\[\psi_{m}(z)=\sum_{z^{\prime}\in S(z)}\frac{\phi_{m}(z^{\prime})}{N_{1}!\ldots N _{c}!},\qquad m=0,1,\ldots,\]

and

\[\phi_{m}(z)=\left\{\begin{array}{ll}\phi_{m-1}(z)+[\alpha-\psi_{m-1}(z)]\\ \qquad\qquad\qquad\qquad\mbox{if both $\phi_{m-1}(z)$ and $\psi_{m-1}(z)$ are $<\alpha$},\\ \phi_{m-1}(z)\qquad\mbox{otherwise}.\end{array}\right.\]

The function \(\phi(z)=\lim\phi_{m}(z)\) then satisfies the required conditions.

[The functions \(\phi_{m}\) are nondecreasing and between \(0\) and \(1\). It is further seen by induction that \(0\leq\alpha-\psi_{m}(z)\leq(1-\gamma)^{m}[\alpha-\psi_{0}(z)]\), where \(\gamma=1/(N_{1}!\ldots N_{c}!)\).]

**Problem 5.52**: Consider the problem of testing \(H:\eta=\xi\) in the family of densities (5.61) when it is given that \(\sigma>c>0\) and that the point \((\zeta_{11},\ldots,\zeta_{cN_{c}})\) of (5.62) lies in a bounded region \(R\) containing a rectangle, where \(c\) and \(R\) are known. Then Theorem 5.11.1 is no longer applicable. However, unbiasedness of a test \(\phi\) of \(H\) implies (5.66), and therefore reduces the problem to the class of permutation tests.

[Unbiasedness implies \(\int(\phi(z)\,p_{\sigma,\zeta}(z)\,dz=\alpha\) and hence \[\alpha=\int\psi(z)p_{\sigma,\zeta}(z)\,dz=\int\,\psi(z)\frac{1}{(\sqrt{2\pi}\sigma)^{ N}}\exp\left[-\frac{1}{2\sigma^{2}}\sum\sum(z_{ij}-\zeta_{ij})^{2}\right]dz\]

for all \(\sigma>c\) and \(\zeta\) in \(R\). The result follows from completeness of this last family.]

**Problem 5.53** To generalize Theorem 5.11.1 to other designs, let \(Z=(Z_{1},\ldots,Z_{N})\) and let \(G=\{g_{1},\ldots,g_{r}\}\) be a group of permutations of \(N\) coordinates or more generally a group of orthogonal transformations of \(N\)-space. If

\[P_{\sigma,\zeta}(z)=\frac{1}{r}\sum_{k=1}^{r}\frac{1}{(\sqrt{2\pi}\sigma)^{N}} \exp\left(-\frac{1}{2\sigma^{2}}|z-g_{k}\zeta|^{2}\right), \tag{5.80}\]

where \(|z|^{2}=\sum z_{i}^{2}\), then \(\int\phi(z)p_{\sigma,\zeta}(z)\,dz\leq\alpha\) for all \(\sigma>0\) and all \(\zeta\) implies

\[\frac{1}{r}\sum_{z^{\prime}\in S(z)}\phi(z^{\prime})\leq\alpha\quad\quad\mbox {a.e.,} \tag{5.81}\]

where \(S(z)\) is the set of points in \(N\)-space obtained from \(z\) by applying to it all the transformations \(g_{k}\), \(k=1,\ldots,r\).

**Problem 5.54**: _Generalization of Corollary 5.11.1._ Let \(H\) be the class of densities (5.80) with \(\sigma>0\) and \(-\infty<\zeta_{i}<\infty\) (\(i=1,\ldots,N\)). A complete family of tests of \(H\) at level of significance \(\alpha\) is the class of permutation tests satisfying

\[\frac{1}{r}\sum_{z^{\prime}\in S(z)}\phi(z^{\prime})=\alpha\quad\quad\mbox{a. e.} \tag{5.82}\]

### _Section 5.12_

**Problem 5.55** If \(c=1\), \(m=n=3\), and if the ordered \(x\)'s and \(y\)'s are respectively 1.97, 2.19, 2.61 and 3.02, 3.28, 3.41, determine the points \(\delta_{(1)}\),..., \(\delta_{(19)}\) defined as the ordered values of (5.72).

**Problem 5.56** If \(c=4\), \(m_{i}=n_{i}=1\), and the pairs \((x_{i},y_{i})\) are (1.56,2.01), (1.87,2.22), (2.17,2.73), and (2.31,2.60), determine the points \(\delta_{(1)}\),..., \(\delta_{(15)}\) which define the intervals (5.71).

**Problem 5.57** If \(m\), \(n\) are positive integers with \(m\leq n\), then

\[\sum_{K=1}^{m}\binom{m}{K}\binom{n}{K}=\binom{m+n}{m}-1.\]

[MISSING_PAGE_EMPTY:1850]

### Section 5.13

**Problem 5.64**:
1. If the joint distribution of \(X\) and \(Y\) is the bivariate normal distribution (5.69), then the conditional distribution of \(Y\) given \(x\) is the normal distribution with variance \(\tau^{2}(1-\rho^{2})\) and mean \(\eta+(\rho\tau/\sigma)(x-\xi)\).
2. Let \((X_{1},\,Y_{1}),\,\ldots,\,(X_{n},\,Y_{n})\) be a sample from a bivariate normal distribution, let \(R\) be the sample correlation coefficient, and suppose that \(\rho=0\). Then the conditional distribution of \(\sqrt{n-2}R/\sqrt{1-R^{2}}\) given \(x_{1},\,\ldots,\,x_{n}\) is Student's \(t\)-distribution with \(n-2\) degrees of freedom provided \(\sum(x_{i}-\bar{x})^{2}>0\). This is therefore also the unconditional distribution of this statistic.
3. The probability density of \(R\) itself is then \[p(r)=\frac{1}{\sqrt{n}}\frac{\Gamma[\frac{1}{2}(n-1)]}{\Gamma[\frac{1}{2}(n-2) ]}(1-r^{2})^{\frac{1}{2}n-2}.\] (5.83)

[(ii): If \(v_{i}=(x_{1}-\bar{x})/\sqrt{\sum(x_{j}-\bar{x})^{2}}\) so that \(\sum v_{i}=0\), \(\sum v_{1}^{2}=1\), the statistic can be written as \[\frac{\sum v_{i}Y_{i}}{\sqrt{\left[\sum Y_{i}^{2}-n\bar{Y}^{2}-\left(\sum v_{i }Y_{i}\right)^{2}\right]/(n-2)}}.\] Since its distribution depends only on \(\rho\) one can assume \(\eta=0\), \(\tau=1\). The desired result follows from Problem 5.6 by making an orthogonal transformation from \((Y_{1},\,\ldots,\,Y_{n})\) to \((Z_{1},\,\ldots,\,Z_{n})\) such that \(Z_{1}=\sqrt{n}\bar{Y}\), \(Z_{2}=\sum v_{i}Y_{i}\).]

**Problem 5.65**:
1. Let \((X_{1},\,Y_{1}),\,\ldots,\,(X_{n},\,Y_{n})\) be a sample from the bivariate normal distribution (5.69), and let \(S_{1}^{2}=\sum(X_{i}-\bar{X})^{2}\), \(S_{2}^{2}=\sum(Y_{i}-\bar{Y})^{2}\), \(S_{12}=\sum(X_{i}-\bar{X})(Y_{i}-\bar{Y})\). There exists a UMP unbiased test for testing the hypothesis \(\tau/\sigma=\Delta\). Its acceptance region is \[\frac{|\Delta^{2}S_{1}^{2}-S_{2}^{2}|}{\sqrt{(\Delta^{2}S_{1}^{2}+S_{2}^{2})^{ 2}-4\Delta^{2}S_{12}^{2}}}\leq C,\] and the probability density of the test statistic is given by (5.83) when the hypothesis is true.
2. Under the assumption \(\tau=\sigma\), there exists a UMP unbiased test for testing \(\eta=\xi\), with acceptance region \(|\bar{Y}-\bar{X}|/\sqrt{S_{1}^{2}+S_{2}^{2}-2S_{12}}\leq C\). On multiplication by a suitable constant the test statistic has Student's \(t\)-distribution with \(n-1\) degrees of freedom when \(\eta=\xi\).

[Due to Morgan (1939) and Hsu (1940). (i): The transformation \(U=\Delta X+Y\), \(V=X-(1/\Delta)Y\) reduces the problem to that of testing that the correlation coefficient in a bivariate normal distribution is zero.

(ii): Transform to new variables \(V_{i}=Y_{i}-X_{i}\), \(U_{i}=Y_{i}+X_{i}\).]

[MISSING_PAGE_EMPTY:1852]

\[\Gamma\big{[}\tfrac{1}{2}(m-1)\big{]}\Gamma\big{(}\tfrac{1}{2}m\big{)}=\frac{\sqrt{ \pi}\Gamma(m-1)}{2^{m-2}}.\]

(iii): If \((X^{\prime},\,Y^{\prime})=(X^{\prime}_{1},\,Y^{\prime}_{1};\,\dots;\,X^{\prime} _{m},\,Y^{\prime}_{m})\) is a sample from a bivariate normal distribution with \(\xi=\eta=0\), then \(T=(\sum X^{\prime}_{i}{}^{2},\,\sum X^{\prime}_{i}Y^{\prime}_{i},\,\sum Y^{ \prime}_{i}{}^{2})\) is sufficient for \(\theta(\sigma,\,\rho,\,\tau)\), and the density of \(T\) is obtained from that given in part (ii) for \(\theta_{0}=(1,\,0,\,1)\) through the identity (Problem 3.41 (i))

\[p^{T}_{\theta}(t)=p^{T}_{\theta_{0}}(t)\frac{p^{X^{\prime},\,Y^{\prime}}_{\theta }(x^{\prime},\,y^{\prime})}{p^{X^{\prime},\,Y^{\prime}}_{\theta_{0}}(x^{\prime },\,y^{\prime})}.\]

The result now follows from part (i) with \(m=n-1\).]

**Problem 5.67**: If \((X_{1},\,Y_{1}),\,\dots,\,(X_{n},\,Y_{n})\) is a sample from a bivariate normal distribution, the probability density of the sample correlation coefficient \(R\) is16

Footnote 16: The distribution of \(R\) is reviewed by Johnson and Kotz (1970, Vol. 2, Section 32) and Patel and Read (1982).

\[p_{\rho}(r) = \frac{2^{n-3}}{\pi(n-3)!}(1-\rho^{2})^{\frac{1}{2}(n-1)}(1-r^{2}) ^{\frac{1}{2}(n-4)}\] \[\times\sum_{k=0}^{\infty}\Gamma^{2}\big{[}\tfrac{1}{2}(n+k-1) \big{]}\frac{(2\rho r)^{k}}{k!}\]

or alternatively

\[p_{\rho}(r) = \frac{n-2}{\pi}(1-\rho^{2})^{\frac{1}{2}(n-1)}(1-r^{2})^{\frac{1} {2}(n-4)}\] \[\times\int_{0}^{1}\frac{t^{n-2}}{(1-\rho rt)^{n-1}}\frac{1}{\sqrt{ 1-t^{2}}}\,dt.\]

Another form is obtained by making the transformation \(t=(1-v)/(1-\rho rv)\) in the integral on the right-hand side of (5.66). The integral then becomes

\[\frac{1}{(1-\rho r)^{\frac{1}{2}(2n-3)}}\int_{0}^{1}\frac{(1-v)^{n-2}}{\sqrt{ 2v}}\big{[}1-\tfrac{1}{2}v(1+\rho r)\big{]}^{-1/2}\,dv. \tag{5.87}\]

Expanding the last factor in powers of \(v\), the density becomes

\[\frac{n-2}{\sqrt{2\pi}}\frac{\Gamma(n-1)}{\Gamma(n-\tfrac{1}{2})} (1-\rho^{2})^{\frac{1}{2}(n-1)}(1-r^{2})^{\frac{1}{2}(n-4)}(1-\rho r)^{-n+ \frac{3}{2}}\] \[\times F\left(\tfrac{1}{2};\,\tfrac{1}{2};\,n-\tfrac{1}{2};\, \frac{1+\rho r}{2}\right),\]where

\[F(a,b,c,x)=\sum_{j=0}^{\infty}\frac{\Gamma(a+j)}{\Gamma(a)}\frac{\Gamma(b+j)}{ \Gamma(b)}\frac{\Gamma(c)}{\Gamma(c+j)}\frac{x^{j}}{j!} \tag{5.89}\]

is a hypergeometric function.

[To obtain the first expression make a transformation from \((S_{1}^{2},S_{2}^{2},S_{12})\) with density (5.84) to \((S_{1}^{2},S_{2}^{2},R)\) and expand the factor \(\exp\{\rho s_{12}/(1-\rho^{2})\sigma\tau\}=\exp\{\rho rs_{1}s_{2}/(1-\rho^{2} )\sigma\tau\}\) into a power series. The resulting series can be integrated term by term with respect to \(s_{1}^{2}\) and \(s_{2}^{2}\). The equivalence with the second expression is seen by expanding the factor \((1-\rho rt)^{-(n-1)}\) under the integral in (5.86) and integrating term by term.]

**Problem 5.68**: If \(X\) and \(Y\) have a bivariate normal distribution with correlation coefficient \(\rho>0\), they are positively regression dependent.

[The conditional distribution of \(Y\) given \(x\) is normal with mean \(\eta+\rho\tau\sigma^{-1}(x-\xi)\) and variance \(\tau^{2}(l-\rho^{2})\). Through addition to such a variable of the positive quantity \(\rho\tau\sigma^{-1}(x^{\prime}-x)\) it is transformed into one with the conditional distribution of \(Y\) given \(x^{\prime}>x\).]

**Problem 5.69**: (i) The functions (5.78) are bivariate cumulative distributions functions.

(ii) A pair of random variables with distribution (5.78) is positively regression dependent. [The distributions (5.78) were introduced by Morgenstern (1956).]

**Problem 5.70**: If \(X\), \(Y\) are positively regression dependent, they are positively quadrant dependent.

[Positive regression dependence implies that

\[P[Y\leq y\mid X\leq x]\geq P[Y\leq y\mid X\leq x^{\prime}]\ \ \ \mbox{for all}\ \ \ x<x^{\prime}\ \mbox{and}\ y, \tag{5.90}\]

and (5.90) implies positive quadrant dependence.]

### 5.15 Notes

The optimal properties of the one- and two-sample normal-theory tests were obtained by Neyman and Pearson (1933) as some of the principal applications of their general theory. Theorem 5.1.2 is due to Basu (1955), and its uses are reviewed in Boos and Hughes-Oliver (1998). For converse aspects of this theorem see Basu (1958), Koehn and Thomas (1975), Bahadur (1979), Lehmann (1980), and Basu (1982). An interesting application is discussed in Boos and Hughes-Oliver (1998). In some exponential family regression models where UMPU tests do not exist, classes of admissible, unbiased tests are obtained in Cohen et al. (1994).

The roots of the randomization model of Section 5.10 can be traced to Neyman (1923); see Speed (1990) and Fienberg and Tanur (1996). Permutation tests, as alternatives to the standard tests having fixed critical levels, were initiated by Fisher(1935a) and further developed, among others, by Pitman (1937,1938a), Lehmann (1949), Hoeffding (1952), and Box and Andersen (1955). Some aspects of these tests are reviewed in Bell and Sen (1984) and Good (1994). Applications to various experimental designs are given in Welch (1990). Optimality of permutation tests in a multivariate nonparametric two-sample setting is studied in Runger and Eaton (1992). Explicit confidence intervals based on subsampling were given by Hartigan (1969). The theory of unbiased confidence sets and its relation to that of unbiased tests is due to Neyman (1937a).

## Chapter 6 Invariance

### 6.1 Symmetry and Invariance

Many statistical problems exhibit symmetries, which provide natural restrictions to impose on the statistical procedures that are to be employed. Suppose, for example, that \(X_{1},\,\ldots,\,X_{n}\) are independently distributed with probability densities \(p_{\theta_{1}}(x_{1}),\,\ldots,\,p_{\theta_{n}}(x_{n})\). For testing the hypothesis \(H:\theta_{1}=\cdots=\theta_{n}\) against the alternative that the \(\theta\)'s are not all equal, the test should be symmetric in \(x_{1},\,\ldots,\,x_{n}\), since otherwise the acceptance or rejection of the hypothesis would depend on the (presumably quite irrelevant) numbering of these variables.

As another example consider a circular target with center \(O\), on which are marked the impacts of a number of shots. Suppose that the points of impact are independent observations on a bivariate normal distribution centered on \(O\). In testing this distribution for circular symmetry with respect to \(O\), it seems reasonable to require that the test itself exhibit such symmetry. For if it lacks this feature, a two-dimensional (for example, Cartesian) coordinate system is required to describe the test, and acceptance or rejection will depend on the choice of this system, which under the assumptions made is quite arbitrary and has no bearing on the problem.

The mathematical expression of symmetry is invariance under a suitable group of transformations. In the first of the two examples above the group is that of all permutations of the variables \(x_{1},\,\ldots,\,x_{n}\) since a function of \(n\) variables is symmetric if and only if it remains invariant under all permutations of these variables. In the second example, circular symmetry with respect to the center \(O\) is equivalent to invariance under all rotations about \(O\). A third example is the following.

**Example 6.1.1**: **(Testing a Fair Coin)** Suppose \(X\) is the number of successes in \(n\) i.i.d. Bernoulli trials, each with success probability \(p\). The problem is to test \(H:p=1/2\) against \(K:\theta\neq 1/2\). Erich is given the data \(X\) and he seeks a test function \(\phi=\phi(X)\) to test \(H\). Meanwhile, Julie is given the number of failures \(X^{\prime}=n-X\). Then, \(X^{\prime}\) is also binomial with parameters \(n\) and \(\theta^{\prime}=1-\theta\). From Julie's point of view, she faces the identical problem Erich faces, testing that a binomial distribution with \(n\) trials has success probability \(1/2\) versus not \(1/2\). It would be inconsistentfor Erich and Julie to reach different conclusions, and thus invariance considerations would then require that

\[\phi(X)=\phi(n-X). \tag{6.1}\]

Tests satisfying (6.1) are said to be invariant, and such tests represent a restriction on the class of tests. The goal is to find a UMP level \(\alpha\) test among such invariant tests.

In general, let \(X\) be distributed according to a probability distribution \(P_{\theta},\,\theta\in\Omega\), and let \(g\) be a transformation of the sample space \(\mathcal{X}\). All such transformations considered in connection with invariance will be assumed to be \(1:1\) transformations of \(\mathcal{X}\) onto itself. Denote by \(gX\) the random variable that takes on the value \(gx\) when \(X=x\), and suppose that when the distribution of \(X\) is \(P_{\theta},\,\theta\in\Omega\), the distribution of \(gX\) is \(P_{\theta^{\prime}}\) with \(\theta^{\prime}\) also in \(\Omega\). The element \(\theta^{\prime}\) of \(\Omega\) which is associated with \(\theta\) in this manner will be denoted by \(\bar{g}\theta\), so that

\[P_{\theta}\{gX\in A\}=P_{\bar{g}\theta}\{X\in A\}. \tag{6.2}\]

Here the subscript \(\theta\) on the left member indicates the distribution of \(X\), not that of \(gX\). Equation (6.2) can also be written as \(P_{\theta}(g^{-1}A)=P_{\bar{g}\theta}(A)\) and hence as

\[P_{\bar{g}\theta}(gA)=P_{\theta}(A). \tag{6.3}\]

The parameter set \(\Omega\) remains invariant under \(g\) (or is preserved by \(g\)) if \(\bar{g}\theta\in\Omega\) for all \(\theta\in\Omega\), and if in addition for any \(\theta^{\prime}\in\Omega\) there exists \(\theta\in\Omega\) such that \(\bar{g}\theta=\theta^{\prime}\). These two conditions can be expressed by the equation

\[\bar{g}\Omega=\Omega. \tag{6.4}\]

The transformation \(\bar{g}\) of \(\Omega\) onto itself defined in this way is \(1:1\) provided the distributions \(P_{\theta}\) corresponding to different values of \(\theta\) are distinct. To see this let \(\bar{g}\theta_{1}=\bar{g}\theta_{2}\). Then \(P_{\bar{g}\theta_{1}}(gA)=P_{\bar{g}\theta_{2}}(gA)\) and therefore \(P_{\theta_{1}}(A)=P_{\theta_{2}}(A)\) for all \(A\), so that \(\theta_{1}=\theta_{2}\).

**Lemma 6.1.1**: _Let \(g,\,g^{\prime}\) be two transformations preserving \(\Omega\). Then the transformations \(g^{\prime}g\) and \(g^{-1}\) defined by_

\[(g^{\prime}g)x=g^{\prime}(gx)\ \ \mbox{and}\ \ \ g(g^{-1}x)=x\ \ \mbox{for all}\ \ \ x\in\mathcal{X}\]

_also preserve \(\Omega\) and satisfy_

\[\overline{g^{\prime}g}=\overline{g^{\prime}}\cdot\bar{g}\ \ \ \ \ \mbox{and}\ \ \ \ \ (\overline{g^{-1}})=(\bar{g})^{-1}. \tag{6.5}\]

Proof. If the distribution of \(X\) is \(P_{\theta}\) then that of \(gX\) is \(P_{\bar{g}\theta}\) and that of \(g^{\prime}gX=g^{\prime}(gX)\) is therefore \(P_{\bar{g}^{\prime}\bar{g}\theta}\). This establishes the first equation of (6.5); the proof of the second one is analogous.

We shall say that _the problem of testing_\(H:\theta\in\Omega_{H}\) against \(K:\theta\in\Omega_{K}\)_remains invariant_ under a transformation \(g\) if \(\bar{g}\) preserves both \(\Omega_{H}\) and \(\Omega_{K}\), so that the equation

\[\bar{g}\Omega_{H}=\Omega_{H} \tag{6.6}\]

holds in addition to (6.4). Let \(\mathcal{C}\) be a class of transformations satisfying these two conditions, and let \(G\) be the smallest class of transformations containing \(\mathcal{C}\) such that \(g\), \(g^{\prime}\in G\) implies that \(g^{\prime}g\) and \(g^{-1}\) belong to \(G\). Then \(G\) is a group of transformations, all of which by Lemma 6.1.1 preserve both \(\Omega\) and \(\Omega_{H}\). Any class \(\mathcal{C}\) of transformations leaving the problem invariant can therefore be extended to a group \(G\). It follows further from Lemma 6.1.1 that the class of induced transformations \(\bar{g}\) form a group \(\bar{G}\). The two equations (6.5) express the fact that \(\bar{G}\) is a homomorphism of \(G\).

In the presence of symmetries in both the sample and parameter spaces represented by the groups \(G\) and \(\bar{G}\), it is natural to restrict attention to tests \(\phi\) which are also symmetric, that is, which satisfy

\[\phi(gx)=\phi(x)\qquad\text{for all}\ \ \ x\in X\ \ \ \text{and}\ g\in G. \tag{6.7}\]

A test \(\phi\) satisfying (6.7) is said to be _invariant under_\(G\). The restriction to invariant tests is a particular case of the principle of invariance formulated in Section 1.5. As was indicated there and in the examples above, a transformation \(g\) can be interpreted as a change of coordinates. From this point of view, a test is invariant if it is independent of the particular coordinate system in which the data are expressed.1

Footnote 1: The relationship between this concept of invariance under reparametrization and that considered in differential geometry is discussed in Barndorff–Nielson, Cox and Reid (1986).

A transformation \(g\), in order to leave a problem invariant, must in particular preserve the class \(\mathcal{A}\) of measurable sets over which the distributions \(P_{\theta}\) are defined. This means that any set \(A\in\mathcal{A}\) is transformed into a set of \(\mathcal{A}\) and is the image of such a set, so that \(gA\) and \(g^{-1}A\) both belong to \(\mathcal{A}\). Any transformation satisfying this condition is said to be _bimeasurable_. Since a group with each element \(g\) also contains \(g^{-1}\) its elements are automatically bimeasurable if all of them are measurable. If \(g^{\prime}\) and \(g\) are bimeasurable, so are \(g^{\prime}g\) and \(g^{-1}\). The transformations of the group \(G\) above generated by a class \(\mathcal{C}\) are therefore all bimeasurable provided this is the case for the transformations of \(\mathcal{C}\).

### Maximal Invariants

If a problem is invariant under a group of transformations, the _principle of invariance_ restricts attention to invariant tests. In order to obtain the best of these, it is convenient first to characterize the totality of invariant tests.

Let two points \(x_{1}\), \(x_{2}\) be considered equivalent under \(G\),

\[x_{1}\sim x_{2}(\bmod G),\]

if there exists a transformation \(g\in G\) for which \(x_{2}=gx_{1}\). This is a true equivalence relation, since \(G\) is a group and the sets of equivalent points, the _orbits_ of \(G\), therefore constitute a partition of the sample space. (Cf. Appendix, Section A.1.) A point \(x\) traces out an orbit as all transformations \(g\) of \(G\) are applied to it; this means that the orbit containing \(x\) consists of the totality of points \(gx\) with \(g\in G\). It follows from the definition of invariance that a function is invariant if and only if it is constant on each orbit.

A function \(M\) is said to be _maximal invariant_ if it is invariant and if

\[M(x_{1})=M(x_{2})\ \ \ \mbox{implies}\ \ \ x_{2}=gx_{1}\ \ \ \mbox{for some $g\in G$}, \tag{6.8}\]

that is, if it is constant on the orbits but for each orbit takes on a different value. All maximal invariants are equivalent in the sense that their sets of constancy coincide.

**Theorem 6.2.1**: _Let \(M(x)\) be a maximal invariant with respect to \(G\). Then, a necessary and sufficient condition for \(\phi\) to be invariant is that it depends on \(x\) only through \(M(x)\); that is, that there exists a function \(h\) for which \(\phi(x)=h[M(x)]\) for all \(x\)._

Proof. If \(\phi(x)=h[M(x)]\) for all \(x\), then \(\phi(gx)=h[M(gx)]=h[M(x)]=\phi(x)\) so that \(\phi\) is invariant. On the other hand, if \(\phi\) is invariant and if \(M(x_{1})=M(x_{2})\), then \(x_{2}=gx_{1}\) for some \(g\) and therefore \(\phi(x_{2})=\phi(x_{1})\).

**Example 6.2.1**:
1. Let \(x=(x_{1},\ldots,x_{n})\), and let \(G\) be the group of translations \[gx=(x_{1}+c,\ldots,x_{n}+c),\ \ \ \ \ -\infty<c<\infty.\]

Then the set of differences \(y=(x_{1}-x_{n},\ldots,x_{n-1}-x_{n})\) is invariant under \(G\). To see that it is maximal invariant suppose that \(x_{i}-x_{n}=x^{\prime}_{i}-x^{\prime}_{n}\) for \(i=1,\ldots,n-1\). Putting \(x^{\prime}_{n}-x_{n}=c\), one has \(x^{\prime}_{i}=x_{i}+c\) for all \(i\), as was to be shown. The function \(y\) is of course only one representation of the maximal invariant. Others are for example \((x_{1}-x_{2},x_{2}-x_{3},\ldots,x_{n-1}-x_{n})\) or the redundant \((x_{1}-\bar{x},\ldots,x_{n}-\bar{x})\). In the particular case that \(n=1\), there are no invariants. The whole space is a single orbit, so that for any two points there exists a transformation of \(G\) taking one into the other. In such a case the transformation group \(G\) is said to be _transitive_. The only invariant functions are then the constant functions \(\phi(x)\equiv c\).
2. if \(G\) is the group of transformations \[gx=(cx_{1},\ldots,cx_{n}),\ \ \ \ \ c\neq 0,\]

a special role is played by any zero coordinates. However, in statistical applications the set of points for which none of the coordinates is zero typically has probability 1; attention can then be restricted to this part of the sample space, and the set of 

[MISSING_PAGE_FAIL:258]

Since \((x_{i}^{\top}x_{i})^{-1}\) is positive definite, there exist nonsingular matrices \(C_{i}\) such that \((x_{i}^{\top}x_{i})^{-1}=C_{i}C_{i}^{\top}\) and hence

\[(x_{1}C_{1})(x_{1}C_{1})^{\top}=(x_{2}C_{2})(x_{2}C_{2})^{\top}.\]

This implies the existence of an orthogonal matrix \(Q\) such that \(x_{2}C_{2}=x_{1}C_{1}Q\) and thus \(x_{2}=x_{1}B\) with \(B=C_{1}QC_{2}^{-1}\), as was to be shown.

In the special case \(s=n\), we have \(t(x)=I\), so that there are no nontrivial invariants. This corresponds to the fact that in this case \(G\) is transitive, since any two nonsingular \(n\times n\) matrices \(x_{1}\) and \(x_{2}\) satisfy \(x_{2}=x_{1}B\) with \(B=x_{1}^{-1}x_{2}\). This result can be made more intuitive through a geometric interpretation. Consider the \(s\)-dimensional subspace \(S\) of \(R^{n}\) spanned by the \(s\) columns of \(x\). Then \(P=x(x^{\top}x)^{-1}x^{\top}\) has the property that for any \(y\) in \(R^{n}\), the vector \(Py\) is the projection of \(y\) onto \(S\). (This will be proved in Section 7.2.) The invariance of \(P\) expresses the fact that the projection of \(y\) onto \(S\) is independent of the choice of vectors spanning S. To see that it is maximal invariant, suppose that the projection of every \(y\) onto the spaces \(S_{1}\) and \(S_{2}\) spanned by two different sets of \(s\) vectors is the same. Then \(S_{1}=S_{2}\), so that the two sets of vectors span the same space. There then exists a nonsingular transformation taking one of these sets into the other.

A somewhat more systematic way of determining maximal invariants is obtained by selecting, by means of a specified rule, a unique point \(M(x)\) on each orbit. Then clearly \(M(X)\) is maximal invariant. To illustrate this method, consider once more two of the earlier examples.

**Example 6.2.1** (i) (continued).: The orbit containing the point \((a_{1},\ldots,a_{n})\) under the group of translations is the set {(\(a_{1}+c\),..., \(a_{n}+c\)), \(-\infty<c<\infty\)}, which is a line in \(E_{n}\).

1. As representative point \(M(x)\) on this line, take its intersection with the hyperplane \(x_{n}=0\). Since then \(a_{n}+c=0\), this point corresponds to the value \(c=-a_{n}\) and thus has coordinates \((a_{1}-a_{n}\),..., \(a_{n-1}-a_{n}\), 0). This leads to the maximal invariant \((x_{1}-x_{n}\),..., \(x_{n-1}-x_{n})\).
2. An alternative point on the line is its intersection with the hyperplane \(\sum x_{i}=0\). Then \(c=-\bar{a}\), and \(M(a)=(a_{1}-\bar{a}\),..., \(a_{n}-\bar{a})\).
3. The point need not be specified by an intersection property. It can, for instance, be taken as the point on the line that is closest to the origin. Since the value of \(c\) minimizing \(\sum(a_{i}+c)^{2}\) is \(c=-\bar{a}\), this leads to the same point as (b).

**Example 6.2.1**(iii) (continued).: The orbit containing the point \((a_{1}\),..., \(a_{n})\) under the group of orthogonal transformations is the hypersphere containing \((a_{1}\),..., \(a_{n})\) and with center at the origin. As representative point on this sphere, take its north pole, i.e., the point with \(a_{1}=\cdots=a_{n-1}=0\). The coordinates of this point are \((0\),..., \(0\), \(\sqrt{\sum a_{i}^{2}})\) and hence lead to the maximal invariant \(\sum x_{i}^{2}\). (Note that in this example, the determination of the orbit is essentially equivalent to the determination of the maximal invariant.)
Frequently, it is convenient to obtain a maximal invariant in a number of steps, each corresponding to a subgroup of \(G\). To illustrate the process and a difficulty that may arise in its application, let \(x=(x_{1},\ldots,x_{n})\), suppose that the coordinates are distinct, and consider the group of transformations

\[gx=(ax_{1}+b,\ldots,ax_{n}+b),\ \ \ \ \ a\neq 0,\ \ \ \ \ -\infty<b<\infty.\]

Applying first the subgroup of translations \(x_{i}^{\prime}=x_{i}+b\), a maximal invariant is \(y=(y_{1},\ldots,y_{n-1})\) with \(y_{i}=x_{i}-x_{n}\). Another subgroup consists of the scale changes \(x_{i}^{\prime\prime}=ax_{i}\). This induces a corresponding change of scale in the \(y\)'s: \(y_{i}^{\prime\prime}=ay_{i}\), and a maximal invariant with respect to this group acting on the \(y\)-space is \(z=(z_{1},\ldots,z_{n-2})\) with \(z_{i}=y_{i}/y_{n-1}\). Expressing this in terms of the \(x\)'s, we get \(z_{i}=(x_{i}-x_{n})/(x_{n-1}-x_{n})\), which is maximal invariant with respect to \(G\).

Suppose now the process is carried out in the reverse order. Application first of the subgroup \(x_{i}^{\prime\prime}=ax_{i}\) yields as maximal invariant \(u=(u_{1},\ldots,u_{n-1})\) with \(u_{i}=x_{i}/x_{n}\). However, the translations \(x_{i}^{\prime}=x_{i}+b\) do not induce transformations in \(u\)-space, since \((x_{i}+b)/(x_{n}+b)\) is not a function of \(x_{i}/x_{n}\).

Quite generally, let a transformation group \(G\) be _generated_ by two subgroups \(D\) and \(E\) in the sense that it is the smallest group containing \(D\) and \(E\). Then \(G\) consists of the totality of products \(e_{m}d_{m}\ldots e_{1}d_{1}\) for \(m=1,2,\ldots,\) with \(d_{i}\in D\), \(e_{i}\in E\) (\(i=1,\ldots,m\)).2 The following theorem shows that whenever the process of determining a maximal invariant in steps can be carried out at all, it leads to a maximal invariant with respect to \(G\).

Footnote 2: See Section A.1 of the Appendix.

**Theorem 6.2.2**: _Let \(G\) be a group of transformations, and let \(D\) and \(E\) be two subgroups generating \(G\). Suppose that \(y=s(x)\) is maximal invariant with respect to \(D\), and that for any \(e\in E\)_

\[s(x_{1})=s(x_{2})\ \ \ \ \mbox{ implies }\ \ \ \ s(ex_{1})=s(ex_{2}). \tag{6.9}\]

_If \(z=t(y)\) is maximal invariant under the group \(E^{*}\) of transformations \(e^{*}\) defined by_

\[e^{*}y=s(ex)\ \ \ \ \mbox{ when }\ \ \ \ y=s(x),\]

_then \(z=t[s(x)]\) is maximal invariant with respect to \(G\)._

Proof. To show that \(t[s(x)]\) is invariant, let \(x^{\prime}=gx\), \(g=e_{m}d_{m}\cdots e_{1}d_{1}\). Then

\[t[s(x^{\prime})]=t[s(e_{m}d_{m}\cdots e_{1}d_{1}x)] = t[e_{m}^{*}s(d_{m}\cdots e_{1}d_{1}x)]\] \[= t[s(e_{m-1}d_{m-1}\cdots e_{1}d_{1}x)],\]

and the last expression can be reduced by induction to \(t[s(x)]\). To see that \(t[s(x)]\) is in fact maximal invariant, suppose that \(t[s(x^{\prime})]=t[s(x)]\). Setting \(y^{\prime}=s(x^{\prime})\), \(y=s(x)\)one has \(t(y^{\prime})=t(y)\), and since \(t(y)\) is maximal invariant with respect to \(E^{*}\), there exists \(e^{*}\) such that \(y^{\prime}=e^{*}y\). Then \(s(x^{\prime})=e^{*}s(x)=s(ex)\), and by the maximal invariance of \(s(x)\) with respect to \(D\) there exists \(d\in D\) such that \(x^{\prime}=dex\). Since \(de\) is an element of \(G\) this completes the proof.

Techniques for obtaining the distribution of maximal invariants are discussed by Andersson (1982), Eaton (1983, 1989), Farrell (1985b), Wijsman (1990) and Anderson (2003).

### Uniformly Most Powerful Invariant Tests

In the presence of symmetries, one may wish to restrict attention to invariant tests, and it then becomes of interest to determine the uniformly most powerful invariant (UMPI) test. The following is a simple example.

**Example 6.3.1** Let \(X_{1}\),..., \(X_{n}\) be i.i.d. on \((0,\,1)\) and consider testing the hypothesis \(H_{0}\) that the common distribution of the \(X\)'s is uniform on \((0,\,1)\) against the two alternatives \(H_{1}\):

\[p_{1}(x_{1},\,\ldots,\,x_{n})=f(x_{1})\cdots f(x_{n})\]

and

\[p_{2}(x_{1},\,\ldots,\,x_{n})=f(1-x_{1})\cdots f(1-x_{n})\,\]

where \(f\) is a fixed (known) density.

(i) This problem remains invariant under the 2 element group \(G\) consisting of the transformations

\[g:\ x_{i}^{\prime}=1-x_{i}\,\quad i=1,\,\ldots,\,n\]

and the identity transformation \(x_{i}^{\prime}=x_{i}\) for \(i=1,\,\ldots,\,n\).

(ii) The induced transformation \(\bar{g}\) in the space of alternatives takes \(p_{1}\) into \(p_{2}\) and \(p_{2}\) into \(p_{1}\).

(iii) A test \(\phi(x_{1},\,\ldots,\,x_{n})\) remains invariant under \(G\) if and only if

\[\phi(x_{1},\,\ldots,\,x_{n})=\phi(1-x_{1},\,\ldots,\,1-x_{n})\.\]

(iv) There exists a UMP invariant test (i.e., an invariant test which is simultaneously most powerful against both \(p_{1}\) and \(p_{2}\)), and it rejects \(H_{0}\) when the average

\[\bar{p}(x_{1},\,\ldots,\,x_{n})=\frac{1}{2}\left[p_{1}(x_{1},\,\ldots,\,x_{n} )+\,p_{2}(x_{1},\,\ldots,\,x_{n})\right]\]

is sufficiently large.

We leave the proof of (i)-(iii) to Problem 6.5. To prove (iv), note that any invariant test satisfies\[E_{p_{1}}[\phi(X_{1},\ldots,X_{n})]=E_{p_{2}}[\phi(X_{1},\ldots,X_{n})]=E_{\tilde{p }}[\phi(X_{1},\ldots,X_{n})]\.\]

Therefore, maximizing the power against \(p_{1}\) or \(p_{2}\) is equivalent to maximizing the power under \(\tilde{p}\), and the result follows from the Neyman-Pearson Lemma.

This example is a special case of the following result.

**Theorem 6.3.1**: _Suppose the problem of testing \(\Omega_{0}\) against \(\Omega_{1}\) remains invariant under a finite group \(G=\{g_{1},\ldots,g_{N}\}\) and that \(\tilde{G}\) is transitive over \(\Omega_{0}\) and over \(\Omega_{1}\). Then there exists a UMP invariant test of \(\Omega_{0}\) against \(\Omega_{1}\), and it rejects \(\Omega_{0}\) when_

\[\frac{\sum_{i=1}^{N}p_{\tilde{g}_{i}\theta_{1}}(x)/N}{\sum_{i=1}^{N}p_{\tilde {g}_{i}\theta_{0}}(x)/N} \tag{6.10}\]

_is sufficiently large, where \(\theta_{0}\) and \(\theta_{1}\) are any elements of \(\Omega_{0}\) and \(\Omega_{1}\), respectively._

The proof is exactly analogous to that of the preceding example; see Problem 6.6.

The results of the previous section provide an alternative approach to the determination of most powerful invariant tests. By Theorem 6.2.1, the class of all invariant functions can be obtained as the totality of functions of a maximal invariant \(M(x)\). Therefore, in particular the class of all invariant tests is the totality of tests depending only on the maximal invariant statistic \(M\). The latter statement, while correct for all the usual situations, actually requires certain qualifications regarding the class of measurable sets in \(M\)-space. These conditions will be discussed at the end of the section; they are satisfied in the examples below.

**Example 6.3.2**: Let \(X=(X_{1},\ldots,X_{n})\), and suppose that the density of \(X\) is \(f_{i}(x_{1}-\theta,\ldots,x_{n}-\theta)\) under \(H_{i}\) (\(i=0,1\)), where \(\theta\) ranges from \(-\infty\) to \(\infty\). The problem of testing \(H_{0}\) against \(H_{1}\) is invariant under the group \(G\) of transformations

\[gx=(x_{1}+c,\ldots,x_{n}+c),\ \ \ \ \ -\infty<c<\infty\]

which in the parameter space induces the transformations

\[\tilde{g}\theta=\theta+c.\]

By Example 6.2.1, a maximal invariant under \(G\) is \(Y=(X_{1}-X_{n},\ldots,X_{n-1}-X_{n})\). The distribution of \(Y\) is independent of \(\theta\) and under \(H_{i}\) has the density

\[\int_{-\infty}^{\infty}f_{i}(y_{1}+z,\ldots,y_{n-1}+z,z)\,dz.\]

When referred to \(Y\), the problem of testing \(H_{0}\) against \(H_{1}\) therefore becomes one of testing a simple hypothesis against a simple alternative. The most powerful test is then independent of \(\theta\), and therefore UMP among all invariant tests. Its rejection region by the Neyman-Pearson Lemma is \[\begin{array}{l}\frac{\int_{-\infty}^{\infty}f_{1}(y_{1}+z,\ldots,y_{n-1}+z,z) \,dz}{\int_{-\infty}^{\infty}f_{0}(y_{1}+z,\ldots,y_{n-1}+z,z)\,dz}=\\ \frac{\int_{-\infty}^{\infty}f_{1}(x_{1}+u,\ldots,x_{n}+u)\,du}{\int_{-\infty}^ {\infty}f_{0}(x_{1}+u,\ldots,x_{n}+u)\,du}>C.\end{array}\]

A general theory of _separate families of hypotheses_ (in which the family \(K\) of alternatives does not adjoin the hypothesis \(H\) but, as above, is separated from it) was initiated by Cox (1961, 1962). A bibliography of the subject is given in Pereira (1977); see also Loh (1985), Pace and Salvan (1990) and Rukhin (1993).

Example 6.3.2 illustrates the fact, also utilized in Theorem 6.3.1, that if the group \(\bar{G}\) is transitive over both \(\Omega_{0}\) and \(\Omega_{1}\), then the problem reduces to one of testing a simple hypothesis against a simple alternative, and a UMP invariant test is then obtained by the Neyman-Pearson Lemma. Note also the close similarity between Theorem 6.3.1 and Example 6.3.2 shown by a comparison of (6.10) and the right side of (6.11), where the summation in (6.10) is replaced by integration with respect to Lebesgue measure.

In \(\bar{G}\) is not transitive, the existence of a UMPI test is not guaranteed. The problem then is to determine whether or not there exists a UMP test based on a maximal invariant \(T\). If the family of distributions of \(T\) has monotone likelihood ratio, then a UMPI test may exist, as illustrated in the following example.

**Example 6.3.3 Testing many normal means)** Assume \(X_{1},\ldots,X_{n}\) are independent with \(X_{i}\sim N(\xi_{i},\sigma^{2})\), where we assume \(\sigma\) is known and equal to 1. The parameter space \(\Omega\) is \(n\)-dimensional Euclidean space as \(\xi_{1},\ldots,\xi_{n}\) vary freely. The problem is to test

\[H_{0}:\ \xi_{1}=\xi_{2}=\cdots=\xi_{n}=0\]

against the alternative where not all \(\xi_{i}\) are 0. Note there does not exist even a UMPU test unless \(n=1\). Let \(X=(X_{1},\ldots,X_{n})^{\top}\) (where the superscript \(T\) denotes transpose), and consider an orthogonal matrix \(O\). If \(X^{\prime}=OX\), then \(X^{\prime}\) consists of independent normals, each with variance one and possibly different means. Moreover, \(X^{\prime}\) has mean 0 iff \(X\) has mean 0. (Here the prime in \(X^{\prime}\) just denotes that it is a transformation of \(X\), so not \(X^{\top}\).) So, the problem of testing the mean vector is 0 based on \(X^{\prime}\) is identical to the problem based on \(X\). For this reason, invariance or symmetry requires restricting to tests \(\phi\) satisfying

\[\phi(X)=\phi(OX)\ \ \ \mbox{for all orthogonal matrices}\ O\.\]

If \(G\) is the group of orthogonal transformations, then from Example 6.2.1(iii), \(T=\sum_{i=1}^{n}X_{i}^{2}\) is a maximal invariant. The distribution of \(T\) is noncentral Chi-squared with \(n\) degrees of freedom and noncentrality parameter \(\psi^{2}=\sum_{i=1}^{n}\xi_{i}^{2}\). Its density function is 

[MISSING_PAGE_FAIL:264]

\[\frac{c(\Delta)z^{\frac{1}{2}(n-3)}}{\left(\Delta+\frac{n-1}{m-1}z\right)^{\frac{1 }{2}(m+n-2)}},\qquad z>0.\]

For varying \(\Delta\), these densities constitute a family with monotone likelihood ratio, so that among all tests of \(H\) based on \(Z\), and therefore among all invariant tests, there exists a UMP one given by the rejection region \(Z>C\). This coincides with the UMP unbiased test (5.20).

**Example 6.3.6** In the method of _paired comparisons_ for testing whether a treatment has a beneficial effect, the experimental material consists of \(n\) pairs of subjects. From each pair, a subject is selected at random for treatment while the other serves as control. Let \(X_{i}\) be 1 or 0 as for the \(i\)th pair the experiment turns out in favor of the treated subject or the control, and let \(p_{i}=P\{X_{i}=1\}\). The hypothesis of no effect, \(H:p_{i}=\frac{1}{2}\) for \(i=1,\ldots,n\), is to be tested against the alternatives that \(p_{i}>\frac{1}{2}\) for all \(i\).

The problem remains invariant under all permutations of the \(n\) variables \(X_{1},\ldots,X_{n}\), and a maximal invariant under this group is the total number of successes \(X=X_{1}+\cdots+X_{n}\). The distribution of \(X\) is

\[P\{X=k\}=q_{1}\cdots q_{n}\sum\frac{p_{i_{1}}}{q_{i_{1}}}\cdots\frac{p_{i_{k}} }{q_{i_{k}}},\]

where \(q_{i}=1-p_{i}\) and where the summation extends over all \({n\choose k}\) choices of subscripts \(i_{1}<\cdots<i_{k}\). The most powerful invariant test against an alternative (\(p^{\prime}_{1},\ldots,p^{\prime}_{n}\)) rejects \(H\) when

\[f(k)=\frac{1}{{n\choose k}}\sum\frac{p^{\prime}_{i_{1}}}{q^{\prime}_{i_{1}}} \cdots\frac{p^{\prime}_{i_{k}}}{q^{\prime}_{i_{k}}}>C.\]

To see that \(f\) is an increasing function of \(k\), note that \(a_{i}=p^{\prime}_{i}/q^{\prime}_{i}>1\), and that

\[\sum_{j}\sum a_{j}a_{i_{1}}\cdots a_{i_{k}}=(k+1)\sum a_{i_{1}}\cdots a_{i_{k +1}}\]

and

\[\sum_{j}\sum a_{i_{1}}\cdots a_{i_{k}}=(n-k)\sum a_{i_{1}}\cdots a_{i_{k}}.\]

Here, in both equations, the second summation on the left-hand side extends over all subscripts \(i_{1}<\cdots<i_{k}\) of which none is equal to \(j\), and the summation on the right-hand side extends over all subscripts \(i_{1}<\cdots<i_{k+1}\) and \(i_{1}<\cdots<i_{k}\) respectively without restriction. Then\[f(k+1)=\frac{1}{{n\choose k+1}}\sum a_{i_{1}}\cdots a_{i_{k+1}} = \frac{1}{{(n-k){n\choose k}}}\sum_{j}\sum a_{j}a_{i_{1}}\cdots a_{i_ {k}}\] \[> \frac{1}{{n\choose k}}\sum a_{i_{1}}\cdots a_{i_{k}}=f(k),\]

as was to be shown. Regardless of the alternative chosen, the test therefore rejects when \(X>C\), and hence is UMP invariant. If the \(i\)th comparison is considered plus or minus as \(X_{i}\) is 1 or 0, this is seen to be another example of the sign test. (Cf. Example 3.8.1 and Section 4.9.)

Sufficient statistics provide a simplification of a problem by reducing the sample space; this process involves no change in the parameter space. Invariance, on the other hand, by reducing the data to a maximal invariant statistic \(M\), whose distribution may depend only on a function of the parameter, typically also shrinks the parameter space. The details are given in the following theorem.

**Theorem 6.3.2**: _If \(M(x)\) is invariant under \(G\), and if \(\upsilon(\theta)\) maximal invariant under the induced group \(\bar{G}\), then the distribution of \(M(X)\) depends only on \(\upsilon(\theta)\)._

Proof. Let \(\upsilon(\theta_{1})=\upsilon(\theta_{2})\). Then \(\theta_{2}=\bar{g}\theta_{1}\), and hence

\[P_{\theta_{2}}\{M(X)\in B\} = P_{\bar{g}\theta_{1}}\{M(X)\in B\}=P_{\theta_{1}}\{M(gX)\in B\}\] \[= P_{\theta_{1}}\{M(X)\in B\}.\]

This result can be paraphrased by saying that the principle of invariance identifies all parameter points that are equivalent with respect to \(\bar{G}\).

In applications, for instance, in Examples 6.3.4 and 6.3.5, the maximal invariants \(M(x)\) and \(\delta=\upsilon(\theta)\) under \(G\) and \(\bar{G}\) are frequently real-valued, and the family of probability densities \(p_{\delta}(m)\) of \(M\) has monotone likelihood ratio. For testing the hypothesis \(H:\delta\leq\delta_{0}\) there exists then a UMP test among those depending only on \(M\), and hence a UMP invariant test. Its rejection region is \(M\geq C\), where

\[\int_{C}^{\infty}P_{\delta_{0}}(m)\,dm=\alpha. \tag{6.13}\]

Consider this problem now as a two-decision problem with decisions \(d_{0}\) and \(d_{1}\) of accepting or rejecting \(H\), and a loss function \(L(\theta,d_{i})=L_{i}(\theta)\). Suppose that \(L_{i}(\theta)\) depends only on the parameter \(\delta\), \(L_{i}(\theta)=L_{i}^{\prime}(\delta)\) say, and satisfies

\[L_{1}^{\prime}(\delta)-L_{0}^{\prime}(\delta)\gtrless 0\quad\text{ as }\ \delta\lessgtr\delta_{0}. \tag{6.14}\]

It then follows from Theorem 3.4.2 that the family of rejection regions \(M\geq C(\alpha)\), as \(\alpha\) varies from 0 to 1, forms a complete family of decision procedures among those depending only on \(M\), and hence a complete family of invariant procedures.

As before, the choice of a particular significance level \(\alpha\) can be considered as a convenient way of specifying a test from this family.

At the beginning of the section it was stated that the class of invariant tests coincides with the class of tests based on a maximal invariant statistic \(M=M(X)\). However, a statistic is not completely specified by a function, but requires also specification of a class \(\mathcal{B}\) of measurable sets. If in the present case \(\mathcal{B}\) is the class of all sets \(B\) for which \(M^{-1}(B)\in\mathcal{A}\), the desired statement is correct. For let \(\phi(x)=\psi[M(x)]\) and \(\phi\) by \(\mathcal{A}\)-measurable, and let \(C\) be a Borel set on the line. Then \(\phi^{-1}(C)=M^{-1}[\psi^{-1}(C)]\in\mathcal{A}\) and hence \(\psi^{-1}(C)\in\mathcal{B}\), so that \(\psi\) is \(\mathcal{B}\)-measurable and \(\phi(x)=\psi[M(x)]\) is a test based on the statistic \(M\).

In most applications, \(M(x)\) is a measurable function taking on values in a Euclidean space and it is convenient to take \(\mathcal{B}\) as the class of Borel sets. If \(\phi(x)=\psi[M(x)]\) is then an arbitrary measurable function depending only on \(M(x)\), it is not clear that \(\psi(m)\) is necessarily \(\mathcal{B}\)-measurable. This measurability can be concluded if \(\mathcal{X}\) is also Euclidean with \(\mathcal{A}\) the class of Borel sets, and if the range of \(M\) is a Borel set. We shall prove it here only under the additional assumption (which in applications is usually obvious, and which will not be verified explicitly in each case) that there exists a vector-valued Borel-measurable function \(Y(x)\) such that \([M(x),\,Y(x)]\) maps \(\mathcal{X}\) onto a Borel subset of the product space \(\mathcal{M}\times\mathcal{Y}\), that this mapping is \(1:1\), and that the inverse mapping is also Borel-measurable. Given any measurable function \(\phi\) of \(x\), there exists then a measurable function \(\phi^{\prime}\) of \((m,\,y)\) such that \(\phi(x)\equiv\phi^{\prime}[M(x),\,Y(x)]\). If \(\phi\) depends only on \(M(x)\), then \(\phi^{\prime}\) depends only on \(m\), so that \(\phi^{\prime}(m,\,y)=\psi(m)\) say, and \(\psi\) is a measurable function of \(m\).3 In Example 6.2.1(i) for instance, where \(x\,=\,(x_{1},\ldots x_{n})\) and \(M(x)\,=\,(x_{1}-x_{n},\,\ldots,\,x_{n-1}-x_{n})\), the function \(Y(x)\) can be taken as \(Y(x)=x_{n}\).

Footnote 3: The last statement follows, for example, from Theorem 18.1 of Billingsley (1995).

### Sample Inspection by Variables

A sample is drawn from a lot of some manufactured product in order to decide whether the lot is of acceptable quality. In the simplest case, each sample item is classified directly as satisfactory or defective (_inspection by attributes_), and the decision is based on the total number of defectives. More generally, the quality of an item is characterized by a variable \(Y\) (_inspection by variables_), and an item is considered satisfactory if \(Y\) exceeds a given constant \(u\). The probability of a defective is then

\[p\,=\,P\{Y\,\leq\,u\}\]

and the problem becomes that of testing the hypothesis \(H:p\,\geq\,p_{0}\).

As was seen in Example 3.8.1, no use can be made of the actual value of \(Y\) unless something is known concerning the distribution of \(Y\). In the absence of such information, the decision will be based, as before, simply on the number of defectivesin the sample. We shall consider the problem now under the assumption that the measurements \(Y_{1}\),..., \(Y_{n}\) constitute a sample from \(N(\eta,\sigma^{2})\). Then

\[p=\int_{-\infty}^{u}\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{1}{2\sigma^{2}}( y-\eta)^{2}\right]dy=\Phi\left(\frac{u-\eta}{\sigma}\right),\]

where

\[\Phi(y)=\int_{-\infty}^{y}\frac{1}{\sqrt{2\pi}}\exp\left(-\tfrac{1}{2}t^{2} \right)\,dt\]

denotes the cumulative distribution function of a standard normal distribution, and the hypothesis \(H\) becomes \((u-\eta)/\sigma\geq\Phi^{-1}(p_{0})\). In terms of the variables \(X_{i}=Y_{i}-u\), which have mean \(\xi=\eta-u\) and variance \(\sigma^{2}\), this reduces to

\[H:\frac{\xi}{\sigma}\leq\theta_{0}\]

with \(\theta_{0}=-\Phi^{-1}(p_{0})\). This hypothesis, which was considered in Section 5.2, for \(\theta_{0}=0\), occurs also in other contexts. It is appropriate when one is interested in the mean \(\xi\) of a normal distribution, expressed in \(\sigma\) units rather than on a fixed scale.

For testing \(H\), attention can be restricted to the pair of variables \(\bar{X}\) and \(S=\sqrt{\sum(X_{i}-\bar{X})^{2}}\), since they form a set of sufficient statistics for \((\xi,\sigma)\), which satisfy the conditions of Theorem 6.5.3 of the next section. These variables are independent, the distribution of \(\bar{X}\) being \(N(\xi,\sigma^{2}/n)\) and that of \(S/\sigma\) being \(\chi_{n-1}\). Multiplication of \(\bar{X}\) and \(S\) by a common constant \(c>0\) transforms the parameters into \(\xi^{\prime}=c\xi\), \(\sigma^{\prime}=c\sigma\), so that \(\xi/\sigma\) and hence the problem of testing \(H\) remain invariant. A maximal invariant under these transformations is \(\bar{x}/s\) or

\[t=\frac{\sqrt{n}\bar{x}}{s/\sqrt{n-1}},\]

the distribution of which depends only on the maximal invariant in the parameter space \(\theta=\xi/\sigma\) (cf. Section 5.2). Thus, the invariant tests are those depending only on \(t\), and it remains to find the most powerful test of \(H:\theta\leq\theta_{0}\) within this class.

The probability density of \(t\) is (Problem 5.3)

\[p_{\delta}(t)=C\int_{0}^{\infty}\exp\left[-\frac{1}{2}\left(t\sqrt{\frac{w}{n -1}}-\delta\right)^{2}\right]w^{\frac{1}{2}(n-2)}\exp\left(-\tfrac{1}{2}w \right)\,dw,\]

where \(\delta=\sqrt{n}\theta/\sigma\) is the noncentrality parameter, and this will now be shown to constitute a family with monotone likelihood ratio. To see that the ratio \[r(t)=\frac{\int_{0}^{\infty}\exp\left[-\frac{1}{2}\left(t\sqrt{\frac{w}{n-1}}- \delta_{1}\right)^{2}\right]w^{\frac{1}{2}(n-2)}\exp(-\frac{1}{2}w)\,dw}{\int_{0 }^{\infty}\exp\left[-\frac{1}{2}\left(t\sqrt{\frac{w}{n-1}}-\delta_{0}\right)^{ 2}\right]w^{\frac{1}{2}(n-2)}\exp(-\frac{1}{2}w)\,dw}\]

is an increasing function of \(t\) for \(\delta_{0}<\delta_{1}\), suppose first that \(t<0\) and let \(\upsilon=-t\sqrt{w/(n-1)}\). The ratio then becomes proportional to

\[\frac{\int_{0}^{\infty}f(\upsilon)\exp\left[-(\delta_{1}-\delta_{0})\upsilon- \frac{(n-1)\upsilon^{2}}{2t^{2}}\right]dv}{\int_{0}^{\infty}f(\upsilon)\exp \left[-\frac{(n-1)\upsilon^{2}}{2t^{2}}\right]dv}\]

\[=\int\exp[-(\delta_{1}-\delta_{0})\upsilon]g_{t^{2}}(\upsilon)\,dv,\]

where

\[f(\upsilon)=\exp(-\delta_{0}\upsilon)\upsilon^{n-1}\exp(-\upsilon^{2}/2)\]

and

\[g_{t^{2}}(\upsilon)=\frac{f(\upsilon)\exp\left[-\frac{(n-1)\upsilon^{2}}{2t^{2 }}\right]}{\int_{0}^{\infty}f(z)\exp\left[-\frac{(n-1)\varsigma^{2}}{2t^{2}} \right]dz}.\]

Since the family of probability densities \(g_{t^{2}}(\upsilon)\) is a family with monotone likelihood ratio, the integral of \(\exp[-(\delta_{1}-\delta_{0})\upsilon]\) with respect to this density is a decreasing function of \(t^{2}\) (Problem 3.41), and hence an increasing function of \(t\) for \(t<0\). Similarly one finds that \(r(t)\) is an increasing function of \(t\) for \(t>0\) by making the transformation \(\upsilon=t\sqrt{w/(n-1)}\). By continuity it is then an increasing function of \(t\) for all \(t\).

There exists therefore a UMP invariant test of \(H:\xi/\sigma\leq\theta_{0}\), which rejects when \(t>C\), where \(C\) is determined by (6.13). In terms of the original variables \(Y_{i}\) the rejection region of the UMP invariant test of \(H:p\geq p_{0}\) becomes

\[\frac{\sqrt{n}(\bar{y}-u)}{\sqrt{\sum(y_{i}-\bar{y})^{2}/(n-1)}}>C. \tag{6.15}\]

If the problem is considered as a two-decision problem with losses \(L_{0}(p)\) and \(L_{1}(p)\) for accepting or rejecting \(p\geq p_{0}\), which depend only on \(p\) and satisfy the condition corresponding to (6.14), the class of tests (6.15) constitutes a complete family of invariant procedures as \(C\) varies from \(-\infty\) to \(\infty\).

Consider next the comparison of two probabilities on the basis of samples \(X_{1}\),..., \(X_{m}\); \(Y_{1}\),..., \(Y_{n}\) from \(N(\xi,\sigma^{2})\) and \(N(\eta,\sigma^{2})\). If

\[p=\Phi\left(\frac{u-\xi}{\sigma}\right),\ \ \ \ \ \ \pi=\Phi\left(\frac{u-\eta}{ \sigma}\right),\]one wishes to test the hypothesis \(p\leq\pi\), which is equivalent to

\[H:\eta\leq\xi.\]

The statistics \(\bar{X},\bar{Y}\), and \(S=\sqrt{\sum(X_{i}-\bar{X})^{2}+\sum(Y_{j}-\bar{Y})^{2}}\) are a set of sufficient statistics for \(\xi\), \(\eta\), \(\sigma\). The problem remains invariant under the addition of an arbitrary common constant to \(\bar{X}\) and \(\bar{Y}\), which leaves \(\bar{Y}-\bar{X}\) and \(S\) as maximal invariants. It is also invariant under multiplication of \(\bar{X},\bar{Y}\), and \(S\), and hence of \(\bar{Y}-\bar{X}\) and \(S\), by a common positive constant, which reduces the data to the maximal invariant \((\bar{Y}-\bar{X})/S\). Since

\[t=\frac{(\bar{y}-\bar{x})/\sqrt{\frac{1}{m}+\frac{1}{n}}}{s/\sqrt{m+n-2}}\]

has a noncentral \(t\)-distribution with noncentrality parameter \(\delta=\sqrt{mn}(\eta-\xi)/\sqrt{(m+n)}\sigma\), the UMP invariant test of \(H:\eta-\xi\leq 0\) rejects when \(t>C\). This coincides with the UMP unbiased test (5.27). Analogously, the corresponding two-sided test (5.30), with rejection region \(|t|\geq C\), is UMP invariant for testing the hypothesis \(p=\pi\) against the alternatives \(p\neq\pi\) (Problem 6.19).

### Almost Invariance

Let \(G\) be a group of transformations leaving a family \(\mathcal{P}=\{P_{\theta},\theta\in\Omega\}\) of distributions of \(X\) invariant. A test \(\phi\) is said to be _equivalent to an invariant test_ if there exists an invariant test \(\psi\) such that \(\phi(x)=\psi(x)\) for all \(x\) except possibly on a \(\mathcal{P}\)-null set \(N\); \(\phi\) is said to be _almost invariant with respect to \(G\) if_

\[\phi(gx)=\phi(x)\quad\text{ for all }\quad x\in\mathcal{X}-N_{g},\quad g\in G, \tag{6.16}\]

where the exceptional null set \(N_{g}\) is permitted to depend on \(g\). This concept is required for investigating the relationship of invariance to unbiasedness and to certain other desirable properties. In this connection it is important to know whether a UMP invariant test is also UMP among almost invariant tests. This turns out to be the case under assumptions which are made precise in Theorem 6.5.1 below and which are satisfied in all the usual applications.

If \(\phi\) is equivalent to an invariant test, then \(\phi(gx)=\phi(x)\) for all \(x\notin N\cup g^{-1}N\). Since \(P_{\theta}(g^{-1}N)=P_{\bar{g}\theta}(N)=0\), it follows that \(\phi\) is then almost invariant. The following theorem gives conditions under which conversely any almost invariant test is equivalent to an invariant one.

**Theorem 6.5.1**: _Let \(G\) be a group of transformations of \(\mathcal{X}\), and let \(\mathcal{A}\) and \(\mathcal{B}\) be \(\sigma\)-fields of subsets of \(\mathcal{X}\) and \(G\) such that for any set \(A\in\mathcal{A}\) the set of pairs \((x,g)\) for which \(gx\in A\) is measurable \(\mathcal{A}\times\mathcal{B}\). Suppose further that there exists a \(\sigma\)-finitemeasure \(\nu\) over \(G\) such that \(\nu(B)=0\) implies \(\nu(Bg)=0\) for all \(g\in G\). Then any measurable function that is almost invariant under \(G\) (where "almost" refers to some \(\sigma\)-finite measure \(\mu\)) is equivalent to an invariant function._

Proof. Because of the measurability assumptions, the function \(\phi(gx)\) considered as a function of the two variables \(x\) and \(g\) is measurable \({\cal A}\times{\cal B}\). It follows that \(\phi(gx)-\phi(x)\) is measurable \({\cal A}\times{\cal B}\), and so therefore is the set \(S\) of points \((x,g)\) with \(\phi(gx)\neq\phi(x)\). If \(\phi\) is almost invariant, any section of \(S\) with fixed \(g\) is a \(\mu\)-null set. By Fubini's Theorem (Theorem 2.2.4), there exists therefore a \(\mu\)-null set \(N\) such that for all \(x\in{\cal X}-N\)

\[\phi(gx)=\phi(x)\ \ \ {\rm a.e.}\ \nu.\]

Without loss of generality suppose that \(\nu(G)=1\), and let \(A\) be the set of points \(x\) for which

\[\int\phi(g^{\prime}x)\,d\nu(g^{\prime})=\phi(gx)\ \ \ {\rm a.e.}\ \nu.\]

If

\[f(x,g)=\left|\int\phi(g^{\prime}x)\,d\nu(g^{\prime})-\phi(gx)\right|,\]

then \(A\) is the set of points \(x\) for which

\[\int f(x,g)\,d\nu(g)=0.\]

Since this integral is a measurable function of \(x\), it follows that \(A\) is measurable. Let

\[\psi(x)=\left\{\begin{array}{ll}\int\phi(gx)d\nu(g)&{\rm if}\ x\in A,\\ 0&{\rm if}\ x\notin A.\end{array}\right.\]

Then \(\psi\) is measurable and \(\psi(x)=\phi(x)\) for \(x\notin N\), since \(\phi(gx)=\phi(x)\) a.e. \(\nu\) implies that \(\int\phi(g^{\prime}x)\,d\nu(g^{\prime})=\phi(x)\) and that \(x\in A\). To show that \(\psi\) is invariant it is enough to prove that the set \(A\) is invariant. For any point \(x\in A\), the function \(\phi(gx)\) is constant except on a null subset \(N_{x}\) of \(G\). Then \(\phi(ghx)\) has the same constant value for all \(g\notin N_{x}h^{-1}\), which by assumption is again a \(\nu\)-null set. Hence \(hx\in A\), which completes the proof.

Additional results concerning the relation of invariance and almost invariance are given by Berk and Bickel (1968) and Berk (1970). In particular, the basic idea of the following example is due to Berk (1970).

**Example 6.5.1**: **(Counterexample)** Let \(Z\), \(Y_{1}\),..., \(Y_{n}\) be independently distributed as \(N(\theta,1)\), and consider the \(1:1\) transformations \(y^{\prime}_{i}=y_{i}(i=1,\ldots,n)\) and If the group \(G\) is generated by taking for \((a_{1},\ldots,a_{k})\), \(k=1,2,\ldots,\) all finite sets and for \((j_{1},\ldots,j_{k})\) all permutations of \((1,\ldots,k)\), then \((z,\,y_{1},\ldots,y_{n})\) is almost invariant. It is however not equivalent to an invariant function, since \((y_{1},\ldots,\,y_{n})\) is maximal invariant.

**Corollary 6.5.1**: _Suppose that the problem of testing \(H:\theta\in\omega\) against \(K:\theta\in\Omega-\omega\) remains invariant under \(G\) and that the assumptions of Theorem 6.5.1 hold. Then if \(\phi_{0}\) is UMP invariant, it is also UMP within the class of almost invariant tests._

Proof. If \(\phi\) is almost invariant, it is equivalent to an invariant test \(\psi\) by Theorem 6.5.1. The tests \(\phi\) and \(\psi\) have the same power function, and hence \(\phi_{0}\) is uniformly at least as powerful as \(\phi\).

In applications, \({\cal P}\) is usually a dominated family, and \(\mu\) any \(\sigma\)-finite measure equivalent to \({\cal P}\) (which exists by Theorem A.4.2 of the Appendix). If \(\phi\) is almost invariant with respect to \({\cal P}\), it is then almost invariant with respect to \(\mu\) and hence equivalent to an invariant test. Typically, the sample space \({\cal X}\) is \(n\)-dimensional Euclidean space, \({\cal A}\) is the class of Borel sets, and the elements of \(G\) are transformations of the form \(y=f(x,\,\tau)\), where \(\tau\) ranges over a set of positive measure in an \(m\)-dimensional space and \(f\) is a Borel-measurable vector-valued function of \(m+n\) variables. If \({\cal B}\) is taken as the class of Borel sets in \(m\)-space the measurability conditions of the theorem are satisfied.

The requirement that for all \(g\in G\) and \(B\in{\cal B}\)

\[\nu(B)=0\ \ \ {\rm implies}\ \ \ \nu(Bg)=0 \tag{6.17}\]

is satisfied in particular when

\[\nu(Bg)=\nu(B)\ \ \ \ \ {\rm for\ all}\ \ \ g\in G,\ \ \ B\in{\cal B}. \tag{6.18}\]

The existence of such a _right invariant measure_ is guaranteed for a large class of groups by the theory of Haar measure. (See, for example, Eaton 1989.) Alternatively, it is usually not difficult to check Condition (6.17) directly.

**Example 6.5.2**: Let \(G\) be the group of all nonsingular linear transformations of \(n\)-space. Relative to a fixed coordinate system the elements of \(G\) can be represented by nonsingular \(n\times n\) matrices \(A=(a_{ij})\), \(A^{\prime}=(a^{\prime}_{ij}),\ldots\) with the matrix product serving as the group product of two such elements. The \(\sigma\)-field \({\cal B}\) can be taken to be the class of Borel sets in the space of the \(n^{2}\) elements of the matrices, and the measure \(\nu\) can be taken as Lebesgue measure over \({\cal B}\). Consider now a set \(S\) of matrices with \(\nu(S)=0\), and the set \(S^{*}\) of matrices \(A^{\prime}A\) with \(A^{\prime}\in S\) and \(A\) fixed. If \(a=\max|a_{ij}|\), \(C^{\prime}=A^{\prime}A\), and \(C^{\prime\prime}=A^{\prime\prime}A\), the inequalities \(|a^{\prime\prime}_{ij}-a^{\prime}_{ij}|\leq\epsilon\) for all \(i\), \(j\) imply \(|c^{\prime\prime}_{ij}-c^{\prime}_{ij}|\leq na\epsilon\). Since a set has \(\nu\)-measure zero if and only if it can be covered by a union of rectangles whose total measure does not exceed any given \(\epsilon>0\), it follows that \(\nu(S^{*})=0\), as was to be proved.

In the preceding chapters, tests were compared purely in terms of their power functions (possibly weighted according to the seriousness of the losses involved). Since the restriction to invariant tests is a departure from this point of view, it is of interest to consider the implications of applying invariance to the power functions rather than to the tests themselves. Any test that is invariant or almost invariant under a group \(G\) has a power function which is invariant under the group \(\bar{G}\) induced by \(G\) in the parameter space.

To see that the converse is in general not true, let \(X_{1}\), \(X_{2}\), \(X_{3}\) be independently, normally distributed with mean \(\xi\) and variance \(\sigma^{2}\), and consider the hypothesis \(\sigma\geq\sigma_{0}\). The test with rejection region

\[\begin{array}{lcl}|X_{2}-X_{1}|>k&\mbox{when}&\bar{X}<0,\\ |X_{3}-X_{2}|>k&\mbox{when}&\bar{X}\geq 0\end{array}\]

is not invariant under the group \(G\) of transformations \(X^{\prime}_{i}=X_{i}+c\), but its power function is invariant under the associated group \(\bar{G}\).

The two properties, almost invariance of a test \(\phi\) and invariance of its power function, become equivalent if before the application of invariance considerations the problem is reduced to a sufficient statistic whose distributions constitute a boundedly complete family.

**Lemma 6.5.1**: _Let the family \({\cal P}^{T}=\{P_{\theta}^{T},\,\theta\in\Omega\}\) of distributions of \(T\) be boundedly complete, and let the problem of testing \(H:\theta\in\Omega_{H}\) remain invariant under a group \(G\) of transformations of \(T\). Then a necessary and sufficient condition for the power function of a test \(\psi(t)\) to be invariant under the induced group \(\bar{G}\) over \(\Omega\) is that \(\psi(t)\) is almost invariant under \(G\)._

Proof. For all \(\theta\in\Omega\) we have \(E_{\bar{g}\theta}\psi(T)=E_{\theta}\psi(gT)\). If \(\psi\) is almost invariant, \(E_{\theta}\psi(T)=E_{\theta}\psi(gT)\) and hence \(E_{\bar{g}\theta}\psi(T)=E_{\theta}\psi(T)\), so that the power function of \(\psi\) is invariant. Conversely, if \(E_{\theta}\psi(T)=E_{\bar{g}\theta}\psi(T)\), then \(E_{\theta}\psi(T)=E_{\theta}\psi(gT)\), and by the bounded completeness of \({\cal P}^{T}\), we have \(\psi(gt)=\psi(t)\) a.e. \({\cal P}^{T}\).

As a consequence, it is seen that UMP almost invariant tests also possess the following optimum property.

**Theorem 6.5.2**: _Under the assumptions of Lemma 6.5.1, let \(v(\theta)\) be maximal invariant with respect to \(\bar{G}\), and suppose that among the tests of \(H\) based on the sufficient statistic \(T\) there exists a UMP almost invariant one, say \(\psi_{0}(t)\). Then \(\psi_{0}(t)\) is UMP in the class of all tests based on the original observations \(X\), whose power function depends only on \(v(\theta)\)._

Proof. Let \(\phi(x)\) be any such test, and let \(\psi(t)=E[\phi(X)|t]\). The power function of \(\psi(t)\), being identical with that of \(\phi(x)\), depends then only on \(v(\theta)\), and hence is invariant under \(\bar{G}\). It follows from Lemma 6.5.1 that \(\psi(t)\) is almost invariant under \(G\), and \(\psi_{0}(t)\) is uniformly at least as powerful as \(\psi(t)\) and therefore as \(\phi(x)\).

**Example 6.5.3**: For the hypothesis \(\tau^{2}\leq\sigma^{2}\) concerning the variances of two normal distributions, the statistics \((\tilde{X},\,\tilde{Y},\,S_{x}^{2},\,S_{Y}^{2})\) constitute a complete set of sufficient statistics. It was shown in Example 6.3.5 that there exists a UMP invariant test with respect to a suitable group \(G\), which has rejection region \(S_{Y}^{2}/S_{X}^{2}>C_{0}\). Since in the present case almost invariance of a test with respect to \(G\) implies that it is equivalent to an invariant one (Problem 6.23), Theorem 6.5.2 is applicable with \(v(\theta)=\Delta=\tau^{2}/\sigma^{2}\), and the test is therefore UMP among all tests whose power function depends only on \(\Delta\).

Theorem 6.5.1 makes it possible to establish a simple condition under which reduction to sufficiency before the application of invariance is legitimate.

**Theorem 6.5.3**: _Let \(X\) be distributed according to \(P_{\theta}\), \(\theta\in\Omega\), and let \(T\) be sufficient for \(\theta\). Suppose \(G\) leaves invariant the problem of testing \(H:\theta\in\Omega_{H}\), and that \(T\) satisfies_

\[T(x_{1})=T(x_{2})\ \ \mbox{implies}\ \ \ T(gx_{1})=T(gx_{2})\ \ \mbox{for all}\ \ \ g\in G,\]

_so that \(G\) induces a group \(\tilde{G}\) of transformations of \(T\)-space through_

\[\tilde{g}T(x)=T(gx).\]

* _If_ \(\varphi(x)\) _is any invariant test of_ \(H\)_, there exists an almost invariant test_ \(\psi\) _based on_ \(T\)_, which has the same power function as_ \(\varphi\)_._
* _If in addition the assumptions of Theorem_ 6.5.1 _are satisfied, the test_ \(\psi\) _of (i) can be taken to be invariant._
* _If there exists a test_ \(\psi_{0}(T)\) _which is UMP among all_ \(\tilde{G}\)_-invariant tests based on_ \(T\)_, then under the assumptions of (ii),_ \(\psi_{0}\)_, is also UMP among all_ \(G\)_-invariant tests based on_ \(X\)_._

This theorem justifies the derivation of the UMP invariant tests of Examples 6.3.4 and 6.3.5.

Proof. (i): Let \(\psi(t)=E[\varphi(X)|t]\). Then \(\psi\) has the same power function as \(\varphi\). To complete the proof, it suffices to show that \(\psi(t)\) is almost invariant, i.e., that

\[\psi(\tilde{g}t)=\psi(t)\ \ \ \ \ \ (\mbox{a.e.}\ {\cal P}^{\cal T}).\]

It follows from (6.2) that

\[E_{\theta}[\varphi(gX)|\tilde{g}t]=E_{\tilde{g}\theta}[\varphi(X)|t]\ \ \ \ \ (\mbox{a.e.}\ P_{\theta}).\]

Since \(T\) is sufficient, both sides of this equation are independent of \(\theta\). Furthermore \(\varphi(gx)=\varphi(x)\) for all \(x\) and \(g\), and this completes the proof.

Part (ii) follows immediately from (i) and Theorem 6.5.1, and part (iii) from (ii).

### Unbiasedness and Invariance

The principles of unbiasedness and invariance complement each other in that each is successful in cases where the other is not. For example, there exist UMP unbiased tests for the comparison of two binomial or Poisson distributions, problems to which invariance considerations are not applicable. UMP unbiased tests also exist for testing the hypothesis \(\sigma=\sigma_{0}\) against \(\sigma\neq\sigma_{0}\) in a normal distribution, while invariance does not reduce this problem sufficiently far. Conversely, there exist UMP invariant tests of hypotheses specifying the values of more than one parameter (to be considered in Chapter 7) but for which the class of unbiased tests has no UMP member. There are also hypotheses, for example, the one-sided hypothesis \(\xi/\sigma\leq\theta_{0}\) in a univariate normal distribution or \(\rho\leq\rho_{0}\) in a bivariate one (Problem 6.20) with \(\theta_{0}\), \(\rho_{0}\neq 0\), where a UMP invariant test exists but the existence of a UMP unbiased test does not follow by the methods of Chapter 5 and is an open question.

On the other hand, to some problems both principles have been applied successfully. These include Student's hypotheses \(\xi\leq\xi_{0}\) and \(\xi=\xi_{0}\) concerning the mean of a normal distribution, and the corresponding two-sample problems \(\eta-\xi\leq\Delta_{0}\) and \(\eta-\xi=\Delta_{0}\) when the variances of the two samples are assumed equal. Other examples are the one-sided hypotheses \(\sigma^{2}\geq\sigma_{0}^{2}\) and \(\tau^{2}/\sigma^{2}\geq\Delta_{0}\) concerning the variances of one or two normal distributions. The hypothesis of independence \(\rho=0\) in a bivariate normal distribution is still another case in point (Problem 6.20). In all these examples the two optimum procedures coincide. We shall now show that this is not accidental but is the case whenever the UMP invariant test is UMP also among all almost invariant tests and the UMP unbiased test is unique. In this sense, the principles of unbiasedness and of almost invariance are consistent.

**Theorem 6.6.1**: _Suppose that for a given testing problem there exists a UMP unbiased test \(\phi^{*}\) which is unique (up to sets of measure zero), and that there also exists a UMP almost invariant test with respect to some group \(G\). Then the latter is also unique (up to sets of measure zero), and the two tests coincide a.e._

Proof. If \(U(\alpha)\) is the class of unbiased level-\(\alpha\) tests, and if \(g\in G\), then \(\phi\in U(\alpha)\) if and only if \(\phi g\in U(\alpha)\).4 Denoting the power function of the test \(\phi\) by \(\beta_{\phi}(\theta)\), we thus have

Footnote 4: \(\phi g\) denotes the critical function which assigns to \(x\) the value \(\phi(gx)\).

\[\beta_{\phi^{*}g}(\theta) = \beta_{\phi^{*}}(\bar{g}\theta)=\sup_{\phi\in U(\alpha)}\beta_{ \phi}(\bar{g}\theta)=\sup_{\phi\in U(\alpha)}\beta_{\phi g}(\theta)\] \[= \sup_{\phi g\in U(\alpha)}\beta_{\phi g}(\theta)=\beta_{\phi^{*} }(\theta).\]

It follows that \(\phi^{*}\) and \(\phi^{*}g\) have the same power function, and, because of the uniqueness assumption, that \(\phi^{*}\) is almost invariant. Therefore, if \(\phi^{\prime}\) is UMP almost invariant, we have \(\beta_{\phi^{\prime}}(\theta)\geq\beta_{\phi^{*}}(\theta)\) for all \(\theta\). On the other hand, \(\phi^{\prime}\) is unbiased, as is seen by comparing it with the invariant test \(\phi(x)\equiv\alpha\), and hence \(\beta_{\phi^{\prime}}(\theta)\leq\beta_{\phi^{*}}(\theta)\)

[MISSING_PAGE_FAIL:276]

multiplication of all variables by a common positive constant to \((\bar{Y}-\bar{X})/\sqrt{S_{X}^{2}+S_{Y}^{2}}\) and \(S_{Y}^{2}/S_{X}^{2}\). One would expect any reasonable invariant rejection region to be of the form

\[\frac{\bar{Y}-\bar{X}}{\sqrt{S_{X}^{2}+S_{Y}^{2}}}\geq g\left(\frac{S_{Y}^{2}}{S _{X}^{2}}\right) \tag{6.19}\]

for some suitable function \(g\). If this test is also to be unbiased, the probability of (6.19) must equal \(\alpha\) when \(\eta=\xi\) for all values of \(\tau/\sigma\). It has been shown by Linnik and others that only pathological functions \(g\) with this property can exist. [This work is reviewed by Pfanzagl (1974).] However, approximate solutions are available which provide tests that are satisfactory for all practical purposes. These are the Welch approximate \(t\)-solution described in Section 13.2, and the Welch-Aspin test. Both are discussed, and evaluated, in Scheffe (1970) and Wang (1971); see also Chernoff (1949), Wallace (1958), Davenport and Webster (1975) and Robinson (1982). The Behrens-Fisher problem will be revisited in Examples 15.5.4 and 18.5.4 and Section 17.3.

The property of a test \(\phi_{1}\) being UMP invariant is relative to a particular group \(G_{1}\), and does not exclude the possibility that there might exist another test \(\phi_{2}\) which is UMP invariant with respect to a different group \(G_{2}\). Simple instances can be obtained from Examples 6.5.1 and 6.6.2.

**Example 6.6.1**: If \(G_{1}\) is the group \(G\) of Example 6.5.1, a UMP invariant test of \(H:\theta\leq\theta_{0}\) against \(\theta>\theta_{0}\) rejects when \(Y_{1}+\cdots+Y_{n}>C\). Let \(G_{2}\) be the group obtained by interchanging the role of \(Z\) and \(Y_{1}\). Then a UMP invariant test with respect to \(G_{2}\) rejects when \(Z+Y_{2}+\cdots+Y_{n}>C\). Analogous UMP invariant tests are obtained by interchanging the role of \(Z\) and any one of the other \(Y\)'s and further examples by applying the transformations of \(G\) in Example 6.5.1 to more than one variable. In particular, if it is applied independently to all \(n+1\) variables, only the constants remain invariant, and the test \(\phi\equiv\alpha\) is UMP invariant.

**Example 6.6.2**: For another example (due to Charles Stein), let \((X_{11},X_{12})\) and \((X_{21},X_{22})\) be independent and have bivariate normal distributions with zero means and covariance matrices

\[\left(\begin{array}{cc}\sigma_{1}^{2}&\rho\sigma_{1}\sigma_{2}\\ \rho\sigma_{1}\sigma_{2}&\sigma_{2}^{2}\end{array}\right)\quad\mbox{and}\quad \left(\begin{array}{cc}\Delta\sigma_{1}^{2}&\Delta\rho\sigma_{1}\sigma_{2}\\ \Delta\rho\sigma_{1}\sigma_{2}&\Delta\sigma_{2}^{2}\end{array}\right).\]

Suppose that these matrices are nonsingular, or equivalently that \(|\rho|\neq 1\), but that all \(\sigma_{1}\), \(\sigma_{2}\), \(\rho\), and \(\Delta\) are otherwise unknown. The problem of testing \(\Delta=1\) against \(\Delta>1\) remains invariant under the group \(G_{1}\) of all nonsingular transformations

\[\begin{array}{l}X_{i1}^{\prime}=bX_{i1}\\ X_{i2}^{\prime}=a_{1}X_{i1}+a_{2}X_{i2},\qquad(a_{2},b>0).\end{array}\]Since the probability is 0 that \(X_{11}X_{22}=X_{12}X_{21}\), the \(2\times 2\) matrix \((X_{ij})\) is nonsingular with probability 1, and the sample space can therefore be restricted to be the set of all nonsingular such matrices. A maximal invariant under the subgroup corresponding to \(b=1\) is the pair \((X_{11},\,X_{21})\). The argument of Example 6.3.5 then shows that there exists a UMP invariant test under \(G_{1}\) which rejects when \(X_{21}^{2}/X_{11}^{2}>C\).

By interchanging 1 and 2 in the second subscript of the \(X\)'s one sees that under the corresponding group \(G_{2}\) the UMP invariant test rejects when \(X_{22}^{2}/X_{12}^{2}>C\).

A third group leaving the problem invariant is the smallest group containing both \(G_{1}\) and \(G_{2}\), namely, the group \(G\) of all common nonsingular transformations

\[\begin{array}{l}X_{i1}^{\prime}=a_{i1}X_{i1}+a_{12}X_{i2}\\ X_{i2}^{\prime}=a_{21}X_{i1}+a_{22}X_{i2},\end{array}\quad(i=1,2).\]

Given any two nonsingular sample points \(Z=(X_{ij})\) and \(Z^{\prime}=(X_{ij}^{\prime})\), there exists a nonsingular linear transformation \(A\) such that \(Z^{\prime}=AZ\). There are therefore no invariants under \(G\), and the only invariant size-\(\alpha\) test is \(\phi\equiv\alpha\). It follows vacuously that this is UMP invariant under G.

### Admissibility

Any UMP unbiased test has the important property of admissibility (Problem 4.1), in the sense that there cannot exist another test which is uniformly at least as powerful and against some alternatives actually more powerful than the given one. The corresponding property does not necessarily hold for UMP invariant tests, as is shown by the following example.

**Example 6.7.1**: **(continued)** Under the assumptions of Example 6.6.2 it was seen that the UMP invariant test under \(G\) is the test \(\varphi\equiv\alpha\) which has power \(\beta(\Delta)\equiv\alpha\). On the other hand, \(X_{11}\) and \(X_{21}\) are independently distributed as \(N(0,\,\sigma_{1}^{2})\) and \(N(0,\,\Delta\sigma_{1}^{2})\). On the basis of these observations there exists a UMP test for testing \(\Delta=1\) against \(\Delta>1\) with rejection region \(X_{21}^{2}/X_{11}^{2}>C\) (Problem 3.67). The power function of this test is strictly increasing in \(\Delta\) and hence \(>\alpha\) for all \(\Delta>1\).

Admissibility of optimum invariant tests therefore cannot be taken for granted but must be established separately for each case.

We shall distinguish two slightly different concepts of admissibility. A test \(\varphi_{0}\) will be called \(\alpha\)_-admissible_ for testing \(H:\theta\in\Omega_{H}\) against a class of alternatives \(\theta\in\Omega^{\prime}\) if for any other level-\(\alpha\) test \(\varphi\)

\[E_{\theta}\varphi(X)\geq E_{\theta}\varphi_{0}(X)\quad\text{ for all }\ \ \theta\in\Omega^{\prime} \tag{6.20}\]

implies \(E_{\theta}\varphi(X)=E_{\theta}\varphi_{0}(X)\) for all \(\theta\in\Omega^{\prime}\). This definition takes no account of the relationship of \(E_{\theta}\varphi(X)\) and \(E_{\theta}\varphi_{0}(X)\) for \(\theta\in\Omega_{H}\) beyond the requirement that both tests are of level \(\alpha\). For some unexpected, and possibly undesirable consequences of \(\alpha\)-admissibility, see Example 8.7.1 and Perlman and Wu (1999). A concept closer to the decision-theoretic notion of admissibility discussed in Section 1.8, defines \(\varphi_{0}\) to be _\(d\)-admissible_ for testing \(H\) against \(\Omega^{\prime}\) if (6.20) and

\[E_{\theta}\varphi(X)\leq E_{\theta}\varphi_{0}(X)\quad\text{ for all }\ \ \theta\in\Omega_{H} \tag{6.21}\]

jointly imply \(E_{\theta}\varphi(X)=E_{\theta}\varphi_{0}(X)\) for all \(\theta\in\Omega_{H}\cup\Omega^{\prime}\) (see Problem 6.34).

Any level-\(\alpha\) test \(\varphi_{0}\) that is \(\alpha\)-admissible is also \(d\)-admissible provided no other test \(\varphi\) exists with \(E_{\theta}\varphi(X)=E_{\theta}\varphi_{0}(X)\) for all \(\theta\in\Omega^{\prime}\) but \(E_{\theta}\varphi(X)\neq E_{\theta}\varphi_{0}(X)\) for some \(\theta\in\Omega_{H}\). That the converse does not hold is shown by the following example.

**Example 6.7.2**: Let \(X\) be normally distributed with mean \(\xi\) and known variance \(\sigma^{2}\). For testing \(H:\xi\leq-1\) or \(\geq 1\) against \(\Omega^{\prime}:\xi=0\), there exists a level-\(\alpha\) test \(\varphi_{0}\), which rejects when \(C_{1}\leq X\leq C_{2}\) and accepts otherwise, such that (Problem 6.35)

\[E_{\xi}\varphi_{0}(X)\leq E_{\xi=-1}\varphi_{0}(X)=\alpha\quad\text{ for }\ \ \xi\leq-1\]

and

\[E_{\xi}\varphi_{0}(X)\leq E_{\xi=+1}\varphi_{0}(X)=\alpha^{\prime}<\alpha\quad \text{ for }\ \ \xi\geq+1.\]

A slight modification of the proof of Theorem 3.7.1 shows that \(\varphi_{0}\) is the unique test maximizing the power at \(\xi=0\) subject to

\[E_{\xi}\varphi(X)\leq\alpha\quad\text{for }\ \ \xi\leq-1\ \ \text{ and }\ \ E_{\xi}\varphi(X)\leq\alpha^{\prime}\ \ \text{ for }\ \ \xi\geq 1,\]

and hence that \(\varphi_{0}\) is \(d\)-admissible.

On the other hand, the test \(\varphi\) with rejection region \(|X|\leq C\), where \(E_{\xi=-1}\varphi(X)=E_{\xi=1}\varphi(X)=\alpha\), is the unique test maximizing the power at \(\xi=0\) subject to \(E_{\xi}\varphi(X)\leq\alpha\) for \(\xi\leq-1\) or \(\geq 1\), and hence is more powerful against \(\Omega^{\prime}\) than \(\varphi_{0}\), so that \(\varphi_{0}\) is not \(\alpha\)-admissible.

A test that is admissible under either definition against \(\Omega^{\prime}\) is also admissible against any \(\Omega^{\prime\prime}\) containing \(\Omega^{\prime}\) and hence in particular against the class of all alternatives \(\Omega_{K}=\Omega-\Omega_{H}\). The terms \(\alpha\)- and \(d\)-admissible without qualification will be reserved for admissibility against \(\Omega_{K}\). Unless a UMP test exists, any \(\alpha\)-admissible test will be admissible against some \(\Omega^{\prime}\subset\Omega_{K}\) and inadmissible against others. Both the strength of an admissibility result and the method of proof will depend on the set \(\Omega^{\prime}\).

Consider in particular the admissibility of a UMP unbiased test mentioned at the beginning of the section. This does not rule out the existence of a test with greater power for all alternatives of practical importance and smaller power only for alternatives so close to \(H\) that the value of the power there is immaterial. In the present section, we shall discuss two methods for proving admissibility against various classes of alternatives.

**Theorem 6.7.1**: _Let \(X\) be distributed according to an exponential family with density_

\[p_{\theta}(x)=C(\theta)\exp\left(\sum_{j=1}^{s}\theta_{j}T_{j}(x)\right)\]

_with respect to a \(\sigma\)-finite measure \(\mu\) over a Euclidean sample space \((\mathcal{X},\mathcal{A})\), and let \(\Omega\) be the natural parameter space of this family. Let \(\Omega_{H}\) and \(\Omega^{\prime}\) be disjoint nonempty subsets of \(\Omega\), and suppose that \(\varphi_{0}\) is a test of \(H:\theta\in\Omega_{H}\) based on \(T=(T_{1},\ldots,T_{s})\) with acceptance region \(A_{0}\) which is a closed convex subset of \(R^{s}\) possessing the following property: If \(A_{0}\cap\{\sum a_{i}t_{i}>c\}\) is empty for some \(c\), there exists a point \(\theta^{*}\in\Omega\) and a sequence \(\lambda_{n}\to\infty\) such that \(\theta^{*}+\lambda_{n}a\in\Omega^{\prime}\) [where \(\lambda_{n}\) is a scalar and \(a=(a_{1},\ldots,a_{s})\)]. Then if \(A\) is any other acceptance region for \(H\) satisfying_

\[P_{\theta}(X\in A)\leq P_{\theta}(X\in A_{0})\quad\mbox{ for all }\ \ \theta\in\Omega^{\prime},\]

\(A\) _is contained in \(A_{0}\), except for a subset of measure 0, i.e., \(\mu(A\cap A_{0}^{c})=0\)._

Proof. Suppose to the contrary that \(\mu(A\cap A_{0}^{c})>0\). Then it follows from the closure and convexity of \(A_{0}\), that there exist \(a\in R^{s}\) and a real number \(c\) such that

\[A_{0}\cap\left\{t:\sum a_{i}t_{i}>c\right\}\mbox{ is empty} \tag{6.22}\]

and

\[A\cap\left\{t:\sum a_{i}t_{i}>c\right\}\mbox{ has positive $\mu$-measure}, \tag{6.23}\]

that is, the set \(A\) protrudes in some direction from the convex set \(A_{0}\). We shall show that this fact and the exponential nature of the densities imply that

\[P_{\theta}(A)>P_{\theta}(A_{0})\quad\mbox{ for some }\ \ \theta\in\Omega^{ \prime}, \tag{6.24}\]

which provides the required contradiction. Let \(\varphi_{0}\) and \(\varphi\) denote the indicators of \(A_{0}^{c}\) and \(A^{c}\), respectively, so that (6.24) is equivalent to

\[\int[\varphi_{0}(t)-\varphi(t)]\,dP_{\theta}(t)>0\quad\mbox{ for some }\ \ \theta\in\Omega^{\prime}.\]

If \(\theta=\theta^{*}+\lambda_{n}a\in\Omega^{\prime}\), the left side becomes

\[\frac{C(\theta^{*}+\lambda_{n}a)}{C(\theta^{*})}e^{c\lambda_{n}}\int[\varphi_ {0}(t)-\varphi(t)]e^{\lambda_{n}(\sum a_{i}t_{i}-c)}\,dP_{\theta^{*}}(t).\]

Let this integral be \(I_{n}^{+}+I_{n}^{-}\), where \(I_{n}^{+}\) and \(I_{n}^{-}\) denote the contributions over the regions of integration \(\{t:\sum a_{i}t_{i}>c\}\) and \(\{t:\sum a_{i}t_{i}\leq c\}\), respectively. Since \(I_{n}^{-}\) is bounded, it is enough to show that \(I_{n}^{+}\to\infty\) as \(n\to\infty\). By (6.22), \(\varphi_{0}(t)=1\) and hence \(\varphi_{0}(t)-\varphi(t)\geq 0\) when \(\sum a_{i}t_{i}>c\), and by (6.23)\[\mu\left\{\varphi_{0}(t)-\varphi(t)>0\ \ \ \mbox{and}\ \ \ \sum a_{i}t_{i}>c \right\}>0.\]

This shows that \(I_{n}^{+}\to\infty\) as \(\lambda_{n}\to\infty\) and therefore completes the proof.

**Corollary 6.7.1**: _Under the assumptions of Theorem 6.7.1, the test with acceptance region \(A_{0}\) is \(d\)-admissible. If its size is \(\alpha\) and there exists a finite point \(\theta_{0}\) in the closure \(\bar{\Omega}_{H}\) of \(\Omega_{H}\) for which \(E_{\theta_{0}}\varphi_{0}(X)=\alpha\), then \(\varphi_{0}\) is also \(\alpha\)-admissible._

Proof.

1. Suppose \(\varphi\) satisfies (6.20). Then by Theorem 6.7.1, \(\varphi_{0}(x)\leq\varphi(x)\) (a.e. \(\mu\)). If \(\varphi_{0}(x)<\varphi(x)\) on a set of positive measure, then \(E_{\theta}\varphi_{0}(X)<E_{\theta}\varphi(X)\) for all \(\theta\) and hence (6.21) cannot hold.
2. By the argument of part (i), (6.20) implies \(\alpha=E_{\theta_{0}}\varphi_{0}(X)<E_{\theta_{0}}\varphi(X)\), and hence by the continuity of \(E_{\theta}\varphi(X)\) there exists a point \(\theta\in\Omega_{H}\) for which \(\alpha<E_{\theta}\varphi(X)\). Thus \(\varphi\) is not a level-\(\alpha\) test.

Theorem 6.7.1 and the corollary easily extend to the case where the competitors \(\varphi\) of \(\varphi_{0}\) are permitted to be randomized but the assumption that \(\varphi_{0}\) is nonrandomized is essential. Thus, the main applications of these results are to the case that \(\mu\) is absolutely continuous with respect to Lebesgue measure. The boundary of \(A_{0}\) will then typically have measure zero, so that the closure requirement for \(A_{0}\) can be dropped.

**Example 6.7.3**: **(Normal mean)** If \(X_{1},\ldots,X_{n}\) is a sample from the normal distribution \(N(\xi,\sigma^{2})\), the family of distributions is exponential with \(T_{1}=\bar{X}\), \(T_{2}=\sum X_{i}^{2}\), \(\theta_{1}=n\xi/\sigma^{2}\), \(\theta_{2}=-1/2\sigma^{2}\). Consider first the one-sided problem \(H:\theta_{1}\leq 0\), \(K:\theta_{1}>0\) with \(\alpha<\frac{1}{2}\). Then the acceptance region of the \(t\)-test is \(A:T_{1}/\sqrt{T_{2}}\leq C\) (\(C>0\)), which is convex (Problem 6.36(i)). The alternatives \(\theta\in\Omega^{\prime}\subseteq K\) will satisfy the conditions of Theorem 6.7.1 if for any half plane \(a_{1}t_{1}+a_{2}t_{2}>c\) that does not intersect the set \(t_{1}\leq C\sqrt{t_{2}}\) there exists a ray \((\theta_{1}^{*}+\lambda a_{1},\theta_{2}^{*}+\lambda a_{2})\) in the direction of the vector \((a_{1},a_{2})\) for which \((\theta_{1}^{*}+\lambda a_{1},\theta_{2}^{*}+\lambda a_{2})\in\Omega^{\prime}\) for all sufficiently large \(\lambda\). In the present case, this condition must hold for all \(a_{1}>0>a_{2}\). Examples of sets \(\Omega^{\prime}\) satisfying this requirement (and against which the \(t\)-test is therefore admissible) are

\[\Omega^{\prime}_{1}:\theta_{1}>k_{1}\mbox{ or }\frac{\xi}{\sigma^{2}}>k^{ \prime}_{1}\]

and

\[\Omega^{\prime}_{2}:\frac{\theta_{1}}{\sqrt{-\theta_{2}}}>k_{2}\mbox{ or } \frac{\xi}{\sigma}>k^{\prime}_{2}.\]

On the other hand, the condition is not satisfied for \(\Omega^{\prime}:\xi>k\) (Problem 6.36).

Analogously, the acceptance region \(A:T_{1}^{2}\leq CT_{2}\) of the two-sided \(t\)-test for testing \(H:\theta_{1}=0\) against \(\theta_{1}\neq 0\) is convex, and the test is admissible against \(\Omega^{\prime}_{1}:|\xi/\sigma^{2}|>k_{1}\) and \(\Omega^{\prime}_{2}:|\xi/\sigma|>k_{2}\).

In decision theory, a quite general method for proving admissibility consists in exhibiting a procedure as a unique Bayes solution. In the present case, this is justified by the following result, which is closely related to Theorem 3.8.1.

**Theorem 6.7.2**: _Assume the set \(\{x:f_{\theta}(x)>0\}\) is independent of \(\theta\), and let a \(\sigma\)-field be defined over the parameter space \(\Omega\), containing both \(\Omega_{H}\) and \(\Omega_{K}\) and such that the densities \(f_{\theta}(x)\) (with respect to \(\mu\)) of \(X\) are jointly measurable in \(\theta\) and \(x\). Let \(\Lambda_{0}\) and \(\Lambda_{1}\) be probability distributions over this \(\sigma\)-field with \(\Lambda_{0}(\Omega_{H})=\Lambda_{1}(\Omega_{K})=1\), and let_

\[h_{i}(x)=\int f_{\theta}(x)\,d\Lambda_{i}(\theta).\]

_Suppose \(\varphi_{0}\) is a nonrandomized test of \(H\) against \(K\) defined by_

\[\varphi_{0}(x)=\left\{\begin{array}{ll}1&when\ h_{1}(x)\geq kh_{0}(x)\\ 0&when\ h_{1}(x)<kh_{0}(x)\end{array}\right. \tag{6.25}\]

_and that \(\mu\{x:h_{1}(x)/h_{0}(x)=k\}=0\)._

* _Then,_ \(\varphi_{0}\) _is_ \(d\)_-admissible for testing_ \(H\) _against_ \(K\)_._
* _Let_ \(\sup_{\Omega_{H}}E_{\theta}\varphi_{0}(X)=\alpha\) _and_ \[\omega=\{\theta:\in\Omega_{H}:E_{\theta}\varphi_{0}(X)=\alpha\}\.\]

_If \(\Lambda_{0}(\omega)=1\), then \(\varphi_{0}\) is also \(\alpha\)-admissible._

* _If_ \(\Lambda_{1}\) _assigns probability 1 to_ \(\Omega^{\prime}\subset\Omega_{K}\)_, then the conclusions of (i) and (ii) apply with_ \(\Omega^{\prime}\) _in place of_ \(\Omega_{K}\)_._

Proof. (i): Suppose \(\varphi\) is any other test, satisfying (6.20) and (6.21) with \(\Omega^{\prime}=\Omega_{K}\). Then also

\[\int E_{\theta}\varphi(X)\,d\Lambda_{0}(\theta)\leq\int E_{\theta}\varphi_{0}( X)\,d\Lambda_{0}(\theta)\]

and

\[\int E_{\theta}\varphi(X)\,d\Lambda_{1}(\theta)\geq\int E_{\theta}\varphi_{0}( X)\,d\Lambda_{1}(\theta).\]

By the argument of Theorem 3.8.1, these inequalities are equivalent to

\[\int\varphi(x)h_{0}(x)\,d\mu(x)\leq\int\varphi_{0}(x)h_{0}(x)\,d\mu(x)\]

and

\[\int\varphi(x)h_{1}(x)\,d\mu(x)\geq\int\varphi_{0}(x)h_{1}(x)\,d\mu(x),\]

and the \(h_{i}(x)\) (\(i=0\), \(1\)) are probability densities with respect to \(\mu\). This contradicts the uniqueness of the most powerful test of \(h_{0}\) against \(h_{1}\) at level \(\int\varphi(x)h_{0}(x)\,d\mu(x)\).

(ii): By assumption, \(\int E_{\theta}\varphi_{0}(x)\,d\Lambda_{0}(\theta)=\alpha\), so that \(\varphi_{0}\) is a level-\(\alpha\) test of \(h_{0}\). If \(\varphi\) is any other level-\(\alpha\) test of \(H\) satisfying (6.20) with \(\Omega^{\prime}=\Omega_{K}\), it is also a level-\(\alpha\) test of \(h_{0}\) and the argument of part (i) can be applied as before.

(iii): This follows immediately from the proofs of (i) and (ii).

**Example 6.7.4** In the two-sided normal problem of Example 6.7.3 with \(H:\xi=0\), \(K:\xi\neq 0\) consider the class \(\Omega^{\prime}_{a,b}\) of alternatives (\(\xi\), \(\sigma\)) satisfying

\[\sigma^{2}=\frac{1}{a+\eta^{2}},\quad\xi=\frac{b\eta}{a+\eta^{2}},\qquad- \infty<\eta<\infty \tag{6.26}\]

for some fixed \(a,b>0\), and the subset \(\omega\), of \(\Omega_{H}\) of points (\(0\), \(\sigma^{2}\)) with \(\sigma^{2}<1/a\). Let \(\Lambda_{0}\), \(\Lambda_{1}\) be distributions over \(\omega\) and \(\Omega^{\prime}_{a,b}\) defined by the densities (Problem 6.37(i))

\[\lambda_{0}(\eta)=\frac{C_{0}}{(a+\eta^{2})^{n/2}}\]

and

\[\lambda_{1}(\eta)=\frac{C_{1}e^{(n/2)b^{2}\eta^{2}/(a+\eta^{2})}}{(a+\eta^{2} )^{n/2}}.\]

Straightforward calculation then shows (Problem 6.37(ii)) that the densities \(h_{0}\) and \(h_{1}\) of Theorem 6.7.2 become

\[h_{0}(x)=\frac{C_{0}e^{-(a/2)\sum x_{i}^{2}}}{\sqrt{\sum x_{i}^{2}}}\]

and

\[h_{1}(x)=\frac{C_{1}\exp\left(-\frac{a}{2}\sum x_{i}^{2}+\frac{b^{2}\left(\sum x _{i}\right)^{2}}{2\sum x_{i}^{2}}\right)}{\sqrt{\sum x_{i}^{2}}},\]

so that the Bayes test \(\varphi_{0}\) of Theorem 6.7.2 rejects when \(\bar{x}^{2}/\sum x_{i}^{2}>k\) and hence reduces to the two-sided \(t\)-test.

The condition of part (ii) of the theorem is clearly satisfied so that the \(t\)-test is both \(d\)- and \(\alpha\)-admissible against \(\Omega^{\prime}_{a,b}\).

When dealing with invariant tests, it is of particular interest to consider admissibility against invariant classes of alternatives. In the case of the two-sided test \(\varphi_{0}\), this means sets \(\Omega^{\prime}\) depending only on \(|\xi/\sigma|\). It was seen in Example 6.7.4 that \(\varphi_{0}\) is admissible against \(\Omega^{\prime}:|\xi/\sigma|\geq B\) for any \(B\), that is, against distant alternatives, and it follows from the test being UMP unbiased or from Example 6.7.4 (continued) that \(\varphi_{0}\), is admissible against \(\Omega^{\prime}:|\xi/\sigma|\leq A\) for any \(A>0\), that is, against alternatives close to \(H\). This leaves open the question whether \(\varphi_{0}\) is admissible against sets \(\Omega^{\prime}:0<A<|\xi/\sigma|<B<\infty\), which include neither nearby nor distant alter 

[MISSING_PAGE_FAIL:284]

difficulty tends to be the presence of additional (nuisance) means. These can often be eliminated by use of the following lemma.

**Lemma 6.7.1**: _For any given \(\sigma^{2}\) and \(M^{2}>\sigma^{2}\) there exists a distribution \(\Lambda_{\sigma}\) such that_

\[I(z)=\int\frac{1}{\sqrt{2\pi}\sigma}e^{-(1/2\sigma^{2})(z-\zeta)^{2}}d\Lambda_ {\sigma}(\zeta)\]

_is the normal density with mean zero and variance \(M^{2}\)._

Proof. Let \(\theta=\zeta/\sigma\), and let \(\theta\) be normally distributed with zero mean and variance \(\tau^{2}\). Then it is seen (Problem 6.38(ii)) that

\[I(z)=\frac{1}{\sqrt{2\pi}\sigma\sqrt{1+\tau^{2}}}\exp\left[-\frac{1}{2\sigma^ {2}(1+\tau^{2})}z^{2}\right].\]

The result now follows by letting \(\tau^{2}=(M^{2}/\sigma^{2})-1\), so that \(\sigma^{2}(1+\tau^{2})=M^{2}\).

**Example 6.7.6**: Let \(X_{1},\ldots,X_{m}\); \(Y_{1},\ldots,Y_{n}\) be samples from \(N(\xi,\sigma^{2})\) and \(N(\eta,\tau^{2})\), respectively, and consider the problem of testing \(H:\tau/\sigma=1\) against \(\tau/\sigma=\Delta>1\).

(i) Suppose first that \(\xi=\eta=0\). If \(\Lambda_{0}\) and \(\Lambda_{1}\) assign probability 1 to the points (\(\sigma_{0}\), \(\tau_{0}=\sigma_{0}\)) and (\(\sigma_{1}\), \(\tau_{1}=\Delta\sigma_{1}\)), respectively, the ratio \(h_{1}/h_{0}\) of Theorem 6.7.2 is proportional to

\[\exp\left\{-\frac{1}{2}\left[\left(\frac{1}{\Delta^{2}\sigma_{1}^{2}}-\frac{1} {\sigma_{0}^{2}}\right)\sum y_{j}^{2}-\left(\frac{1}{\sigma_{0}^{2}}-\frac{1}{ \sigma_{1}^{2}}\right)\sum x_{i}^{2}\right]\right\},\]

and for suitable choice of critical value and \(\sigma_{1}<\sigma_{0}\), the rejection region of the Bayes test reduces to

\[\frac{\sum y_{j}^{2}}{\sum x_{i}^{2}}>\frac{\Delta^{2}\sigma_{1}^{2}-\sigma_{0 }^{2}}{\sigma_{0}^{2}-\sigma_{1}^{2}}.\]

The values \(\sigma_{0}^{2}\) and \(\sigma_{1}^{2}\) can then be chosen to give this test any preassigned size \(\alpha\).

(ii) If \(\xi\) and \(\eta\) are unknown, then \(\bar{X}\), \(\bar{Y}\), \(S_{X}^{2}=\sum(X_{i}-\bar{X})^{2}\), \(S_{Y}^{2}=\sum(Y_{j}-\bar{Y})^{2}\) are sufficient statistics, and \(S_{X}^{2}\) and \(S_{Y}^{2}\) can be represented as \(S_{X}^{2}=\sum_{i=1}^{m-1}U_{i}^{2}\), \(S_{Y}^{2}=\sum_{j=1}^{n-1}V_{j}^{2}\), with the \(U_{i}\), \(V_{j}\) independent normal with means 0 and variances \(\sigma^{2}\) and \(\tau^{2}\) respectively.

To \(\sigma\) and \(\tau\) assign the distributions \(\Lambda_{0}\) and \(\Lambda_{1}\) of part (i) and conditionally, given \(\sigma\) and \(\tau\), let \(\xi\) and \(\eta\) be independently distributed according to \(\Lambda_{0\sigma}\), \(\Lambda_{0\tau}\), over \(\Omega_{H}\) and \(\Lambda_{1\sigma}\), \(\Lambda_{1\tau}\) over \(\Omega_{K}\), with these four conditional distributions determined from Lemma 6.7.1 in such a way that

\[\int\frac{\sqrt{m}}{\sqrt{2\pi}\sigma_{0}}e^{-(m/2\sigma_{0}^{2})(\bar{x}-\xi) ^{2}}\,d\,\Lambda_{0\sigma_{0}}(\xi)=\int\frac{\sqrt{m}}{\sqrt{2\pi}\sigma_{ 1}}e^{-(m/2\sigma_{1}^{2})(\bar{x}-\xi)^{2}}\,d\,\Lambda_{0\sigma_{1}}(\xi),\]and analogously for \(\eta\). This is possible by choosing the constant \(M^{2}\) of Lemma 6.7.1 greater than both \(\sigma_{0}^{2}\) and \(\sigma_{1}^{2}\). With this choice of priors, the contribution from \(\bar{x}\) and \(\bar{y}\) to the ratio \(h_{1}/h_{0}\) of Theorem 6.7.2 disappears, so that \(h_{1}/h_{0}\) reduces to the expression for this ratio in part (i), with \(\sum x_{i}^{2}\) and \(\sum y_{j}^{2}\) replaced by \(\sum(x_{i}-\bar{x})^{2}\) and \(\sum(y_{j}-\bar{y})^{2}\), respectively.

This approach applies quite generally in normal problems with nuisance means, provided the prior distribution of the variances \(\sigma^{2}\), \(\tau^{2}\),... assigns probability 1 to a bounded set, so that \(M^{2}\) can be chosen to exceed all possible values of these variances.

Admissibility questions have been considered not only for tests but also for confidence sets. These will not be treated here (but see Example 8.5.4); convenient entries to the literature are Cohen and Strawderman (1973) and Joshi (1982). For additional results, see Hooper (1982a) and Arnold (1984).

### Rank Tests

One of the basic problems of statistics is the two-sample problem of testing the equality of two distributions. A typical example is the comparison of a treatment with a control, where the hypothesis of no treatment effect is tested against the alternatives of a beneficial effect. This was considered in Chapter 5 under the assumption of normality, and the appropriate test was seen to be based on Student's \(t\). It was also shown that when approximate normality is suspected but the assumption cannot be trusted, one is led to replacing the \(t\)-test by its permutation analogue, which in turn can be approximated by the original \(t\)-test. For further details, see Chapter 17.

We shall consider the same problem below without, at least for the moment, making any assumptions concerning even the approximate form of the underlying distributions, assuming only that they are continuous. The observations then consist of samples \(X_{1}\),..., \(X_{m}\) and \(Y_{1}\),..., \(Y_{n}\) from two distributions with continuous cumulative distribution functions \(F\) and \(G\), and the problem becomes that of testing the hypothesis

\[H_{1}:G=F.\]

If the treatment effect is assumed to be additive, the alternatives are \(G(y)=F(y-\Delta)\). We shall here consider the more general possibility that the size of the effect may depend on the value of \(y\) (so that \(\Delta\) becomes a nonnegative function of \(y\)) and therefore test \(H_{1}\) against the one-sided alternatives that the \(Y\)'s are stochastically larger than the \(X\)'s,

\[K_{1}:G(z)\leq F(z)\ \ \ \mbox{for all}\ z,\ \ \ \mbox{and}\ \ \ G\neq F.\]

An alternative experiment that can be performed to test the effect of a treatment consists of the comparison of \(N\) pairs of subjects, which have been matched so as to eliminate as far as possible any differences not due to the treatment. One member of each pair is chosen at random to receive the treatment while the other serves as control. If the normality assumption of Section 5.10 is dropped and the pairs of subjects can be considered to constitute a sample, the observations \((X_{1},Y_{1}),\ldots,\) (\(X_{N},Y_{N}\)) are a sample from a continuous bivariate distribution \(F\). The hypothesis of no effect is then equivalent to the assumption that \(F\) is symmetric with respect to the line \(y=x\):

\[H_{2}:F(x,\,y)=F(y,x).\]

Another basic problem, which occurs in many different contexts, concerns the dependence or independence of two variables. In particular, if \((X_{1},\,Y_{1}),\ldots,\) (\(X_{N},\,Y_{N}\)) is a sample from a bivariate distribution \(F\), one will be interested in the hypothesis

\[H_{3}:F(x,\,y)=G_{1}(x)G_{2}(y)\]

that \(X\) and \(Y\) are independent, which was considered for normal distributions in Section 5.13. The alternatives of interest may, for example, be that \(X\) and \(Y\) are positively dependent. An alternative formulation results when \(x\), instead of being random, can be selected for the experiment. If the chosen values are \(x_{1}<\cdots<x_{N}\) and \(F_{i}\) denotes the distribution of \(Y\) given \(x_{i}\), the \(Y\)'s are independently distributed with continuous cumulative distribution functions \(F_{1},\ldots,F_{N}\). The hypothesis of independence of \(Y\) from \(x\) becomes

\[H_{4}:F_{1}=\cdots=F_{N},\]

while under the alternatives of positive regression dependence the variables \(Y_{i}\) are stochastically increasing with \(i\).

In these and other similar problems, invariance reduces the data so completely that the actual values of the observations are discarded and only certain order relations between different groups of variables are retained. It is nevertheless possible on this basis to test the various hypotheses in question, and the resulting tests frequently are nearly as powerful as the standard normal tests. We shall now carry out this reduction for the four problems above.

The two-sample problem of testing \(H_{1}\) against \(K_{1}\) remains invariant under the group \(G\) of all transformations

\[x_{i}^{\prime}=\rho(x_{i}),\quad y_{j}^{\prime}=\rho(y_{j})\quad\quad(i=1, \ldots,m,\quad j=1,\ldots,n)\]

such that \(\rho\) is continuous and strictly increasing. This follows from the fact that these transformations preserve both the continuity of a distribution and the property of two variables being either identically distributed or one being stochastically larger than the other. As was seen (with a different notation) in Example 6.2.3, a maximal invariant under \(G\) is the set of ranks

\[(R^{\prime};\,S^{\prime})=(R_{1}^{\prime},\ldots,\,R_{m}^{\prime};\,S_{1}^{ \prime},\ldots,\,S_{n}^{\prime})\]of \(X_{1},\ldots,X_{m};Y_{1},\ldots,Y_{n}\) in the combined sample. Since the distribution of \((R^{\prime}_{1},\ldots,R^{\prime}_{m};S^{\prime}_{1},\ldots,S^{\prime}_{n})\) is symmetric in the first \(m\) and in the last \(n\) variables for all distributions \(F\) and \(G\), a set of sufficient statistics for \((R^{\prime},S^{\prime})\) is the set of the \(X\)-ranks and that of the \(Y\)-ranks without regard to the subscripts of the \(X\)'s and \(Y\)'s. This can be represented by the ordered \(X\)-ranks and \(Y\)-ranks

\[R_{1}<\cdots<R_{m}\ \ \ {\rm and}\ \ \ S_{1}<\cdots<S_{n},\]

and therefore by one of these sets alone since each of them determines the other. Any invariant test is thus a _rank test_, that is, it depends only on the ranks of the observations, for example, on \((S_{1},\ldots,S_{n})\).

That almost invariant tests are equivalent to invariant ones in the present context was shown first by Bell (1964). A streamlined and generalized version of his approach is given by Berk and Bickel (1968) and Berk (1970), who also show that the conclusion of Theorem 6.5.3 remains valid in this case.

To obtain a similar reduction for \(H_{2}\), it is convenient first to make the transformation \(Z_{i}=Y_{i}-X_{i}\), \(W_{i}=X_{i}+Y_{i}\). The pairs of variables \((Z_{i},W_{i})\) are then again a sample from a continuous bivariate distribution. Under the hypothesis this distribution is symmetric with respect to the \(w\)-axis, while under the alternatives the distribution is shifted in the direction of the positive \(z\)-axis. The problem is unchanged if all the \(w\)'s are subjected to the same transformation \(w^{\prime}_{i}=\lambda(w_{i})\), where \(\lambda\) is \(1:1\) and has at most a finite number of discontinuities, and \((Z_{1},\ldots,Z_{N})\) constitutes a maximal invariant under this group. [Cf. Problem 6.2(ii).]

The \(Z\)'s are a sample from a continuous univariate distribution \(D\), for which the hypothesis of symmetry with respect to the origin,

\[H^{\prime}_{2}:D(z)+D(-z)=1\ \ \ {\rm for\ all}\ z,\]

is to be tested against the alternatives that the distribution is shifted toward positive \(z\)-values. This problem is invariant under the group \(G\) of all transformations

\[z^{\prime}_{i}=\rho(z_{i})\ \ \ \ \ (i=1,\ldots,N)\]

such that \(\rho\) is continuous, odd, and strictly increasing. If \(z_{i_{1}},\ldots,z_{i_{n}}<0<z_{j_{1}},\ldots,z_{j_{n}}\), where \(i_{1}<\cdots<i_{m}\) and \(j_{1}<\cdots<j_{n}\), let \(s^{\prime}_{1},\ldots,s^{\prime}_{n}\) denote the ranks of \(z_{j_{i}},\ldots,z_{j_{n}}\), among the absolute values \(|z_{1}|,\ldots,|z_{N}|\), and \(r^{\prime}_{1},\ldots,r^{\prime}_{m}\) the ranks of \(|z_{i_{1}}|,\ldots,|z_{i_{m}}|\) among \(|z_{1}|,\ldots,|z_{N}|\). The transformations \(\rho\) preserve the sign of each observation, and hence in particular also the numbers \(m\) and \(n\). Since \(\rho\) is a continuous, strictly increasing function of \(|z|\), it leaves the order of the absolute values invariant and therefore the ranks \(r^{\prime}_{i}\) and \(s^{\prime}_{j}\). To see that the latter are maximal invariant, let \((z_{1},\ldots,z_{N})\) and \((z^{\prime}_{1},\ldots,z^{\prime}_{N})\) be two sets of points with \(m^{\prime}=m\), \(n^{\prime}=n\), and the same \(r^{\prime}_{i}\) and \(s^{\prime}_{j}\). There exists a continuous, strictly increasing function on the positive real axis such that \(|z^{\prime}_{i}|=\rho(|z_{i}|)\) and \(\rho(0)=0\). If \(\rho\) is defined for negative \(z\) by \(\rho(-z)=-\rho(z)\), it belongs to \(G\) and \(z^{\prime}_{i}=\rho(z_{i})\) for all \(i\), as was to be proved. As in the preceding problem, sufficiency permits the further reduction to the ordered ranks \(r_{1}<\cdots<r_{m}\) and \(s_{1}<\cdots<s_{n}\). This retains the information for the rank of each absolute value whether it belongs to a positive or negative observation, but not with which positive or negative observation it is associated.

The situation is very similar for the hypotheses \(H_{3}\) and \(H_{4}\). The problem of testing for independence in a bivariate distribution against the alternatives of positive dependence is unchanged if the \(X_{i}\) and \(Y_{i}\) are subjected to transformations \(X_{i}^{\prime}=\rho(X_{i})\), \(Y_{i}^{\prime}=\lambda(Y_{i})\) such that \(\rho\) and \(\lambda\) are continuous and strictly increasing. This leaves as maximal invariant the ranks \((R_{1}^{\prime},\ldots,R_{N}^{\prime})\) of \((X_{1},\ldots,X_{N})\) among the \(X\)'s and the ranks \((S_{1}^{\prime},\ldots,S_{N}^{\prime})\) of \((Y_{1},\ldots,Y_{N})\) among the \(Y\)'s. The distribution of \((R_{1}^{\prime},S_{1}^{\prime})\), \(\ldots,(R_{N}^{\prime},S_{N}^{\prime})\) is symmetric in these \(N\) pairs for all distributions of \((X,Y)\). It follows that a sufficient statistic is \((S_{1},\ldots,S_{N})\) where \((1,S_{1}),\ldots,(N,S_{N})\) is a permutation of \((R_{1}^{\prime},S_{1}^{\prime})\), \(\ldots,(R_{N}^{\prime},S_{N}^{\prime})\) and where therefore \(S_{i}\) is the rank of the variable \(Y\) associated with the \(i\)th smallest \(X\).

The hypothesis \(H_{4}\) that \(Y_{1},\ldots,Y_{n}\) constitutes a sample is to be tested against the alternatives \(K_{4}\) that the \(Y_{i}\) are stochastically increasing with \(i\). This problem is invariant under the group of transformations \(y_{i}^{\prime}=\rho(y_{i})\) where \(\rho\) is continuous and strictly increasing. A maximal invariant under this group is the set of ranks \(S_{1},\ldots,S_{N}\) of \(Y_{1},\ldots,Y_{N}\).

Some invariant tests of the hypotheses \(H_{1}\) and \(H_{2}\) will be considered in the next two sections. Corresponding results concerning \(H_{3}\) and \(H_{4}\) are given in Problems 6.62-6.64.

### The Two-Sample Problem

The problem of testing the two-sample hypothesis \(H:G=F\) against the one-sided alternatives \(K\) that the \(Y\)'s are stochastically larger than the \(X\)'s is reduced by the principle of invariance to the consideration of tests based on the ranks \(S_{1}<\cdots<S_{n}\) of the \(Y\)'s. The specification of the \(S_{i}\) is equivalent to specifying for each of the \(N=m+n\) positions within the combined sample (the smallest, the next smallest, etc.) whether it is occupied by an \(x\) or a \(y\). Since for any set of observations \(n\) of the \(N\) positions are occupied by \(y\)'s and since the \({N\choose n}\) possible assignments of \(n\) positions to the \(y\)'s are all equally likely when \(G=F\), the joint distribution of the \(S_{i}\) under \(H\) is

\[P\{S_{1}=s_{1},\ldots,S_{n}=s_{n}\}=1\Big{/}{N\choose n} \tag{6.28}\]

for each set \(1\leq s_{1}<s_{2}<\cdots<s_{n}\leq N\). Any rank test of \(H\) of size

\[\alpha=k\Big{/}{N\choose n}\]

therefore has a rejection region consisting of exactly \(k\) points \((s_{1},\ldots,s_{n})\).

For testing \(H\) against \(K\) there exists no UMP rank test, and hence no UMP invariant test. This follows, for example, from a consideration of two of the standard tests for this problem, since each is most powerful among all rank tests against some alternative. The two tests in question have rejection regions of the form

\[h(s_{1})+\cdots+h(s_{n})>C. \tag{6.29}\]

One, the Wilcoxon _two-sample test_, is obtained from (6.29) by letting \(h(s)=s\), so that it rejects \(H\) when the sum of the \(y\)-ranks is too large. We shall show below that for sufficiently small \(\Delta\), this is most powerful against the alternatives that \(F\) is the logistic distribution \(F(x)=1/(1+e^{-x})\), and that \(G(y)=F(y-\Delta)\). The other test, the _normal scores test_, has the rejection region (6.29) with \(h(s)=E(W_{(s)})\), where \(W_{(1)}<\cdots<W_{(N)}\), is an ordered sample of size \(N\) from a standard normal distribution.5 This is most powerful against the alternatives that \(F\) and \(G\) are normal distributions with common variance and means \(\xi\) and \(\eta=\xi+\Delta\), when \(\Delta\) is sufficiently small.

Footnote 5: Tables of the expected order statistics from a normal distribution are given in _Biometrika Tables for Statisticians_, Vol. 2, Cambridge U. P., 1972, Table 9. For additional references, see David (1981, Appendix, Section 3.2).

To prove that these tests have the stated properties it is necessary to know the distribution of (\(S_{1}\),..., \(S_{n}\)) under the alternatives. If \(F\) and \(G\) have densities \(f\) and \(g\) such that \(f\) is positive whenever \(g\) is, the joint distribution of the \(S_{i}\) is given by

\[P\{S_{1}=s_{1},\ldots,\,S_{n}=s_{n}\}=E\bigg{[}\frac{g(V_{(s_{1})})}{f(V_{(s_{1 })})}\cdots\frac{g(V_{(s_{n})})}{f(V_{(s_{n})})}\bigg{]}\bigg{/}\binom{N}{n}, \tag{6.30}\]

where \(V_{(1)}<\cdots<V_{(N)}\) is an ordered sample of size \(N\) from the distribution \(F\). (See Problem 6.44.) Consider in particular the translation (or shift) alternatives

\[g(y)=f(y-\Delta),\]

and the problem of maximizing the power for small values of \(\Delta\). Suppose that \(f\) is differentiable and that the probability (6.30), which is now a function of \(\Delta\), can be differentiated with respect to \(\Delta\) under the expectation sign. The derivative of (6.30) at \(\Delta=0\) is then

\[\frac{\partial}{\partial\Delta}P_{\Delta}\{S_{1}=s_{1},\ldots,S_{n}=S_{n}\} \bigg{|}_{\Delta=0}=-\sum_{i=1}^{n}E\bigg{[}\frac{f^{\prime}(V_{(s_{i})})}{f( V_{(s_{i})})}\bigg{]}\bigg{/}\binom{N}{n}.\]

Since under the hypothesis the probability of any ranking is given by (6.28), it follows from the Neyman-Pearson Lemma in the extended form of Theorem 3.6.1, that the derivative of the power function at \(\Delta=0\) is maximized by the rejection region

\[-\sum_{i=1}^{n}E\bigg{[}\frac{f^{\prime}(V_{(s_{i})})}{f(V_{(s_{i})})}\bigg{]} >C. \tag{6.31}\]The same test maximizes the power itself for sufficiently small \(\Delta\). To see this let \(s\) denote a general rank point \((s_{1},\ldots,s_{n})\), and denote by \(s^{(j)}\) the rank point giving the \(j\)th largest value to the left-hand side of (6.31). If

\[\alpha=k\Big{/}\binom{N}{n},\]

the power of the test is then

\[\beta(\Delta)=\sum_{j=1}^{k}P_{\Delta}(s^{(j)})=\sum_{j=1}^{k}\Biggl{[}\frac{1 }{\binom{N}{n}}+\Delta\frac{\partial}{\partial\Delta}P_{\Delta}(s^{(j)}) \Biggr{|}_{\Delta=0}+\cdots\Biggr{]}.\]

Since there is only a finite number of points \(s\), there exists for each \(j\) a number \(\Delta_{j}>0\) such that the point \(s^{(j)}\) also gives the \(j\)th largest value to \(P_{\Delta}(s)\) for all \(\Delta<\Delta_{j}\). If \(\Delta\) is less than the smallest of the numbers

\[\Delta_{j},\qquad j=1,\ldots,\binom{N}{n},\]

the test also maximizes \(\beta(\Delta)\).

If \(f(x)\) is the normal density \(N(\xi,\sigma^{2})\), then

\[-\frac{f^{\prime}(x)}{f(x)}=-\frac{d}{dx}\log f(x)=\frac{x-\xi}{\sigma^{2}},\]

and the left-hand side of (6.31) becomes

\[\sum E\frac{V_{(s_{i})}-\xi}{\sigma^{2}}=\frac{1}{\sigma}\sum E(W_{(s_{i})}),\]

where \(W_{(1)}<\cdots<W_{(N)}\) is an ordered sample from \(N(0,1)\). The test that maximizes the power against these alternatives (for sufficiently small \(\Delta\)) is therefore the normal scores test.

In the case of the logistic distribution,

\[F(x)=\frac{1}{1+e^{-x}},\qquad f(x)=\frac{e^{-x}}{(1+e^{-x})^{2}},\]

and hence

\[-\frac{f^{\prime}(x)}{f(x)}=2F(x)-1.\]

The locally most powerful rank test therefore rejects when \(\sum E[F(V_{(s_{i})})]>C\). If \(V\) has the distribution \(F\), then \(U=F(V)\) is uniformly distributed over \((0,1)\) (Problem 3.22). The rejection region can therefore be written as \(\sum E(U_{(s_{i})})>C\)where \(U_{(1)}<\cdots<U_{(N)}\) is an ordered sample of size \(N\) from the uniform distribution \(U(0,1)\). Since \(E(U_{(s_{i})})=s_{i}/(N+1)\), the test is seen to be the Wilcoxon test.

Both the normal scores test and the Wilcoxon test are unbiased against the one-sided alternatives \(K\). In fact, let \(\phi\) be the critical function of any test determined by (6.29) with \(h\) nondecreasing. Then \(\phi\) is nondecreasing in the \(y\)'s and the probability of rejection is \(\alpha\) for all \(F=G\). By Lemma 5.9.1 the test is therefore unbiased against all alternatives of \(K\).

It follows from the unbiasedness properties of these tests that the most powerful invariant tests in the two cases considered are also most powerful against their respective alternatives among all tests that are invariant and unbiased. The nonexistence of a UMP test is thus not relieved by restricting the tests to be unbiased as well as invariant. Nor does the application of the unbiasedness principle alone lead to a solution, as was seen in the discussion of permutation tests in Section 5.9. With the failure of these two principles, both singly and in conjunction, the problem is left not only without a solution but even without a formulation. A possible formulation (stringency) will be discussed in Chapter 8. However, the determination of a most stringent test for the two-sample hypothesis is an open problem.

For testing \(H:G=F\) against the two-sided alternatives that the \(Y\)'s are either stochastically smaller or larger than the \(X\)'s, two-sided versions of the rank tests of this section can be used. In particular, suppose that \(h\) is increasing and that \(h(s)+h(N+1-s)\) is independent of \(s\), as is the case for the Wilcoxon and normal scores statistics. Then under \(H\), the statistic \(\Sigma h(s_{j})\) is symmetrically distributed about \(n\,\Sigma_{i=1}^{N}h(i)/N=\mu\), and (6.29) suggests the rejection region

\[\left|\sum h(s_{j})-\mu\right|=\frac{1}{N}\left|m\sum_{j=1}^{n}h(s_{j})-n\sum _{i=1}^{m}h(r_{i})\right|>C.\]

The theory here is still less satisfactory than in the one-sided case. These tests need not even be unbiased [Sugiura (1965)], and it is not known whether they are admissible within the class of all rank tests. On the other hand, the relative asymptotic efficiencies are the same as in the one-sided case.

The two-sample hypothesis \(G=F\) can also be tested against the general alternatives \(G\neq F\). This problem arises in deciding whether two products, two sets of data, or the like can be pooled when nothing is known about the underlying distributions. Since the alternatives are now unrestricted, the problem remains invariant under all transformations \(x_{i}^{\prime}=f(x_{i})\), \(y_{j}^{\prime}=f(y_{j})\), \(i=1\),..., \(m\), \(j=1\),..., \(n\), such that \(f\) has only a finite number of discontinuities. There are no invariants under this group, so that the only invariant test is \(\phi(x,\,y)\equiv\alpha\). This is however not admissible, since there do exist tests of \(H\) that are strictly unbiased against all alternatives \(G\neq F\) (Problem 6.56). One of the tests most commonly employed for this problem is the _Kalmogorov-Smirnov test_. Let the _empirical distribution functions_ of the two samples be defined by

\[S_{x_{1},\ldots,x_{m}}(z)=\frac{a}{m},\qquad S_{y_{1},\ldots,y_{m}}(z)=\frac{ b}{n},\]where \(a\) and \(b\) are the numbers of \(x\)'s and \(y\)'s less or equal to \(z\) respectively. Then \(H\) is rejected according to this test when

\[\sup_{z}\big{|}S_{x_{1},\ldots,x_{n}}(z)-S_{y_{1},\ldots,y_{n}}(z)\big{|}>C.\]

Accounts of the theory of this and related tests are given, for example, in Durbin (1973), Serfling (1980), Gibbons and Chakraborti (1992) and Hajek, Sidak, and Sen (1999).

Two-sample rank tests are distribution-free for testing \(H:G=F\) but not for the nonparametric Behrens-Fisher situation of testing \(H:\eta=\xi\) when the \(X\)'s and \(Y\)'s are samples from \(F((x-\xi)/\sigma)\) and \(F((y-\eta)/\tau)\) with \(\sigma\), \(\tau\) unknown. A detailed study of the effect of the difference in scales on the levels of the Wilcoxon and normal scores tests is provided by Pratt (1964).

### The Hypothesis of Symmetry

When the method of paired comparisons is used to test the hypothesis of no treatment effect, the problem was seen in Section 6.8 to reduce through invariance to that of testing the hypothesis

\[H_{2}^{\prime}:D(z)+D(-z)=1\mbox{ for all }z,\]

which states that the distribution \(D\) of the differences \(Z_{i}=Y_{i}-X_{i}\) (\(i=1,\ldots,N\)) is symmetric with respect to the origin. The distribution \(D\) can be specified by the triple (\(\rho\), \(F\), \(G\)) where

\[\rho=P\{Z\leq 0\}, F(z)=P\{|Z|\leq z\mid Z>0\},\] \[G(z)=P\{Z\leq z\mid Z>0\},\]

and the hypothesis of symmetry with respect to the origin then becomes

\[H:p=\tfrac{1}{2},\,G=F.\]

Invariance and sufficiency were shown to reduce the data to the ranks \(S_{1}<\cdots<S_{n}\) of the positive \(Z\)'s among the absolute values \(|Z_{1}|,\ldots,|Z_{N}|\). The probability of \(S_{1}=s_{1},\ldots,S_{n}=s_{n}\) is the probability of this event given that there are \(n\) positive observations multiplied by the probability that the number of positive observations is \(n\). Hence

\[P\{S_{1}=s_{1},\ldots,S_{n}=s_{n}\}\] \[\qquad={N\choose n}(1-\rho)^{n}\rho^{N-n}\,P_{F,G}\{S_{1}=s_{1}, \ldots,S_{n}=s_{n}\mid n\},\]where the second factor is given by (6.30). Under \(H\), this becomes

\[P\{S_{1}=s_{1},\ldots,S_{n}=s_{n}\}=\frac{1}{2^{N}}\]

for each of the

\[\sum_{n=0}^{N}\binom{N}{n}=2^{N}\]

\(n\)-tuples \((s_{1},\ldots,s_{n})\) satisfying \(1\leq s_{1}<\cdots<s_{n}\leq N\). Any rank test of size \(\alpha=k/2^{N}\) therefore has a rejection region containing exactly \(k\) such points \((s_{1},\ldots,s_{n})\).

The alternatives \(K\) of a beneficial treatment effect are characterized by the fact that the variable \(Z\) being sampled is stochastically larger than some random variable which is symmetrically distributed about \(0\). It is again suggestive to use rejection regions of the form \(h(s_{1})+\cdots+h(s_{n})>C\), where however \(n\) is no longer a constant as it was in the two-sample problem, but depends on the observations. Two particular cases are the _Wilcoxon one-sample test_, which is obtained by putting \(h(s)=s\), and the analogue of the normal scores test with \(h(s)=E(W_{(s)})\) where \(W_{(1)}<\cdots<W_{(N)}\) are the ordered values of \(|V_{1}|,\ldots,|V_{N}|\), the \(V\)'s being a sample from \(N(0,1)\). The \(W\)'s are therefore an ordered sample of size \(N\) from a distribution with density \(\sqrt{2/\pi}e^{-w^{2}/2}\) for \(w\geq 0\).

As in the two-sample problem, it can be shown that each of these tests is most powerful (among all invariant tests) against certain alternatives, and that they are both unbiased against the class \(K\). Their asymptotic efficiencies relative to the \(t\)-test for testing that the mean of \(Z\) is zero have the same values \(3/\pi\) and \(1\) as the corresponding two-sample tests, when the distribution of \(Z\) is normal.

In certain applications, for example, when the various comparisons are made under different experimental conditions or by different methods, it may be unrealistic to assume that the variables \(Z_{1},\ldots,Z_{N}\) have a common distribution. Suppose instead that the \(Z_{i}\) are still independently distributed but with arbitrary continuous distributions \(D_{i}\). The hypothesis to be tested is that each of these distributions is symmetric with respect to the origin.

This problem remains invariant under all transformations \(z_{i}^{\prime}=f_{i}(z_{i})\)\(i=1,\ldots,N\), such that each \(f_{i}\) is continuous, odd, and strictly increasing. A maximal invariant is then the number \(n\) of positive observations, and it follows from Example 6.5.1 that there exists a UMP invariant test, the _sign test_, which rejects when \(n\) is too large. This test reflects the fact that the magnitude of the observations or of their absolute values can be explained entirely in terms of the spread of the distributions \(D_{i}\), so that only the signs of the \(Z\)'s are relevant.

Frequently, it seems reasonable to assume that the \(Z\)'s are identically distributed, but the assumption cannot be trusted. One would then prefer to use the information provided by the ranks \(s_{i}\) but require a test which controls the probability of false rejection even when the assumption fails. As is shown by the following lemma, this requirement is in fact satisfied for every (symmetric) rank test. Actually, the lemma will not require even the independence of the \(Z\)'s; it will show that any symmetric rank test continues to correspond to the stated level of significance provided only the treatment is assigned at random within each pair.

**Lemma 6.10.1**: _Let \(\phi(z_{1},\ldots,z_{N})\) be symmetric in its \(N\) variables and such that_

\[E_{D}\phi(Z_{1},\ldots,Z_{N})=\alpha \tag{6.32}\]

_when the \(Z\)'s are a sample from any continuous distribution \(D\) which is symmetric with respect to the origin. Then_

\[E\phi(Z_{1},\ldots,Z_{N})=\alpha \tag{6.33}\]

_if the joint distribution of the \(Z\)'s is unchanged under the \(2^{N}\) transformations \(Z_{1}^{\prime}=\pm Z_{1},\ldots,Z_{N}^{\prime}=\pm Z_{N}\)._

Proof. Condition (6.32) implies

\[\sum_{(j_{1},\ldots,j_{N})}\sum\frac{\phi(\pm z_{j_{1}},\ldots,\pm z_{j_{N}})} {2^{N}\cdot N!}=\alpha\ \ \ {\rm a.e.}, \tag{6.34}\]

where the outer summation extends over all \(N!\) permutations \((j_{1},\ldots,j_{N})\) and the inner one over all \(2^{N}\) possible choices of the signs \(+\) and \(-\). This is proved exactly as was Theorem 5.8.1. If in addition \(\phi\) is symmetric, (6.34) implies

\[\sum\frac{\phi(\pm z_{1},\ldots,\pm z_{N})}{2^{N}}=\alpha. \tag{6.35}\]

Suppose that the distribution of the \(Z\)'s is invariant under the \(2^{N}\) transformations in question. Then the conditional probability of any sign combination of \(Z_{1},\ldots,Z_{N}\) given \(|Z_{1}|,\ldots,|Z_{N}|\) is \(1/2^{N}\). Hence (6.35) is equivalent to

\[E[\phi(Z_{1},\ldots,Z_{N})\ |\ |Z_{1}|,\ldots,|Z_{N}|]=\alpha\ \ \ {\rm a.e.}, \tag{6.36}\]

and this implies (6.33) which was to be proved.

The tests discussed above can be used to test symmetry about any known value \(\theta_{0}\) by applying them to the variables \(Z_{i}-\theta_{0}\). The more difficult problem of testing for symmetry about an unknown point \(\theta\) will not be considered here. Tests of this hypothesis are discussed, among others, by Antille et al. (1982), Bhattacharya et al. (1982), Boos (1982), and Koziol (1983).

As will be seen in Section 13.2.1, the one-sample \(t\)-test is not robust against dependence. Unfortunately, this is also true-although to a somewhat lesser extent--of the sign and one-sample Wilcoxon tests [Gastwirth and Rubin (1971)].

### 6.11 Equivariant Confidence Sets

Confidence sets for a parameter \(\theta\) in the presence of nuisance parameters \(\vartheta\) were discussed in Chapter 5 (Sections 5.4 and 5.5) under the assumption that \(\theta\) is real-valued. The correspondence between acceptance regions \(A(\theta_{0})\) of the hypotheses \(H(\theta_{0}):\theta=\theta_{0}\) and confidence sets \(S(x)\) for \(\theta\) given by (5.33) and (5.34) is, however, independent of this assumption; it is valid regardless of whether \(\theta\) is real-valued, vector-valued, or possibly a label for a completely unknown distribution function (in the latter case, confidence intervals become confidence bands for the distribution function). This duality, which can be summarized by the relationship

\[\theta\in S(x)\ \ \ \mbox{if and only if}\ \ \ x\in A(\theta), \tag{6.37}\]

was the basis for deriving uniformly most accurate and uniformly most accurate unbiased confidence sets. In the present section, it will be used to obtain uniformly most accurate equivariant confidence sets.

We begin by defining equivariance for confidence sets. Let \(G\) be a group of transformations of the variable \(X\) preserving the family of distributions \(\{P_{\theta,\vartheta},\,(\theta,\,\vartheta)\in\Omega\}\) and let \(\bar{G}\) be the induced group of transformations of \(\Omega\). If \(\bar{g}(\theta,\,\vartheta)=(\theta^{\prime},\,\vartheta^{\prime})\), we shall suppose that \(\theta^{\prime}\) depends only on \(\bar{g}\) and \(\theta\) and not on \(\vartheta\), so that \(\bar{g}\) induces a transformation in the space of \(\theta\). In order to keep the notation from becoming unnecessarily complex, it will then be convenient to write also \(\theta^{\prime}=\bar{g}\theta\). For each transformation \(g\in G\), denote by \(g^{*}\) the transformation acting on sets \(S\) in \(\theta\)-space and defined by

\[g^{*}S=\{\bar{g}\theta:\theta\in S\}, \tag{6.38}\]

so that \(g^{*}S\) is the set obtained by applying the transformation \(\bar{g}\) to each point \(\theta\) of \(S\). The invariance argument of Section 1.5 then suggests restricting consideration to confidence sets satisfying

\[g^{*}S(x)=S(gx)\ \ \ \ \ \mbox{for all}\ \ \ x\in{\cal X},\ \ \ g\in G. \tag{6.39}\]

We shall say that such confidence sets are _equivariant_ under \(G\). This terminology is preferable to the older term invariance which creates the impression that the confidence sets remain unchanged under the transformation \(X^{\prime}=gX\). If the transformation \(g\) is interpreted as a change of coordinates, (6.39) means that the confidence statement does not depend on the coordinate system used to express the data. The statement that the transformed parameter \(\bar{g}\theta\) lies in \(S(gx)\) is equivalent to stating that \(\theta\in g^{*-1}S(gx)\), which is equivalent to the original statement \(\theta\in S(x)\) provided (6.39) holds.

**Example 6.11.1**: Let \(X\), \(Y\) be independently normally distributed with means \(\xi\), \(\eta\) and unit variance, and let \(G\) be the group of all rigid motions of the plane, which is generated by all translations and orthogonal transformations. Here \(\bar{g}=g\) for all \(g\in G\). An example of an equivariant class of confidence sets is given by\[S(x,y)=\left\{(\xi,\eta):(x-\xi)^{2}+(y-\eta)^{2}\leq C\right\},\]

the class of circles with radius \(\sqrt{C}\) and center \((x,y)\). The set \(g^{*}S(x,y)\) is the set of all points \(g(\xi,\eta)\) with \((\xi,\eta)\in S(x,y)\) and hence is obtained by subjecting \(S(x,y)\) to the rigid motion \(g\). The result is the circle with radius \(\sqrt{C}\) and center \(g(x,y)\), and (6.39) is therefore satisfied.

In accordance with the definitions given in Chapters 3 and 5, a class of confidence sets for \(\theta\) will be said to be _uniformly most accurate equivariant_ at confidence level \(1-\alpha\) if among all equivariant classes of sets \(S(x)\) at that level it minimizes the probability

\[P_{\theta,\vartheta}\{\theta^{\prime}\in S(X)\}\quad\mbox{ for all }\ \ \theta^{\prime}\neq\theta.\]

In order to derive confidence sets with this property from families of UMP invariant tests, we shall now investigate the relationship between equivariance of confidence sets and invariance of the associated tests.

Suppose that for each \(\theta_{0}\) there exists a group of transformations \(G_{\theta_{0}}\) which leaves invariant the problem of testing \(H(\theta_{0}):\theta=\theta_{0}\), and denote by \(G\) the group of transformations generated by the totality of groups \(G_{\theta}\).

**Lemma 6.11.1**: (i) _Let \(S(x)\) be any class of confidence sets that is equivariant under \(G\), and let \(A(\theta)=\{x:\theta\in S(x)\}\); then the acceptance region \(A(\theta)\) is invariant under \(G_{\theta}\) for each \(\theta\)._

(ii) _If in addition, for each \(\theta_{0}\) the acceptance region \(A(\theta_{0})\) is UMP invariant for testing \(H(\theta_{0})\) at level \(\alpha\), the class of confidence sets \(S(x)\) is uniformly most accurate among all equivariant confidence sets at confidence level \(1-\alpha\)._

Proof. (i): Consider any fixed \(\theta\), and let \(g\in G_{\theta}\). Then

\[\begin{array}{l}gA(\theta)=\{gx:\theta\in S(x)\}=\{x:\theta\in S(g^{-1}x)\} =\{x:\theta\in g^{*-1}S(x)\}\\ \phantom{gA(\theta)=}=\{x:\bar{g}\theta\in S(x)\}=\{x:\theta\in S(x)\}=A( \theta).\end{array}\]

Here the third equality holds because \(S(x)\) is equivariant, and the fifth one because \(g\in G_{\theta}\) and therefore \(\bar{g}\theta=\theta\).

(ii): If \(S^{\prime}(x)\) is any other equivariant class of confidence sets at the prescribed level, the associated acceptance regions \(A^{\prime}(\theta)\) by (i) define invariant tests of the hypotheses \(H(\theta)\). It follows that these tests are uniformly at most as powerful as those with acceptance regions \(A(\theta)\) and hence that

\[P_{\theta,\vartheta}\{\theta^{\prime}\in S(X)\}\leq P_{\theta,\vartheta}\{ \theta^{\prime}\in S^{\prime}(X)\}\quad\mbox{ for all }\ \ \theta^{\prime}\neq\theta\]

as was to be proved.

It is an immediate consequence of the lemma that if UMP invariant acceptance regions \(A(\theta)\) have been found for each hypothesis \(H(\theta)\) (invariant with respect to \(G_{\theta}\)), and if the confidence sets \(S(x)=\{\theta:x\in A(\theta)\}\) are equivariant under \(G\), then they are uniformly most accurate equivariant.

[MISSING_PAGE_FAIL:298]

which are distribution-free in the sense that the coverage probability is independent of the true \(f\). The hypothesis \(H(\Delta_{0}):\Delta=\Delta_{0}\) can be tested, for example, by means of the Wilcoxon test applied to the observations \(X_{i}\), \(Y_{j}-\Delta_{0}\), and confidence sets for \(\Delta\) can then be obtained by the usual inversion process. The resulting confidence intervals are of the form \(D_{(k)}<\Delta<D_{(mn+1-k)}\) where \(D_{(1)}<\cdots<D_{(mn)}\) are the \(mn\) ordered differences \(Y_{j}-X_{i}\). [For details see Problem 6.54 and for fuller accounts nonparametric books such as Randles and Wolfe (1979), Gibbons and Chakraborti (1992) and Lehmann (1998).] By their construction, these intervals have coverage probability \(1-\alpha\), which is independent of \(f\). However, the invariance considerations of Sections 6.8 and 6.9 do not apply. The hypothesis \(H(\Delta_{0})\) is invariant under the transformations \(X_{i}^{\prime}=\rho(X_{i})\), \(Y_{j}^{\prime}=\rho(Y_{j}-\Delta_{0})+\Delta_{0}\) with \(\rho\) continuous and strictly increasing, but the shift model, and hence the problem under consideration, is not invariant under these transformations.

### Average Smallest Equivariant Confidence Sets

In the examples considered so far, the invariance and equivariance properties of the confidence sets corresponded to invariant properties of the associated tests. In the following examples this is no longer the case.

**Example 6.12.1**: Let \(X_{1}\),..., \(X_{n}\), be a sample from \(N(\xi,\sigma^{2})\), and consider the problem of estimating \(\sigma^{2}\).

The model is invariant under translations \(X_{i}^{\prime}=X_{i}+a\), and sufficiency and invariance reduce the data to \(S^{2}=\sum(X_{i}-\bar{X})^{2}\). The problem of estimating \(\sigma^{2}\) by confidence sets also remains invariant under scale changes \(X_{i}^{\prime}=bX_{i}\), \(S^{\prime}=bS\), \(\sigma^{\prime}=b\sigma\) (\(0<b\)), although these do not leave the corresponding problem of testing the hypothesis \(\sigma=\sigma_{0}\) invariant. (Instead, they leave invariant the _family_ of these testing problems, in the sense that they transform one such hypothesis into another.) The totality of equivariant confidence sets based on \(S\) is given by

\[\frac{\sigma^{2}}{S^{2}}\in A, \tag{6.41}\]

where \(A\) is any fixed set on the line satisfying

\[P_{\sigma=1}\left(\frac{1}{S^{2}}\in A\right)=1-\alpha. \tag{6.42}\]

That any set \(\sigma^{2}\in S^{2}\cdot A\) is equivariant is obvious. Conversely, suppose that \(\sigma^{2}\in C(S^{2})\) is an equivariant family of confidence sets for \(\sigma^{2}\). Then \(C(S^{2})\) must satisfy \(b^{2}C(S^{2})=C(b^{2}S^{2})\) and hence \[\sigma^{2}\in C(S^{2})\ \ \ \text{if and only if}\ \ \ \frac{\sigma^{2}}{S^{2}} \in\frac{1}{S^{2}}C(S^{2})=C(1),\]

which establishes (6.41) with \(A=C(1)\).

Among the confidence sets (6.41) with \(A\) satisfying (6.42) there does not exist one that uniformly minimizes the probability of covering false values (Problem 6.75). Consider instead the problem of determining the confidence sets that are physically smallest in the sense of having minimum Lebesgue measure. This requires minimizing \(\int_{A}dv\) subject to (6.42). It follows from the Neyman-Pearson Lemma that the minimizing \(A^{*}\) is

\[A^{*}=\{v:p(v)>C\}, \tag{6.43}\]

where \(p(v)\) is the density of \(V=1/S^{2}\) when \(\sigma=1\), and where \(C\) is determined by (6.42). Since \(p(v)\) is unimodal (Problem 6.76), these smallest confidence sets are intervals, \(aS^{2}<\sigma^{2}<bS^{2}\). Values of \(a\) and \(b\) are tabled by Tate and Klett (1959), who also table the corresponding (different) values \(a^{\prime}\), \(b^{\prime}\) for the uniformly most accurate unbiased confidence intervals \(a^{\prime}S^{2}<\sigma^{2}<b^{\prime}S^{2}\) (given in Example 5.5.1).

Instead of minimizing the Lebesgue measure \(\int_{A}dv\) of the confidence sets \(A\), one may prefer to minimize the scale-invariant measure

\[\int_{A}\frac{1}{v}\,dv. \tag{6.44}\]

To an interval \((a,b)\), (6.44) assigns, in place of its length \(b-a\), its logarithmic length \(\log b-\log a=\log(b/a)\). The optimum solution \(A^{**}\) with respect to this new measure is again obtained by applying the Neyman-Pearson Lemma, and is given by

\[A^{**}=\{v:vp(v)>C\}, \tag{6.45}\]

which coincides with the uniformly most accurate unbiased confidence sets (Problem 6.77(i)).

One advantage of minimizing (6.44) instead of Lebesgue measure is that it then does not matter whether one estimates \(\sigma\) or \(\sigma^{2}\) (or \(\sigma^{r}\) for some other power of \(r\)), since under (6.44), if \((a,b)\) is the best interval for \(\sigma\), then \((a^{r},b^{r})\) is the best interval for \(\sigma^{r}\) (Problem 6.77(ii)).

**Example 6.12.2**: Let \(X_{i}\) (\(i=1,\ldots,r\)) be independently normally distributed as \(N(\xi,1)\). A slight generalization of Example 6.11.2 shows that uniformly most accurate equivariant confidence sets for \((\xi_{1},\ldots,\xi_{r})\) exist with respect to the group \(G\) of all rigid transformations and are given by

\[\sum(X_{i}-\xi_{i})^{2}\leq C. \tag{6.46}\]

Suppose that the context of the problem does not possess the symmetry which would justify invoking invariance with respect to \(G\), but does allow the weaker assump

[MISSING_PAGE_FAIL:301]

for all \(a_{1},\ldots,a_{r}\) and all \(b>0\). It follows that \((\xi_{1},\ldots,\xi_{r})\in C\) if and only if

\[\left(\frac{X_{1}-\xi_{1}}{S},\ldots,\frac{X_{r}-\xi_{r}}{S}\right) \in\frac{(X_{1},\ldots,X_{r})-C(X_{1},\ldots,X_{r};\,S)}{S} = C(0,\ldots,0;\,1)\] \[= A.\]

The equivariant confidence sets of smallest volume are obtained by choosing for \(A\) the set \(A^{*}\) given by (6.43) with \(v=(v_{1},\ldots,v_{r})\) and \(p(v)\) the joint density of \((X_{1}/S,\ldots,X_{r}/S)\) when \(\xi_{1}=\cdots=\xi_{r}=0\). This density is a decreasing function of \(\sum v_{i}^{2}\) (Problem 6.78), and the smallest equivariant confidence sets are therefore given by

\[\sum(X_{i}-\xi_{i})^{2}\leq CS^{2}. \tag{6.52}\]

[Under the larger group \(G\) generated by all rigid transformations of \((X_{1},\ldots,X_{r})\) together with the scale changes \(X_{i}^{\prime}=bX_{i}\), \(S^{\prime}=bS\), the same sets have the stronger property of being uniformly most accurate equivariant; see Problem 6.79.]

Examples 6.12.1-6.12.3 have the common feature that the equivariant confidence sets \(S(X)\) for \(\theta=(\theta_{1},\ldots,\theta_{r})\) are characterized by an \(r\)-valued _pivotal quantity_, that is, a function \(h(X,\theta)=(h_{1}(X,\theta),\ldots,h_{r}(X,\theta))\) of the observations \(X\) and parameters \(\theta\) being estimated that has a fixed distribution, and such that the most general equivariant confidence sets are of the form

\[h(X,\theta)\in A \tag{6.53}\]

for some fixed set \(A\).6 When the functions \(h_{i}\) are linear in \(\theta\), the confidence sets \(C(X)\) obtained by solving (6.53) for \(\theta\) are linear transforms of \(A\) (with random coefficients), so that the volume or invariant measure of \(C(X)\) is minimized by minimizing

Footnote 6: More general results concerning the relationship of equivariant confidence sets and pivotal quantities are given in Problems 6.71–6.74.

\[\int_{A}\rho(v_{1},\ldots,v_{r})\,dv_{1}\ldots dv_{r} \tag{6.54}\]

for the appropriate \(\rho\). The problem thus reduces to that of minimizing (6.54) subject to

\[P_{\theta_{0}}\{h(X,\theta_{0})\in A\}=\int_{A}p(v_{1},\ldots,v_{r})\,dv_{1} \ldots dv_{r}=1-\alpha, \tag{6.55}\]

where \(p(v_{1},\ldots,v_{r})\) is the density of the pivotal quantity \(h(X,\theta)\). The minimizing \(A\) is given by

\[A^{*}=\left\{v:\frac{p(v_{1},\ldots,v_{r})}{\rho(v_{1},\ldots,v_{r})}>C\right\}, \tag{6.56}\]

with \(C\) determined by (6.55).

The following is one more illustration of this approach.

**Example 6.12.4**: Let \(X_{1}\),..., \(X_{m}\) and \(Y_{1},\ldots,Y_{n}\) be samples from \(N(\xi,\sigma^{2})\) and \(N(\eta,\tau^{2})\) respectively, and consider the problem of estimating \(\Delta=\tau^{2}/\sigma^{2}\). Sufficiency and invariance under translations \(X^{\prime}_{i}=X_{i}+a_{1}\), \(Y^{\prime}_{j}=Y_{j}+a_{2}\) reduce the data to \(S^{2}_{X}=\sum(X_{i},-\bar{X})^{2}\) and \(S^{2}_{Y}=\sum(Y_{j}-\bar{Y})^{2}\). The problem of estimating \(\Delta\) also remains invariant under the scale changes

\[X^{\prime}_{i}=b_{1}X_{i},\quad Y^{\prime}_{j}=b_{2}Y_{j},\quad\quad 0<b_{1},b_{ 2}<\infty,\]

which induce the transformations

\[S^{\prime}_{X}=b_{1}S_{X},\quad\quad S^{\prime}_{Y}=b_{2}S_{Y},\quad\quad\sigma ^{\prime}=b_{1}\sigma,\quad\quad\tau^{\prime}=b_{2}\tau. \tag{6.57}\]

The totality of equivariant confidence sets for \(\Delta\) is given by \(\Delta/V\in A\), where \(V=S^{2}_{Y}/S^{2}_{X}\) and \(A\) is any fixed set on the line satisfying

\[P_{\Delta=1}\left(\frac{1}{V}\in A\right)=1-\alpha. \tag{6.58}\]

To see this, suppose that \(C(S_{X},\,S_{Y})\) are any equivariant confidence sets for \(\Delta\). Then \(C\) must satisfy

\[C(b_{1}S_{X},\,b_{2}S_{Y})=\frac{b_{2}^{2}}{b_{1}^{2}}C(S_{X},\,S_{Y}), \tag{6.59}\]

and hence \(\Delta\in C(S_{X},\,S_{Y})\) if and only if the pivotal quantity \(V/\Delta\) satisfies

\[\frac{\Delta}{V}=\frac{S^{2}_{X}\Delta}{S^{2}_{Y}}\in\frac{S^{2}_{X}}{S^{2}_{Y }}C(S_{X},\,S_{Y})=C(1,1)=A.\]

As in Example 6.12.1, one may now wish to choose \(A\) so as to minimize either its Lebesgue measure \(\int_{A}dv\) or the invariant measure \(\int_{A}(1/v)\,dv\). The resulting confidence sets are of the form

\[p(v)>C\quad\text{and}\quad vp(v)>C, \tag{6.60}\]

respectively. In both cases, they are intervals \(V/b<\Delta<V/a\) (Problem 6.80(i)). The values of \(a\) and \(b\) minimizing Lebesgue measure are tabled by Levy and Narula (1974); those for the invariant measure coincide with the uniformly most accurate unbiased intervals (Problem 6.80(ii)).

### Confidence Bands for a Distribution Function

Suppose that \(X=(X_{1},\ldots,X_{n})\) is a sample from an unknown continuous cumulative distribution function \(F\), and that lower and upper bounds \(L_{X}\) and \(M_{X}\) are to be determined such that with preassigned probability \(1-\alpha\) the inequalities

\[L_{X}(u)\leq F(u)\leq M_{X}(u)\quad\quad\mbox{for all }u\]

hold for all continuous cumulative distribution functions \(F\). This problem is invariant under the group \(G\) of transformations

\[X_{i}^{\prime}=g(X_{i}),\quad\quad i=1,\ldots,n,\]

where \(g\) is any continuous strictly increasing function. The induced transformation in the parameter space is \(\tilde{g}F=F(g^{-1})\).

If \(S(x)\) is the set of continuous cumulative distribution functions

\[S(x)=\{F:L_{x}(u)\leq F(u)\leq M_{x}(u)\mbox{ for all }u\},\]

then

\[g^{*}S(x) = \{\tilde{g}F:L_{x}(u)\leq F(u)\leq M_{x}(u)\mbox{ for all }u\}\] \[= \{F:L_{x}[g^{-1}(u)]\leq F(u)\leq M_{x}[g^{-1}(u)]\mbox{ for all }u\}.\]

For an equivariant procedure, this must coincide with the set

\[S(gx)=\left\{F:L_{g(x_{1}),\ldots,g(x_{n})}(u)\leq F(u)\leq M_{g(x_{1}),\ldots,g(x_{n})}(u)\mbox{ for all }u\right\}.\]

The condition of equivariance is therefore

\[L_{g(x_{1}),\ldots,g(x_{n})}[g(u)] = L_{x}(u),\] \[M_{g(x_{1}),\ldots,g(x_{n})}[g(u)] = M_{x}(u)\quad\quad\mbox{for all }x\mbox{ and }u.\]

To characterize the totality of equivariant procedures, consider the _empirical distribution function_ (EDF) \(T_{x}\) given by

\[T_{x}(u)=\frac{i}{n}\quad\mbox{for}\quad x_{(i)}\leq u<x_{(i+1)},\quad i=0, \ldots,n,\]

where \(x_{(1)}<\cdots<x_{(n)}\) is the ordered sample and where \(x_{(0)}=-\infty\), \(x_{(n+1)}=\infty\). Then a necessary and sufficient condition for \(L\) and \(M\) to satisfy the above equivariance condition is the existence of numbers \(a_{0}\), \(\ldots\), \(a_{n}\); \(a_{0}^{\prime}\), \(\ldots\), \(a_{n}^{\prime}\) such that

\[L_{x}(u)=a_{i},\quad M_{x}(u)=a_{i}^{{}^{\prime}}\quad\quad\mbox{for}\quad x_ {(i)}<u<x_{(i+1)}.\]That this condition is sufficient is immediate. To see that it is also necessary, let \(u\), \(u^{\prime}\) be any two points satisfying \(x_{(i)}<u<u^{\prime}<x_{(i+1)}\). Given any \(y_{1},\ldots,y_{n}\) and \(v\) with \(y_{(i)}<v<y_{(i+1)}\), there exist \(g\), \(g^{\prime}\in G\) such that

\[g(y_{(i)})=g^{\prime}(y_{(i)})=x_{(i)},\quad g(v)=u,\quad g^{\prime}(v)=u^{ \prime}.\]

If \(L_{x}\), \(M_{x}\) are equivariant, it then follows that \(L_{x}(u^{\prime})=L_{y}(v)\) and \(L_{x}(u)=L_{y}(v)\), and hence that \(L_{x}(u^{\prime})=L_{x}(u)\) and similarly \(M_{x}(u^{\prime})=M_{x}(u)\), as was to be proved. This characterization shows \(L_{x}\) and \(M_{x}\) to be step functions whose discontinuity points are restricted to those of \(T_{x}\).

Since any two continuous strictly increasing cumulative distribution functions can be transformed into one another through a transformation \(\bar{g}\), it follows that all these distributions have the same probability of being covered by an equivariant confidence band. (See Problem 6.86.) Suppose now that \(F\) is continuous but no longer strictly increasing. If \(I\) is any interval of constancy of \(F\), there are no observations in \(I\), so that \(I\) is also an interval of constancy of the sample cumulative distribution function. It follows that the probability of the confidence band covering \(F\) is not affected by the presence of \(I\) and hence is the same for all continuous cumulative distribution functions \(F\).

For any numbers \(a_{i}\), \(a^{\prime}_{i}\) let \(\Delta_{i}\), \(\Delta^{\prime}_{i}\) be determined by

\[a_{i}=\frac{i}{n}-\Delta_{i},\quad\quad a^{\prime}_{i}=\frac{i}{n}-\Delta^{ \prime}_{i}.\]

Then it was seen above that any numbers \(\Delta_{0}\),..., \(\Delta_{n}\); \(\Delta^{\prime}_{0}\),..., \(\Delta^{\prime}_{n}\) define a confidence band for \(F\), which is equivariant and hence has constant probability of covering the true \(F\). From these confidence bands a test can be obtained of the hypothesis of _goodness of fit_\(F=F_{0}\) that the unknown \(F\) equals a hypothetical distribution \(F_{0}\). The hypothesis is accepted if \(F_{0}\) ties entirely within the band, that is, if

\[-\Delta_{i}<F_{0}(u)-T_{x}(u)<\Delta^{\prime}_{i}\] \[\mbox{for all}\quad x_{(i)}<u<x_{(i+1)}\quad\mbox{and all}\quad i =1,\ldots,n.\]

Within this class of tests there exists no UMP member, and the most common choice of the \(\Delta\)'s is \(\Delta_{i}=\Delta^{\prime}_{i}=\Delta\) for all \(i\). The acceptance region of the resulting _Kolmogorov-Smirnov test_ can be written as

\[\sup_{-\infty<u<\infty}|F_{0}(u)-T_{x}(u)|<\Delta. \tag{6.61}\]

Tables of the null distribution of the Kolmogorov-Smirnov statistic are given by Birnbaum (1952). For large \(n\), approximate critical values can be obtained from the limit distribution \(K\) of \(\sqrt{n}\sup|F_{0}(u)-T_{x}(u)|\), due to Kolmogorov and tabled by Smirnov (1948). Derivations of \(K\) can be found, for example, in Feller (1948), Billingsley (1968), and Hajek et al. (1999). The large-sample properties of this test

will be studied in Example 11.4.2 and Section 16.2. The more general problem of testing goodness of fit will be presented in Chapter 16.

### Problems

#### Section 6.1

**Problem 6.1**: Let \(G\) be a group of measurable transformations of \((\mathcal{X},\mathcal{A})\) leaving \(\mathcal{P}=\{P_{\theta},\theta\in\Omega\}\) invariant, and let \(T(x)\) be a measurable transformation to \((\mathcal{T},\mathcal{B})\). Suppose that \(T(x_{1})=T(x_{2})\) implies \(T(gx_{1})=T(gx_{2})\) for all \(g\in G\), so that \(G\) induces a group \(G^{*}\) on \(\mathcal{T}\) through \(g^{*}T(x)=T(gx)\), and suppose further that the induced transformations \(g^{*}\) are measurable \(\mathcal{B}\). Then \(G^{*}\) leaves the family \(\mathcal{P}^{T}=\{P_{\theta}^{T},\theta\in\Omega\}\) of distributions of \(T\) invariant.

#### Section 6.2

**Problem 6.2**:
1. Let \(\mathcal{X}\) be the totality of points \(x=(x_{1},\ldots,x_{n})\) for which all coordinates are different from zero, and let \(G\) be the group of transformations \(x_{i}^{\prime}=cx_{i}\), \(c>0\). Then a maximal invariant under \(G\) is \((\operatorname{sgn}x_{n},x_{1}/x_{n},\ldots,x_{n-1}/x_{n})\) where \(\operatorname{sgn}x\) is \(1\) or \(-1\) as \(x\) is positive or negative.
2. Let \(\mathcal{X}\) be the space of points \(x=(x_{1},\ldots,x_{n})\) for which all coordinates are distinct, and let \(G\) be the group of all transformations \(x_{i}^{\prime}=f(x_{i})\), \(i=1,\ldots,n\), such that \(f\) is a \(1:1\) transformation of the real line onto itself with at most a finite number of discontinuities. Then \(G\) is transitive over \(\mathcal{X}\).
3. Let \(x=(x_{1},\ldots,x_{n})\) and \(x^{\prime}=(x_{1}^{\prime},\ldots,x_{n}^{\prime})\) be any two points of \(\mathcal{X}\). Let \(I_{1}\),..., \(I_{n}\) be a set of mutually exclusive open intervals which (together with their end points) cover the real line and such that \(x_{j}\in I_{j}\). Let \(I_{1}^{\prime}\),..., \(I_{n}^{\prime}\) be a corresponding set of intervals for \(x_{1}^{\prime}\),..., \(x_{n}^{\prime}\). Then there exists a transformation \(f\) which maps each \(I_{j}\) continuously onto \(I_{j}^{\prime}\), maps \(x_{j}\) into \(x_{j}^{\prime}\), and maps the set of \(n-1\) end points of \(I_{1}\),..., \(I_{n}\) onto the set of end points of \(I_{1}^{\prime}\),..., \(I_{n}^{\prime}\).]

**Problem 6.3**: Suppose \(M\) is any \(m\times p\) matrix. Show that \(M^{\top}M\) is positive semidefinite. Also, show the rank of \(M^{\top}M\) equals the rank of \(M\), so that in particular \(M^{\top}M\) is nonsingular if and only if \(m\geq p\) and \(M\) is of rank \(p\).

**Problem 6.4**:
1. A sufficient condition for (6.9) to hold is that \(D\) is a normal subgroup of \(G\).
2. If \(G\) is the group of transformations \(x^{\prime}=ax+b\), \(a\neq 0,-\infty<b<\infty\), then the subgroup of translations \(x^{\prime}=x+b\) is normal but the subgroup \(x^{\prime}=ax\) is not.

[The defining property of a normal subgroup is that given \(d\in D\), \(g\in G\), there exists \(d^{\prime}\in D\) such that \(gd=d^{\prime}g\). The equality \(s(x_{1})=s(x_{2})\) implies \(x_{2}=dx_{1}\) for some \(d\in D\), and hence \(ex_{2}=edx_{1}=d^{\prime}ex_{1}\). The result (i) now follows, since \(s\) is invariant under \(D\).]

_Section 6.3_

**Problem 6.5**: Prove statements (i)-(iii) of Example 6.3.1.

**Problem 6.6**: Prove Theorem 6.3.1

(i) by analogy with Example 6.3.1, and

(ii) by the method of Example 6.3.2. [_Hint:_ A maximal invariant under \(G\) is the set \(\{g_{1}x,\,\ldots,\,g_{N}x\}\).]

**Problem 6.7**: Consider the situation of Example 6.3.1 with \(n=1\), and suppose that \(f\) is strictly increasing on (0, 1).

(i) The likelihood ratio test rejects if \(X<\alpha/2\) or \(X>1-\alpha/2\).

(ii) The MP invariant test agrees with the likelihood ratio test when \(f\) is convex.

(iii) When \(f\) is concave, the MP invariant test rejects when

\[\frac{1}{2}-\frac{\alpha}{2}<X<\frac{1}{2}+\frac{\alpha}{2}\,\]

and the likelihood ratio test is the least powerful invariant test against both alternatives and has power \(\leq\alpha\). When does the power \(=\alpha\)?

**Problem 6.8**: In Example 6.1.1, find a maximal invariant and the UMPI level \(\alpha\) test.

**Problem 6.9**: Let \(X\), \(Y\) have the joint probability density \(f(x,\,y)\). Then the integral \(h(z)=\int_{-\infty}^{\infty}f(y-z,\,y)dy\) is finite for almost all \(z\), and is the probability density of \(Z=Y-X\).

[Since \(P\{Z\leq b\}=\int_{-\infty}^{b}h(z)dz\), it is finite and hence \(h\) is finite almost everywhere.]

**Problem 6.10**: (i) Let \(X=(X_{1},\,\ldots,\,X_{n})\) have probability density \((1/\theta^{n})f[(x_{1}-\xi)/\theta,\,\ldots,\,(x_{n}-\xi)/\theta]\), where \(-\infty<\xi<\infty\), \(0<\theta\) are unknown, and where \(f\) is even. The problem of testing \(f=f_{0}\) against \(f=f_{1}\) remains invariant under the transformations \(x_{i}^{\prime}=ax_{i}+b\) (\(i=1,\,\ldots,\,n\)), \(a\neq 0,\,-\infty<b<\infty\) and the most powerful invariant test is given by the rejection region

\[\int_{-\infty}^{\infty}\int_{0}^{\infty}v^{n-2}f_{1}(vx_{1}+u, \,\ldots,\,vx_{n}+u)\,dv\,du\] \[\quad>C\int_{-\infty}^{\infty}\int_{0}^{\infty}v^{n-2}f_{0}(vx_{ 1}+u,\ldots,\,vx_{n}+u)\,dv\,du.\]2. Let \(X=(X_{1},\ldots,X_{n})\) have probability density \(f(x_{1}-\sum_{j=1}^{k}w_{1j}\beta_{j},\ldots,x_{n}-\sum_{j=1}^{k}w_{nj}\beta_{j})\) where \(k<n\), the \(w\)'s are given constants, the matrix \((w_{ij})\) is of rank \(k\), the \(\beta\)'s are unknown, and we wish to test \(f=f_{0}\) against \(f=f_{1}\). The problem remains invariant under the transformations \(x_{i}^{\prime}=x_{i}+\Sigma_{j=1}^{k}w_{ij}\gamma_{j},-\infty<\gamma_{1},\ldots,\gamma_{k}<\infty\), and the most powerful invariant test is given by the rejection region \[\frac{\int\cdots\int f_{1}(x_{1}-\sum w_{1j}\beta_{j},\ldots,x_{n}-\sum w_{nj} \beta_{j})d\beta_{1},\ldots,d\beta_{k}}{\int\cdots\int f_{0}(x_{1}-\sum w_{1j} \beta_{j},\ldots,x_{n}-\sum w_{nj}\beta_{j})d\beta_{1},\ldots,d\beta_{k}}>C.\]

[A maximal invariant is given by \(y=\) \[\left(x_{1}-\sum_{r=n-k+1}^{n}a_{1,}x_{r},\ x_{2}-\sum_{r=n-k+1}^{n}a_{2,}x_{ r},\ldots,x_{n-k}-\sum_{r=n-k+1}^{n}a_{n-k,r}x_{r}\right)\] for suitably chosen constants \(a_{ir}\).]

**Problem 6.11**: Let \(X_{1},\ldots,X_{m}\); \(Y_{1},\ldots,Y_{n}\) be samples from exponential distributions with densities for \(\sigma^{-1}e^{-(x-\zeta)/\sigma}\), for \(x\geq\xi\), and \(\tau^{-1}e^{-(y-\eta)/\tau}\) for \(y\geq\eta\).

1. For testing \(\tau/\sigma\leq\Delta\) against \(\tau/\sigma>\Delta\), there exists a UMP invariant test with respect to the group \(G:X_{i}^{\prime}=aX_{i}+b\), \(Y_{j}^{\prime}=aY_{j}+c\), \(a>0,-\infty<b\), \(c<\infty\), and its rejection region is \[\frac{\sum[y_{j}-\min(y_{1},\ldots,y_{n})]}{\sum[x_{i}-\min(x_{1},\ldots,x_{m} )]}>C.\]
2. This test is also UMP unbiased.
3. Extend these results to the case that only the \(r\) smallest \(X\)'s and the \(s\) smallest \(Y\)'s are observed.

[(ii): See Problem 5.15.]

**Problem 6.12**: If \(X_{1},\ldots,X_{n}\) and \(Y_{1},\ldots,Y_{n}\) are samples from \(N(\xi,\sigma^{2})\) and \(N(\eta,\tau^{2})\), respectively, the problem of testing \(\tau^{2}=\sigma^{2}\) against the two-sided alternatives \(\tau^{2}\neq\sigma^{2}\) remains invariant under the group \(G\) generated by the transformations \(X_{i}^{\prime}=aX_{i}+b\), \(Y_{i}^{\prime}=aY_{i}+c\), \((a\neq 0)\), and \(X_{i}^{\prime}=Y_{i}\), \(Y_{i}^{\prime}=X_{i}\). There exists a UMP invariant test under \(G\) with rejection region \[W=\max\left\{\frac{\sum(Y_{i}-\bar{Y})^{2}}{\sum(X_{i}-\bar{X})},\frac{\sum(X_ {i}-\bar{X})^{2}}{\sum(Y_{i}-\bar{Y})^{2}}\right\}\geq k.\]

[The ratio of the probability densities of \(W\) for \(\tau^{2}/\sigma^{2}=\Delta\) and \(\tau^{2}/\sigma^{2}=1\) is proportional to \([(1+w)/(\Delta+w)]^{n-1}+[(1+w)/(1+\Delta w)]^{n-1}\) for \(w\geq 1\). The derivative of this expression is \(\geq 0\) for all \(\Delta\).]

**Problem 6.13**: Let \(X_{1},\,\ldots,\,X_{n}\) be a sample from a distribution with density

\[\frac{1}{\tau^{n}}\,f\,\left(\frac{x_{1}}{\tau}\right)\ldots f\,\left(\frac{x_{n }}{\tau}\right),\]

where \(f(x)\) is either zero for \(x<0\) or symmetric about zero. The most powerful scale-invariant test for testing \(H:f=f_{0}\) against \(K:f=f_{1}\) rejects when

\[\frac{\int_{0}^{\infty}v^{n-1}f_{1}(vx_{1})\ldots f_{1}(vx_{n})\,dv}{\int_{0}^ {\infty}v^{n-1}f_{0}(vx_{1})\ldots f_{0}(vx_{n})\,dv}>C.\]

**Problem 6.14**: _Normal versus double exponential. For \(f_{0}(x)=e^{-x^{2}/2}/\sqrt{2\pi}\), \(f_{1}(x)=e^{-|x|}/2\), the test of the preceding problem reduces to rejecting when \(\sqrt{\sum x_{i}^{2}}/\sum|x_{i}|<C\)._

(Hogg, 1972.)

_Note_. The corresponding test when both location and scale are unknown is obtained in Uthoff (1973). Testing normality against Cauchy alternatives is discussed by Franck (1981).

**Problem 6.15**: _Uniform versus triangular._

(i): For \(f_{0}(x)=1\) (\(0<x<1\)), \(f_{1}(x)=2x\) (\(0<x<1\)), the test of Problem 6.13 reduces to rejecting when \(T=x_{(n)}/\tilde{x}<C\). (ii): Under \(f_{0}\), the statistic \(2n\,\log T\) is distributed as \(\chi^{2}_{2n}\).

(Quesenberry and Starbuck, 1976.)

**Problem 6.16**: Show that the test of Problem 6.10(i) reduces to

(i): \([x_{(n)}-x_{(1)}]/S<c\) for normal versus uniform;

(ii): \([\tilde{x}-x_{(1)}]/S<c\) for normal versus exponential;

(iii): \([\tilde{x}-x_{(1)}]/[x_{(n)}-x_{(1)}]<c\) for uniform versus exponential.

(Uthoff, 1970.)

_Note_. When testing for normality, one is typically not interested in distinguishing the normal from some other given shape but would like to know more generally whether the data are or are not consonant with a normal distribution. This is a special case of the problem of testing for goodness of fit, which is briefly discussed at the end of Section 6.13 and forms the topic of Chapter 16; also, see the many references in the notes to Chapter 16.

**Problem 6.17**: Let \(X_{1},\,\ldots,\,X_{n}\) be independent and normally distributed. Suppose \(X_{i}\) has mean \(\mu_{i}\) and variance \(\sigma^{2}\) (which is the same for all \(i\)). Consider testing the null hypothesis that \(\mu_{i}=0\) for all \(i\). Using invariance considerations, find a UMP invariant test with respect to a suitable group of transformations in each of the following cases:(i) \(\sigma^{2}\) is known and equal to one.

(ii) \(\sigma^{2}\) is unknown.

#### Section 6.4

**Problem 6.18**:
1. When testing \(H:p\leq p_{0}\) against \(K:p>p_{0}\) by means of the test corresponding to (6.15), determine the sample size required to obtain power \(\beta\) against \(p=p_{1}\), \(\alpha=0.05\), \(\beta=0.9\) for the cases \(p_{0}=0.1\), \(p_{1}=0.15\), \(0.20\), \(0.25\); \(p_{0}=0.05\), \(p_{1}=0.10\), \(0.15\), \(0.20\), \(0.25\); \(p_{0}=0.01\), \(p_{1}=0.02\), \(0.05\), \(0.10\), \(0.15\), \(0.20\).
2. Compare this with the sample size required if the inspection is by attributes and the test is based on the total number of defectives.

**Problem 6.19**: _Two-sided t-test._

1. Let \(X_{1},\ldots,X_{n}\) be a sample from \(N(\xi,\sigma^{2})\). For testing \(\xi=0\) against \(\xi\neq 0\), there exists a UMP invariant test with respect to the group \(X^{\prime}_{i}=cX_{i}\), \(c\neq 0\), given by the two-sided \(t\)-test (5.17).
2. Let \(X_{1},\ldots,X_{m}\), and \(Y_{1},\ldots,Y_{n}\) be samples from \(N(\xi,\sigma^{2})\) and \(N(\eta,\sigma^{2})\) respectively. For testing \(\eta=\xi\) against \(\eta\neq\xi\) there exists a UMP invariant test with respect to the group \(X^{\prime}_{i}=aX_{i}+b\), \(Y^{\prime}_{j}=aY_{j}+b\), \(a\neq 0\), given by the two-sided \(t\)-test (5.30).

[(i): Sufficiency and invariance reduce the problem to \(|t|\), which in the notation of Section 4 has the probability density \(p\delta(t)+p_{\delta}(-t)\) for \(t>0\). The ratio of this density for \(\delta=\delta_{1}\) to its value for \(\delta=0\) is proportional to \(\int_{0}^{\infty}(e^{\delta_{1}v}+e^{-\delta_{1}v})g_{t^{2}}(v)\,dv\), which is an increasing function of \(t^{2}\) and hence of \(|t|\).]

**Problem 6.20**: _Testing a correlation coefficient._ Let \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) be a sample from a bivariate normal distribution.

1. For testing \(\rho\leq\rho_{0}\) against \(\rho>\rho_{0}\) there exists a UMP invariant test with respect to the group of all transformations \(X^{\prime}_{i}=aX_{i}+b\), \(Y^{\prime}_{i}=cY_{1}+d\) for which \(a\), \(c>0\). This test rejects when the sample correlation coefficient \(R\) is too large.
2. The problem of testing \(\rho=0\) against \(\rho\neq 0\) remains invariant in addition under the transformation \(Y^{\prime}_{i}=-Y_{i}\), \(X^{\prime}_{i}=X_{i}\). With respect to the group generated by this transformation and those of (i) there exists a UMP invariant test, with rejection region \(|R|\geq C\).

[(i): To show that the probability density \(p_{\rho}(r)\) of \(R\) has monotone likelihood ratio, apply the condition of Problem 3.28(i), to the expression given for this density in Problem 5.67. Putting \(t=\rho r+1\), the second derivative \(\partial^{2}\log p_{\rho}(r)/\partial\rho\partial r\) up to a positive factor is

\[\frac{\sum_{i,j=0}^{\infty}c_{i}c_{j}t^{i+j-2}\left[(j-i)^{2}(t-1)+(i+j) \right]}{2\left[\sum_{i=0}^{\infty}c_{i}t^{i}\right]^{2}}.\]To see that the numerator is positive for all \(t>0\), note that it is greater than

\[2\sum_{i=0}^{\infty}c_{i}t^{i-2}\sum_{j=i+1}^{\infty}c_{j}t^{j}\left[(j-i)^{2}(t- 1)+(i+j)\right].\]

Holding \(i\) fixed and using the inequality \(c_{j+1}<\frac{1}{2}c_{j}\), the coefficient of \(t^{j}\) in the interior sum is \(\geq 0\).]

**Problem 6.21**: Let \((X_{i},\,Y_{i})\) be independent \(N(\mu_{i},\,\sigma^{2})\) for \(i=1,\,\ldots,\,n\). The parameters \(\mu_{1},\,\ldots,\,\mu_{n}\) and \(\sigma^{2}\) are all unknown. For testing \(\sigma=1\) against \(\sigma>1\), determine the UMPI level \(\alpha\) test. Is the test also UMPU?

**Problem 6.22**: For testing the hypothesis that the correlation coefficient \(\rho\) of a bivariate normal distribution is \(\leq\rho_{0}\), determine the power against the alternative \(\rho=\rho_{1}\), when the level of significance \(\alpha\) is.05, \(\rho_{0}=.3,\,\rho_{1}=.5\), and the sample size \(n\) is 50, 100, 200.

### _Section 6.5_

**Problem 6.23**: Almost invariance of a test \(\phi\) with respect to the group \(G\) of either Problem 6.11(i) or Example 6.3.5 implies that \(\phi\) is equivalent to an invariant test.

**Problem 6.24**: The totality of permutations of \(K\) distinct numbers \(a_{1},\,\ldots,\,a_{K}\), for varying \(a_{1},\,\ldots,\,a_{K}\) can be represented as a subset \(C_{K}\) of Euclidean \(K\)-space \(R_{K}\), and the group \(G\) of Example 6.5.1 as the union of \(C_{2},\,C_{3},\,\ldots\,\). Let \(\nu\) be the measure over \(G\) which assigns to a subset \(B\) of \(G\) the value \(\sum_{k=2}^{\infty}\mu_{K}(B\cap C_{K})\), where \(\mu_{K}\) denotes Lebesgue measure in \(E_{K}\). Give an example of a set \(B\subset G\) and an element \(g\in G\) such that \(\nu(B)>0\) but \(\nu(Bg)=0\).

[If \(a,\,b,\,c,\,d\) are distinct numbers, the permutations \(g\), \(g^{\prime}\) taking \((a,\,b)\) into \((b,\,a)\) and \((c,\,d)\) into \((d,\,c)\) respectively are points in \(C_{2}\), but \(gg^{\prime}\) is a point in \(C_{4}\).]

### _Section 6.6_

**Problem 6.25**: Show that

1. \(G_{1}\) of Example 6.6.2 is a group;
2. the test which rejects when \(X_{21}^{2}/X_{11}^{2}>C\) is UMP invariant under \(G_{1}\);
3. the smallest group containing \(G_{1}\) and \(G_{2}\) is the group \(G\) of Example 6.6.2.

**Problem 6.26**: Consider a testing problem which is invariant under a group \(G\) of transformations of the sample space, and let \(\mathcal{C}\) be a class of tests which is closed under \(G\), so that \(\phi\in\mathcal{C}\) implies \(\phi g\in\mathcal{C}\), where \(\phi g\) is the test defined by \(\phi g(x)=\phi(gx)\). If there exists an a.e. unique UMP member \(\phi_{0}\) of \(\mathcal{C}\), then \(\phi_{0}\) is almost invariant.

**Problem 6.27**: _Envelope power function_. Let \(S(\alpha)\) be the class of all level-\(\alpha\) tests of a hypothesis \(H\), and let \(\beta_{\alpha}^{*}(\theta)\) be the _envelope power function_, defined by

\[\beta_{\alpha}^{*}(\theta)=\sup_{\phi\in S(\alpha)}\beta_{\phi}(\theta),\]

where \(\beta_{\phi}\) denotes the power function of \(\phi\). If the problem of testing \(H\) is invariant under a group \(G\), then \(\beta_{\alpha}^{*}(\theta)\) is invariant under the induced group \(\tilde{G}\).

**Problem 6.28**:
1. A generalization of equation (6.2) is \[\int_{A}f(x)\,dP_{\theta}(x)=\int_{gA}f(g^{-1}x)\,dP_{\tilde{g}\theta}(x).\]
2. If \(P_{\theta_{1}}\) is absolutely continuous with respect to \(P_{\theta_{0}}\), then \(P_{\tilde{g}\theta_{1}}\) is absolutely continuous with respect to \(P_{\tilde{g}\theta_{0}}\) and \[\frac{dP_{\theta_{1}}}{dP_{\theta_{0}}}(x)=\frac{dP_{\tilde{g}\theta_{1}}}{dP _{\tilde{g}\theta_{0}}}(gx)\qquad\mbox{\rm(a.e. $P_{\theta_{0}}$)}\,.\]
3. The distribution of \(dP_{\theta_{1}}/dP_{\theta_{0}}(X)\) when \(X\) is distributed as \(P_{\theta_{0}}\) is the same as that of \(dP_{\tilde{g}\theta_{1}}/dP_{\tilde{g}\theta_{0}}(X^{\prime})\) when \(X^{\prime}\) is distributed as \(P_{\tilde{g}\theta_{0}}\).

**Problem 6.29**: _Invariance of likelihood ratio_. Let the family of distributions \({\cal P}=\{P_{\theta},\theta\in\Omega\}\) be dominated by \(\mu\), let \(p_{\theta}=dP_{\theta}/d\mu\), let \(\mu g^{-1}\) be the measure defined by \(\mu g^{-1}(A)=\mu[g^{-1}(A)]\), and suppose that \(\mu\) is absolutely continuous with respect to \(\mu g^{-1}\) for all \(g\in G\).

1. Then \[p_{\theta}(x)=p_{\tilde{g}\theta}(gx)\frac{d\mu}{d\mu g^{-1}}(gx)\qquad\mbox{ \rm(a.e. $\mu$)}.\]
2. Let \(\Omega\) and \(\omega\) be invariant under \(\tilde{G}\), and countable. Then the likelihood ratio \(\sup_{\Omega}p_{\theta}(x)/\sup_{\omega}p_{\theta}(x)\) is almost invariant under \(G\).
3. Suppose that \(p_{\theta}(x)\) is continuous in \(\theta\) for all \(x\), that \(\Omega\) is a separable pseudo-metric space, and that \(\Omega\) and \(\omega\) are invariant. Then the likelihood ratio is almost invariant under \(G\).

**Problem 6.30**: _Inadmissible likelihood ratio test_. In many applications in which a UMP invariant test exists, it coincides with the likelihood ratio test. That this is, however, not always the case is seen from the following example. Let \(P_{1},\ldots,\,P_{n}\) be \(n\) equidistant points on the circle \(x^{2}+y^{2}=4\), and \(Q_{1},\ldots,\,Q_{n}\) on the circle \(x^{2}+y^{2}=1\). Denote the origin in the \((x,\,y)\) plane by \(O\), let \(0<\alpha\leq\frac{1}{2}\) be fixed, andlet \((X,Y)\) be distributed over the \(2n+1\) points \(P_{1},\ldots,P_{n},\)\(Q_{1},\ldots,Q_{n},\)\(O\) with probabilities given by the following table:

\[\begin{array}{c|cccc}&P_{i}&Q_{i}&O\\ \hline H&\alpha/n&(1-2\alpha)/n&\alpha\\ K&p_{i}/n&0&(n-1)/n,\end{array}\]

where \(\sum p_{i}=1.\) The problem remains invariant under rotations of the plane by the angles \(2k\pi/n\) (\(k=0,1,\ldots,n-1\)). The rejection region of the likelihood ratio test consists of the points \(P_{1},\ldots,\)\(P_{n},\) and its power is \(1/n.\) On the other hand, the UMP invariant test rejects when \(X=Y=0\) and has power \((n-1)/n.\)

**Problem 6.31**: Let \(G\) be a group of transformations of \(\mathcal{X},\) and let \(\mathcal{A}\) be a \(\sigma\)-field of subsets of \(\mathcal{X},\) and \(\mu\) a measure over \((\mathcal{X},\)\(\mathcal{A}).\) Then a set \(A\in\mathcal{A}\) is said to be almost invariant if its indicator function is almost invariant.

(i): The totality of almost invariant sets forms a \(\sigma\)-field \(\mathcal{A}_{0},\) and a critical function is almost invariant if and only if it is \(\mathcal{A}_{0}\)-measurable.

(ii): Let \(\mathcal{P}=\{P_{\theta},\theta\in\Omega\}\) be a dominated family of probability distributions over \((\mathcal{X},\)\(\mathcal{A}),\) and suppose that \(\bar{g}\theta=\theta\) for all \(\bar{g}\in\bar{G},\)\(\theta\in\Omega.\) Then the \(\sigma\)-field \(\mathcal{A}_{0}\) of almost invariant sets is sufficient for \(\mathcal{P}.\)

[Let \(\lambda=\sum c_{i}P_{\theta_{i}},\) be equivalent to \(\mathcal{P}.\) Then

\[\frac{dP_{\theta}}{d\lambda}(gx)=\frac{dP_{g^{-1}\theta}}{\sum c_{i}\,dP_{g^{- 1}\theta_{i}}}(x)=\frac{dP_{\theta}}{d\lambda}(x)\qquad\text{(a.e. $\lambda$)},\]

so that \(dP_{\theta}/d\lambda\) is almost invariant and hence \(\mathcal{A}_{0}\)-measurable.]

**Problem 6.32**: The UMP invariant test of Problem 6.14 is also UMP similar.

[Consider the problem of testing \(\alpha=0\) versus \(\alpha>0\) in the two-parameter exponential family with density

\[C(\alpha,\tau)\exp\left(-\frac{\alpha}{2\tau^{2}}\sum x_{i}^{2}-\frac{1- \alpha}{\tau}\sum|x_{i}|\right),\qquad 0\leq\alpha<1.\]

_Note_. For the analogous result for the tests of Problem 6.15, 6.16, see Quesenberry and Starbuck (1976).

**Problem 6.33**: The following UMP unbiased tests of Chapter 5 are also UMP invariant under change in scale:

(i): The test of \(g\leq g_{0}\) in a gamma distribution (Problem 5.30).

(ii): The test of \(b_{1}\leq b_{2}\) in Problem 5.18(i).

### _Section 6.7_

**Problem 6.34**: The definition of \(d\)-admissibility of a test coincides with the admissibility definition given in Section 1.8 when applied to a two-decision procedure with loss 0 or 1 as the decision taken is correct or false.

**Problem 6.35**:
1. The following example shows that \(\alpha\)-admissibility does not always imply \(d\)-admissibility. Let \(X\) be distributed as \(U(0,\theta)\), and consider the tests \(\varphi_{1}\) and \(\varphi_{2}\) which reject when, respectively, \(X<1\) and \(X<\frac{3}{2}\) for testing \(H:\theta=2\) against \(K:\theta=1\). Then for \(\alpha=\frac{3}{4}\), \(\varphi_{1}\) and \(\varphi_{2}\) are both \(\alpha\)-admissible but \(\varphi_{2}\) is not \(d\)-admissible.
2. Verify the existence of the test \(\varphi_{0}\) of Example 6.7.2.

**Problem 6.36**:
1. The acceptance region \(T_{1}/\sqrt{T_{2}}\leq C\) of Example 6.7.3 is a convex set in the (\(T_{1}\), \(T_{2}\)) plane.
2. In Example 6.7.3, the conditions of Theorem 6.7.1 are not satisfied for the sets \(A:T_{1}/\sqrt{T_{2}}\leq C\) and \(\Omega^{\prime}:\xi>k\).

**Problem 6.37**:
1. In Example 6.7.4 show that there exist \(C_{0}\), \(C_{1}\) such that \(\lambda_{0}(\eta)\) and \(\lambda_{1}(\eta)\) are probability densities (with respect to Lebesgue measure).
2. Verify the densities \(h_{0}\) and \(h_{1}\).

**Problem 6.38**: Verify

1. the admissibility of the rejection region (6.27);
2. the expression for \(I(z)\) given in the proof of Lemma 6.7.1.

**Problem 6.39**: Let \(X_{1}\),..., \(X_{m}\); \(Y_{1}\),..., \(Y_{n}\) be independent \(N(\xi,\sigma^{2})\) and \(N(\eta,\sigma^{2})\) respectively. The one-sided \(t\)-test of \(H:\delta=\xi/\sigma\leq 0\) is admissible against the alternatives (i) \(0<\delta<\delta_{1}\) for any \(\delta_{1}>0\); (ii) \(\delta>\delta_{2}\) for any \(\delta_{2}>0\).

**Problem 6.40**: For the model of the preceding problem, generalize Example 6.7.3 (continued) to show that the two-sided \(t\)-test is a Bayes solution for an appropriate prior distribution.

**Problem 6.41**: Suppose \(X=(X_{1},\ldots,X_{k})^{\top}\) is multivariate normal with unknown mean vector \((\theta_{1},\ldots,\theta_{k})^{\top}\) and known nonsingular covariance matrix \(\Sigma\). Consider testing the null hypothesis \(\theta_{i}=0\) for all \(i\) against \(\theta_{i}\neq 0\) for some \(i\). Let \(C\) be any closed convex subset of \(k\)-dimensional Euclidean space, and let \(\phi\) be the test that accepts the null hypothesis if \(X\) falls in \(C\). Show that \(\phi\) is admissible. _Hint_: First assume \(\Sigma\) is the identity and use Theorem 6.7.1. [An alternative proof is provided by Strasser (1985, Theorem 30.4).]

### _Section 6.9_

**Problem 6.42**: _Wilcoxon two-sample test_. Let \(U_{ij}=1\) or \(0\) as \(X_{i}<Y_{j}\) or \(X_{i}>Y_{j}\), and let \(U=\sum\sum U_{ij}\) be the number of pairs \(X_{i}\), \(Y_{j}\) with \(X_{i}<Y_{j}\).

1. Then \(U=\sum S_{i}-\frac{1}{2}n(n+1)\), where \(S_{1}<\cdots<S_{n}\) are the ranks of the \(Y\)'s so that the test with rejection region \(U>C\) is equivalent to the Wilcoxon test.
2. Any given arrangement of \(x\)'s and \(y\)'s can be transformed into the arrangement \(x\ldots xy\ldots y\) through a number of interchanges of neighboring elements. The smallest number of steps in which this can be done for the observed arrangement is \(mn-U\).

**Problem 6.43**: _Expectation and variance of Wilcoxon statistic._ If the \(X\)'s and \(Y\)'s are samples from continuous distributions \(F\) and \(G\), respectively, the expectation and variance of the Wilcoxon statistic \(U\) defined in the preceding problem are given by

\[E\left(\frac{U}{mn}\right)=P\{X<Y\}=\int F\,dG \tag{6.62}\]

and

\[mnVar\left(\frac{U}{mn}\right) = \int F\,dG+(n-1)\int(1-G)^{2}\,dF\] \[+(m-1)\int F^{2}\,dG-(m+n-1)\left(\int F\,dG\right)^{2}.\]

Under the hypothesis \(G=F\), these reduce to

\[E\left(\frac{U}{mn}\right)=\frac{1}{2},\qquad Var\left(\frac{U}{mn}\right)= \frac{m+n+1}{12mn}. \tag{6.64}\]

**Problem 6.44**:
1. Let \(Z_{1},\ldots,Z_{N}\) be independently distributed with densities \(f_{1},\ldots,f_{N}\), and let the rank of \(Z_{i}\) be denoted by \(T_{i}\). If \(f\) is any probability density which is positive whenever at least one of the \(f_{i}\) is positive, then \[P\{T_{1}=t_{1},\ldots,T_{N}=t_{n}\}=\frac{1}{N!}E\left[\frac{f_{1}\left(V_{(t_ {1})}\right)}{f\left(V_{(t_{1})}\right)}\cdots\frac{f_{N}\left(V_{(t_{N})} \right)}{f\left(V_{(t_{N})}\right)}\right],\] (6.65) where \(V_{(1)}<\cdots<V_{(N)}\) is an ordered sample from a distribution with density \(f\).
2. If \(N=m+n\), \(f_{1}=\cdots=f_{m}=f\), \(f_{m+1}=\cdots=f_{m+n}=g\), and \(S_{1}<\cdots<S_{n}\) denote the ordered ranks of \(Z_{m+1},\ldots,Z_{m+n}\) among all the \(Z\)'s, the probability distribution of \(S_{1},\ldots,S_{n}\) is given by (6.30).

[(i): The probability in question is \(\int\ldots\int f_{1}(z_{1})\ldots f_{N}(z_{N})\,dz_{1}\cdots dz_{N}\) integrated over the set in which \(z_{i}\) is the \(t_{i}\)th smallest of the \(z\)'s for \(i=1,\ldots,N\). Under the transformation \(w_{t_{i}}=z_{i}\) the integral becomes \(\int\ldots\int f_{1}(w_{t_{1}})\ldots f_{N}(w_{t_{N}})\,dw_{1}\cdots dw_{N}\) integrated over the set \(w_{1}<\cdots<w_{N}\). The desired result now follows from the fact that the probability density of the order statistics \(V_{(1)}<\cdots<V_{(N)}\) is \(N!f(w_{1})\cdots f(w_{N})\) for \(w_{1}<\ldots<w_{N}\).]

**Problem 6.45**:
1. For any continuous cumulative distribution function \(F\), define \(F^{-1}(0)=-\infty\), \(F^{-1}(y)=\inf\{x:F(x)=y\}\) for \(0<y<1\), \(F^{-1}(1)=\infty\) if \(F(x)<1\) for all finite \(x\), and otherwise \(\inf\{x:F(x)=1\}\). Then \(F[F^{-1}(y)]=y\) for all \(0\leq y\leq 1\), but \(F^{-1}[F(y)]\) may be \(<y\).
2. Let \(Z\) have a cumulative distribution function \(G(z)=h[F(z)]\), where \(F\) and \(h\) are continuous cumulative distribution functions, the latter defined over (0,1). If \(Y=F(Z)\), then \(P\{Y<y\}=h(y)\) for all \(0\leq y\leq 1\).
3. If \(Z\) has the continuous cumulative distribution function \(F\), then \(F(Z)\) is uniformly distributed over (0, 1).
4. \(P\{F(Z)<y\}=P\{Z<F^{-1}(y)\}=F[F^{-1}(y)]=y\).]

**Problem 6.46**: Let \(Z_{i}\) have a continuous cumulative distribution function \(F_{i}\) (\(i=1,\ldots,\,N\)), and let \(G\) be the group of all transformations \(Z_{i}^{\prime}=f(Z_{i})\) such that \(f\) is continuous and strictly increasing.

1. The transformation induced by \(f\) in the space of distributions is \(F_{i}^{\prime}=F_{i}(f^{-1})\).
2. Two \(N\)-tuples of distributions (\(F_{1},\ldots,\,F_{N}\)) and (\(F_{1}^{\prime},\ldots,\,F_{N}^{\prime}\)) belong to the same orbit with respect to \(\bar{G}\) if and only if there exist continuous distribution functions \(h_{1}\),..., \(h_{N}\) defined on (0,1) and strictly increasing continuous distribution functions \(F\) and \(F^{\prime}\) such that \(F_{i}=h_{i}(F)\) and \(F_{i}^{\prime}=h_{i}(F^{\prime})\).

[(i): \(P\{f(Z_{i})\leq y\}=P\{Z_{i}\leq f^{-1}(y)\}=F_{i}[f^{-1}(y)]\).

2. If \(F_{i}=h_{i}(F)\) and the \(F_{i}^{\prime}\) are on the same orbit, so that \(F_{i}^{\prime}=F_{i}(f^{-1})\), then \(F_{i}^{\prime}=h_{i}(F^{\prime})\) with \(F^{\prime}=F(f^{-1})\). Conversely, if \(F_{i}=h_{i}(F)\), \(F_{i}^{\prime}=h_{i}(F^{\prime})\), then \(F_{i}^{\prime}=F_{i}(f^{-1})\) with \(f=F^{\prime-1}(F)\).]

**Problem 6.47**: Under the assumptions of the preceding problem, if \(F_{i}=h_{i}(F)\), the distribution of the ranks \(T_{1},\ldots,\,T_{N}\) of \(Z_{1},\ldots,\,Z_{N}\) depends only on the \(h_{i}\), not on \(F\). If the \(h_{i}\) are differentiable, the distribution of the \(T_{i}\) is given by

\[P\{T_{1}=t_{1},\ldots,\,T_{N}=t_{n}\}=\frac{E\left[h_{1}^{\prime}\left(U_{(t_{ 1})}\right)\ldots h_{N}^{\prime}\left(U_{(t_{N})}\right)\right]}{N!}, \tag{6.66}\]

where \(U_{(1)}<\cdots<U_{(N)}\) is an ordered sample of size \(N\) from the uniform distribution \(U(0,1)\). [The left-hand side of (6.66) is the probability that of the quantities \(F(Z_{1})\),..., \(F(Z_{N})\), the \(i\)th one is the \(t_{i}\)th smallest for \(i=1,\ldots,\,N\). This is given by \(\int\ldots\int h_{1}^{\prime}(y_{1})\ldots h_{N}^{\prime}(y_{N})\,dy\) integrated over the region in which \(y_{i}\) is the \(t_{i}\)th smallest of the \(y\)'s for \(i=1,\ldots,\,N\). The proof is completed as in Problem 6.44.]

**Problem 6.48**:
1. _Distribution of order statistics._
2. If \(Z_{1},\ldots,\,Z_{N}\) is a sample from a cumulative distribution function \(F\) with density \(f\), the joint density of \(Y_{i}=Z_{(s_{i})},\ i=1,\ldots,\,n\), is \[\frac{N!f(y_{1})\ldots f(y_{n})}{(s_{1}-1)!(s_{2}-s_{1}-1)!\ldots(N-s_{n})!}\] (6.67) \[\times[F(y_{1})]^{s_{1}-1}[F(y_{2})-F(y_{1})]^{s_{2}-s_{1}-1} \ldots[1-F(y_{n})]^{N-s_{n}}\]for \(y_{1}<\cdots<y_{n}\).
2. For the particular case that the \(Z\)'s are a sample from the uniform distribution on (0,1), this reduces to \[\frac{N!}{(s_{1}-1)!(s_{2}-s_{1}-1)!\ldots(N-s_{n})!}\] (6.68) \[y_{1}^{s_{1}-1}(y_{2}-y_{1})^{s_{2}-s_{1}-1}\ldots(1-y_{n})^{N-s_{n}}.\] For \(n=1,(6.68)\) is the density of the beta distribution \(B_{s,N-s+1}\), which therefore is the distribution of the single order statistic \(Z_{(s)}\) from \(U(0,1)\).
3. Let the distribution of \(Y_{1}\),..., \(Y_{n}\) be given by (6.68), and let \(V_{i}\) be defined by \(Y_{i}=V_{i}\,V_{i+1}\ldots V_{n}\) for \(i=1,\ldots,n\). Then the joint distribution of the \(V_{i}\) is \[\frac{N!}{(s_{1}-1)!\ldots(N-s_{n})!}\prod_{i=1}^{n}v_{i}^{s_{i}-1}(1-v_{i})^ {s_{i+1}-s_{i}-1}\ \ \ \ \ (s_{n+1}=N+1),\] so that the \(V_{i}\) are independently distributed according to the beta distribution \(B_{s_{i}-i+1,s_{i+1}-s_{i}}\).

[(i): If \(Y_{1}=Z_{(s_{1})}\),..., \(Y_{n}=Z_{(s_{n})}\) and \(Y_{n+1},\ldots,Y_{N}\) are the remaining \(Z\)'s in the original order of their subscripts, the joint density of \(Y_{1}\),..., \(Y_{n}\) is \(N(N-1)\ldots(N-n+1)\int\ldots\int f(y_{n+1})\ldots f(y_{N})\,dy_{n+1}\ldots dy _{N}\) integrated over the region in which \(s_{1}-1\) of the \(y\)'s are \(<y_{1},s_{2}-s_{1}-1\) between \(y_{1}\) and \(y_{2}\),..., and \(N-s_{n}>y_{n}\). Consider any set where a particular \(s_{1}-1\) of the \(y\)'s is \(<y_{1}\), a particular \(s_{2}-s_{1}-1\) of them is between \(y_{1}\) and \(y_{2}\), and so on, There are \(N!/(s_{1}-1)!\ldots(N-s_{n})!\) of these regions, and the integral has the same value over each of them, namely \([F(y_{1})]^{s_{1}-1}[F(y_{2})-F(y_{1})]^{s_{2}-s_{1}-1}\ldots[1-F(y_{n})]^{N- s_{n}}\).]

**Problem 6.49**:
1. If \(X_{1}\),..., \(X_{m}\) and \(Y_{1},\ldots,Y_{n}\) are samples with continuous cumulative distribution functions \(F\) and \(G=h(F)\) respectively, and if \(h\) is differentiable, the distribution of the ranks \(S_{1}<\ldots<S_{n}\) of the \(Y\)'s is given by \[P\{S_{1}=s_{1},\ldots,\,S_{n}=s_{n}\}=\frac{E\left[h^{\prime}\left(U_{(s_{1}) }\right)\ldots h^{\prime}\left(U_{(s_{n})}\right)\right]}{\binom{m+n}{m}},\] (6.69) where \(U_{(1)}<\cdots<U_{(m+n)}\) is an ordered sample from the uniform distribution \(U(0,1)\).
2. If in particular \(G=F^{k}\), where \(k\) is a positive integer, (6.69) reduces to \[P\{S_{1} =s_{1},\ldots,S_{n}=s_{n}\}\] (6.70) \[=\frac{k^{n}}{\binom{m+n}{m}}\prod_{j=1}^{n}\frac{\Gamma\left(s_ {j}+jk-j\right)}{\Gamma\left(s_{j}\right)}\cdot\frac{\Gamma\left(s_{j+1}\right) }{\Gamma\left(s_{j+1}+jk-j\right)}.\]

[MISSING_PAGE_EMPTY:1920]

[MISSING_PAGE_EMPTY:1921]

among \(|Z_{1}|,\ldots|Z_{N}|\). Consider the \(N+\frac{1}{2}N(N-1)\) distinct sums \(Z_{i}+Z_{j}\) with \(i=j\) as well as \(i\neq j\). The Wilcoxon signed-rank statistic \(\sum S_{j}\), is equal to the number of these sums that are positive.
2. If the common distribution of the \(Z\)'s is \(D\), then \[E\left(\sum S_{j}\right)=\tfrac{1}{2}N(N+1)-ND(0)-\tfrac{1}{2}N(N-1)\int D(-z) \,d\,D(z).\]

[(i) Let \(K\) be the required number of positive sums. Since \(Z_{i}+Z_{j}\) is positive if and only if the \(Z\) corresponding to the larger of \(|Z_{i}|\) and \(|Z_{j}|\) is positive, \(K=\sum_{i=1}^{N}\sum_{j=1}^{N}U_{ij}\) where \(U_{ij}=1\) if \(Z_{j}>0\) and \(|Z_{i}|\leq Z_{j}\) and \(U_{ij}=0\) otherwise.]

**Problem 6.59**: Let \(Z_{1},\ldots,Z_{N}\) be a sample from a distribution with density \(f(z-\theta)\), where \(f(z)\) is positive for all \(z\) and \(f\) is symmetric about \(0\), and let \(m\), \(n\), and the \(S_{j}\) be defined as in the preceding problem.

1. The distribution of \(n\) and the \(S_{j}\) is given by \[P\{\text{the number of positive $Z$'s is $n$ and $S_{1}=s_{1},\ldots,S_{n}=s_{n}$}\}\] (6.71) \[=\frac{1}{2^{N}}E\left[\frac{f\left(V_{(r_{1})}+\theta\right) \ldots f\left(V_{(r_{n})}+\theta\right)f\left(V_{(s_{1})}-\theta\right)\ldots f \left(V_{(s_{n})}-\theta\right)}{f\left(V_{(1)}\right)\ldots f\left(V_{(N)} \right)}\right],\] where \(V_{(1)}<\cdots<V_{(N)}\), is an ordered sample from a distribution with density \(2f(v)\) for \(v>0\), and \(0\) otherwise.
2. The rank test of the hypothesis of symmetry with respect to the origin, which maximizes the derivative of the power function at \(\theta=0\) and hence maximizes the power for sufficiently small \(\theta>0\), rejects, under suitable regularity conditions, when \[-E\left[\sum_{j=1}^{n}\frac{f^{\prime}(V_{(s_{j})}}{f(V_{(s_{j})}}\right]>C.\]
3. In the particular case that \(f(z)\) is a normal density with zero mean, the rejection region of (ii) reduces to \(\sum E(V(s_{j})>C,\) where \(V_{(1)}<\cdots<V_{(N)}\) is an ordered sample from a \(\chi\)-distribution with \(1\) degree of freedom.
4. Determine a density \(f\) such that the one-sample Wilcoxon test is most powerful against the alternatives \(f(z-\theta)\) for sufficiently small positive \(\theta\).

[(i): Apply Problem 6.44(i) to find an expression for \(P\{S_{1}=s_{1},\ldots,S_{n}=s_{n}\) given that the number of positive \(Z\)'s is \(n\}\).]

**Problem 6.60**: An alternative expression for (6.71) is obtained if the distribution of \(Z\) is characterized by (\(\rho\), \(F\), \(G\)). If then \(G=h(F)\) and \(h\) is differentiable, the distribution of \(n\) and the \(S_{j}\) is given by \[\rho^{m}(1-\rho)^{n}E\left[h^{\prime}(U_{(s_{1})})\cdots h^{\prime}(U_{(s_{n})}) \right], \tag{6.72}\]

where \(U_{(1)}\), \(<\cdots<U_{(N)}\) is an ordered sample from \(U\,(0,\,1)\).

**Problem 6.61**: _Unbiased tests of symmetry._ Let \(Z_{1}\),..., \(Z_{N}\), be a sample, and let \(\phi\) be any rank test of the hypothesis of symmetry with respect to the origin such that \(z_{i}\leq z_{i}^{\prime}\) for all \(i\) implies \(\phi(z_{1},\,\ldots,\,z_{N})\leq\phi(z_{1}^{\prime},\,\ldots,\,z^{\prime}N)\). Then \(\phi\) is unbiased against the one-sided alternatives that the \(Z\)'s are stochastically larger than some random variable that has a symmetric distribution with respect to the origin.

**Problem 6.62**: _The hypothesis of randomness.7 Let \(Z_{1},\,\ldots,\,Z_{N}\) be independently distributed with distributions \(F_{1}\),..., \(F_{N}\), and let \(T_{i}\) denote the rank of \(Z_{i}\) among the \(Z\)'s. For testing the hypothesis of randomness \(F_{1}=\cdots=F_{N}\) against the alternatives \(K\) of an upward trend, namely, that \(Z_{i}\) is stochastically increasing with \(i\), consider the rejection regions_

Footnote 7: Some tests of randomness are treated in Diaconis (1988).

\[\sum it_{i}>C \tag{6.73}\]

_and_

\[\sum iE(V_{(i)})>C, \tag{6.74}\]

_where \(V_{(1)}<\cdots<V_{(N)}\) is an ordered sample from a standard normal distribution and where \(t_{i}\) is the value taken on by \(T_{i}\)._

1. _The second of these tests is most powerful among rank tests against the normal alternatives \(F=N(\gamma+i\delta,\,\sigma^{2})\) for sufficiently small \(\delta\)._
2. _Determine alternatives against which the first test is a most powerful rank test._
3. _Both tests are unbiased against the alternatives of an upward trend; so is any rank test_ \(\phi\) _satisfying_ \(\phi(z_{1},\ldots,z_{N})\leq\phi(z_{1}^{\prime},\,\ldots,\,z_{N}^{\prime})\) _for any two points for which_ \(i<j\)_,_ \(z_{i}<z_{j}\) _implies_ \(z_{i}^{\prime}<z_{j}^{\prime}\) _for all_ \(i\) _and_ \(j\)_._

[(iii): Apply Problem 6.52 with \({\cal C}\) the class of transformations \(z_{1}^{\prime}=z_{1}\), \(z_{i}^{\prime}=f_{i}(z_{i})\) for \(i>1\), where \(z<f_{2}(z)<\cdots<f_{N}(z)\) and each \(f_{i}\) is nondecreasing. If \({\cal F}_{0}\) is the class of \(N\)-tuples (\(F_{1}\),..., \(F_{N}\)) with \(F_{1}=\cdots=F_{N}\), then \({\cal F}_{1}\) coincides with the class \(K\) of alternatives.]

**Problem 6.63**: In the preceding problem let \(U_{ij}=1\) if \((j-i)(Z_{j}-Z_{i})>0\), and = 0 otherwise.

1. The test statistic \(\sum iT_{i}\), can be expressed in terms of the \(U\)'s through the relation \[\sum_{i=1}^{N}iT_{i}=\sum_{i<j}(j-i)U_{ij}+\frac{N(N+1)(N+2)}{6}.\]
2. The smallest number of steps [in the sense of Problem 6.42(ii)] by which \((Z_{1},\,\ldots,\,Z_{N})\) can be transformed into the ordered sample \((Z_{(1)},\,\ldots,\,Z_{(N)})\) is \([N(N-1)/2]-U\), where \(U=\sum_{i<j}U_{ij}\). This suggests \(U>C\) as another rejection region for the preceding problem.

[(i): Let \(V_{ij}=1\) or \(0\) as \(Z_{i}\leq Z_{i}\) or \(Z_{i}>Z_{j}\). Then \(T_{j}=\sum_{i=1}^{N}V_{ij}\), and \(V_{ij}=U_{ij}\) or \(1-U_{ij}\) as \(i<j\) or \(i\geq j\). Expressing \(\sum_{j=1}^{N}jT_{j}=\sum_{j=1}^{N}j\sum_{i=1}^{N}V_{ij}\) in terms of the \(U\)'s and using the fact that \(U_{ij}=U_{ji}\), the result follows by a simple calculation.]

**Problem 6.64**: _The hypothesis of independence. Let \((X_{1},\,Y_{1}),\ldots,\,(X_{N},\,Y_{N})\) be a sample from a bivariate distribution, and \((X_{(1)},\,Z_{1}),\ldots,\,(X_{(N)},\,Z_{N})\) be the same sample arranged according to increasing values of the \(X\)'s so that the \(Z\)'s are a permutation of the \(Y\)'s. Let \(R_{i}\) be the rank of \(X_{i}\) among the \(X\)'s, \(S_{i}\) the rank of \(Y_{i}\) among the \(Y\)'s, and \(T_{i}\) the rank of \(Z_{i}\) among the \(Z\)'s, and consider the hypothesis of independence of \(X\) and \(Y\) against the alternatives of positive regression dependence._

1. _Conditionally, given \((X_{(1)},\,\ldots,\,X_{(N)})\), this problem is equivalent to testing the hypothesis of randomness of the \(Z\)'s against the alternatives of an upward trend._
2. _The test (_6.73_) is equivalent to rejecting when the rank correlation coefficient_8 _is_

Footnote 8: For further material on these and other tests of independence, see Kendall (1970), Aiyar, Guillier, and Albers (1979), Kallenberg and Ledwina (1999).
3. _An alternative expression for the rank correlation coefficient_8 _is_

Footnote 8: For further material on these and other tests of independence, see Kendall (1970), Aiyar, Guillier, and Albers (1979), Kallenberg and Ledwina (1999).
4. \[1-\frac{6}{N^{3}-N}\sum(S_{i}-R_{i})^{2}=1-\frac{6}{N^{3}-N}\sum(T_{i}-i)^{2}.\]
5. _The test_ \(U>C\) _of Problem_ 6.63_(ii) is equivalent to rejecting when Kendall's \(t\)-statistic \(\sum_{i<j}V_{ij}/N(N-1)\) is too large where \(V_{ij}\) is \(+1\) or \(-1\) as \((Y_{j}-Y_{i})(X_{j}-X_{i})\) is positive or negative._
6. _The tests (ii) and (iv) are unbiased against the alternatives of positive regression dependence._

**Section 6.11**:

**Problem 6.65**: In Example 6.11.1, a family of sets \(S(x,\,y)\) is a class of equivariant confidence sets if and only if there exists a set \(\mathcal{R}\) of real numbers such that

\[S(x,\,y)=\bigcup_{r\in\mathcal{R}}\{(\xi,\eta):(x-\xi)^{2}+(y-\eta)^{2}=r^{2}\}.\]

[MISSING_PAGE_FAIL:323]

\[P_{\theta}\left\{\frac{\sqrt{n}\bar{X}}{\sqrt{\sum(X_{i}-\bar{X})^{2}/(n-1)}}\leq C (\theta)\right\}=1-\alpha.\]
* Determine \(\underline{\theta}\) when the \(x\)'s are 7.6, 21.2, 15.1, 32.0, 19.7, 25.3, 29.1, 18.4 and the confidence level is \(1-\alpha=.95\).

**Problem 6.70**:
* Let \((X_{1},\,Y_{1}),\,\ldots,\,(X_{n},\,Y_{n})\) be a sample from a bivariate normal distribution, and let \[\underline{\rho}=C^{-1}\left(\frac{\sum(X_{i}-\bar{X})(Y_{i}-\bar{Y})}{\sqrt{ \sum(X_{i}-\bar{X})^{2}\sum(Y_{i}-\bar{Y})^{2}}}\right),\] where \(C(\rho)\) is determined such that \[P_{\theta}\left\{\frac{\sum(X_{i}-\bar{X})(Y_{i}-\bar{Y})}{\sqrt{\sum(X_{i}- \bar{X})^{2}\sum(Y_{i}-\bar{Y})^{2}}}\leq C(\rho)\right\}=1-\alpha.\] Then \(\underline{\rho}\) is a lower confidence limit for the population correlation coefficient \(\rho\) at confidence level \(1-\alpha\); it is uniformly most accurate invariant with respect to the group of transformations \(X_{i}^{\prime}=a\,X_{i}+b\), \(Y_{i}^{\prime}=c\,Y_{i}+d\), with \(ac>0\), \(-\infty<b\), \(d<\infty\).
* Determine \(\underline{\rho}\) at level \(1-\alpha=.95\) when the observations are (12.9,.56), (9.8,.92), (13.1,.42), (12.5,1.01), (8.7,.63), (10.7,.58), (9.3,.72), (11.4,.64).

_Note_. The following problems explore the relationship between pivotal quantities and equivariant confidence sets. For more details see Arnold (1984).

Let \(X\) be distributed according \(P_{\theta,\vartheta}\), and consider confidence sets for \(\theta\) that are equivariant under a group \(G^{*}\), as in Section 6.11. If \(w\) is the set of possible \(\theta\)-values, define a group \(\tilde{G}\) on \(\mathcal{X}\times w\) by \(\tilde{g}(\theta,x)=(gx,\,\tilde{g}\theta)\).

**Problem 6.71**: Let \(V(X,\,\theta)\) be any pivotal quantity [i.e., have a fixed probability distribution independent of \((\theta,\,\vartheta)\)], and let \(B\) be any set in the range space of \(V\) with probability \(P(V\in B)=1-\alpha\). Then the sets \(S(x)\) defined by

\[\theta\in S(x)\ \ \ \mbox{if and only if}\ \ \ V(\theta,x)\in B \tag{6.75}\]

are confidence sets for \(\theta\) with confidence coefficient \(1-\alpha\).

**Problem 6.72**:
* If \(\tilde{G}\) is transitive over \(\mathcal{X}\times w\) and \(V(X,\,\theta)\) is maximal invariant under \(\tilde{G}\), then \(V(X,\theta)\) is pivotal.
* By (i), any quantity \(W(X,\theta)\) which is invariant under \(\tilde{G}\) is pivotal; give an example showing that the converse need not be true.

**Problem 6.73**: Under the assumptions of the preceding problem, the confidence set \(S(x)\) is equivariant under \(G^{*}\).

**Problem 6.74**: Under the assumptions of Problem 6.72, suppose that a family of confidence sets \(S(x)\) is equivariant under \(G^{*}\). Then there exists a set \(B\) in the range space of the pivotal \(V\) such that (6.75) holds. In this sense, all equivariant confidence sets can be obtained from pivotals.

[Let \(A\) be the subset of \(\mathcal{X}\times w\) given by \(A=\{(x,\,\theta):\,\theta\in S(x)\}\). Show that \(\tilde{g}A=A\), so that any orbit of \(\tilde{G}\) is either in \(A\) or in the complement of \(A\). Let the maximal invariant \(V(x,\,\theta)\) be represented as in Section 6.2 by a uniquely defined point on each orbit, and let \(B\) be the set of these points whose orbits are in \(A\). Then \(V(x,\,\theta)\in B\) if and only if \((x,\theta)\in A\).] Note. Problem 6.74 provides a simple check of the equivariance of confidence sets. In Example 6.12.2, for instance, the confidence sets (6.46) are based on the pivotal vector \((X_{1}-\xi_{1},\,\ldots,\,X_{r}-\xi_{r})\), and hence are equivariant.

### _Section 6.12_

**Problem 6.75**: In Examples 6.12.1 and 6.12.2 there do not exist equivariant sets that uniformly minimize the probability of covering false values.

**Problem 6.76**: In Example 6.12.1, the density \(p(v)\) of \(V=1/S^{2}\) is unimodal.

**Problem 6.77**: Show that in Example 6.12.1,

(i): the confidence sets \(\sigma^{2}/S^{2}\in A^{**}\) with \(A^{**}\) given by (6.45) coincide with the uniformly most accurate unbiased confidence sets for \(\sigma^{2}\);

(ii): if \((a,b)\) is best with respect to (6.44) for \(\sigma\), then \((a^{r},b^{r})\) is best for \(\sigma^{r}\) (\(r>0\)).

**Problem 6.78**: Let \(X_{1},\,\ldots,\,X_{r}\) be i.i.d. \(N(0,\,1)\), and let \(S^{2}\) be independent of the \(X\)'s and distributed as \(\chi^{2}_{\nu}\). Then the distribution of \((X_{1}/S\sqrt{\nu},\,\ldots,\,X_{r}/S\sqrt{\nu})\) is a central multivariate \(t\)-distribution, and its density is

\[p(v_{1},\,\ldots,\,v_{r})=\frac{\Gamma(\frac{1}{2}(\nu+r))}{(\pi\nu)^{r/2} \Gamma(\nu/2)}\left(1+\frac{1}{\nu}\sum v_{i}^{2}\right)^{-\frac{1}{2}(\nu+r)}\,.\]

**Problem 6.79**: The confidence sets (6.52) are uniformly most accurate equivariant under the group \(G\) defined at the end of Example 6.12.3.

**Problem 6.80**: In Example 6.12.4, show that

(i): both sets (6.60) are intervals;

(ii): the sets given by \(vp(v)>C\) coincide with the intervals (5.41).

**Problem 6.81**: Let \(X_{1},\,\ldots,\,X_{m}\); \(Y_{1},\,\ldots,\,Y_{n}\) be independently normally distributed as \(N(\xi,\,\sigma^{2})\) and \(N(\eta,\,\sigma^{2})\) respectively. Determine the equivariant confidence sets for \(\eta-\xi\) that have smallest Lebesgue measure when1. \(\sigma\) is known;
2. \(\sigma\) is unknown.

**Problem 6.82**: Generalize the confidence sets of Example 6.11.3 to the case that the \(X_{i}\) are \(N(\xi_{i},d_{i}\sigma^{2})\) where the \(d\)'s are known constants.

**Problem 6.83**: Solve the problem corresponding to Example 6.12.1 when

1. \(X_{1}\),..., \(X_{n}\) is a sample from the exponential density \(E(\xi,\sigma)\), and the parameter being estimated is \(\sigma\);
2. \(X_{1}\),..., \(X_{n}\) is a sample from the uniform density \(U(\xi,\xi+\tau)\), and the parameter being estimated is \(\tau\).

**Problem 6.84**: Let \(X_{1}\),..., \(X_{n}\) be a sample from the exponential distribution \(E(\xi,\sigma)\). With respect to the transformations \(X^{\prime}_{i}=bX_{i}+a\) determine the smallest equivariant confidence sets

1. for \(\sigma\), both when size is defined by Lebesgue measure and by the equivariant measure (6.44);
2. for \(\xi\).

**Problem 6.85**: Let \(X_{ij}\) (\(j=1,\ldots,n_{i}\); \(i=1,\ldots,s\)) be samples from the exponential distribution \(E(\xi_{i},\sigma)\). Determine the smallest equivariant confidence sets for (\(\xi_{1}\),..., \(\xi_{r}\)) with respect to the group \(X^{\prime}_{ij}=bX_{ij}+a_{i}\).

**Section 6.13**:

**Problem 6.86**: If the confidence sets \(S(x)\) are equivariant under the group \(G\), then the probability \(P_{\theta}\{\theta\in S(X)\}\) of their covering the true value is invariant under the induced group \(\bar{G}\).

**Problem 6.87**: Consider the problem of obtaining a (two-sided) confidence band for an unknown continuous cumulative distribution function \(F\).

1. Show that this problem is invariant both under strictly increasing and strictly decreasing continuous transformations \(X^{\prime}_{i}=f(X_{i})\), \(i=1,\ldots,n\), and determine a maximal invariant with respect to this group.
2. Show that the problem is not invariant under the transformation \[X^{\prime}_{i}=\left\{\begin{array}{lcl}X_{i}&\mbox{if}&|X_{i}|\geq 1,\\ X_{i}-1&\mbox{if}&0<X_{i}<1,\\ X_{i}+1&\mbox{if}&-1<X_{i}<0.\end{array}\right.\]

[(ii): For this transformation \(g\), the set \(g^{*}S(x)\) is no longer a band.]

### Notes

Invariance considerations were introduced for particular classes of problems by Hotelling (1936) and Pitman (1939b). The general theory of invariant and almost invariant tests, together with its principal parametric applications, was developed by Hunt and Stein (1946) in an unpublished paper. In their paper, invariance was not proposed as a desirable property in itself but as a tool for deriving most stringent tests (cf. Chapter 8). Apart from this difference in point of view, the present account is based on the ideas of Hunt and Stein, about which E. L. Lehmann learned through conversations with Charles Stein during the years 1947-1950.

Of the admissibility results of Section 6.7, Theorem 6.7.1 is due to Birnbaum (1955) and Stein (1956a); Example 6.7.3 (continued) and Lemma 6.7.1, to Kiefer and Schwartz (1965).

The problem of minimizing the volume or diameter of confidence sets is treated in DasGupta (1991).

Deuchler (1914) appears to contain the first proposal of the two-sample procedure known as the Wilcoxon test, which was later discovered independently by many different authors. A history of this test is given by Kruskal (1957). Hoeffding (1951) derives a basic rank distribution of which (6.22) is a special case, and from it obtains locally optimum tests of the type (6.23).

## Chapter 7 Linear Hypotheses

### 7.1 A Canonical Form

Many testing problems concern the means of normal distributions and are special cases of the following _general univariate linear hypothesis_. Let \(X_{1},\ldots,X_{n}\) be independently normally distributed with means \(\xi_{1},\ldots,\xi_{n}\) and common variance \(\sigma^{2}\). The vector of means1\(\underline{\xi}\) is known to lie in a given \(s\)-dimensional linear subspace \(\prod_{\Omega}\ (s<n)\), and the hypothesis \(H\) to be tested is that \(\underline{\xi}\) lies in a given \((s-r)\)-dimensional subspace \(\prod_{\omega}\) of \(\prod_{\Omega}\ (r\leq s)\).

Footnote 1: Throughout this chapter, a fixed coordinate system is assumed given in \(n\)-space. A vector with components \(\xi_{1},\ldots,\xi_{n}\) is denoted by \(\underline{\xi}\), and an \(n\times 1\) column matrix with elements \(\xi_{1},\ldots,\xi_{n}\) by \(\hat{\xi}\). © The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 315 E. L. Lehmann and J. P. Romano, _Testing Statistical Hypotheses_, Springer Texts in Statistics, [https://doi.org/10.1007/978-3-030-70578-7_7](https://doi.org/10.1007/978-3-030-70578-7_7)

**Example 7.1.1**: In the two-sample problem of testing equality of two normal means (considered with a different notation in Section 5.3), it is given that \(\xi_{i}=\xi\) for \(i=1,\ldots,n_{1}\) and \(\xi_{i}=\eta\) for \(i=n_{1}+1,\ldots,n_{1}+n_{2}\), and the hypothesis to be tested is \(\eta=\xi\). The space \(\prod_{\Omega}\) is then the space of vectors

\[(\xi,\ldots,\xi,\eta,\ldots,\eta)=\xi(1,\ldots,1,0,\ldots,0)+\eta(0,\ldots,0,1, \ldots,1)\]

spanned by \((1,\ldots,1,0,\ldots,0)\) and \((0,\ldots,0,1,\ldots,1)\), so that \(s=2\). Similarly, \(\prod_{\omega}\) is the set of all vectors \((\xi,\ldots,\xi)=\xi(1,\ldots,1)\) and hence \(r=1\).

Another hypothesis that can be tested in this situation is \(\eta=\xi=0\). The space \(\prod_{\omega}\) is then the origin, \(s-r=0\) and hence \(r=2\). The more general hypothesis \(\xi=\xi_{0},\eta=\eta_{0}\) is not a linear hypothesis, since \(\prod_{\omega}\) does not contain the origin. However, it reduces to the previous case through the transformation \(X^{\prime}_{i}=X_{i}-\xi_{0}\ (i=1,\ldots,n_{1}),\ X^{\prime}_{i}=X_{i}-\eta_{0}\ (i=n_{1}+1,\ldots,n_{1}+n_{2})\).

**Example 7.1.2**: The regression problem of Section 5.6 is essentially a linear hypothesis. Changing the notation to make it conform with that of the present section, let \(\xi_{i}=\alpha+\beta t_{i}\), where \(\alpha\), \(\beta\) are unknown, and the \(t_{i}\) known and not all equal. Since \(\prod_{\Omega}\) is the space of all vectors \(\alpha(1,\ldots,1)+\beta(t_{1},\ldots,t_{n})\), it has dimension \(s=2\).

The hypothesis to be tested may be \(\alpha=\beta=0\) (\(r=2\)) or it may only specify that one of the parameters is zero (\(r=1\)). The more general hypotheses \(\alpha=\alpha_{0}\), \(\beta=\beta_{0}\) can be reduced to the previous case by letting \(X^{\prime}_{i}=X_{i}-\alpha_{0}-\beta_{0}t_{i}\), since then \(E(X^{\prime}_{i})=\alpha^{\prime}+\beta^{\prime}t_{i}\) with \(\alpha^{\prime}=\alpha-\alpha_{0}\), \(\beta^{\prime}=\beta-\beta_{0}\).

Higher polynomial regression and regression in several variables also fall under the linear-hypothesis scheme. Thus if \(\xi_{i}=\alpha+\beta t_{i}+\gamma t_{i}^{2}\) or more generally \(\xi_{i}=\alpha+\beta t_{i}+\gamma u_{i}\), where the \(t_{i}\) and \(u_{i}\) are known, it can be tested whether one or more of the regression coefficients \(\alpha\), \(\beta\), \(\gamma\) are zero, and by transforming to the variables \(X^{\prime}_{i}=X_{i}-\alpha_{0}-\beta_{0}t_{i}-\gamma_{0}u_{i}\) also whether these coefficients have specified values other than zero.

In the general case, the hypothesis can be given a simple form by making an orthogonal transformation to variables \(Y_{1}\),..., \(Y_{n}\)

\[Y=CX,\qquad C=(c_{ij})\quad i,\,j=1,\ldots,n, \tag{7.1}\]

such that the first \(s\) row vectors \(c_{1}\),..., \(c_{s}\) of the matrix \(C\) span \(\prod_{\Omega}\), with \(c_{r+1}\),..., \(c_{s}\), spanning \(\prod_{\omega}\). Then \(Y_{s+1}=\cdots=Y_{n}=0\) if and only if \(\underline{X}\) is in \(\prod_{\Omega}\), and \(Y_{1}=\cdots=Y_{r}=Y_{s+1}=\cdots=Y_{n}=0\) if and only if \(\underline{X}\) is in \(\prod_{\omega}\). Let \(\eta_{i}=E(Y_{i})\), so that \(\eta=C\xi\). Then since \(\underline{\xi}\) lies in \(\prod_{\Omega}\) a priori and in \(\prod_{\omega}\) under \(H\), it follows that \(\eta_{i}=0\) for \(i=s+1\),..., \(n\) in both cases, and \(\eta_{i}=0\) for \(i=1\),..., \(r\) when \(H\) is true. Finally, since the transformation is orthogonal, the variables \(Y_{1}\),..., \(Y_{n}\) are again independent and normally distributed with common variance \(\sigma^{2}\), and the problem reduces to the following canonical form.

The variables \(Y_{1}\),..., \(Y_{n}\) are independently, normally distributed with common variance \(\sigma^{2}\) and means \(E(Y_{i})=\eta_{i}\) for \(i=1,\ldots,s\) and \(E(Y_{i})=0\) for \(i=s+1\),..., \(n\), so that their joint density is

\[\frac{1}{(\sqrt{2\pi}\sigma)^{n}}\exp\left[-\frac{1}{2\sigma^{2}}\left(\sum_{i =1}^{s}(y_{i}-\eta_{i})^{2}+\sum_{i=s+1}^{n}y_{i}^{2}\right)\right]. \tag{7.2}\]

The \(\eta\)'s and \(\sigma^{2}\) are unknown, and the hypothesis to be tested is

\[H:\eta_{1}=\cdots=\eta_{r}=0\qquad(r\leq s<n). \tag{7.3}\]

**Example 7.1.3** To illustrate the determination of the transformation (7.1), consider once more the regression model \(\xi_{i}=\alpha+\beta t_{i}\), of Example 7.1.2. It was seen there that \(\prod_{\Omega}\) is spanned by (1,..., 1) and (\(t_{1}\),..., \(t_{n}\)). If the hypothesis being tested is \(\beta=0\), \(\prod_{\omega}\) is the one-dimensional space spanned by the first of these vectors. The row vector \(c_{2}\) is in \(\prod_{\omega}\) and of length 1, and hence \(c_{2}=(1/\sqrt{n}\),..., \(1/\sqrt{n})\). Since \(c_{1}\) is in \(\prod_{\Omega}\), of length 1, and orthogonal to \(c_{2}\), its coordinates are of the form \(a+bt_{i}\), \(i=1\),..., \(n\), where \(a\) and \(b\) are determined by the conditions \(\sum(a+bt_{i})=0\) and \(\sum(a+bt_{i})^{2}=1\). The solutions of these equations are \(a=-b\overline{i}\), \(b=1/\sqrt{\sum(t_{j}-\overline{t})^{2}}\), and therefore \(a+bt_{i}=(t_{i}-\overline{t})/\sqrt{\sum(t_{j}-\overline{t})^{2}}\), and \[Y_{1}=\frac{\sum X_{i}(t_{i}-\bar{t})^{2}}{\sqrt{\sum(t_{j}-\bar{t})^{2}}}=\frac{ \sum(X_{i}-\bar{X})(t_{i}-\bar{t})}{\sqrt{\sum(t_{j}-\bar{t})^{2}}}.\]

The remaining row vectors of \(C\) can be taken to be any set of orthogonal unit vectors that are orthogonal to \(\prod_{\Omega}\); it turns out not to be necessary to determine them explicitly.

If the hypothesis to be tested is \(\alpha=0\), \(\prod_{\omega}\) is spanned by \((t_{1},\ldots,t_{n})\), so that the \(i\)th coordinate of \(\underline{c}_{2}\) is \(t_{i}/\sqrt{\sum t_{j}^{2}}\). The coordinates of \(\underline{c}_{1}\) are again of the form \(a+bt_{i}\) with \(a\) and \(b\) now determined by the equations \(\sum(a+bt_{i})t_{i}=0\) and \(\sum(a+bt_{i})^{2}=1\). The solutions are \(b=-an\bar{t}/\sum t_{j}^{2},\ a=\sqrt{\sum t_{j}^{2}/n\sum(t_{j}-\bar{t})^{2}}\), and therefore

\[Y_{1}=\sqrt{\frac{n\sum t_{j}^{2}}{\sum(t_{j}-\bar{t})^{2}}}\left(\bar{X}- \frac{\bar{t}}{\sum t_{j}^{2}}\sum t_{i}X_{i}\right).\]

In the case of the hypothesis \(\alpha=\beta=0\), \(\prod_{\omega}\) is the origin, and \(\underline{c}_{1}\), \(\underline{c}_{2}\) can be taken as any two orthogonal unit vectors in \(\prod_{\Omega}\). One possible choice is that appropriate to the hypothesis \(\beta=0\), in which case \(Y_{1}\) is the linear function given there and \(Y_{2}=\sqrt{n}\bar{X}\).

The general linear-hypothesis problem in terms of the \(Y\)'s remains invariant under the group \(G_{1}\) of transformations \(Y_{i}^{\prime}=Y_{i}+c_{i}\) for \(i=r+1\), \(\ldots\), \(s\); \(Y_{i}^{\prime}=Y_{i}\) for \(i=1\), \(\ldots\), \(r\); \(s+1\), \(\ldots\), \(n\). This leaves \(Y_{1}\), \(\ldots\), \(Y_{r}\) and \(Y_{s+1}\), \(\ldots\), \(Y_{n}\) as maximal invariants. Another group of transformations leaving the problem invariant is the group \(G_{2}\) of all orthogonal transformations of \(Y_{1}\), \(\ldots\), \(Y_{r}\). The middle set of variables having been eliminated, it follows from Example 6.2.1(iii) that a maximal invariant under \(G_{2}\) is \(U=\sum_{i=1}^{r}Y_{i}^{2}\), \(Y_{s+1}\), \(\ldots\), \(Y_{n}\). This can be reduced to \(U\) and \(V=\sum_{i=s+1}^{n}Y_{i}^{2}\) by sufficiency. Finally, the problem also remains invariant under the group \(G_{3}\) of scale changes \(Y_{i}^{\prime}=cY_{i}\), \(c\neq 0\), for \(i=1\), \(\ldots\), \(n\). In the space of \(U\) and \(V\) this induces the transformation \(U^{*}=c^{2}U\), \(V^{*}=c^{2}V\), under which \(W=U/V\) is maximal invariant. Thus the principle of invariance reduces the data to the single statistic 2

Footnote 2: A corresponding reduction without assuming normality is discussed by Jagers (1980).

\[W=\frac{\sum\limits_{i=1}^{r}Y_{i}^{2}}{\sum\limits_{i=s+1}^{n}Y_{i}^{2}}. \tag{7.4}\]

Each of the three transformation groups \(G_{i}\) (\(i=1\), \(2\), \(3\)) which lead to the above reduction induces a corresponding group \(\bar{G}_{i}\) in the parameter space. The group \(\bar{G}_{1}\) consists of the translations \(\eta_{i}^{\prime}=\eta_{i}+c_{i}\) (\(i=r+1\), \(\ldots\), \(s\)), \(\eta_{i}^{\prime}=\eta_{i}\) (\(i=1\), \(\ldots\), \(r\)), \(\sigma^{\prime}=\sigma\), which leaves (\(\eta_{1}\), \(\ldots\), \(\eta_{r}\), \(\sigma\)) as maximal invariants. Since any orthogonal transformation of \(Y_{1}\), \(\ldots\), \(Y_{r}\) induces the same transformation on \(\eta_{1}\), \(\ldots\), \(\eta_{r}\) and leaves \(\sigma^{2}\) unchanged, a maximal invariant under \(\bar{G}_{2}\) is \(\left(\sum_{i=1}^{r}\eta_{i}^{2},\sigma^{2}\right)\). Finally the elements of \(\bar{G}_{3}\) are the transformations \(\eta^{\prime}_{i}=c\eta_{i}\), \(\sigma^{\,\prime}=|c|\sigma\), and hence a maximal invariant with respect to the totality of these transformations is

\[\psi^{2}=\frac{\sum\limits_{i=1}^{r}\eta_{i}^{2}}{\sigma^{\,2}}. \tag{7.5}\]

It follows from Theorem 6.3.2 that the distribution of \(W\) depends only on \(\psi^{\,2}\), so that the principle of invariance reduces the problem to that of testing the simple hypothesis \(H:\psi=0\). More precisely, the probability density of \(W\) is (cf. Problems 7.2 and 7.3)

\[p_{\psi}(w)=e^{-\frac{1}{2}\psi^{2}}\sum\limits_{k=0}^{\infty}c_{k}\frac{( \frac{1}{2}\psi^{\,2})^{k}}{k!}\frac{w^{\frac{1}{2}r-1+k}}{(1+w)^{\frac{1}{2} (r+n-s)+k}}, \tag{7.6}\]

where

\[c_{k}=\frac{\Gamma\left[\frac{1}{2}(r+n-s)+k\right]}{\Gamma(\frac{1}{2}r+k) \Gamma[\frac{1}{2}(n-s)]}.\]

For any \(\psi_{1}\) the ratio \(p_{\psi_{1}}(w)/p_{0}(w)\) is an increasing function of \(w\), and it follows from the Neyman-Pearson fundamental lemma that the most powerful invariant test for testing \(\psi=0\) against \(\psi=\psi_{1}\) rejects when \(W\) is too large, or equivalently when

\[W^{*}=\frac{\sum\limits_{i=1}^{r}Y_{i}^{2}/r}{\sum\limits_{i=s+1}^{n}Y_{i}^{2} /(n-s)}>C. \tag{7.7}\]

The cutoff point \(C\) is determined so that the probability of rejection is \(\alpha\) when \(\psi=0\). Since in this case \(W^{*}\) is the ratio of two independent \(\chi^{2}\) variables, each divided by the number of its degrees of freedom, the distribution of \(W^{*}\) is the \(F\)-distribution with \(r\) and \(n-s\) degrees of freedom, and hence \(C\) is determined by

\[\int_{C}^{\infty}F_{r,n-s}(y)dy=\alpha. \tag{7.8}\]

The test is independent of \(\psi_{1}\), and hence is UMP among all invariant tests. By Theorem 6.5.2, it is also UMP among all tests whose power function depends only on \(\psi^{\,2}\).

The rejection region (7.7) can also be expressed in the form

\[\frac{\sum\limits_{i=1}^{r}Y_{i}^{2}}{\sum\limits_{i=1}^{r}Y_{i}^{2}+\sum \limits_{i=s+1}^{n}Y_{i}^{2}}>C^{\,\prime}. \tag{7.9}\]When \(\psi=0\), the left-hand side is distributed according to the beta-distribution with \(r\) and \(n\,-s\) degrees of freedom [defined through (5.24)], so that \(C^{\prime}\) is determined by

\[\int_{C^{\prime}}^{1}B_{\frac{1}{2}r,\frac{1}{2}(n-s)}(y)\,dy=\alpha. \tag{7.10}\]

For an alternative value of \(\psi\), the left-hand side of (7.9) is distributed according to the _noncentral beta-distribution_ with noncentrality parameter \(\psi\), the density of which is (Problem 7.3)

\[g_{\psi}(y)=e^{-\frac{1}{2}\psi^{2}}\sum_{k=0}^{\infty}\frac{\left(\frac{1}{2} \psi^{2}\right)^{k}}{k!}B_{\frac{1}{2}r+k,\frac{1}{2}(n-s)}(y). \tag{7.11}\]

The power of the test against an alternative \(\psi\) is therefore 3

Footnote 3: Tables of the power of the F-test are provided by Tiku (1967, 1972) [reprinted in Graybill (1976)] and Cohen (1977); charts are given in Pearson and Hartley (1972). Various approximations are discussed by Johnson, Kotz, and Balakrishnan (1995).

\[\beta(\psi)=\int_{C^{\prime}}^{1}g_{\psi}(y)\,dy.\]

In the particular case \(r=1\) the rejection region (7.7) reduces to

\[\frac{|Y_{1}|}{\sqrt{\sum_{i=s+1}^{n}Y_{i}^{2}/(n-s)}}>C_{0}. \tag{7.12}\]

This is a two-sided \(t\)-test which by the theory of Chapter 5 (see for example Problem 5.5) is UMP unbiased. On the other hand, no UMP unbiased test exists for \(r>1\).

The \(F\)-test (7.7) shares the admissibility properties of the two-sided \(t\)-test discussed in Section 6.7. In particular, the test is admissible against distant alternatives \(\psi^{2}\geq\psi_{1}^{2}\) (Problem 7.6) and against nearby alternatives \(\psi^{2}\leq\psi_{2}^{2}\) (Problem 7.7). It was shown by Lehmann and Stein (1953) that the test is in fact admissible against the alternatives \(\psi^{2}\leq\psi_{1}^{2}\) for any \(\psi_{1}\) and hence against all invariant alternatives.

### Linear Hypotheses and Least Squares

In applications to specific problems it is usually not convenient to carry out the reduction to canonical form explicitly. The test statistic \(W\) can be expressed in terms of the original variables by noting that \(\sum_{i=s+1}^{n}Y_{i}^{2}\) is the minimum value of 

[MISSING_PAGE_FAIL:333]

\[W^{*}=\frac{\sum\limits_{i=1}^{n}(\hat{\xi}_{i}-\hat{\xi}_{i})^{2} \big{/}r}{\sum\limits_{i=1}^{n}(X_{i}-\hat{\xi}_{i})^{2}/(n-s)}. \tag{7.15}\]

It is desirable to express also the _noncentrality parameter_\(\psi^{2}=\sum_{i=1}^{r}\eta_{i}^{2}/\sigma^{2}\) in terms of the \(\xi\)'s. Now \(X=C^{-1}Y\), \(\xi=C^{-1}\eta\), and

\[\sum\limits_{i=1}^{r}Y_{i}^{2}=\sum\limits_{i=1}^{n}(X_{i}-\hat{ \xi}_{i})^{2}-\sum\limits_{i=1}^{n}(X_{i}-\hat{\xi}_{i})^{2}. \tag{7.16}\]

If the right-hand side of (7.16) is denoted by \(f(X)\), it follows that \(\sum_{i=1}^{r}\eta_{i}^{2}=f(\xi)\).

A slight generalization of a linear hypothesis is the inhomogeneous hypothesis which specifies for the vector of means \(\underline{\xi}\) a sub-hyperplane \(\prod_{\omega}^{\prime}\) of \(\prod_{\Omega}\) not passing through the origin. Let \(\prod_{\omega}\) denote the subspace of \(\prod_{\Omega}\) which passes through the origin and is parallel to \(\prod_{\omega}^{\prime}\). If \(\underline{\xi}^{0}\) is any point of \(\prod_{\omega}^{\prime}\), the set \(\prod_{\omega}^{\prime}\) consists of the totality of points \(\underline{\xi}=\underline{\xi}^{*}+\underline{\xi}^{0}\) as \(\underline{\xi}^{*}\) ranges over \(\prod_{\omega}\). Applying the transformation (7.1) with respect to \(\prod_{\omega}\), the vector of means \(\underline{\eta}\) for \(\underline{\xi}\in\prod_{\omega}^{\prime}\) is then given by \(\eta=C\xi=C\xi^{*}+C\xi^{0}\) in the canonical form (7.2), and the totality of these vectors is therefore characterized by the equations \(\eta_{1}=\eta_{1}^{0}\),..., \(\eta_{r}=\eta_{r}^{0}\), \(\eta_{s+1}=\cdots=\eta_{n}=0\), where \(\eta_{i}^{0}\) is the \(i\)th coordinate of \(C\xi^{0}\). In the canonical form, the inhomogeneous hypothesis \(\underline{\xi}\in\prod_{\omega}^{\prime}\) therefore becomes \(\eta_{i}=\eta_{i}^{0}\) (\(i=1,\)..., \(r\)). This reduces to the homogeneous case on replacing \(Y_{i}\) with \(Y_{i}-\eta_{i}^{0}\), and it follows from (7.7) that the UMP invariant test has the rejection region

Figure 7.1: Least squares projection

\[\frac{\sum_{i=1}^{r}(Y_{i}-\eta_{i}^{o})^{2}/r}{\sum_{i=s+1}^{n}Y_{i}^{2}/(n-s)}>C\, \tag{7.17}\]

and that the noncentrality parameter is \(\psi^{2}=\sum_{i=1}^{r}(\eta_{i}-\eta_{i}^{0})^{2}/\sigma^{2}\).

In applications it is usually most convenient to apply the transformation \(X_{i}-\xi_{i}^{0}\) directly to (7.14) or (7.15). It follows from (7.17) that such a transformation always leaves the denominator unchanged. This can also be seen geometrically, since the transformation is a translation of \(n\)-space parallel to \(\prod_{\Omega}\) and therefore leaves the distance \(\sum(X_{i}-\hat{\xi}_{i})^{2}\) from \(\underline{X}\) to \(\prod_{\Omega}\) unchanged. The noncentrality parameter can be computed as before by replacing \(X\) with \(\xi\) in the transformed numerator (7.16).

Some examples of linear hypotheses, all with \(r=1\), were already discussed in Chapter 5. The following treats two of these from the present point of view.

**Example 7.2.1**: Let \(X_{1},\ldots,X_{n}\) be independently, normally distributed with common mean \(\mu\) and variance \(\sigma^{2}\), and consider the hypothesis \(H:\mu=0\). Here \(\prod_{\Omega}\) is the line \(\xi_{i}=\cdots=\xi_{n}\), \(\prod_{o}\) is the origin, and \(s=r=1\). Let \(\bar{X}=n^{-1}\sum_{i}X_{i}\). From the identity

\[\sum(X_{i}-\mu)^{2}=\sum(X_{i}-\bar{X})^{2}+n(\bar{X}-\mu)^{2}\,\]

it is seen that \(\hat{\xi}_{i}=\bar{X}\), while \(\hat{\xi}_{i}=0\). The test statistic and \(\psi^{2}\) are therefore given by

\[W=\frac{n\bar{X}^{2}}{\sum(X_{i}-\bar{X})^{2}}\ \ \ \mbox{and}\ \ \ \psi^{2}= \frac{n\mu^{2}}{\sigma^{2}}.\]

Under the hypothesis, the distribution of \((n-1)W\) is that of the square of a variable having Student's \(t\)-distribution with \(n-1\) degrees of freedom.

**Example 7.2.2**: In the two-sample problem considered in Example 7.1.1 with \(n=n_{1}+n_{2}\), the sum of squares

\[\sum_{i=1}^{n_{1}}(X_{i}-\xi)^{2}+\sum_{i=n_{1}+1}^{n}(X_{i}-\eta)^{2}\]

is minimized by

\[\hat{\xi}=X_{\cdot}^{(1)}=\sum_{i=1}^{n_{1}}\frac{X_{i}}{n_{1}},\ \ \ \ \ \hat{\eta}=X_{\cdot}^{(2)}=\sum_{i=n_{1}+1}^{n}\frac{X_{i}}{n_{2}},\]

while, under the hypothesis \(\eta-\xi=0\),

\[\hat{\hat{\xi}}=\hat{\hat{\eta}}=\bar{X}=\frac{n_{1}X_{\cdot}^{(1)}+n_{2}X_{ \cdot}^{(2)}}{n}.\]

[MISSING_PAGE_FAIL:336]

### Tests of Homogeneity

The UMP invariant test obtained in the preceding section for testing the equality of the means of two normal distributions with common variance is also UMP unbiased (Section 5.3). However, when a number of populations greater than 2 is to be tested for homogeneity of means, a UMP unbiased test no longer exists, so that invariance considerations lead to a new result. Let \(X_{ij}\) (\(j=1,\ldots,n_{i}\); \(i=1,\ldots,s\)) be independently distributed as \(N\)(\(\mu_{i}\), \(\sigma^{2}\)), and consider the hypothesis

\[H:\mu_{1}=\cdots=\mu_{s}.\]

This arises, for example, in the comparison of a number of different treatments, processes, varieties, or locations, when one wishes to test whether these differences have any effect on the outcome \(X\). It may arise more generally in any situation involving a _one-way classification_ of the outcomes, that is, in which the outcomes are classified according to a single factor. In such situations, when rejecting \(H\) one will frequently want to know more about the \(\mu s\) than just that they are unequal. The resulting multiple comparison problem will be discussed in Section 9.5.

The hypothesis \(H\) is a linear hypothesis with \(r=s-1\), with \(\prod_{\Omega}\) given by the equations \(\hat{\xi}_{ij}=\hat{\xi}_{ik}\) for \(j\), \(k=1,\ldots,n\), \(i=1,\ldots,s\) and with \(\prod_{\omega}\) the line on which all \(n=\sum n_{i}\) coordinates \(\hat{\xi}_{ij}\) are equal. We have

\[\sum\sum(X_{ij}-\mu_{i})^{2}=\sum\sum(X_{ij}-X_{i.})^{2}+\sum n_{i}(X_{i.}-\mu _{i})^{2}\]

with \(X_{i.}=\sum_{j=1}^{n_{i}}X_{ij}/n_{i}\), and hence \(\hat{\xi}_{ij}=X_{i.}\). Also,

\[\sum\sum(X_{ij}-\mu)^{2}=\sum\sum(X_{ij}-X_{..})^{2}+n(X_{..}-\mu)^{2}\]

with \(X_{..}=\sum\sum X_{ij}/n\), so that \(\hat{\hat{\xi}}_{ij}=X_{..}\). Using the form (7.15) of \(W^{*}\), the test therefore becomes

\[W^{*}=\frac{\sum n_{i}(X_{i.}-X_{..})^{2}/(s-1)}{\sum\sum(X_{ij}-X_{i.})^{2}/( n-s)}>C. \tag{7.19}\]

The noncentrality parameter is

\[\psi^{2}=\frac{\sum n_{i}(\mu_{i}-\mu_{.})^{2}}{\sigma^{2}}\]

with

\[\mu_{.}=\frac{\sum n_{i}\mu_{i}}{n}.\]The sum of squares in both numerator and denominator of (7.19) admits three interpretations, which are closely related: (i) as the two components in the decomposition of the total variation

\[\sum\sum(X_{ij}-X_{..})^{2}=\sum\sum(X_{ij}-X_{i.})^{2}+\sum n_{i}(X_{i.}-X_{..})^ {2},\]

of which the first represents the variation within, and the second the variation between populations; (ii) as a basis, through the test (7.19), for comparing these two sources of variation; (iii) as estimates of their expected values, \((n-s)\sigma^{2}\) and \((s-1)\sigma^{2}+\sum n_{i}(\mu_{i}-\mu.)^{2}\) (Problem 7.11). This breakdown of the total variation, together with the various interpretations of the components, is an example of an _analysis of variance_,4 which will be applied to more complex problems in the succeeding sections.

Footnote 4: For conditions under which such a breakdown is possible, see Albert (1976).

When applying the principle of invariance, it is important to make sure that the underlying symmetry assumptions really are satisfied. In the problem of testing the equality of a number of normal means \(\mu_{1},\ldots,\mu_{s}\), for example, all parameter points, which have the same value of \(\psi^{2}=\sum n_{i}(\mu_{i}-\mu.)^{2}/\sigma^{2}\), are identified under the principle of invariance. This is appropriate only when these alternatives can be considered as being equidistant from the hypothesis. In particular, it should then be immaterial whether the given value of \(\psi^{2}\) is built up by a number of small contributions or a single large one. Situations where instead the main emphasis is on the detection of large individual deviations do not possess the required symmetry, and the test based on (7.19) need no longer be optimum.

The robustness properties against nonnormality of the \(F\)-test for testing equality of means will be discussed using a large sample approach in Section 13.2, as well as the corresponding test for equality of variances. Alternatively, permutation tests will be applied in Section 17.2.

Instead of assuming \(X_{ij}\) is normally distributed, suppose that \(X_{ij}\) has distribution \(F(x-\mu_{i})\), where \(F\) is an arbitrary distribution with finite variance. If \(F\) has heavy tails, the test (7.19) tends to be inefficient. More efficient tests can be obtained by generalizing the considerations of Sections 6.8 and 6.9. Suppose the \(X_{ij}\) are samples of size \(n_{i}\) from continuous distributions \(F_{i}\) (\(i=1,\ldots,s\)) and that we wish to test \(H:F_{1}=\cdots=F_{s}\). Invariance, by the argument of Section 6.8, then reduces the data to the ranks \(R_{ij}\) of the \(X_{ij}\) in the combined sample of \(n=\sum n_{i}\) observations. A natural analogue of the two-sample Wilcoxon test is the Kruskal-Wallis test, which rejects \(H\) when \(\sum n_{i}(R_{i.}-R_{..})^{2}\) is too large. For the shift model \(F_{i}(y)=F(y-\mu_{i})\), the performance of this test relative to (7.19) is similar to that of the Wilcoxon to the \(t\)-test in the case \(s=2\); the notion of asymptotic relative efficiency will be developed in Section 15.2. The theory of this and related rank tests is developed in books on nonparametric statistics such as Randles and Wolfe (1979), Hettmansperger (1984), Gibbons and Chakraborti (1992), Lehmann (1998), and Hajek, Sidak and Sen (1999).

Unfortunately, such rank tests are available only for the simplest linear models. An alternative approach capable of achieving similar efficiencies for much widerclasses of linear models can be obtained through large-sample theory, which will be studied in Chapters 11-13. Briefly, the least squares estimators may be replaced by estimators with better efficiency properties for nonnormal distributions. Furthermore, asymptotically valid significance levels can be obtained through "Studentization",5 that is, by dividing the statistic by a suitable estimator of its standard deviation; see Section 13.2. Different ways of implementing such a program are reviewed, for example, by Draper (1981, 1983), McKean and Schrader (1982), Ronchetti (1982) and Hettmansperger, McKean and Sheather (2000). [For a simple alternative of this kind to Student's \(t\)-test, see Prescott (1975).]

Footnote 5: This term (after Student, the pseudonym of W. S. Gosset) is a misnomer. The procedure of dividing the sample mean \(\bar{X}\) by its estimated standard deviation and referring the resulting statistic to the standard normal distribution (without regard to the distribution of the \(X\)’s) was used already by Laplace. Student’s contribution consisted of pointing out that if the \(X\)’s are normal, the approximate normal distribution of the \(t\)-statistic can be replaced by its exact distribution—Student’s \(t\).

Sometimes, it is of interest to test the hypothesis \(H:\mu_{1}=\cdots=\mu_{s}\) considered at the beginning of the section, against only the ordered alternatives \(\mu_{1}\leq\cdots\leq\mu_{s}\) rather than against the general alternatives of any inequalities among the \(\mu\)'s. Then the \(F\)-test (7.19) is no longer reasonable; more powerful alternative tests for this and other problems involving ordered alternatives are discussed by Robertson, Wright, and Dykstra (1988). The problem of testing \(H\) against one-sided alternatives such as \(K:\xi_{i}\geq 0\) for all \(i\), with at least one inequality strict, is treated by Perlman (1969) and in Barlow et al. (1972), which gives a survey of the literature; also see Tang (1994), Liu and Berger (1995) and Perlman and Wu (1999). Minimal complete classes and admissibility for this and related problems are discussed by Marden (1982a) and Cohen and Sackrowitz (1992).

### Two-Way Layout: One Observation Per Cell

The hypothesis of equality of several means arises when a number of different treatments, procedures, varieties, or manifestations of some other factors are to be compared. Frequently one is interested in studying the effects of more than one factor, or the effects of one factor as certain other conditions of the experiment vary, which then play the role of additional factors. In the present section we shall consider the case that the number of factors affecting the outcomes of the experiment is two.

Suppose that one observation is obtained at each of a number of levels of these factors, and denote by \(X_{ij}\) (\(i=1,\ldots,a\); \(j=1,\ldots,b\)) the value observed when the first factor is at the \(i\)th and the second at the \(j\)th level. It is assumed that the \(X_{ij}\) are independently normally distributed with constant variance \(\sigma^{2}\), and for the moment also that the two factors act independently (they are then said to be _additive_), so that \(\xi_{ij}\) is of the form \(\alpha^{\prime}_{i}+\beta^{\prime}_{j}\). Putting \(\mu=\alpha^{\prime}_{\cdot}+\beta^{\prime}_{\cdot}\) and \(\alpha_{i}=\alpha^{\prime}_{i}-\alpha^{\prime}\), \(\beta_{j}=\beta^{\prime}_{j}-\beta^{\prime}_{\cdot}\), this can be written as

\[\xi_{ij}=\mu+\alpha_{i}+\beta_{j},\qquad\sum\alpha_{i}=\sum\beta_{j}=0, \tag{7.20}\]where the \(\alpha\)'s and \(\beta\)'s (the _main effects_ of \(A\) and \(B\)) and \(\mu\) are uniquely determined by (7.20) as6

Footnote 6: The replacing of a subscript by a dot indicates that the variable has been averaged with respect to that subscript.

\[\alpha_{i}=\xi_{i.}-\xi_{\cdot.},\qquad\beta_{j}=\xi_{\cdot j}-\xi_{\cdot.}, \qquad\mu=\xi_{\cdot.} \tag{7.21}\]

Consider the hypothesis

\[H:\alpha_{1}=\cdots=\alpha_{a}=0 \tag{7.22}\]

that the first factor has no effect on the outcome being observed. This arises in two quite different contexts. The factor of interest, corresponding say to a number of treatments, may be \(\beta\), while \(\alpha\) corresponds to a classification according to, for example, the site on which the observations are obtained (farm, laboratory, city, etc.). The hypothesis then represents the possibility that this subsidiary classification has no effect on the experiment so that it need not be controlled. Alternatively, \(\alpha\) may be the (or a) factor of primary interest. In this case, the formulation of the problem as one of hypothesis testing would usually be an oversimplification, since in case of rejection of \(H\), one would require estimates of the \(\alpha\)'s or at least a grouping according to high and low values.

The hypothesis \(H\) is a linear hypothesis with \(r=a-1\), \(s=1+(a-1)+(b-1)=a+b-1\), and \(n-s=(a-1)(b-1)\). The least squares estimates of the parameters under \(\Omega\) can be obtained from the identity

\[\sum\sum\big{(}X_{ij}-\xi_{ij}\big{)}^{2} = \sum\sum\big{(}X_{ij}-\mu-\alpha_{i}-\beta_{j}\big{)}^{2}\] \[= \sum\sum\big{[}\big{(}X_{ij}-X_{i.}-X_{\cdot.j}+X_{\cdot.}\big{)} +(X_{i.}-X_{\cdot.}-\alpha_{i})\] \[\qquad\qquad+\big{(}X_{\cdot.j}-X_{\cdot.}-\beta_{j}\big{)}+(X_{ \cdot.}-\mu)\big{]}^{2}\] \[= \sum\sum\big{(}X_{ij}-X_{i.}-X_{\cdot.j}+X_{\cdot.}\big{)}^{2}\] \[\qquad\qquad+b\sum\big{(}X_{i.}-X_{\cdot.}-\alpha_{i}\big{)}^{2}\] \[+a\sum\big{(}X_{\cdot.j}-X_{\cdot.}-\beta_{j}\big{)}^{2}+ab\;(X_{ \cdot.}-\mu)^{2}\,,\]

which is valid because in the expansion of the third sum of squares the cross-product terms vanish. It follows that

\[\hat{\alpha}_{i}=X_{i.}-X_{\cdot.},\qquad\hat{\beta}_{j}=X_{\cdot.j}-X_{\cdot.}, \qquad\hat{\mu}=X_{\cdot.}, \tag{7.23}\]

and that

\[\sum\sum\big{(}X_{ij}-\hat{\xi}_{ij}\big{)}^{2}=\sum\sum\big{(}X_{ij}-X_{i.}- X_{\cdot.j}+X_{\cdot.}\big{)}^{2}\,.\]Under the hypothesis \(H\) we still have \(\hat{\beta}_{j}=X_{.j}-X_{..}\) and \(\hat{\tilde{\mu}}=X_{..}\), and hence \(\hat{\xi}_{ij}-\hat{\tilde{\xi}}_{ij}=X_{i.}-X_{..}\) The best invariant test therefore rejects when

\[W^{*}=\frac{b\sum\left(X_{i.}-X_{..}\right)^{2}/(a-1)}{\sum\sum\left(X_{ij}-X_{i.}-X_{.j}+X_{..}\right)^{2}/(a-1)(b-1)}>C. \tag{7.24}\]

The noncentrality parameter, on which the power of the test depends, is given by

\[\psi^{2}=\frac{b\sum(\xi_{i.}-\xi_{..})^{2}}{\sigma^{2}}=\frac{b\sum\alpha_{i} ^{2}}{\sigma^{2}}. \tag{7.25}\]

This problem provides another example of an analysis of variance. The total variation can be broken into three components,

\[\begin{array}{ll}\sum\sum(X_{ij}-X_{..})^{2}=&b\sum(X_{i.}-X_{..})^{2}+a\sum (X_{.j}-X_{..})^{2}\\ &+\sum\sum(X_{ij}-X_{i.}-X_{.j}+X_{..})^{2}.\end{array}\]

Of these, the first contains the variation due to the \(\alpha\)'s, the second that due to the \(\beta\)'s. The last component, in the canonical form of Section 7.1, is equal to \(\sum_{i=s+1}^{n}Y_{i}^{2}\). It is therefore the sum of squares of those variables whose means are zero even under \(\Omega\). Since this residual part of the variation, which on division by \(n-s\) is an estimate of \(\sigma^{2}\), cannot be attributed to any effects such as the \(\alpha\)'s or \(\beta\)'s, it is frequently labeled "error", as an indication that it is due solely to the randomness of the observations, not to any differences of the means. Actually, the breakdown is not quite as sharp as is suggested by the above description. Any component such as that attributed to the \(\alpha\)'s always also contains some "error", as is seen for example from its expectation, which is

\[E\sum(X_{i.}-X_{..})^{2}=(a-1)\sigma^{2}+b\sum\alpha_{i}^{2}.\]

Instead of testing whether a certain factor has any effect, one may wish to estimate the size of the effect at the various levels of the factor. Other parameters that are sometimes interesting to estimate are the average outcomes (for example, yields) \(\xi_{1},\ldots,\xi_{a.}\) when the factor is at the various levels. If \(\theta_{i}=\mu+\alpha_{i}=\xi_{i.}\), confidence sets for (\(\theta_{1},\ldots,\theta_{a}\)) are obtained by considering the hypotheses \(H(\theta^{0}):\theta_{i}=\theta_{i}^{0}(i=1,\ldots,a)\). For testing \(\theta_{1}=\cdots=\theta_{a}=0\), the least squares estimates of the \(\xi_{ij}\) are \(\hat{\xi}_{ij}=X_{i.}+X_{.j}-X_{..}\) and \(\hat{\tilde{\xi}}_{ij}=X_{.j}-X_{..}\). The denominator sum of squares is therefore \(\sum\sum(X_{ij}-X_{i.}-X_{.j}+X_{..})^{2}\) as before, while the numerator sum of squares is

\[\sum\sum\left(\hat{\xi}_{ij}-\hat{\tilde{\xi}}_{ij}\right)^{2}=b\sum X_{i.}^{ 2}.\]

The general hypothesis reduces to this special case on replacing \(X_{ij}\) with the variable \(X_{ij}-\theta_{i}^{0}\). Since \(s=a+b-1\) and \(r=a\), the hypothesis \(H(\theta^{0})\) is rejected when \[\frac{b\sum(X_{i.}-\theta_{i}^{0})^{2}/a}{\sum\sum(X_{ij}-X_{i.}-X_{.j}+X_{..})^{2} /(a-1)(b-1)}>C.\]

The associated confidence sets for \((\theta_{1},\ldots,\theta_{a})\) are the spheres

\[\sum(\theta_{i}-X_{i.})^{2}\leq\frac{aC\sum\sum(X_{ij}-X_{i.}-X_{..j}+X_{..})^{2 }}{(a-1)(b-1)b}\.\]

When considering confidence sets for the effects \(\alpha_{1},\ldots,\alpha_{a}\), one must take account of the fact that the \(\alpha\)'s are not independent. Since they add up to zero, it would be enough to restrict attention to \(\alpha_{1},\ldots,\alpha_{a-1}\). However, an easier and more symmetric solution is found by retaining all the \(\alpha\)'s. The rejection region of \(H:\alpha_{i}=\alpha_{i}^{0}\) for \(i=1,\ldots,a\) (with \(\sum\alpha_{i}^{0}=0\)) is obtained from (7.24) by letting \(X_{ij}^{\prime}=X_{ij}-\alpha_{i}^{0}\), and hence is given by

\[b\sum(X_{i.}-X_{..}-\alpha_{i}^{0})^{2}>\frac{C\sum\sum(X_{ij}-X_{i.}-X_{..j}+ X_{..})^{2}}{(b-1)}.\]

The associated confidence set consists of the totality of points \((\alpha_{1},\ldots,\alpha_{a})\) satisfying \(\sum\alpha_{i}=0\) and

\[\sum[\alpha_{i}-(X_{i.}-X_{..})]^{2}\leq\frac{C\sum(X_{ij}-X_{i.}-X_{..j}+X_{.. })^{2}}{b(b-1)}.\]

In the space of \((\alpha_{1},\ldots,\alpha_{a})\), this inequality defines a sphere whose center (\(X_{1.}-X..,\ldots,X_{a.}-X_{..}\)) lies on the hyperplane \(\sum\alpha_{i}=0\). The confidence sets for the \(\alpha\)'s therefore consist of the interior and surface of the great hyperspheres obtained by cutting the \(a\)-dimensional spheres with the hyperplane \(\sum\alpha_{i}=0\).

In both this and the previous case, the usual method shows the class of confidence sets to be invariant under the appropriate group of linear transformations, and the sets are therefore uniformly most accurate invariant.

A rank test of (7.22) analogous to the Kruskal-Wallis test for the one-way layout is Friedman's test, obtained by ranking the \(s\) observations \(X_{1j},\ldots,X_{sj}\) separately from 1 to \(s\) at each level \(j\) of the second factor. If these ranks are denoted by \(R_{1j},\ldots,R_{sj}\), Friedman's test rejects for large values of \(\sum(R_{i.}-R_{..})^{2}\). Unless \(s\) is large, this test suffers from the fact that comparisons are restricted to observations at the same level of factor 2. The test can be improved by "aligning" the observations from different levels, for example, by subtracting from each observation at the \(j\)th level its mean \(X_{.j}\) for that level, and then ranking the aligned observations from 1 to \(ab\). For a discussion of these tests and their efficiency see Lehmann (1998, Chapter 6), and for an extension to tests of (7.22) in the model (7.20) when there are several observations per cell, Mack and Skillings (1980). Further discussion is provided by Hettmansperger (1984) and Gibbons and Chakraborti (1992).

That in the experiment described at the beginning of the section there is only one observation per cell, and that as a consequence hypotheses about the \(\alpha\)'s and \(\beta\)'scannot be tested without some restrictions on the means \(\xi_{ij}\), does not of course justify the assumption of additivity. Rather, it is the other way around: the experiment should not be performed with just one observation per cell unless the factors can safely be assumed to be additive. Faced with such an experiment without prior assurance that the assumption holds, one should test the hypothesis of additivity. A number of tests for this purpose are discussed, for example, in Hegemann and Johnson (1976) and Marasinghe and Johnson (1981).

### Two-Way Layout: \(m\) Observations Per Cell

In the preceding section it was assumed that the effects of the two factors \(\alpha\) and \(\beta\) are independent and hence additive. The factors may, however, interact in the sense that the effect of one depends on the level of the other. Thus the effectiveness of a teacher depends for example on the quality or the age of the students, and the benefit derived by a crop from various amounts of irrigation depends on the type of soil as well as on the variety being planted. If the additivity assumption is dropped, the means \(\xi_{ij}\) of \(X_{ij}\) are no longer given by (7.20) under \(\Omega\) but are completely arbitrary. More than \(ab\) observations, one for each combination of levels, are then required, since otherwise \(s=n\). We shall here consider only the simple case in which the number of observations is the same at each combination of levels.

Let \(X_{ijk}\) (\(i=1,\ldots,a\); \(j=1,\ldots,b\); \(k=1,\ldots,m\)) be independent normal with common variance \(\sigma^{2}\) and mean \(E(X_{ijk})=\xi_{ij}\). In analogy with the previous notation we write

\[\xi_{ij} = \xi_{\cdot\cdot}+(\xi_{i\cdot}-\xi_{\cdot\cdot})+(\xi_{\cdot j}- \xi_{\cdot\cdot})+(\xi_{ij}-\xi_{i\cdot}-\xi_{\cdot j}+\xi_{\cdot\cdot})\] \[= \mu+\alpha_{i}+\beta_{j}+\gamma_{ij}\]

with \(\sum_{i}\alpha_{i}=\sum_{j}\beta_{j}=\sum_{i}\gamma_{ij}=\sum_{j}\gamma_{ij}=0\). Then \(\alpha_{i}\) is the average effect of factor 1 at level \(i\), averaged over the \(b\) levels of factor 2, and a similar interpretation holds for the \(\beta\)'s. The \(\gamma\)'s are called _interactions_, since \(\gamma_{ij}\) measures the extent to which the joint effect \(\xi_{ij}-\xi_{\cdot\cdot}\) of factors 1 and 2 at levels \(i\) and \(j\) exceeds the sum (\(\xi_{i\cdot}-\xi_{\cdot\cdot}\)) + (\(\xi_{\cdot j}-\xi_{\cdot\cdot}\)) of the individual effects. Consider again the hypothesis that the \(\alpha\)'s are zero. Then \(r=a-1\), \(s=ab\), and \(n-s=(m-1)ab\). From the decomposition

\[\sum\sum\sum(X_{ijk}-\xi_{ij})^{2}=\sum\sum\sum(X_{ijk}-X_{ij\cdot})^{2}+m\sum \sum(X_{ij\cdot}-\xi_{ij})^{2}\]

and

\[\sum\sum(X_{ij\cdot}-\xi_{ij})^{2} = \sum\sum(X_{ij\cdot}-X_{i\cdot\cdot}-X_{\cdot j\cdot}+X_{\cdot \cdot\cdot}-\gamma_{ij})^{2}\] \[+b\sum(X_{i\cdot\cdot}-X_{\cdot\cdot\cdot}-\alpha_{i})^{2}+a\sum (X_{\cdot j\cdot}-X_{\cdot\cdot\cdot}-\beta_{j})^{2}\] \[+ab(X_{\cdot\cdot\cdot}-\mu)^{2}\]it follows that

\[\hat{\mu}=\hat{\hat{\mu}}=\hat{\xi}_{..}=X_{...},\ \ \ \ \ \hat{\alpha}_{i}=\hat{\xi}_{i}.-\hat{\xi}_{..}=X_{i..}-X_{...},\] \[\hat{\beta}_{j}=\hat{\hat{\beta}}_{j}=\hat{\xi}_{..j}-\hat{\xi}_{.. }=X_{..j}.-X_{...},\] \[\hat{\gamma}_{ij}=\hat{\hat{\gamma}}_{ij}=X_{ij..}-X_{i..}-X_{..j} +X_{...},\]

and hence that

\[\sum\sum\sum(X_{ijk}-\hat{\xi}_{ij})^{2}=\sum\sum\sum(X_{ijk}-X_{ij.})^{2},\] \[\sum\sum\sum(\hat{\xi}_{ij}-\hat{\hat{\xi}}_{ij})^{2}=mb\sum(X_{i..}-X_{...})^{2}.\]

The most powerful invariant test therefore rejects when

\[W^{*}=\frac{mb\sum(X_{i..}-X_{...})^{2}/(a-1)}{\sum\sum\sum(X_{ijk}-X_{ij.})^{ 2}/(m-1)ab}>C, \tag{7.26}\]

and the noncentrality parameter in the distribution of \(W^{*}\) is

\[\frac{mb\sum(\xi_{i..}-\xi_{..})^{2}}{\sigma^{2}}=\frac{mb\sum\alpha_{i}^{2}} {\sigma^{2}}. \tag{7.27}\]

Another hypothesis of interest is the hypothesis \(H^{\prime}\) that the two factors are additive,7

Footnote 7: A test of \(H^{\prime}\) against certain restricted alternatives has been proposed for the case of one observation per cell by Tukey (1949a); see Hegemann and Johnson (1976) for further discussion.

\[H^{\prime}:\gamma_{ij}=0\ \ \ \mbox{for all}\ i,\ j.\]

The least squares estimates of the parameters are easily derived as before, and the UMP invariant test is seen to have the rejection region (Problem 7.13)

\[W^{*}=\frac{m\sum\sum(X_{ij..}-X_{i..}-X_{..j.}+X_{...})^{2}/(a-1)(b-1)}{\sum \sum\sum(X_{ijk}-X_{ij.})^{2}/(m-1)ab}>C. \tag{7.28}\]

Under \(H^{\prime}\), the statistic \(W^{*}\) has the \(F\)-distribution with \((a-1)(b-1)\) and \((m-1)ab\) degrees of freedom; the noncentrality parameter for any alternative set of \(\gamma\)'s is

\[\psi^{2}=\frac{m\sum\sum\gamma_{ij}^{2}}{\sigma^{2}}. \tag{7.29}\]

The decomposition of the total variation into its various components, in the present case, is given by \[\sum\sum\sum(X_{ijk}-X_{\ldots})^{2} = mb\sum(X_{i..}-X_{\ldots})^{2}+ma\sum(X_{.j..}-X_{\ldots})^{2}\] \[+m\sum\sum(X_{ij.}-X_{i..}-X_{.j.}+X_{\ldots})^{2}\] \[+\sum\sum\sum(X_{ijk}-X_{ij.})^{2}.\]

Here the first three terms contain the variation due to the \(\alpha\)'s, \(\beta\)'s and \(\gamma\)'s, respectively, and the last component corresponds to error. The tests for the hypotheses that the \(\alpha\)'s, \(\beta\)'s, or \(\gamma\)'s are zero, the first and third of which have the rejection regions (7.26) and (7.28), are then obtained by comparing the \(\alpha\), \(\beta\), or \(\gamma\) sum of squares with that for error.

An analogous decomposition is possible when the \(\gamma\)'s are assumed a priori to be equal to zero. In that case, the third component which previously was associated with \(\gamma\) represents an additional contribution to error, and the breakdown becomes

\[\sum\sum\sum(X_{ijk}-X_{\ldots})^{2} = mb\sum(X_{i..}-X_{\ldots})^{2}+ma\sum(X_{.j.}-X_{\ldots})^{2}\] \[+\sum\sum\sum(X_{ijk}-X_{i..}-X_{.j.}+X_{\ldots})^{2},\]

with the last term corresponding to error. The hypothesis \(H:\alpha_{1}=\cdots=\alpha_{a}=0\) is then rejected when

\[\frac{mb\sum(X_{i..}-X_{\ldots})^{2}/(a-1)}{\sum\sum\sum(X_{ijk}-X_{i..}-X_{.j. }+X_{\ldots})^{2}/(abm-a-b+1)}>C.\]

Suppose now that the assumption of no interaction, under which this test was derived, is not justified. The denominator sum of squares then has a noncentral \(\chi^{2}\)-distribution instead of a central one; and is therefore stochastically larger than was assumed (Problem 7.15). It follows that the actual rejection probability is less than it would be for \(\sum\sum\gamma_{ij}^{2}=0\). This shows that the probability of an error of the first kind will not exceed the nominal level of significance, regardless of the values of the \(\gamma\)'s. However, the power also decreases with increasing \(\sum\sum\gamma_{ij}^{2}/\sigma^{2}\) and tends to zero as this ratio tends to infinity.

The analysis of variance and the associated tests derived in this section for two factors extend in a straightforward manner to a larger number of factors (see, for example, Problem 7.16). On the other hand, if the number of observations is not the same for each combination of levels (each _cell_), explicit formulae for the least squares estimators may no longer be available, but there is no difficulty in computing these estimators and the associated UMP invariant tests numerically. However, in applications it is then not always clear how to define main effects, interactions, and other parameters of interest, and hence what hypothesis to test. These issues are discussed, for example, in Hocking and Speed (1975) and Speed, Hocking, and Hackney (1979). See also Lehmann and Casella (1998), Chapter 3, Example 4.9,Arnold (1981, Section 7.4), Searle (1987), McCulloch and Searle (2001), and Hocking (2003).

Of great importance are arrangements in which only certain combinations of levels occur, since they permit reducing the size of the experiment. Thus, for example, three independent factors, at \(m\) levels each, can be analyzed with only \(m^{2}\) observations, instead of the \(m^{3}\) required if 1 observation were taken at each combination of levels, by adopting a Latin-square design (Problem 7.17).

The class of problems considered here contains as a special case the two-sample problem treated in Chapter 5, which concerns a single factor with only two levels. The questions discussed in that connection regarding possible inhomogeneities of the experimental material and the randomization required to offset it are of equal importance in the present, more complex situations. If inhomogeneous material is subdivided into more homogeneous groups, this classification can be treated as constituting one or more additional factors. The choice of these groups is an important aspect in the determination of a suitable experimental design.8 A very simple example of this is discussed in Problems 5.49 and 5.50.

Footnote 8: For a discussion of various designs and the conditions under which they are appropriate see, for example, Box, Hunter, and Hunter (1978), Montgomery (2001) and Wu and Hamada (2000). Optimum properties of certain designs, proved by Wald, Ehrenfeld, Kiefer, and others, are discussed by Kiefer (1958), Silvey (1980), Atkinson and Donev (1992) and Pukelsheim (1993). The role of randomization, treated for the two-sample problem in Section 5.10, is studied by Kempthorne (1955), Wilk and Kempthorne (1955), Scheffe (1959), and others; see, for example, Lorenzen (1984) and Giesbrecht and Gumpertz (2004).

Multiple comparison procedures for two-way (and higher) layouts are discussed by Spjotvoll (1974); additional references can be obtained from Miller (1977b, 1986) and Westfall and Young (1993). The more general problem of multiple testing will be treated in Chapter 9.

### Regression

Hypotheses specifying one or both of the regression coefficients \(\alpha\), \(\beta\) when \(X_{1}\),..., \(X_{n}\) are independently normally distributed with common variance \(\sigma^{2}\) and means

\[\xi_{i}=\alpha+\beta t_{i} \tag{7.30}\]

are essentially linear hypotheses, as was pointed out in Example 7.1.2. The hypotheses \(H_{1}:\alpha=\alpha_{0}\) and \(H_{2}:\beta=\beta_{0}\) were treated in Section 5.6, where they were shown to possess UMP unbiased tests. We shall now consider \(H_{1}\) and \(H_{2}\), as well as the hypothesis \(H_{3}:\alpha=\alpha_{0}\), \(\beta=\beta_{0}\), from the present point of view. By the general theory of Section 7.1, the resulting tests will be UMP invariant under suitable groups of linear transformations. For the first two cases, in which \(r=1\), this also provides, by the argument of Section 6.6, an alternative proof of their being UMP unbiased.

The space \(\prod_{\Omega}\) is the same for all three hypotheses. It is spanned by the vectors \((1,\,\ldots,\,1)\) and \((t_{1},\,\ldots,\,t_{n})\) and therefore has dimension \(s=2\) unless the \(t_{i}\) are all equal, which we shall assume not to be the case. The least squares estimates \(\alpha\) and \(\beta\) under \(\Omega\) are obtained by minimizing \(\sum(X_{i}\,-\,\alpha\,-\,\beta t_{i})^{2}\). For any fixed value of \(\beta\), this is achieved by the value \(\alpha=\bar{X}-\beta\bar{t}\), for which the sum of squares reduces to \(\sum[(X_{i}\,-\,\bar{X})\,-\,\beta(t_{i}\,-\,\bar{t})]^{2}\). By minimizing this with respect to \(\beta\) one finds

\[\hat{\beta}=\frac{\sum(X_{i}\,-\,\bar{X})(t_{i}\,-\,\bar{t})}{\sum(t_{j}\,-\, \bar{t})^{2}},\ \ \ \ \ \hat{\alpha}=\bar{X}-\hat{\beta}\bar{t}; \tag{7.31}\]

and

\[\sum(X_{i}\,-\,\hat{\alpha}\,-\,\hat{\beta}t_{i})^{2}=\sum(X_{i}\,-\,\bar{X}) ^{2}-\hat{\beta}^{2}\sum(t_{i}\,-\,\bar{t})^{2}\]

is the denominator sum of squares for all three hypotheses. The numerator of the test statistic (7.7) for testing the two hypotheses \(\alpha=0\) and to \(\beta=0\) is \(Y_{1}^{2}\), and for testing \(\alpha=\beta=0\) is \(Y_{1}^{2}+Y_{2}^{2}\).

For the hypothesis \(\alpha=0\), the statistic \(Y_{1}\) was shown in Example 7.1.3 to be equal to

\[\left(\bar{X}\,-\,\bar{t}\,\frac{\sum t_{i}\,X_{i}}{\sum t_{j}^{2}}\right) \sqrt{n\,\frac{\sum t_{j}^{2}}{\sum(t_{j}\,-\,\bar{t})^{2}}}=\hat{\alpha}\sqrt {n\,\frac{\sum(t_{j}\,-\,\bar{t})^{2}}{\sum t_{j}^{2}}}.\]

Since then

\[E(Y_{1})=\alpha\sqrt{n\,\frac{\sum(t_{j}\,-\,\bar{t})^{2}}{\sum t_{j}^{2}}},\]

the hypothesis \(\alpha=\alpha_{0}\) is equivalent to the hypothesis

\[E(Y_{1})=\eta_{1}^{0}=\alpha_{0}\sqrt{n\sum(t_{j}\,-\,\bar{t})^{2}/\sum t_{j} ^{2}}\,\]

for which the rejection region (7.17) is

\[(n\,-\,s)(Y_{1}\,-\,\eta_{1}^{0})^{2}/\sum_{i=s+1}^{n}Y_{i}^{2}>C_{0}\]

and hence

\[\frac{|\hat{\alpha}-\alpha_{0}|\sqrt{n\,\sum(t_{j}\,-\,\bar{t})^{2}/\sum t_{j }^{2}}}{\sqrt{\sum(X_{i}\,-\,\hat{\alpha}\,-\,\hat{\beta}t_{i})^{2}/(n\,-\,2) }}>C_{0}. \tag{7.32}\]

For the hypothesis \(\beta=0\), \(Y_{1}\) was shown to be equal to \[\frac{\sum(X_{i}-\bar{X})(t_{i}-\bar{t})}{\sqrt{\sum(t_{j}-\bar{t})^{2}}}=\hat{ \beta}\sqrt{\sum(t_{j}-\bar{t})^{2}}.\]

Since then \(E(Y_{1})=\beta\sqrt{\sum(t_{j}-\bar{t})^{2}}\), the hypothesis \(\beta=\beta_{0}\) is equivalent to \(E(Y_{1})=\eta_{1}^{0}=\beta_{0}\sqrt{\sum(t_{j}-\bar{t})^{2}}\) and the rejection region is

\[\frac{|\hat{\beta}-\beta_{0}|\sqrt{\sum(t_{j}-\bar{t})^{2}}}{\sqrt{\sum(X_{i}- \hat{\alpha}-\hat{\beta}t_{i})^{2}/(n-2)}}>C_{0}.\]

For testing \(\alpha=\beta=0\), it was shown in Example 7.1.3 that

\[Y_{1}=\hat{\beta}\sqrt{\sum(t_{j}-\bar{t})^{2}},\qquad Y_{2}=\sqrt{n}\bar{X}= \sqrt{n}(\hat{\alpha}+\hat{\beta}\bar{t});\]

the numerator of (7.7) is therefore

\[\frac{Y_{1}^{2}+Y_{2}^{2}}{2}=\frac{n(\hat{\alpha}+\hat{\beta}\bar{t})^{2}+ \hat{\beta}^{2}\sum(t_{j}-\bar{t})^{2}}{2}.\]

The more general hypothesis \(\alpha=\alpha_{0}\), \(\beta=\beta_{0}\) is equivalent to \(E(Y_{1})=\eta_{1}^{0}\), \(E(Y_{2})=\eta_{2}^{0}\), where \(\eta_{1}^{0}=\beta_{0}\sqrt{\sum(t_{j}-\bar{t})^{2}}\), \(\eta_{2}^{0}=\sqrt{n}(\alpha_{0}+\beta_{0}\bar{t})\); and the rejection region (7.17) can therefore be written as

\[\frac{\left[n(\hat{\alpha}-\alpha_{0})^{2}+2n\bar{t}(\hat{\alpha}-\alpha_{0})( \hat{\beta}-\beta_{0})+\sum t_{i}^{2}(\hat{\beta}-\beta_{0})^{2}\right]/2}{\sum( X_{i}-\hat{\alpha}-\hat{\beta}t_{i})^{2}/(n-2)}>C.\]

The associated confidence sets for (\(\alpha\), \(\beta\)) are obtained by reversing this inequality and replacing \(\alpha_{0}\) and \(\beta_{0}\) by \(\alpha\) and \(\beta\). The resulting sets are ellipses centered at (\(\hat{\alpha}\), \(\hat{\beta}\)).

The simple regression model (7.30) can be generalized in many directions; the means \(\xi_{i}\) may, for example, be polynomials in \(t_{1}\) of higher than the first degree (see Problem 7.20), or more complex functions such as trigonometric polynomials; or they may be functions of several variables, \(t_{i}\), \(u_{i}\), \(v_{i}\). Some further extensions will now be illustrated by a number of examples.

**Example 7.6.1** A variety of problems arise when there is more than one regression-line. Suppose that the variables \(X_{ij}\) are independently normally distributed with common variance and means

\[\xi_{ij}=\alpha_{i}+\beta_{i}t_{ij}\qquad(j=1,\ldots,n_{i};\quad i=1,\ldots,b).\]

The hypothesis that these regression lines have equal slopes

\[H:\beta_{1}=\cdots=\beta_{b}\]may occur for example when the equality of a number of growth rates is to be tested. The parameter space \(\prod_{\Omega}\) has dimension \(s=2b\) provided none of the sums \(\sum_{j}(t_{ij}-t_{i.})^{2}\) is zero; the number of constraints imposed by the hypothesis is \(r=b-1\). The minimum value of \(\sum\sum(X_{ij}-\hat{\xi}_{ij})^{2}\) under \(\Omega\) is obtained by minimizing \(\sum_{j}(X_{ij}-\alpha_{i}-\beta_{i}t_{ij})^{2}\) for each \(i\), so that by (7.31),

\[\hat{\beta}_{i}=\frac{\sum_{j}(X_{ij}-X_{i.})(t_{ij}-t_{i.})}{\sum_{j}(t_{ij}-t _{i.})^{2}},\quad\hat{\alpha}_{i}=X_{i.}-\hat{\beta}_{i}t_{i.}\.\]

Under \(H\), one must minimize \(\sum\sum(X_{ij}-\alpha_{i}-\beta t_{ij})^{2}\), which for any fixed \(\beta\) leads to \(\alpha_{i}=X_{i.}-\beta t_{i.}\) and reduces the sum of squares to \(\sum\sum[(X_{ij}-X_{i.})-\beta(t_{ij}-t_{i.})]^{2}\). Minimizing this with respect to \(\beta\), one finds

\[\hat{\hat{\beta}}=\frac{\sum\sum(X_{ij}-X_{i.})(t_{ij}-t_{i.})}{\sum\sum(t_{ij }-t_{i.})^{2}},\quad\hat{\hat{\hat{\alpha}}}_{i}=X_{i.}-\hat{\hat{\beta}}_{i.}\]

Since

\[X_{ij}-\hat{\xi}_{ij}=X_{ij}-\hat{\alpha}_{i}-\hat{\beta}_{i}t_{ij}=(X_{ij}-X_ {i.})-\hat{\beta}_{i}(t_{ij}-t_{i.})\]

and

\[\hat{\xi}_{ij}-\hat{\hat{\xi}}_{ij}=(\hat{\alpha}_{i}-\hat{\hat{\alpha}}_{i})+ t_{ij}(\hat{\beta}_{i}-\hat{\hat{\beta}})=(\hat{\beta}_{i}-\hat{\hat{\beta}})(t_{ ij}-t_{i.}),\]

the rejection region (7.15) is

\[\frac{\sum_{i}(\hat{\beta}_{i}-\hat{\hat{\beta}})^{2}\sum_{j}(t_{ij}-t_{i.})^ {2}/(b-1)}{\sum\sum\bigl{[}(X_{ij}-X_{i.})-\hat{\beta}_{i}(t_{ij}-t_{i.}) \bigr{]}^{2}/(n-2b)}>C,\]

where the left-hand side under \(H\) has the \(F\)-distribution with \(b-1\) and \(n-2b\) degrees of freedom.

Since

\[E(\hat{\beta}_{i})=\beta_{i}\quad\mbox{and}\quad E(\hat{\hat{\beta}})=\frac{ \sum_{i}\beta_{i}\sum_{j}(t_{ij}-t_{i.})^{2}}{\sum\sum(t_{ij}-t_{i.})^{2}}\,\]

the noncentrality parameter of the distribution for an alternative set of \(\beta\)'s is \(\psi^{2}=\sum_{i}(\beta_{i}-\tilde{\beta})^{2}\sum_{j}(t_{ij}-t_{i.})^{2}/ \sigma^{2}\), where \(\tilde{\beta}=E(\hat{\tilde{\beta}})\). In the particular case that the \(n_{i}\) and the \(t_{ij}\) are independent of \(i\), \(\tilde{\beta}\) reduces to \(\tilde{\beta}=\sum\beta_{j}/b\).

**Example 7.6.2**: The regression model (7.35) arises in the comparison of a number of treatments when the experimental units are treated as fixed and the unit effects \(u_{ij}\) (defined in Section 5.9) are proportional to known constants \(t_{ij}\). Here \(t_{ij}\) might, for example, be a measure of the fertility of the \(i\), \(j\)th piece of land or the weight of the \(i\), \(j\)th experimental animal prior to the experiment. It is then frequently possible to assume that the proportionality factor \(\beta_{i}\) does not depend on the treatment, in which case (7.35) reduces to

\[\hat{\xi}_{ij}=\alpha_{i}+\beta t_{ij} \tag{7.37}\]

and the hypothesis of no treatment effect becomes

\[H:\alpha_{1}=\cdots=\alpha_{b}.\]

The space \(\prod_{\Omega}\) coincides with \(\prod_{\alpha}\) of the previous example, so that \(s=b+1\) and

\[\hat{\beta}=\frac{\sum\sum(X_{ij}-X_{i.})(t_{ij}-t_{i.})}{\sum\sum(t_{ij}-t_{i. })^{2}},\qquad\hat{\alpha}_{i}=X_{i.}-\hat{\beta}t_{i.}.\]

Minimization of \(\sum\sum(X_{ij}-\alpha-\beta t_{ij})^{2}\) gives

\[\hat{\hat{\beta}}=\frac{\sum\sum(X_{ij}-X_{.})(t_{ij}-t_{.})}{\sum\sum(t_{ij}- t_{.})^{2}},\qquad\hat{\hat{\alpha}}=X_{..}-\hat{\hat{\beta}}t_{..},\]

where \(X_{..}=\sum\sum X_{ij}/n\), \(t_{..}=\sum\sum t_{ij}/n\), \(n=\sum n_{i}\). The sum of squares in the numerator of \(W^{*}\) in (7.15) is thus

\[\sum\sum\bigl{(}\hat{\xi}_{ij}-\hat{\hat{\xi}}_{ij}\bigr{)}^{2}=\sum\sum \bigl{[}(X_{i.}-X_{..})+\hat{\beta}(t_{ij}-t_{i.})-\hat{\hat{\beta}}(t_{ij}-t_ {..})\bigr{]}^{2}.\]

The hypothesis \(H\) is therefore rejected when

\[\frac{\sum\bigl{[}(X_{i.}-X_{..})+\hat{\beta}(t_{ij}-t_{i.})-\hat{\hat{\beta}} (t_{ij}-t_{..})\bigr{]}^{2}/(b-1)}{\sum\sum\bigl{[}(X_{ij}-X_{i.})-\hat{\beta} (t_{ij}-t_{i.})\bigr{]}^{2}/(n-b-1)}>C\, \tag{7.38}\]

where under \(H\) the left-hand side has the \(F\)-distribution with \(b-1\) and \(n-b-1\) degrees of freedom.

The hypothesis \(H\) can be tested without first ascertaining the values of the \(t_{ij}\); it is then the hypothesis of no effect in a one-way classification considered in Section 7.3, and the test is given by (7.19). Actually, since the unit effects \(u_{ij}\) are assumed to be constants, which are now completely unknown, the treatments are assigned to the units either completely at random or at random within subgroups. The appropriate test is then a randomization test for which (7.19) is an approximation.

Example 7.6.2 illustrates the important class of situations in which an analysis of variance (in the present case concerning a one-way classification) is combined with a regression problem (in the present case linear regression on the single "concomitant variable" \(t\)). Both parts of the problem may of course be considerably more complex than was assumed here. Quite generally, in such combined problems one can test (or estimate) the treatment effects as was done above, and a similar analysis can be given for the regression coefficients. The breakdown of the variation into its various treatment and regression components is the so-called _analysis of covariance_.

### Random-Effects Model: One-Way Classification

In the factorial experiments discussed in Sections 7.3, 7.4, and 7.5, the factor levels were considered fixed, and the associated effects (the \(\mu\)'s in Section 7.3, the \(\alpha\)'s, \(\beta\)'s and \(\gamma\)'s in Sections 7.4 and 7.5) to be unknown constants. However, in many applications, these levels and their effects instead are (unobservable) random variables. If all the effects are constant or all random, one speaks of _fixed-effects model_ (_model I_) or _random-effects model_ (_model II_) respectively, and the term _mixed model_ refers to situations in which both types occur.9 Of course, only the model I case constitutes a linear hypothesis according to the definition given at the beginning of the chapter. In the present section we shall treat as model II the case of a single factor (one-way classification), which was analyzed under the model I assumption in Section 7.3.

Footnote 9: For an exposition of random-effects models, see Sahai and Ojeda (2004).

As an illustration of this problem, consider a material such as steel, which is manufactured or processed in batches. Suppose that a sample of size \(n\) is taken from each of \(s\) batches and that the resulting measurements \(X_{ij}\) (\(j=1,\ldots,n\); \(i=1,\ldots,s\)) are independently normally distributed with variance \(\sigma^{2}\) and mean \(\xi_{i}\). If the factor corresponding to \(i\) were constant, with the same effect \(\alpha_{i}\) in each replication of the experiment, we would have

\[\xi_{i}=\mu+\alpha_{i}\qquad\left(\sum\alpha_{i}=0\right)\]

and

\[X_{ij}=\mu+\alpha_{i}+U_{ij}\,\]

where the \(U_{ij}\) are independently distributed as \(N(0,\sigma^{2})\). The hypothesis of no effect is \(\xi_{1}=\cdots=\xi_{s}\), or equivalently \(\alpha_{1}=\cdots=\alpha_{s}=0\). However, the effect is associated with the batches, of which a new set will be involved in each replication of the experiment; the effect therefore does not remain constant. Instead, we shall suppose that the batch effects constitute a sample from a normal distribution, and to indicate their random nature we shall write \(A_{i}\) for \(\alpha_{i}\), so that

\[X_{ij}=\mu+A_{i}+U_{ij}. \tag{7.39}\]

The assumption of additivity (lack of interaction) of batch and unit effect, in the present model, implies that the \(A\)'s and \(U\)'s are independent. If the expectation of \(A_{i}\) is absorbed into \(\mu\), it follows that the \(A\)'s and \(U\)'s are independently normally distributed with zero means and variances \(\sigma_{A}^{2}\) and \(\sigma^{2}\), respectively. The \(X\)'s of course are no longer independent.

The hypothesis of no batch effect, that the \(A\)'s are zero and hence constant, takes the form

\[H:\sigma_{A}^{2}=0.\]This is not realistic in the present situation, but is the limiting case of the hypothesis

\[H(\Delta_{0}):\frac{\sigma_{A}^{2}}{\sigma^{2}}\leq\Delta_{0}\]

that the batch effect is small relative to the variation of the material within a batch. These two hypotheses correspond respectively to the model I hypotheses \(\sum\alpha_{i}^{2}=0\) and \(\sum\alpha_{i}^{2}/\sigma^{2}\leq\Delta_{0}\).

To obtain a test of \(H(\Delta_{0})\) it is convenient to begin with the same transformation of variables that reduced the corresponding model I problem to canonical form. Each set \((X_{i1},\ldots,X_{in})\) is subjected to an orthogonal transformation \(Y_{ij}=\sum_{k=1}^{n}c_{jk}X_{ik}\) such that \(Y_{i1}=\sqrt{n}X_{i}..\) Since \(c_{1k}=1/\sqrt{n}\) for \(k=1,\ldots,n\) (see Example 7.1.3), it follows from the assumption of orthogonality that \(\sum_{k=1}^{n}c_{jk}=0\) for \(j=2,\ldots,n\) and hence that \(Y_{ij}=\sum_{k=1}^{n}c_{jk}U_{ik}\) for \(j>1\). The \(Y_{ij}\) with \(j>1\) are therefore independently normally distributed with zero mean and variance \(\sigma^{2}\). They are also independent of \(U_{i}\). since \((\sqrt{n}U_{i}\cdot-Y_{i2}\ldots Y_{in})^{\prime}=C(U_{i1}U_{i2}\ldots U_{in}) ^{\prime}\) (a prime indicates the transpose of a matrix). On the other hand, the variables \(Y_{i1}=\sqrt{n}X_{i\cdot}=\sqrt{n}(\mu+A_{i}+U_{i\cdot})\) are also independently normally distributed but with mean \(\sqrt{n}\mu\) and variance \(\sigma^{2}+n\sigma_{A}^{2}\). If an additional orthogonal transformation is made from \((Y_{11},\ldots,Y_{s1})\) to \((Z_{11},\ldots,Z_{s1})\) such that \(Z_{11}=\sqrt{s}Y_{\cdot 1}\), the \(Z\)'s are independently normally distributed with common variance \(\sigma^{2}+n\sigma_{A}^{2}\) and means \(E(Z_{11})=\sqrt{sn}\mu\) and \(E(Z_{i1})=0\) for \(i>1\). Putting \(Z_{ij}=Y_{ij}\) for \(j>1\) for the sake of conformity, the joint density of the \(Z\)'s is then

\[(2\pi)^{-ns/2}\sigma^{-(n-1)s}\bigl{(}\sigma^{2}+n\sigma_{A}^{2} \bigr{)}^{-s/2} \tag{7.40}\] \[\times\exp\left[-\frac{1}{2\bigl{(}\sigma^{2}+n\sigma_{A}^{2} \bigr{)}}\left(\bigl{(}z_{11}-\sqrt{sn}\mu\bigr{)}^{2}+\sum_{i=2}^{s}z_{i1}^{ 2}\right)-\frac{1}{2\sigma^{2}}\sum_{i=1}^{s}\sum_{j=2}^{n}z_{ij}^{2}\right].\]

The problem of testing \(H(\Delta_{0})\) is invariant under addition of an arbitrary constant to \(Z_{11}\), which leaves the remaining \(Z\)'s as a maximal set of invariants. These constitute samples of size \(s(n-1)\) and \(s-1\) from two normal distributions with means zero and variances \(\sigma^{2}\) and \(\tau^{2}=\sigma^{2}+n\sigma_{A}^{2}\).

The hypothesis \(H(\Delta_{0})\) is equivalent to \(\tau^{2}/\sigma^{2}\leq 1+\Delta_{0}n\), and the problem reduces to that of comparing two normal variances, which was considered in Example 6.3.5 without the restriction to zero means. The UMP invariant test, under multiplication of all \(Z_{ij}\) by a common positive constant, has the rejection region

\[W^{*}=\frac{1}{1+\Delta_{0}n}\cdot\frac{S_{A}^{2}/(s-1)}{S^{2}/(n-1)s}>C, \tag{7.41}\]

where

\[S_{A}^{2}=\sum_{i=2}^{s}Z_{i1}^{2}\quad\text{and}\quad S^{2}=\sum_{i=1}^{s} \sum_{j=2}^{n}Z_{ij}^{2}=\sum_{i=1}^{s}\sum_{j=2}^{n}Y_{ij}^{2}.\]The constant \(C\) is determined by

\[\int_{C}^{\infty}F_{s-1,(n-1)s}(y)\,dy=\alpha.\]

Since

\[\sum_{j=1}^{n}Y_{ij}^{2}-Y_{i1}^{2}=\sum_{j=1}^{n}U_{ij}^{2}-nU_{i.}^{2}\]

and

\[\sum_{i=1}^{s}Z_{i1}^{2}-Z_{11}^{2}=\sum_{i=1}^{s}Y_{i1}^{2}-Y_{\cdot\cdot 1}^{2},\]

the numerator and denominator sums of squares of \(W^{*}\), expressed in terms of the \(X\)'s, become

\[S_{A}^{2}=n\sum_{i=1}^{s}(X_{i.}-X_{\cdot\cdot})^{2}\ \ \ \text{and}\ \ \ S^{2}=\sum_{i=1}^{s}\sum_{j=1}^{n}(X_{ij}-X_{i.})^{2}.\]

In the particular case \(\Delta_{0}=0\), the test (7.41) is equivalent to the corresponding model I test (7.19), but they are of course solutions of different problems, and also have different power functions. Instead of being distributed according to a noncentral \(\chi^{2}\)-distribution as in model I, the numerator sum of squares of \(W^{*}\) is proportional to a central \(\chi^{2}\)-variable even when the hypothesis is false, and the power of the test (7.41) against an alternative value of \(\Delta\) is obtained from the \(F\)-distribution through

\[\beta(\Delta)=P_{\Delta}\{W^{*}>C\}=\int_{\frac{1+\Delta_{0}n}{1+\Delta n}C}^{ \infty}F_{s-1,(n-1)s}(y)\,dy.\]

The family of tests (7.41) for varying \(\Delta_{0}\) is equivalent to the confidence statements

\[\underline{\Delta}=\frac{1}{n}\left[\frac{S_{A}^{2}/(s-1)}{C\,S^{2}/(n-1)s}-1 \right]\leq\Delta. \tag{7.42}\]

The corresponding upper confidence bounds for \(\Delta\) are obtained from the tests of the hypotheses \(\Delta\geq\Delta_{0}\). These have the acceptance regions \(W^{*}\geq C^{\prime}\), where \(W^{*}\) is given by (7.41) and \(C^{\prime}\) is determined by

\[\int_{C^{\prime}}^{\infty}F_{s-1,(n-1)s}=1-\alpha\.\]

The resulting confidence bounds are \[\Delta\leq\frac{1}{n}\left[\frac{S_{A}^{2}/(s-1)}{C^{\prime}S^{2}/(n-1)s}-1\right]= \bar{\Delta}. \tag{7.43}\]

Both the confidence sets (7.42) and (7.43) are equivariant with respect to the group of transformations generated by those considered for the testing problems, and hence are uniformly most accurate equivariant.

When \(\underline{\Delta}\) is negative, the confidence set (\(\underline{\Delta},\infty\)) contains all possible values of the parameter \(\Delta\). For small \(\Delta\), this will happen with high probability (\(1-\alpha\) for \(\Delta=0\)), as must be the case, since \(\underline{\Delta}\) is then required to be a safe lower bound for a quantity which is equal to or near zero. Even more awkward is the possibility that \(\bar{\Delta}\) is negative, so that the confidence set (\(-\infty,\,\bar{\Delta}\)) is empty. An interpretation is suggested by the fact that this occurs if and only if the hypothesis \(\Delta\geq\Delta_{0}\) is rejected for all positive values of \(\Delta_{0}\). This may be taken as an indication that the assumed model is not appropriate,10 although it must be realized that for small \(\Delta\) the probability of the event \(\bar{\Delta}<0\) is near \(\alpha\) even when the assumptions are satisfied, so that this outcome will occasionally be observed.

Footnote 10: For a discussion of possibly more appropriate alternative models, see Smith and Murray (1984).

The tests of \(\Delta\leq\Delta_{0}\) and \(\Delta\geq\Delta_{0}\) are not only UMP invariant but also UMP unbiased, and UMP unbiased tests also exist for testing \(\Delta=\Delta_{0}\) against the two-sided alternatives \(\Delta\neq\Delta_{0}\). This follows from the fact that the joint density of the \(Z\)'s constitutes an exponential family. The confidence sets associated with these three families of tests are then uniformly most accurate unbiased (Problem 7.21). That optimum unbiased procedures exist in the model II case but not in the corresponding model I problem is explained by the different structure of the two hypotheses. The model II hypothesis \(\sigma_{A}^{2}=0\) imposes one constraint, since it concerns the single parameter \(\sigma_{A}^{2}\). On the other hand, the corresponding model I hypothesis \(\sum_{i=1}^{s}\sigma_{i}^{2}=0\) specifies the values of the \(s\) parameters \(\alpha_{1},\ldots,\alpha_{s}\), and since \(s-1\) of these are independent, imposes \(s-1\) constraints.

A UMP invariant test of \(\Delta\leq\Delta_{0}\) does not exist if the sample sizes \(n_{i}\) are unequal. An invariant test with a weaker optimum property for this case is obtained by Spjotvoll (1967).

Since \(\Delta\) is a ratio of variances, it is not surprising that the test statistic \(W^{*}\) is quite sensitive to the assumption of normality; such robustness issues are discussed in Section 13.2.1. More robust alternatives are discussed, for example, by Arvesen and Layard (1975). Westfall (1989) compares invariant variance ratio tests in mixed models.

Optimality of standard \(F\) tests in balanced ANOVA models with mixed effects is derived in Mathew and Sinha (1988a) and optimal tests in some unbalanced designs are derived in Mathew and Sinha (1988b).

### Nested Classifications

The theory of the preceding section does not carry over even to so simple a situation as the general one-way classification with unequal numbers in the different classes (Problem 7.24). However, the unbiasedness approach does extend to the important case of a _nested_ (hierarchical) classification with equal numbers in each class. This extension is sufficiently well indicated by carrying it through for the case of two factors; it follows for the general case by induction with respect to the number of factors.

Returning to the illustration of a batch process, suppose that a single batch of raw material suffices for several batches of the finished product. Let the experimental material consist of \(ab\) batches, \(b\) coming from each of \(a\) batches of raw material, and let a sample of size \(n\) be taken from each. Then (7.39) becomes

\[X_{ijk}=\mu+A_{i}+B_{ij}+U_{ijk} \tag{7.44}\] \[(i=1,\ldots,a;\quad j=1,\ldots,b;\quad k=1,\ldots,n),\]

where \(A_{i}\) denotes the effect of the \(i\)th batch of raw material, \(B_{ij}\) that of the \(j\)th batch of finished product obtained from this material, and \(U_{ijk}\) the effect of the \(k\)th unit taken from this batch. All these variables are assumed to be independently normally distributed with zero means and with variances \(\sigma_{A}^{2}\), \(\sigma_{B}^{2}\), and \(\sigma^{2}\), respectively. The main part of the induction argument consists of proving the existence of an orthogonal transformation to variables \(Z_{ijk}\), the joint density of which, except for a constant, is

\[\exp\left[-\frac{1}{2\left(\sigma^{2}+n\sigma_{B}^{2}+bn\sigma_{A }^{2}\right)}\left(\left(z_{111}-\sqrt{abn}\mu\right)^{2}+\sum_{i=2}^{a}z_{i11 }^{2}\right)\right.\] \[\left.-\frac{1}{2\left(\sigma^{2}+n\sigma_{B}^{2}\right)}\sum_{i= 1}^{a}\sum_{j=2}^{b}z_{ij1}^{2}-\frac{1}{2\sigma^{2}}\sum_{i=1}^{a}\sum_{j=1}^ {b}\sum_{k=2}^{n}z_{ijk}^{2}\right]. \tag{7.45}\]

As a first step, there exists for each fixed \(i\), \(j\) an orthogonal transformation from \((X_{ij1},\ldots,X_{ijn})\) to \((Y_{ij1},\ldots,Y_{ijn})\) such that

\[Y_{ij1}=\sqrt{n}X_{ij}.=\sqrt{n}\mu+\sqrt{n}(A_{i}+B_{ij}+U_{ij}.).\]

As in the case of a single classification, the variables \(Y_{ijk}\) with \(k>1\) depend only on the \(U\)'s, are independently normally distributed with zero mean and variance \(\sigma^{2}\), and are independent of the \(U_{ij}.\) On the other hand, the variables \(Y_{ij1}\) have exactly the structure of the \(Y_{ij}\) in the one-way classification,

\[Y_{ij1}=\mu^{\prime}+A_{i}^{\prime}+U_{ij}^{\prime},\]where \(\mu^{\prime}=\sqrt{n}\mu\), \(A^{\prime}_{i}=\sqrt{n}A_{i}\), \(U^{\prime}_{ij}=\sqrt{n}(B_{ij}+U_{ij}.)\), and where the variances of \(A^{\prime}_{i}\) and \(U^{\prime}_{ij}\) are \({\sigma^{\prime}}^{2}_{A}=n\sigma^{2}_{A}\) and \({\sigma^{\prime}}^{2}={\sigma}^{2}+n\sigma^{2}_{B}\), respectively. These variables can therefore be transformed to variables \(Z_{ij1}\) whose density is given by (7.40) with \(Z_{ij1}\) in place of \(Z_{ij}\). Putting \(Z_{ijk}=Y_{ijk}\) for \(k>1\), the joint density of all \(Z_{ijk}\) is then given by (7.45).

Two hypotheses of interest can be tested on the basis of (7.45)--\(H_{1}:\sigma^{2}_{A}/({\sigma}^{2}+n\sigma^{2}_{B})\leq\Delta_{0}\) and \(H_{2}:\sigma^{2}_{B}/{\sigma}^{2}\leq\Delta_{0}\). Both state that one or the other of the classifications has little effect on the outcome. Let

\[S^{2}_{A}=\sum_{i=2}^{a}Z_{i11}^{2},\hskip 21.681ptS^{2}_{B}=\sum_{i=1}^{a} \sum_{j=2}^{b}Z_{ij1}^{2},\hskip 21.681ptS^{2}=\sum_{i=1}^{a}\sum_{j=1}^{b} \sum_{k=2}^{n}Z_{ijk}^{2}.\]

To obtain a test of \(H_{1}\), one is tempted to eliminate \(S^{2}\) through invariance under multiplication of \(Z_{ijk}\) for \(k>1\) by an arbitrary constant. However, these transformations do not leave (7.45) invariant, since they do not always preserve the fact that \(\sigma^{2}\) is the smallest of the three variances \(\sigma^{2}\), \(\sigma^{2}+n\sigma^{2}_{B}\), and \(\sigma^{2}+n\sigma^{2}_{B}+bn\sigma^{2}_{A}\). We shall instead consider the problem from the point of view of unbiasedness. For any unbiased test of \(H_{1}\), the probability of rejection is \(\alpha\) whenever \(\sigma^{2}_{A}/({\sigma}^{2}+n\sigma^{2}_{B})=\Delta_{0}\), and hence in particular when the three variances are \(\sigma^{2}\), \(\tau^{2}_{0}\), and \((1+bn\Delta_{0})\tau^{2}_{0}\) for any fixed \(\tau^{2}_{0}\) and all \(\sigma^{2}<\tau^{2}_{0}\). It follows by the techniques of Chapter 4 that the conditional probability of rejection given \(S^{2}=s^{2}\) must be equal to \(\alpha\) for almost all values of \(s^{2}\). With \(S^{2}\) fixed, the joint distribution of the remaining variables is of the same type as (7.45) after the elimination of \(Z_{111}\), and a UMP unbiased conditional test given \(S^{2}=s^{2}\) has the rejection region

\[W^{*}_{1}=\frac{1}{1+bn\Delta_{0}}\cdot\frac{S^{2}_{A}/(a-1)}{S^{2}_{B}/(b-1)a }\geq C_{1}. \tag{7.46}\]

Since \(S^{2}_{A}\) and \(S^{2}_{B}\) are independent of \(S^{2}\), the constant \(C_{1}\) is determined by the fact that when \(\sigma^{2}_{A}/({\sigma}^{2}+n\sigma^{2}_{B})=\Delta_{0}\), the statistic \(W^{*}_{1}\) is distributed as \(F_{a-1,(b-1)a}\) and hence in particular does not depend on \(s\). The test (7.46) is clearly unbiased and hence UMP unbiased.

An alternative proof of this optimality property can be obtained using Theorem 6.6.1. The existence of a UMP unbiased test follows from the exponential family structure of the density (7.45), and the test is the same whether \(\tau^{2}\) is equal to \({\sigma}^{2}+n\sigma^{2}_{B}\) and hence \(\geq\sigma^{2}\), or whether it is unrestricted. However, in the latter case, the test (7.46) is UMP invariant and therefore is UMP unbiased even when \(\tau^{2}\geq\sigma^{2}\).

The argument with respect to \(H_{2}\) is completely analogous and shows the UMP unbiased test to have the rejection region

\[W^{*}_{2}=\frac{1}{1+n\Delta_{0}}\cdot\frac{S^{2}_{B}/(b-1)a}{S^{2}/(n-1)ab} \geq C_{2}, \tag{7.47}\]where \(C_{2}\) is determined by the fact that for \(\sigma_{B}^{2}/\sigma^{2}=\Delta_{0}\), the statistic \(W_{2}^{*}\) is distributed as \(F_{(b-1)a,(n-1)ab}\).

It remains to express the statistics \(S_{A}^{2}\), \(S_{B}^{2}\), and \(S^{2}\) in terms of the \(X\)'s. From the corresponding expressions in the one-way classification, it follows that

\[S_{A}^{2} = \sum_{i=1}^{a}Z_{i11}^{2}-Z_{111}^{2}=b\sum(Y_{i\cdot 1}-Y_{ \cdot\cdot 1})^{2},\] \[S_{B}^{2} = \sum_{i=1}^{a}\left[\sum_{j=1}^{b}Z_{ij1}^{2}-Z_{i11}^{2}\right]= \sum\sum(Y_{ij1}-Y_{i\cdot 1})^{2},\]

and

\[S^{2} = \sum_{i=1}^{a}\sum_{j=1}^{b}\left[\sum_{k=1}^{n}Y_{ijk}^{2}-Y_{ ij1}^{2}\right]=\sum_{i}\sum_{j}\left[\sum_{k=1}^{n}U_{ijk}^{2}-nU_{ij}^{2}.\right]\] \[= \sum_{i}\sum_{j}\sum_{k}(U_{ijk}-U_{ij\cdot})^{2}.\]

Hence

\[S_{A}^{2}=bn\sum(X_{i\cdot\cdot}-X_{\cdot\cdot\cdot\cdot})^{2}, \qquad S_{B}^{2}=n\sum\sum(X_{ij\cdot}-X_{i\cdot\cdot})^{2}, \tag{7.48}\] \[S^{2}=\sum\sum\qquad\sum(X_{ijk}-X_{ij\cdot})^{2}.\]

It is seen from the expression of the statistics in terms of the \(Z\)'s that their expectations are \(E[S_{A}^{2}/(a-1)]=\sigma^{2}+n\sigma_{B}^{2}+bn\sigma_{A}^{2}\), \(E[S_{B}^{2}/(b-1)a]=\sigma^{2}+n\sigma_{B}^{2}\), and \(E[S^{2}/(n-1)ab]=\sigma^{2}\). The decomposition

\[\sum\sum\sum(X_{ijk}-X_{\cdot\cdot\cdot})^{2}=S_{A}^{2}+S_{B}^{2}+S^{2}\]

therefore forms a basis for the analysis of the variance of \(X_{ijk}\),

\[Var(X_{ijk})=\sigma_{A}^{2}+\sigma_{B}^{2}+\sigma^{2}\]

by providing estimates of the _components of variance_\(\sigma_{A}^{2}\), \(\sigma_{B}^{2}\), and \(\sigma^{2}\), and tests of certain ratios of these components.

Nested two-way classifications also occur as mixed models. Suppose for example that a firm produces the material of the previous illustrations in different plants. If \(\alpha_{i}\) denotes the effect of the \(i\)th plant (which is fixed, since the plants do not change in the replication of the experiment), \(B_{ij}\) the batch effect, and \(U_{ijk}\) the unit effect, the observations have the structure

\[X_{ijk}=\mu+\alpha_{i}+B_{ij}+U_{ijk}. \tag{7.49}\]Instead of reducing the \(X\)'s to the fully canonical form in terms of the \(Z\)'s as before, it is convenient to carry out only the reduction to the \(Y\)'s (such that \(Y_{ij1}=\sqrt{n}X_{ij}\).) and the first of the two transformations which take the \(Y\)'s into the \(Z\)'s. If the resulting variables are denoted by \(W_{ijk}\), they satisfy \(W_{i11}=\sqrt{b}Y_{i\cdot 1}\), \(W_{ijk}=Y_{ijk}\) for \(k>1\) and

\[\sum_{i=1}^{a}(W_{i11}-W_{\cdot 11})^{2}=S_{A}^{2},\ \ \ \ \ \sum_{i=1}^{a}\sum_{j=2}^{b}W_{ij1}^{2}=S_{B}^{2},\ \ \ \ \ \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=2}^{n}W_{ijk}^{2}=S^{2}\,\]

where \(S_{A}^{2}\), \(S_{B}^{2}\), and \(S^{2}\) are given by (7.48). The joint density of the \(W\)'s is, except for a constant,

\[\exp\left[-\frac{1}{2(\sigma^{2}+n\sigma_{B}^{2})}\left(\sum_{i=1}^{a}(w_{i11}-\mu-\alpha_{i})^{2}+\sum_{i=1}^{a}\sum_{j=2}^{b}w_{ij1}^{2}\right)\right. \tag{7.50}\] \[-\left.\frac{1}{2\sigma^{2}}\sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=2}^{n}w_{ ijk}^{2}\right].\]

This shows clearly the different nature of the problem of testing that the plant effect is small,

\[H:\alpha_{1}=\cdots=\alpha_{a}=0\ \ \ \mbox{or}\ \ \ H^{\prime}:\frac{\sum\alpha_{i}^{2}}{\sigma^{2}+n\sigma_{B}^{2}}\leq\Delta_{0}\,\]

and testing the corresponding hypothesis for the batch effect: \(\sigma_{B}^{2}/\sigma^{2}\leq\Delta_{0}\). The first of these is essentially a model I problem (linear hypothesis). As before, unbiasedness implies that the conditional rejection probability given \(S^{2}=s^{2}\) is equal to \(\alpha\) a.e. With \(S^{2}\) fixed, the problem of testing \(H\) is a linear hypothesis, and the rejection region of the UMP invariant conditional test given \(S^{2}=s^{2}\) has the rejection region (7.46) with \(\Delta_{0}=0\). The constant \(C_{1}\) is again independent of \(S^{2}\), and the test is UMP among all tests that are both unbiased and invariant. A test with the same property also exists for testing \(H^{\prime}\). Its rejection region is

\[\frac{S_{A}^{2}/(a-1)}{S_{B}^{2}/(b-1)a}\geq C^{\prime},\]

where \(C^{\prime}\) is determined from the noncentral \(F\)-distribution instead of, as before, the (central) \(F\)-distribution.

On the other hand, the hypothesis \(\sigma_{B}^{2}/\sigma^{2}\leq\Delta_{0}\) is essentially model II. It is invariant under addition of an arbitrary constant to each of the variables \(W_{i11}\), which leaves \(\sum_{i=1}^{a}\sum_{j=2}^{b}W_{ij1}^{2}\) and \(\sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=2}^{n}W_{ijk}^{2}\) as maximal invariants, and hence reduces the structure to pure model II with one classification. The test is then given by (7.47) as before. It is both UMP invariant and UMP unbiased.

Very general mixed models (containing general type II models as special cases) are discussed, for example, by Harville (1978), J. Miller (1977a), and Brown (1984), but see the note following Problem 7.36.

The different one- and two-factor models are discussed from a Bayesian point of view, for example, in Box and Tiao (1973) and Broemeling (1985). In distinction to the approach presented here, the Bayesian treatment also includes inferences concerning the values of the individual random components such as the batch means \(\xi_{i}\) of Section 7.7.

### Multivariate Extensions

The univariate linear models studied so far in this chapter arise in the study of the effects of various experimental conditions (factors) on a single characteristic such as yield, weight, length of life, or blood pressure. This characteristic is assumed to be normally distributed with a mean that depends on the various factors under investigation, and a variance that is independent of these factors. We shall now consider the multivariate analogue of this model, which is appropriate when one is concerned with the effect of one or more factors simultaneously on several characteristics, for example, the effect of a change in the diet of dairy cows on both fat content and quantity of milk.

A random vector (\(X_{1}\),..., \(X_{p}\)) has a multivariate normal density if its density is of the form

\[\frac{\sqrt{|A|}}{(2\pi)^{\frac{1}{2}p}}\exp\left[-\tfrac{1}{2}\sum\sum a_{ij} (x_{i}-\xi_{i})(x_{j}-\xi_{j})\right]\;, \tag{7.51}\]

where the matrix \(A=(a_{ij})\) is positive definite, and \(|A|\) denotes its determinant. The means and covariance matrix of the \(X\)'s are given by

\[E(X_{i})=\xi_{i},\qquad E(X_{i}-\xi_{i})(X_{j}-\xi_{j})=\sigma_{ij},\quad( \sigma_{ij})=A^{-1}. \tag{7.52}\]

Such a model was previously introduced in Section 3.9.2.

Consider now \(n\) i.i.d. multivariate normal vectors \(X_{k}=(X_{k,1},\ldots,X_{k,p})\), \(k=1\),..., \(n\), with means \(E(X_{k,i})=\xi_{i}\) and covariance matrix \(A^{-1}\). A natural extension of the one-sample problem of testing the mean \(\xi\) of a normal distribution with unknown variance is that of testing the hypothesis

\[\xi_{1}=\xi_{1,0},\;\;\ldots\;,\xi_{p}=\xi_{p,0}\;;\]

without loss of generality, assume \(\xi_{k,0}=0\) for all \(k\). The joint density of \(X_{1}\),..., \(X_{n}\) is

\[\frac{|A|^{n/2}}{(2\pi)^{np/2}}\exp\left[-\frac{1}{2}\sum_{k=1}^{n}\sum_{i=1}^ {p}\sum_{j=1}^{p}a_{i,j}(x_{k,i}-\xi_{i})(x_{k,j}-\xi_{j})\right]\;.\]Writing the exponent as

\[\sum_{i=1}^{p}\sum_{j=1}^{p}a_{i,\,j}\sum_{k=1}^{n}(x_{k,i}-\xi_{i})(x_{k,j}-\xi_{ j})\,\]

it is seen that the vector of sample means \((\bar{X}_{1},\,\ldots,\,\bar{X}_{p})\) together with

\[S_{i,\,j}=\sum_{k=1}^{n}(X_{k,i}-\bar{X}_{i})(X_{k,j}-\bar{X}_{j})\,\quad i,\,j=1,\ldots\,p \tag{7.53}\]

are sufficient for the unknown mean vector \(\xi\) and unknown covariance matrix \(\Sigma=A^{-1}\) (assumed positive definite). For the remainder of this section, assume \(n>p\), so that the matrix \(S\) with \((i,\,j)\) component \(S_{i,\,j}\) is nonsingular with probability one (Problem 7.38).

We shall now consider the group of transformations

\[X^{\prime}_{k}=CX_{k}\quad(C\ \mbox{nonsingular})\.\]

This leaves the problem invariant, since it preserves the normality of the variables and their means. It simply replaces the unknown covariance matrix by another one. In the space of sufficient statistics, this group induces the transformations

\[\bar{X}^{*}=C\bar{X}\ \ \ \mbox{and}\ \ \ S^{*}=CSC^{\top}\,\ \mbox{where}\ \ S=(S_{i,\,j}). \tag{7.54}\]

Under this group, the statistic

\[W=\bar{X}^{\top}S^{-1}\bar{X} \tag{7.55}\]

is maximal invariant (Problem 7.39).

The distribution of \(W\) depends only on the maximal invariant in the parameter space; this is found to be

\[\psi^{2}=\sum_{i=1}^{p}\sum_{j=1}^{p}a_{ij}\xi_{i}\xi_{j}, \tag{7.56}\]

and the probability density of \(W\) is given by (Problem 7.40)

\[p_{\psi}(w)=e^{-\frac{1}{2}\psi^{2}}\sum_{k=0}^{\infty}\frac{(\frac{1}{2}\psi^ {2})^{k}}{k!}c_{k}\frac{w^{\frac{1}{2}p-1+k}}{(1+w)^{\frac{1}{2}n+k}}. \tag{7.57}\]

This is the same as the density of the test statistic in the univariate case, given as (7.6), with \(r\) and \(s\) there replaced by \(p\). For any \(\psi_{0}<\psi_{1}\) the ratio \(p_{\psi_{1}}(w)/p_{\psi_{0}}(w)\) is an increasing function of \(w\), and it follows from the Neyman-Pearson Lemma that the most powerful invariant test for testing \(H:\xi_{1}=\cdots=\xi_{p}=0\) rejects when is too large, or equivalently when

\[\frac{n-p}{p}W>C. \tag{7.58}\]

The quantity \((n-1)W\), which for \(p=1\) reduces to the square of Student's \(t\), is Hotelling's \(T^{2}\)-statistic. The constant \(C\) is determined from the fact that for \(\psi=0\) the statistic \((n-p)W/p\) has the \(F\)-distribution with \(p\) and \(n-p\) degrees of freedom. As in the univariate case, there also exists a UMP invariant test of the more general hypothesis \(H^{\prime}:\psi^{2}\leq\psi_{0}^{2}\), with rejection region \(W>C^{\prime}\).

The \(T^{2}\)-test was shown by Stein (1956) to be admissible against the class of alternatives \(\psi^{2}\geq c\) for any \(c>0\) by the method of Theorem 6.7.1. Against the class of alternatives \(\psi^{2}\leq c\) admissibility was proved by Kiefer and Schwartz (1965) [see Problem 7.44 and Schwartz (1967, 1969)].

Most accurate equivariant confidence sets for the unknown mean vector \((\xi_{1},\ldots,\xi_{p})\) are obtained from the UMP invariant test of \(H:\xi_{i}=\xi_{i0}\) (\(i=1,\ldots,p\)), which has acceptance region

\[n\sum\sum(\bar{X}_{i}-\xi_{i0})(n-1)S^{i,j}(\bar{X}_{j}-\xi_{j0})\leq C\,\]

where \(S^{i,j}\) are the elements of \(S^{-1}\). The associated confidence sets are therefore ellipsoids

\[n\sum\sum(\xi_{i}-\bar{X}_{i})(n-1)S^{ij}(\xi_{j}-\bar{X}_{j})\leq C \tag{7.59}\]

centered at \((\bar{X}_{1},\ldots,\bar{X}_{p})\). These confidence sets are equivariant under the group of transformations considered in this section (Problem 7.41), and by Lemma 6.10.1 are therefore uniformly most accurate among all equivariant confidence sets at the specified level.

The result extends to the two-sample problem with equal covariances (Problem 7.43), but the situation becomes more complicated for multivariate generalizations of univariate linear hypotheses with \(r>1\). Then, the maximal invariant is no longer univariate and a UMP invariant test no longer exists. For a discussion of this case, see Anderson (2003), Section 8.10.

### Problems

#### Section 7.1

**Problem 7.1**: _Expected sums of squares_. The expected values of the numerator and denominator of the statistic \(W^{*}\) defined by (7.7) are \[E\left(\sum_{i=1}^{r}\frac{Y_{i}^{2}}{r}\right)=\sigma^{2}+\frac{1}{r}\sum_{i=1}^{ r}\eta_{i}^{2}\ \ \ \mbox{and}\ \ \ E\left[\sum_{i=s+1}^{n}\frac{Y_{i}^{2}}{n-s}\right]=\sigma^{2}.\]

**Problem 7.2**: _Noncentral \(\chi^{2}\)-distribution.11_

Footnote 11: The literature on noncentral \(\chi^{2}\), including tables, is reviewed in Tiku (1985a), Chou, Arthur, Rosenstein, and Owen (1994), and Johnson, Kotz, and Balakrishnan (1995).

(i) If \(X\) is distributed as \(N(\psi,1)\), the probability density of \(V=X^{2}\) is \(P_{\psi}^{V}(v)=\sum_{k=0}^{\infty}P_{k}(\psi)f_{2k+1}(v)\), where \(P_{k}(\psi)=(\psi^{2}/2)^{k}e^{-(1/2)\psi^{2}}/k!\) and where \(f_{2k+1}\) is the probability density of a \(\chi^{2}\)-variable with \(2k+1\) degrees of freedom. (ii) Let \(Y_{1},\ldots,Y_{r}\) be independently normally distributed with unit variance and means \(\eta_{1},\ldots,\eta_{r}\). Then \(U=\sum Y_{i}^{2}\) is distributed according to the noncentral \(\chi^{2}\)-distribution with \(r\) degrees of freedom and noncentrality parameter \(\psi^{2}=\sum_{i=1}^{r}\eta_{i}^{2}\), which has probability density

\[p_{\psi}^{U}(u)=\sum_{k=0}^{\infty}P_{k}(\psi)f_{r+2k}(u). \tag{7.60}\]

Here \(P_{k}(\psi)\) and \(f_{r+2k}(u)\) have the same meaning as in (i), so that the distribution is a mixture of \(\chi^{2}\)-distributions with Poisson weights.

[(i): This is seen from

\[p_{\psi}^{V}(v)=\frac{e^{-\frac{1}{2}(\psi^{2}+v)}(e^{\psi\sqrt{v}}+e^{-\psi \sqrt{v}})}{2\sqrt{2\pi v}}\]

by expanding the expression in parentheses into a power series, and using the fact that \(\Gamma(2k)=2^{2k-1}\Gamma(k)\Gamma(k+\frac{1}{2})/\sqrt{\pi}\).

(ii): Consider an orthogonal transformation to \(Z_{1}\),..., \(Z_{r}\) such that \(Z_{1}=\sum\eta_{i}Y_{i}/\psi\). Then the \(Z\)'s are independent normal with unit variance and means \(E(Z_{1})=\psi\) and \(E(Z_{i})=0\) for \(i>1\).]

**Problem 7.3**: _Noncentral \(F\)- and beta-distribution.12_ Let \(Y_{1}\),..., \(Y_{r}\); \(Y_{s+1}\),..., \(Y_{n}\) be independently normally distributed with common variance \(\sigma^{2}\) and means \(E(Y_{i})=\eta_{i}\) (\(i=1,\ldots,r\)); \(E(Y_{i})=0\) (\(i=s+1\),..., \(n\)).

Footnote 12: For literature on noncentral \(F\), see Tiku (1985b) and Johnson, Kotz, and Balakrishnan (1995).

(i) The probability density of \(W=\sum_{i=1}^{r}Y_{i}^{2}/\sum_{i=s+1}^{n}Y_{i}^{2}\) is given by (7.6). The distribution of the constant multiple \((n-s)W/r\) of \(W\) is the _noncentral \(F\)-distribution_. (ii) The distribution of the statistic \(B=\sum_{i=1}^{r}Y_{i}^{2}/(\sum_{i=1}^{r}Y_{i}^{2}+\sum_{i=s+1}^{n}Y_{i}^{2})\) is the _noncentral beta-distribution_, which has probability density

\[\sum_{k=0}^{\infty}P_{k}(\psi)g_{\frac{1}{2}r+k,\frac{1}{2}(n-s)}(b), \tag{7.61}\]where \[g_{p,q}(b)=\frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)}b^{p-1}(1-b)^{q-1},\ \ \ \ \ 0\leq b\leq 1\] (7.62) is the probability density of the (central) beta-distribution.

**Problem 7.4**:
1. The noncentral \(\chi^{2}\) and \(F\) distributions have strictly monotone likelihood ratio.
2. Under the assumptions of Section 7.1, the hypothesis \(H^{\prime}:\psi^{2}\leq\psi_{0}^{2}\) (\(\psi_{0}>0\) given) remains invariant under the transformations \(G_{i}(i=1,2,3)\) that were used to reduce \(H:\psi\,=0\), and there exists a UMP invariant test with rejection region \(W>C^{\prime}\). The constant \(C^{\prime}\) is determined by \(P_{\psi_{0}}\{W>C^{\prime}\}=\alpha\), with the density of \(W\) given by (7.6).

[(i): Let \(f(z)=\sum_{k=0}^{\infty}b_{k}z^{k}/\sum_{k=0}^{\infty}a_{k}z^{k}\) where the constants \(a_{k}\), \(b_{k}\) are \(>0\) and \(\sum a_{k}z^{k}\) and \(\sum b_{k}z^{k}\) converge for all \(z>0\), and suppose that \(b_{k}/a_{k}<b_{k+1}/a_{k+1}\) for all \(k\). Then \[f^{\prime}(z)=\frac{\sum\sum\limits_{k<n}(n-k)(a_{k}b_{n}-a_{n}b_{k})z^{k+n-1 }}{\left(\sum\limits_{k=0}^{\infty}a_{k}z^{k}\right)^{2}}\] is positive, since \((n-k)(a_{k}b_{n}-a_{n}b_{k})>0\) for \(k<n\), and hence \(f\) is increasing.] _Note_. The noncentral \(\chi^{2}\) and \(F\)-distributions are in fact STP\({}_{\infty}\) [see, for example, Marshall and Olkin (1979) and Brown, Johnstone, and MacGibbon (1981)], and there thus exists a test of \(H:\psi\,=\psi_{0}\) against \(\psi\,=\psi_{0}\) which is UMP among all tests that are both invariant and unbiased.

**Problem 7.5**: _Best average power._

1. Consider the general linear hypothesis \(H\) in the canonical form given by (7.2) and (7.3) of Section 7.1, and for any \(\eta_{r+1},\,\ldots,\,\eta_{s},\,\sigma\), and \(\rho\) let \(S=S(\eta_{r+1},\,\ldots,\,\eta_{s},\,\sigma:\rho)\) denote the sphere \(\{(\eta_{1},\,\ldots,\,\eta_{r}):\sum_{i=1}^{r}\eta_{i}^{2}/\sigma^{2}=\rho^{2}\}\). If \(\beta_{\phi}(\eta_{1},\,\ldots,\,\eta_{r},\,\sigma)\) denotes the power of a test \(\phi\) of \(H\), then the test (7.9) maximizes the average power \[\frac{\int_{S}\beta_{\phi}(\eta_{1},\,\ldots,\,\eta_{r},\,\sigma)\,dA}{\int_{S }\,dA}\] for every \(\eta_{r+1},\,\ldots,\,\eta_{s},\,\sigma\), and \(\rho\) among all unbiased (or similar) tests. Here \(dA\) denotes the differential of area on the surface of the sphere.
2. The result (i) provides an alternative proof of the fact that the test (7.9) is UMP among all tests whose power function depends only on \(\sum_{i=1}^{r}\eta_{i}^{2}/\sigma^{2}\).

[(i): if \(U=\sum_{i=1}^{r}Y_{i}^{2}\), \(V=\sum_{i=s+1}^{n}Y_{i}^{2}\), unbiasedness (or similarity) implies that the conditional probability of rejection given \(Y_{r+1},\,\ldots,\,Y_{s}\), and \(U+V\) equals \(\alpha\) a.e. Hence for any given \(\eta_{r+1},\,\ldots,\,\eta_{s},\,\sigma\), and \(\rho\), the average power is maximized by rejecting when the ratio of the average density to the density under \(H\) is larger than a suitable constant \(C(y_{r+1},\,\ldots,\,y_{s},\,u+v)\), and hence when

\[g(y_{1},\,\ldots,\,y_{r};\,\eta_{1},\,\ldots,\,\eta_{r})=\int_{S}\exp\left(\sum _{i=1}^{r}\,\frac{\eta_{i}y_{i}}{\sigma^{2}}\right)\,dA>C(y_{r+1},\,\ldots,\,y _{s},\,u+v).\]

As will be indicated below, the function \(g\) depends on \(y_{1},\,\ldots,\,y_{r}\) only through \(u\) and is an increasing function of \(u\). Since under the hypothesis \(U/(U\,+\,V)\) is independent of \(Y_{r+1},\,\ldots,\,Y_{s}\) and \(U\,+\,V\), it follows that the test is given by (7.9). The exponent in the integral defining \(g\) can be written as \(\sum_{i=1}^{r}\eta_{i}y_{i}/\sigma^{2}=(\rho\sqrt{u}\,\cos\beta)/\sigma\), where \(\beta\) is the angle (\(0\leq\beta\leq\pi\)) between (\(\eta_{1},\,\ldots,\,\eta_{r}\)) and (\(y_{1},\,\ldots,\,y_{r}\)). Because of the symmetry of the sphere, this is unchanged if \(\beta\) is replaced by the angle \(\gamma\) between (\(\eta_{1},\,\ldots,\,\eta_{r}\)) and an arbitrary fixed vector. This shows that \(g\) depends on the \(y\)'s only through \(u\): for fixed \(\eta_{1},\,\ldots,\,\eta_{r},\sigma\) denote it by \(h(u)\). Let \(S^{\prime}\) be the subset of \(S\) in which \(0\leq\gamma\,\leq\pi/2\). Then

\[h(u)=\int_{S^{\prime}}\left[\exp\left(\frac{\rho\sqrt{u}\,\cos\gamma}{\sigma} \right)+\exp\left(\frac{-\rho\sqrt{u}\,\cos\gamma}{\sigma}\right)\right]\,dA,\]

which proves the desired result.]

**Problem 7.6**: Use Theorem 6.7.1 to show that the \(F\)-test (7.7) is \(\alpha\)-admissible against \(\Omega^{\prime}:\,\psi\,\geq\,\psi_{1}\) for any \(\psi_{1}>0\).

**Problem 7.7**: Given any \(\psi_{2}>0\), apply Theorem 6.7.2 and Lemma 6.7.1 to obtain the \(F\)-test (7.7) as a Bayes test against a set \(\Omega^{\prime}\) of alternatives contained in the set \(0<\psi\,\leq\,\psi_{2}\).

### _Section 7.2_

**Problem 7.8**: Under the assumptions of Section 7.1 suppose that the means \(\xi_{t}\) are given by

\[\xi_{i}=\sum_{j=1}^{s}a_{ij}\beta_{j},\]

where the constants \(a_{ij}\) are known and the matrix \(A=(a_{ij})\) has full rank, and where the \(\beta_{j}\) are unknown parameters. Let \(\theta=\sum_{j=1}^{s}e_{j}\beta_{j}\) be a given linear combination of the \(\beta_{j}\).

1. If \(\hat{\beta}_{j}\) denotes the values of the \(\beta_{j}\) minimizing \(\sum(X_{i}-\xi_{i})^{2}\) and if \(\hat{\theta}=\sum_{j=1}^{s}e_{j}\hat{\beta}_{j}=\sum_{j=1}^{n}d_{i}X_{i}\), the rejection region of the hypothesis \(H:\theta=\theta_{0}\) is \[\frac{|\hat{\theta}-\theta_{0}|/\sqrt{\sum\,d_{i}^{2}}}{\sqrt{\sum\left(X_{i}-\hat{ \xi}_{i}\right)^{2}/(n-s)}}>C_{0}\;, \tag{7.63}\]

where the left-hand side under \(H\) has the distribution of the absolute value of Student's \(t\) with \(n-s\) degrees of freedom.
2. The associated confidence intervals for \(\theta\) are \[\hat{\theta}-k\sqrt{\frac{\sum\left(X_{i}-\hat{\xi}_{i}\right)^{2}}{n-s}}\leq \theta\leq\hat{\theta}+k\sqrt{\frac{\sum\left(X_{i}-\hat{\xi}_{i}\right)^{2}}{n -s}}\] (7.64) with \(k=C_{0}\sqrt{\sum\,d_{i}^{2}}\). These intervals are uniformly most accurate equivariant under a suitable group of transformations.

[(i): Consider first the hypothesis \(\theta=0\), and suppose without loss of generality that \(\theta=\beta_{1}\); the general case can be reduced to this by making a linear transformation in the space of the \(\beta\)'s. If \(\underline{a}_{1},\ldots,\underline{a}_{s}\) denote the column vectors of the matrix \(A\) which by assumption span \(\Pi_{\Omega}\), then \(\underline{\xi}=\beta_{1}\underline{a}_{1}+\cdots+\beta_{s}\underline{a}_{s}\), and since \(\underline{\hat{\xi}}\) is in \(\Pi_{\Omega}\) also \(\underline{\hat{\xi}}=\hat{\beta}_{1}\underline{a}_{1}+\cdots+\hat{\beta}_{s} \underline{a}_{s}\). The space \(\Pi_{\omega}\) defined by the hypothesis \(\beta_{1}=0\) is spanned by the vectors \(\underline{a}_{2},\ldots,\underline{a}_{s}\) and also by the row vectors \(\underline{c}_{2},\ldots,\underline{c}_{s}\) of the matrix \(C\) of (7.1), while \(\underline{c}_{1}\) is orthogonal to \(\Pi_{\omega}\). By (7.1), the vector \(\underline{X}\) is given by \(\underline{X}=\sum_{i=1}^{n}Y_{i}\underline{c}_{i}\), and its projection \(\underline{\hat{\xi}}\) on \(\Pi_{\Omega}\) therefore satisfies \(\underline{\hat{\xi}}=\sum_{i=1}^{s}Y_{i}\underline{c}_{i}\). Equating the two expressions for \(\underline{\hat{\xi}}\) and taking the inner product of both sides of this equation with \(\underline{c}_{i}\) gives \(Y_{1}=\hat{\beta}_{1}\sum_{i=1}^{n}a_{i1}c_{i1}\), since the \(\underline{c}\)'s are an orthogonal set of unit vectors. This shows that \(Y_{1}\) is proportional to \(\hat{\beta}_{1}\) and, since the variance of \(Y_{1}\) is the same as that of the \(X\)'s, that \(|Y_{1}|=|\hat{\beta}_{1}|/\sqrt{\sum\,d_{i}^{2}}\). The result for testing \(\beta_{1}=0\) now follows from (7.12) and (7.13). The test for \(\beta_{1}=\beta_{1}^{0}\) is obtained by making the transformation \(X_{i}^{*}=X_{i}-a_{i}\beta_{1}^{0}\).

(ii): The invariance properties of the intervals (7.64) can again be discussed without loss of generality by letting \(\theta\) be the parameter \(\beta_{1}\). In the canonical form of Section 7.1, one then has \(E(Y_{1})=\eta_{1}=\lambda\beta_{1}\) with \(|\lambda|=1/\sqrt{\sum\,d_{1}^{2}}\) while \(\eta_{2},\ldots,\eta_{s}\) do not involve \(\beta_{1}\). The hypothesis \(\beta_{1}=\beta_{1}^{0}\) is therefore equivalent to \(\eta_{1}=\eta_{1}^{0}\), with \(\eta_{1}^{0}=\lambda\beta_{1}^{0}\). This is invariant (a) under addition of arbitrary constants to \(Y_{2}\ldots,Y_{s}\); (b) under the transformations \(Y_{1}^{*}=-(Y_{1}-\eta_{1}^{0})+\eta_{1}^{0}\); (c) under the scale changes \(Y_{i}^{*}=cY_{i}\) (\(i=2,\ldots,n\)), \(Y_{1}^{*}-\eta_{1}^{0*}=c(Y_{1}-\eta_{1}^{0})\). The confidence intervals for \(\theta=\beta_{1}\) are then uniformly most accurate equivariant under the group obtained from (a), (b), and (c) by varying \(\eta_{1}^{0}\).]

**Problem 7.9**: Let \(X_{ij}\) (\(j=1,\ldots,m_{i}\)) and \(Y_{ik}\) (\(k=1,\ldots,n_{i}\)) be independently normally distributed with common variance \(\sigma^{2}\) and means \(E(X_{ij})=\xi_{i}\) and \(\xi_{i}+\Delta\). Then the UMP invariant test of \(H:\Delta=0\) is given by (7.63) with \(\theta=\Delta\), \(\theta_{0}=0\) and

\[\hat{\theta}=\frac{\sum\limits_{i}\frac{m_{i}n_{i}}{N_{i}}(Y_{i.}-X_{i.})}{\sum \limits_{i}\frac{m_{i}n_{i}}{N_{i}}},\quad\quad\hat{\xi}_{i}=\frac{\sum\limits _{j=1}^{m_{i}}X_{ij}+\sum\limits_{k=1}^{n_{i}}(Y_{ik}-\hat{\theta})}{N_{i}},\]

where \(N_{i}=m_{i}+n_{i}\).

**Problem 7.10** Let \(X_{1},\,\ldots,\,X_{n}\) be independently normally distributed with known variance \(\sigma_{0}^{2}\) and means \(E(X_{i})=\xi_{i}\), and consider any linear hypothesis with \(s\leq n\) (instead of \(s<n\) which is required when the variance is unknown). This remains invariant under a subgroup of that employed when the variance was unknown, and the UMP invariant test has rejection region

\[\sum\left(X_{i}-\hat{\hat{\xi}}_{i}\right)^{2}-\left(X_{i}-\hat{\hat{\xi}}_{i} \right)^{2}=\sum\left(\hat{\xi}_{i}-\hat{\hat{\xi}}_{i}\right)^{2}>C\sigma_{0} ^{2} \tag{7.65}\]

with \(C\) determined by

\[\int_{C}^{\infty}\chi_{r}^{2}(y)\,dy=\alpha. \tag{7.66}\]

**Section 7.3**

**Problem 7.11** If the variables \(X_{ij}\) (\(j=1,\,\ldots,\,n_{i}\); \(i=1,\,\ldots,\,s\)) are independently distributed as \(N(\mu_{i},\,\sigma^{2})\), then

\[E\left[\sum n_{i}\,\left(X_{i.}-X_{..}\right)^{2}\right] = (s-1)\sigma^{2}+\sum n_{i}\,\left(\mu_{i}-\mu.\right)^{2},\] \[E\left[\sum\sum\left(X_{ij}-X_{i.}\right)^{2}\right] = (n-s)\sigma^{2}.\]

**Problem 7.12** Let \(Z_{1},\,\ldots,\,Z_{s}\) be independently distributed as \(N(\zeta_{i},a_{i}^{2}),\,i=1,\,\ldots,\,s\), where the \(a_{i}\) are known constants.

1. With respect to a suitable group of linear transformations there exists a UMP invariant test of \(H:\zeta_{1}=\cdots=\zeta_{s}\) given by the rejection region \[\sum\frac{1}{a_{i}^{2}}\left(Z_{i}-\frac{\sum Z_{j}/a_{j}^{2}}{\sum 1/a_{j}^{2}} \right)^{2}=\sum\left(\frac{Z_{i}}{a_{i}}\right)^{2}-\frac{\left(\sum Z_{j}/a _{j}^{2}\right)^{2}}{\sum\left(1/a_{j}^{2}\right)}>C.\] (7.67)2. The power of this test is the integral from \(C\) to \(\infty\) of the noncentral \(\chi^{2}\)-density with \(s-1\) degrees of freedom and noncentrality parameter \(\lambda^{2}\) obtained by substituting \(\zeta_{i}\) for \(Z_{i}\) in the left-hand side of (7.67).

#### 7.5.3 Section 7.5

**Problem 7.13**: The linear-hypothesis test of the hypothesis of no interaction in a two-way layout with \(m\) observations per cell is given by (7.28).

**Problem 7.14**: In the two-way layout of Section 7.5.3 with \(a=b=2\), denote the first three terms in the partition of \(\sum\sum\sum\left(X_{ijk}-X_{ij}.\right)^{2}\) by \(S_{A}^{2}\), \(S_{B}^{2}\), and \(S_{AB}^{2}\), corresponding to the \(A\), \(B\), and \(AB\) effects (i.e., the \(\alpha\)'s, \(\beta\)'s, and \(\gamma\)'s), and denote by \(H_{A}\), \(H_{B}\), and \(H_{AB}\) the hypotheses of these effects being zero. Define a new two-level factor \(B^{\prime}\) which is at level 1 when \(A\) and \(B\) are both at level 1 or both at level 2, and which is at level 2 when \(A\) and \(B\) are at different levels. Then

\[H_{B^{\prime}}=H_{AB},\qquad S_{B^{\prime}}=S_{AB},\qquad H_{AB^{\prime}}=H_{B },\qquad S_{AB^{\prime}}=S_{B},\]

so that the \(B\)-effect has become an interaction, and the \(AB\)-interaction the effect of the factor \(B^{\prime}\). [Shaffer (1977b).]

**Problem 7.15**: Let \(X_{\lambda}\) denote a random variable distributed as noncentral \(\chi^{2}\) with \(f\) degrees of freedom and noncentrality parameter \(\lambda^{2}\). Then \(X_{\lambda^{\prime}}\) is stochastically larger than \(X_{\lambda}\) if \(\lambda<\lambda^{\prime}\).

[It is enough to show that if \(Y\) is distributed as \(N(0,1)\), then \((Y+\lambda^{\prime})^{2}\) is stochastically larger than \((Y+\lambda)^{2}\). The equivalent fact that for any \(z>0\),

\[P\{|Y+\lambda^{\prime}|\leq z\}\leq P\{|Y+\lambda|\leq z\},\]

is an immediate consequence of the shape of the normal density function. An alternative proof is obtained by combining Problem 7.4 with Lemma 3.4.2.]

**Problem 7.16**: Let \(X_{ijk}\) (\(i=1,\ldots,a\); \(j=1,\ldots,b\); \(k=1,\ldots,m\)) be independently normally distributed with common variance \(\sigma^{2}\) and mean

\[E(X_{ijk})=\mu+\alpha_{i}+\beta_{j}+\gamma_{k}\qquad\left(\sum\alpha_{i}=\sum \beta_{j}=\sum\gamma_{k}=0\right).\]

Determine the linear-hypothesis test for testing \(H:\alpha_{i}=\ldots\alpha_{a}=0\).

**Problem 7.17**: In the three-factor situation of the preceding problem, suppose that \(a=b=m\). The hypothesis \(H\) can then be tested on the basis of \(m^{2}\) observations as follows. At each pair of levels (\(i\), \(j\)) of the first two factors one observation is taken, to which we refer as being in the \(i\)th row and the \(j\)th column. If the levelsof the third factor are chosen in such a way that each of them occurs once and only once in each row and column, the experimental design is a _Latin square_. The \(m^{2}\) observations are denoted by \(X_{ij(k)}\), where the third subscript indicates the level of the third factor when the first two are at levels \(i\) and \(j\). It is assumed that \(E(X_{ij(k)})=\xi_{ij(k)}=\mu+\alpha_{i}+\beta_{j}+\gamma_{k}\), with \(\sum\alpha_{i}=\sum\beta_{j}=\sum\gamma_{k}=0\).

1. The parameters are determined from the \(\xi\)'s through the equations \[\xi_{i\cdot(\cdot)}=\mu+\alpha_{i},\hskip 14.226378pt\xi_{\cdot j(\cdot)}=\mu+ \beta_{j},\hskip 14.226378pt\xi_{\cdot\cdot(k)}=\mu+\gamma_{k},\hskip 14.226378pt\xi_{\cdot\cdot(\cdot)}=\mu.\] (Summation over \(j\) with \(i\) held fixed automatically causes summation also over \(k\).)
2. The least squares estimates of the parameters may be obtained from the identity \[\sum_{i}\sum_{j} \left[x_{ij(k)}-\xi_{ij(k)}\right]^{2}\] \[= m\sum\left[x_{i\cdot(\cdot)}-x_{\cdot(\cdot)}-\alpha_{i}\right]^{2} +m\sum\left[x_{\cdot j(\cdot)}-x_{\cdot(\cdot)}-\beta_{j}\right]^{2}\] \[+m\sum\left[x_{\cdot(k)}-x_{\cdot(\cdot)}-\gamma_{k}\right]^{2}+m^{2} \left[x_{\cdot(\cdot)}-\mu\right]^{2}\] \[+\sum_{i}\sum_{k}\left[x_{ij(k)}-x_{i\cdot(\cdot)}-x_{\cdot j( \cdot)}-x_{\cdot\cdot(k)}+2x_{\cdot(\cdot)}\right]^{2}.\]
3. For testing the hypothesis \(H:\alpha_{1}=\cdots=\alpha_{m}=0\), the test statistic \(W^{*}\) of (7.15) is \[\frac{m\sum\left[X_{i\cdot(\cdot)}-X_{\cdot(\cdot)}\right]^{2}}{\sum\sum\left[X_{ij(k)}-X_{i\cdot(\cdot)}-X_{\cdot j(\cdot)}-X_{\cdot\cdot(k)}+2X_{\cdot(\cdot)}\right]^{2}/(m-2)}.\] The degrees of freedom are \(m-1\) for the numerator and \((m-1)(m-2)\) for the denominator, and the noncentrality parameter is \(\psi^{2}=m\sum\alpha_{i}^{2}/\sigma^{2}\).

### _Section 7.6_

**Problem 7.18** In a regression situation, suppose that the observed values \(X_{j}\) and \(Y_{j}\) of the independent and dependent variable differ from certain true values \(X_{j}^{\prime}\) and \(Y_{j}^{\prime}\) by errors \(U_{j}\), \(V_{j}\) which are independently normally distributed with zero means and variances \(\sigma_{U}^{2}\) and \(\sigma_{V}^{2}\). The true values are assumed to satisfy a linear relation: \(Y_{j}^{\prime}=\alpha+\beta X_{j}^{\prime}\). However, the variables which are being controlled, and which are therefore constants, are the \(X_{j}\) rather than the \(X_{j}^{\prime}\). Writing \(x_{j}\) for \(X_{j}\), we have \(x_{j}=X_{j}^{\prime}+U_{j}\), \(Y_{j}=Y_{j}^{\prime}+V_{j}\), and hence \(Y_{j}=\alpha+\beta x_{j}+W_{j}\), where \(W_{j}=V_{j}-\beta U_{j}\). The results of Section 7.6 can now be applied to test that \(\beta\) or \(\alpha+\beta x_{0}\) has a specified value.

**Problem 7.19**: Let \(X_{1}\),..., \(X_{m}\); \(Y_{1}\),..., \(Y_{n}\) be independently normally distributed with common variance \(\sigma^{2}\) and means \(E(X_{i})=\alpha+\beta(u_{i}-\bar{u})\), \(E(Y_{j})=\gamma+\delta(v_{j}-\bar{v})\), where the \(u\)'s and \(v\)'s are known numbers. Determine the UMP invariant tests of the linear hypotheses \(H:\beta=\delta\) and \(H:\alpha=\gamma\), \(\beta=\delta\).

**Problem 7.20**: Let \(X_{1}\),..., \(X_{n}\) be independently normally distributed with common variance \(\sigma^{2}\) and means \(\xi_{i}=\alpha+\beta t_{i}+\gamma t_{i}^{2}\), where the \(t_{i}\) are known. If the coefficient vectors \((t_{1}^{k},\ldots,t_{n}^{k}),\ k=0,1,2\), are linearly independent, the parameter space \(\Pi_{\Omega}\) has dimension \(s=3\), and the least squares estimates \(\hat{\alpha}\), \(\hat{\beta}\), \(\hat{\gamma}\) are the unique solutions of the system of equations

\[\alpha\sum t_{i}^{k}+\beta\sum t_{i}^{k+1}+\gamma\sum t_{i}^{k+2}=\sum t_{i}^{ k}X_{i}\ \ \ \ \ (k=0,1,2).\]

The solutions are linear functions of the \(X\)'s, and if \(\hat{\gamma}=\sum c_{i}X_{i}\), the hypothesis \(\gamma=0\) is rejected when

\[\frac{|\hat{\gamma}|/\sqrt{\sum c_{i}^{2}}}{\sqrt{\sum\left(X_{i}-\hat{\alpha} -\hat{\beta}t_{i}-\hat{\gamma}t_{i}^{2}\right)^{2}/(n-3)}}>C_{0}.\]

**Section 7.7**:
**Problem 7.21**:
* The test (7.41) of \(H:\Delta\leq\Delta_{0}\) is UMP unbiased.
* Determine the UMP unbiased test of \(H:\Delta=\Delta_{0}\) and the associated uniformly most accurate unbiased confidence sets for \(\Delta\).

**Problem 7.22**: In the model (7.39), the correlation coefficient \(\rho\) between two observations \(X_{ij}\), \(X_{ik}\) belonging to the same class, the so-called _intraclass correlation coefficient_, is given by \(\rho=\sigma_{A}^{2}/(\sigma_{A}^{2}+\sigma^{2})\).

**Section 7.8**:
**Problem 7.23**: The tests (7.46) and (7.47) are UMP unbiased.

**Problem 7.24**: If \(X_{ij}\) is given by (7.39) but the number \(n_{i}\) of observations per batch is not constant, obtain a canonical form corresponding to (7.40) by letting \(Y_{i1}=\sqrt{n_{i}}X_{i}\). Note that the set of sufficient statistics has more components than when \(n_{i}\) is constant.

**Problem 7.25**: The general nested classification with a constant number of observations per cell, under model II, has the structure\[X_{ijk\cdots}=\mu+A_{i}+B_{ij}+C_{ijk}+\cdots+U_{ijk\cdots},\] \[i=1,\ldots,a;\,j=1,\ldots,b;\,k=1,\ldots,c;\,\ldots.\]

(i) This can be reduced to a canonical form generalizing (7.45). (ii) There exist UMP unbiased tests of the hypotheses

\[H_{A}:\frac{\sigma_{A}^{2}}{cd\ldots\sigma_{B}^{2}+d\ldots\sigma_{C}^{2}+ \cdots+\sigma^{2}}\leq\Delta_{0},\] \[H_{B}:\frac{\sigma_{B}^{2}}{d\ldots\sigma_{C}^{2}+\cdots+\sigma^ {2}}\leq\Delta_{0}.\]

**Problem 7.26**: Consider the model II analogue of the two-way layout of Section 7.5, according to which

\[X_{ijk}= \mu+A_{i}+B_{j}+C_{ij}+E_{ijk}\] \[(i=1,\ldots,a;\quad j=1,\ldots,b;\quad k=1,\ldots,n),\]

where the \(A_{i}\), \(B_{j}\), \(C_{ij}\), and \(E_{ijk}\) are independently normally distributed with mean zero and with variances \(\sigma_{A}^{2}\), \(\sigma_{B}^{2}\), \(\sigma_{C}^{2}\) and \(\sigma^{2}\) respectively. Determine tests which are UMP among all tests that are invariant (under a suitable group) and unbiased of the hypotheses that the following ratios do not exceed a given constant (which may be zero):

(i) \(\sigma_{C}^{2}/\sigma^{2}\);

(ii) \(\sigma_{A}^{2}/(n\sigma_{C}^{2}+\sigma^{2})\);

(iii) \(\sigma_{B}^{2}/(n\sigma_{C}^{2}+\sigma^{2})\).

Note that the test of (i) requires \(n>1\), but those of (ii) and (iii) do not.

[Let \(S_{A}^{2}=nb\sum(X_{i\cdots}-X_{\cdots})^{2}\), \(S_{B}^{2}=na\sum(X_{\cdot j\cdot}-X_{\cdots})^{2}\), \(S_{C}^{2}=n\sum\sum(X_{ij\cdot}-X_{i\cdots}-X_{\cdot j\cdot}+X_{\cdots})^{2}\), \(S^{2}=\sum\sum(X_{ijk}-X_{ij\cdot})^{2}\), and make a transformation to new variables \(Z_{ijk}\) (independent, normal, and with mean zero except when \(i=j=k=1\)) such that

\[S_{A}^{2}=\sum_{i=2}^{a}Z_{i11}^{2},\qquad S_{B}^{2}=\sum_{j=2}^ {b}Z_{1j1}^{2},\qquad S_{C}^{2}=\sum_{i=2}^{a}\sum_{j=2}^{b}Z_{ij1}^{2},\] \[S^{2}=\sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=2}^{n}Z_{ijk}^{2}.]\]

**Problem 7.27**: Consider the mixed model obtained from (7.68) by replacing the random variables \(A_{i}\) by unknown constants \(\alpha_{i}\) satisfying \(\sum\alpha_{i}=0\). With (ii) replaced by (ii\({}^{\prime}\)) \(\sum\alpha_{i}^{2}/(n\sigma_{C}^{2}+\sigma^{2})\), there again exist tests which are UMP among an tests that are invariant and unbiased, and in cases (i) and (iii) these coincide with the corresponding tests of Problem 7.26.

**Problem 7.28**: Consider the following generalization of the univariate linear model of Section 7.1. The variables \(X_{i}\) (\(i=1,\ldots,n\)) are given by \(X_{i}=\xi_{i}+U_{i}\), where (\(U_{1},\ldots,U_{n}\)) have a joint density which is _spherical_, that is, a function of \(\sum_{i=1}^{n}u_{i}^{2}\), say

\[f\left(U_{1},\ldots,U_{n}\right)=q\left(\sum U_{i}^{2}\right).\]

The parameter spaces \(\Pi_{\Omega}\) and \(\Pi_{\omega}\) and the hypothesis \(H\) are as in Section 7.1.

(i): The orthogonal transformation (7.1) reduces (\(X_{1},\ldots,X_{n}\)) to canonical variables (\(Y_{1},\ldots,Y_{n}\)) with \(Y_{i}=\eta_{i}+V_{i}\), where \(\eta_{i}=0\) for \(i=s+1,\ldots,n\), \(H\) reduces to (7.3), and the \(V\)'s have joint density \(q\left(v_{1},\ldots,v_{n}\right)\). (ii): In the canonical form of (i), the problem is invariant under the groups \(G_{1}\), \(G_{2}\), and \(G_{3}\) of Section 7.1, and the statistic \(W^{*}\) given by (7.7) is maximal invariant.

**Problem 7.29**: Under the assumptions of the preceding problem, the null distribution of \(W^{*}\) is independent of \(q\) and hence the same as in the normal case, namely, \(F\) with \(r\) and \(n--s\) degrees of freedom. [See Problem 5.11]. _Note_. The analogous multivariate problem is treated by Kariya (1981); also see Kariya (1985) and Kariya and Sinha (1985). For a review of work on spherically and elliptically symmetric distributions, see Chmielewski (1981).

**Problem 7.30**: Consider the additive random-effects model

\[X_{ijk}=\mu+A_{i}+B_{j}+U_{ijk}\quad(i=1,\ldots,a;\quad j=1,\ldots,b;\quad k=1, \ldots,n),\]

where the \(A\)'s, \(B\)'s, and \(U\)'s are independent normal with zero means and variances \(\sigma_{A}^{2}\), \(\sigma_{B}^{2}\), and \(\sigma^{2}\)' respectively. Determine

(i): the joint density of the \(X\)'s,

(ii): the UMP unbiased test of \(H:\sigma_{B}^{2}/\sigma^{2}\leq\delta\).

**Problem 7.31**: For the mixed model

\[X_{ij}=\mu+\alpha_{i}+B_{j}+U_{ij}\quad\quad(i=1,\ldots,a;\quad j=1,\ldots,n),\]

where the \(B\)'s and \(U\)'s are as in Problem 7.30 and the \(\alpha\)'s are constants adding to zero, determine (with respect to a suitable group leaving the problem invariant)

(i): a UMP invariant test of \(H:\alpha_{1}=\cdots=\alpha_{a}\);

(ii): a UMP invariant test of \(H:\xi_{1}=\cdots=\xi_{a}=0\quad(\xi_{i}=\mu+\alpha_{i})\);

(iii): a test of \(H:\sigma_{B}^{2}/\sigma^{2}\leq\delta\) which is both UMP invariant and UMP unbiased.

**Problem 7.32**: Let (\(X_{1j},\ldots,X_{pj}\)), \(j=1,\ldots,n\), be a sample from a \(p\)-variate normal distribution with mean \((\xi_{1},\ldots,\xi_{p})\) and covariance matrix \(\Sigma=(\sigma_{ij})\), where \(\sigma_{ij}^{2}=\sigma^{2}\) when \(j=i\), and \(\sigma_{ij}^{2}=\rho\sigma^{2}\) when \(j\neq i\). Show that the covariance matrix is positive definite if and only if \(\rho>-1/(p-1)\).

[For fixed \(\sigma\) and \(\rho<0\), the quadratic form \((1/\sigma^{2})\sum\sum\sigma_{ij}y_{i}y_{j}=\sum y_{i}^{2}+\rho\sum\sum y_{i} y_{j}\) takes on its minimum value over \(\sum y_{i}^{2}=1\) when all the \(y\)'s are equal.]

**Problem 7.33**: Under the assumptions of the preceding problem, determine the UMP invariant test (with respect to a suitable \(G\)) of \(H:\xi_{i}=\ldots=\xi_{p}\).

[Show that this model agrees with that of Problem 7.31 if \(\rho=\sigma_{b}^{2}/(\sigma_{b}^{2}+\sigma^{2})\), except that instead of being positive, \(\rho\) now only needs to satisfy \(\rho>-1/(p-1)\).]

**Problem 7.34**: Permitting interactions in the model of Problem 7.30 leads to the model

\[X_{ijk}=\mu+A_{i}+B_{j}+C_{ij}+U_{ijk}\quad(i=1,\ldots,a;\,j=1,\ldots,b,\,k=1, \ldots,n),\]

where the \(A\)'s, \(B\)'s, \(C\)'s, and \(U\)'s are independent normal with mean zero and variances \(\sigma_{A}^{2}\), \(\sigma_{B}^{2}\), \(\sigma_{C}^{2}\), and \(\sigma^{2}\).

1. Give an example of a situation in which such a model might be appropriate.
2. Reduce the model to a convenient canonical form along the lines of Section 7.4.
3. Determine UMP unbiased tests of (a) \(H_{1}:\sigma_{B}^{2}=0\); (b) \(H_{2}:\sigma_{C}^{2}=0\).

**Problem 7.35**: Formal analogy with the model of Problem 7.34 suggests the mixed model

\[X_{ijk}=\mu+\alpha_{i}+B_{j}+C_{ij}+U_{ijk}\]

with the \(B\)'s, \(C\)'s, and \(U\)'s as in Problem 7.34. Reduce this model to a canonical form involving \(X_{\ldots}\) and the sums of squares

\[\begin{array}{ll}\frac{\sum(X_{i,-}X_{\ldots}-\alpha_{i})^{2}}{n\sigma_{C}^{ 2}+\sigma^{2}},&\frac{\sum\sum(X_{j,-}X_{\ldots})^{2}}{an\sigma_{B}^{2}+n \sigma_{C}^{2}+\sigma^{2}},\\ \frac{\sum\sum(X_{ij,-}X_{i,-}X_{j,+}X_{\ldots})^{2}}{n\sigma_{C}^{2}+\sigma^{ 2}},&\frac{\sum\sum\sum(X_{ij,-}X_{i,-}X_{j,+}X_{\ldots})^{2}}{\sigma^{2}}. \end{array}\]

**Problem 7.36**: Among all tests that are both unbiased and invariant under suitable groups under the assumptions of Problem 7.35, there exist UMP tests of

1. \(H_{1}:\alpha_{1}=\cdots=\alpha_{a}=0\);
2. \(H_{2}:\sigma_{B}^{2}/(n\sigma_{C}^{2}+\sigma^{2})\leq C\);
3. \(H_{3}:\sigma_{C}^{2}/\sigma^{2}\leq C\).

_Note_. The independence assumptions of Problems 7.35 and 7.36 often are not realistic. For alternative models, derived from more basic assumptions, see Scheffe (1956, 1959). Relations between the two types of models are discussed in Hocking (1973), Cohen and Miller (1976), and Stuart and Ord (1991).

**Problem 7.37**: Let \((X_{1j1},\ldots,X_{1jn};\,X_{2j1},\ldots,\,X_{2jn};\ldots;\,X_{aj1},\ldots,\,X_ {ajn})\), \(j=1,\ldots,b\), be a sample from an \(an\)-variate normal distribution. Let \(E(X_{ijk})=\xi_{i}\), and denote by \(\sum_{ii^{\prime}}\) the matrix of covariances of \((X_{ij1},\ldots,X_{ijn})\) with \((X_{i^{\prime}j1},\ldots,X_{i^{\prime}jn})\). Suppose that for all \(i\), the diagonal elements of \(\sum_{ii}\) are \(=\tau^{2}\) and the off-diagonal elements are \(=\rho_{1}\tau^{2}\), and that for \(i\not=i^{\prime}\) all \(n^{2}\) elements of \(\sum_{ii^{\prime}}\) are \(=\rho_{2}\tau^{2}\).

1. Find necessary and sufficient conditions on \(\rho_{1}\) and \(\rho_{2}\) for the overall \(abn\times abn\) covariance matrix to be positive definite.
2. Show that this model agrees with that of Problem 7.35 for suitable values of \(\rho_{1}\) and \(\rho_{2}\).

#### _Section 7.9_

**Problem 7.38**: If \(n\leq p\), the matrix \(S\) with (\(i\), \(j\)) component \(S_{i,j}\) defined in (7.53) is singular. If \(n>p\), it is nonsingular with probability 1. If \(n\leq p\), the test \(\phi\equiv\alpha\) is the only test that is invariant under the group of nonsingular linear transformations.

**Problem 7.39**: Show that the statistic \(W\) given in (7.55) is maximal invariant. [_Hint:_ If (\(\bar{X}\), \(S\)) and (\(\bar{Y}\), \(T\)) are such that

\[\bar{X}^{\top}S^{-1}\bar{X}=\bar{Y}^{\top}T^{-1}\bar{Y}\,\]

then a transformation \(C\) that transforms one to the other is given by \(C=Y(X^{\top}S^{-1}X)^{-1}X^{\top}S^{-1}\).]

**Problem 7.40**: Verify that the density of \(W\) is given by (7.55).

**Problem 7.41**: The confidence ellipsoids (7.59) for (\(\xi_{1},\ldots,\xi_{p}\)) are equivariant under the group of Section 7.9.

**Problem 7.42**: For testing a multivariate mean vector \(\xi\) is zero in the case where \(\Sigma\) is known, derive a UMPI test.

**Problem 7.43**: Extend the one-sample problem to the two-sample problem for testing whether two multivariate normal distributions with common unknown covariance matrix have the same mean vectors.

**Problem 7.44**: _Bayes character and admissibility of Hotelling's \(T^{2}\)._

1. Let (\(X_{\alpha 1}\),..., \(X_{\alpha p}\)), \(\alpha=1\),..., \(n\), be a sample from a \(p\)-variate normal distribution with unknown mean \(\xi=(\xi_{1},\ldots,\xi_{p})\) and covariance matrix \(\Sigma=A^{-1}\), and with \(p\leq n-1\). Then the one-sample \(T^{2}\)-test of \(H:\xi=0\) against \(K:\xi\neq 0\) is a Bayes test with respect to prior distributions \(\Lambda_{0}\) and \(\Lambda_{1}\) which generalize those of Example 6.7.3 (continued).
2. The test of part (i) is admissible for testing \(H\) against the alternatives \(\psi^{\,2}\leq c\) for any \(c>0\).

[If \(\omega\) is the subset of points (0, \(\Sigma\)) of \(\Omega_{H}\) satisfying \(\Sigma^{-1}=A+\eta^{\prime}\eta\) for some fixed positive definite \(p\times p\) matrix \(A\) and arbitrary \(\eta=(\eta_{1},\ldots,\eta_{p})\), and \(\Omega^{\prime}_{A,b}\) is the subset of points \((\xi,\Sigma)\) of \(\Omega_{K}\) satisfying \(\Sigma^{-1}=A+\eta^{\prime}\eta\), \(\xi^{\prime}=b\Sigma\eta^{\prime}\) for the same \(A\) and some fixed \(b>0\), let \(\Lambda_{0}\) and \(\Lambda_{1}\) have densities defined over \(\omega\) and \(\Omega_{A,b}\), respectively, by

\[\lambda_{0}(\eta)=C_{0}|A+\eta^{\prime}\eta|^{-n/2}\]

and

\[\lambda_{1}(\eta)=C_{1}|A+\eta^{\prime}\eta|^{-n/2}\exp\left\{\frac{nb^{2}}{2} \left[\eta(A+\eta^{\prime}\eta)^{-1}\eta^{\prime}\right]\right\}.\]

(Kiefer and Schwartz 1965).]

**Problem 7.45**: Suppose \((X_{1},\ldots,X_{p})\) have the multivariate normal density (7.51), so that \(E(X_{i})=\xi_{i}\) and \(A^{-1}\) is the known positive definite covariance matrix. The vector of means \(\xi=(\xi_{1},\ldots,\xi_{p})\) is known to lie in a given \(s\)-dimensional linear space \(\Pi_{\Omega}\) with \(s\leq p\); the hypothesis to be tested is that \(\xi\) lies in a given \((s-r)\)-dimensional linear subspace \(\Pi_{\omega}\) of \(\Pi_{\Omega}(r\leq s)\).

(i) Determine the UMPI test under a suitable group of transformations as explicitly as possible. Find an expression for the power function.

(ii) Specialize to the case of a simple null hypothesis.

### Notes

The general linear model in the parametric form (7.18) was formulated at the beginning of the 19th century by Legendre and Gauss, who were concerned with estimating the unknown parameters. [For an account of its history, see Seal (1967).] The canonical form (7.2) of the model is due to Kolodziejczyk (1935). The analysis of variance, including the concept of interaction, was developed by Fisher in the 1920s and 1930s, and a systematic account is provided by Scheffe (1959) in a book that includes a careful treatment of alternative models and of robustness questions.

Different approaches to analysis of variance than that given here are considered in Speed (1987) and the discussion following this paper, and in Diaconis (1988, Section 8C). Rank tests are discussed in Marden and Muyot (1995). Admissibility results for testing homogeneity of variances in a normal balanced one-way layout are given in Cohen and Marden (1989). Linear models have been generalized in many directions. Loglinear models provide extensions to important discrete data. [Both are reviewed in Christensen (2000).] These two classes of models are subsumed in generalized linear models discussed for example in McCullagh and Nelder (1983), Dobson (1990) and Agresti (2002), and they in turn are a subset of additive linear models which are discussed in Hastie and Tibshirani (1990, 1997). Modern treatments of regression analysis can be found, for example, in Weisberg (1985), Atkinson and Riani (2000) and Ruppert, Wand, and Carroll (2003). UMPI tests can be constructed for tests of lack of fit in some regression models; see Christensen (1989) and Miller, Neill, and Sherfey (1998).

Hsu (1941) shows that the test (7.7) is UMP among all tests whose power function depends only on the noncentrality parameter. Hsu (1945) obtains a result on best average power for the \(T^{2}\)-test analogous to that of Chapter 7, Problem 7.5.

Tests of multivariate linear hypotheses and the associated confidence sets have their origin in the work of Hotelling (1931). More details on these procedures and discussion of other multivariate techniques can be found in the comprehensive books by Anderson (2003) and Seber (1984). A more geometric approach stressing invariance is provided by Eaton (1983).

For some recent work on using rank tests in multivariate problems, see Choi and Marden (1997), Hettmansperger, Mottonen and Oja (1997), and Akritas, Arnold, and Brunner (1997).

## Chapter 8 The Minimax Principle

### 8.1 Tests with Guaranteed Power

The criteria discussed so far, unbiasedness and invariance, suffer from the disadvantage of being applicable, or leading to optimum solutions, only in rather restricted classes of problems. We shall therefore turn now to an alternative approach, which potentially is of much wider applicability. Unfortunately, its application to specific problems is in general not easy, unless there exists a UMP invariant test.

One of the important considerations in planning an experiment is the number of observations required to ensure that the resulting statistical procedure will have the desired precision or sensitivity. For problems of hypothesis testing this means that the probabilities of the two kinds of errors should not exceed certain preassigned bounds, say \(\alpha\) and \(1-\beta\), so that the tests must satisfy the conditions

\[\begin{array}{ll}E_{\theta}\varphi(X)&\leq\alpha&\text{for}\,\theta\in \Omega_{H},\\ E_{\theta}\varphi(X)&\geq\beta&\text{for}\,\theta\in\Omega_{K}.\end{array} \tag{8.1}\]

If the power function \(E_{\theta}\varphi(X)\) is continuous and if \(\alpha<\beta\), (8.1) cannot hold when the sets \(\Omega_{H}\) and \(\Omega_{K}\) are contiguous. This mathematical difficulty corresponds in part to the fact that the division of the parameter values \(\theta\) into the classes \(\Omega_{H}\) and \(\Omega_{K}\) for which the two different decisions are appropriate is frequently not sharp. Between the values for which one or the other of the decisions is clearly correct there may lie others for which the relative advantages and disadvantages of acceptance and rejection are approximately in balance. Accordingly we shall assume that \(\Omega\) is partitioned into three sets

\[\Omega=\Omega_{H}+\Omega_{I}+\Omega_{K},\]

of which \(\Omega_{I}\) designates the _indifference zone_, and \(\Omega_{K}\) the class of parameter values differing so widely from those postulated by the hypothesis that false acceptance of \(H\) is a serious error, which should occur with probability at most \(1-\beta\).

To see how the sample size is determined in this situation, suppose that \(X_{1}\), \(X_{2}\),... constitute the sequence of available random variables, and for a moment let \(n\) be fixed and let \(X=(X_{1},\ldots,X_{n})\). In the usual applications (for a more precise statement, see Problem 8.1), there exists a test \(\varphi_{n}\) which maximizes

\[\inf_{\Omega_{k}}E_{\theta}\varphi(X)\]

among all level-\(\alpha\) tests based on \(X\). Let \(\beta_{n}=\inf_{\Omega_{K}}E_{\theta}\varphi_{n}(X)\), and suppose that for sufficiently large \(n\) there exists a test satisfying (8.1). [Conditions under which this is the case are given by Berger (1951a) and Kraft (1955).] The desired sample size, which is the smallest value of \(n\) for which \(\beta_{n}\geq\beta\), is then obtained by trial and error. This requires the ability of determining for each fixed \(n\) the test that maximizes (8.2) subject to

\[E_{\theta}\varphi(X)\leq\alpha\quad\quad\text{for}\ \ \ \theta\in\Omega_{H}.\]

A method for determining a test with this _maximin_ property (of maximizing the minimum power over \(\Omega_{K}\)) is obtained by generalizing Theorem 3.8.1. It will be convenient in this discussion to make a change of notation, and to denote by \(\omega\) and \(\omega^{\prime}\) the subsets of \(\Omega\) previously denoted by \(\Omega_{H}\) and \(\Omega_{K}\). Let \({\cal P}=\{P_{\theta},\theta\in\omega\cup\omega^{\prime}\}\) be a family of probability distributions over a sample space \(({\cal X},{\cal A})\) with densities \(p_{\theta}=dP_{\theta}/d\mu\) with respect to a \(\sigma\)-finite measure \(\mu\), and suppose that the densities \(p_{\theta}(x)\) considered as functions of the two variables \((x,\theta)\) are measurable \(({\cal A}\times{\cal B})\) and \(({\cal A}\times{\cal B}^{\prime})\), where \({\cal B}\) and \({\cal B}^{\prime}\) are given \(\sigma\)-fields over \(\omega\) and \(\omega^{\prime}\). Under these assumptions, the following theorem gives conditions under which a solution of a suitable Bayes problem provides a test with the required properties.

**Theorem 8.1.1**: _For any distributions \(\Lambda\) and \(\Lambda^{\prime}\) over \({\cal B}\) and \({\cal B}^{\prime}\), let \(\varphi_{\Lambda,\Lambda^{\prime}}\) be the most powerful test for testing_

\[h(x)=\int_{\omega}p_{\theta}(x)\,d\Lambda(\theta)\]

_at level \(\alpha\) against_

\[h^{\prime}(x)=\int_{\omega^{\prime}}p_{\theta}(x)\,d\Lambda^{\prime}(\theta)\]

_and let \(\beta_{\Lambda,\Lambda^{\prime}}\) be its power against the alternative \(h^{\prime}\). If there exist \(\Lambda\) and \(\Lambda^{\prime}\) such that_

\[\eqalign{\sup_{\omega}E_{\theta}\varphi_{\Lambda,\Lambda^{\prime}}(X)&\leq \alpha,\cr\inf_{\omega^{\prime}}E_{\theta}\varphi_{\Lambda,\Lambda^{\prime}}(X )&=\beta_{\Lambda,\Lambda^{\prime}},\cr}\]

[MISSING_PAGE_FAIL:378]

Proof. If \(h\), \(h^{\prime}\), and \(\beta_{\Lambda,\Lambda^{\prime}}\) are defined as in Theorem 8.1.1, the assumptions imply that \(\varphi_{\Lambda,\Lambda^{\prime}}\) is a most powerful level-\(\alpha\) test for testing \(h\) against \(h^{\prime}\), that

\[\sup_{\omega}E_{\theta}\varphi_{\Lambda,\Lambda^{\prime}}(X)=\int_{\omega}E_{ \theta}\varphi_{\Lambda,\Lambda^{\prime}}(X)\,d\Lambda(\theta)=\alpha,\]

and that

\[\inf_{\omega^{\prime}}E_{\theta}\varphi_{\Lambda,\Lambda^{\prime}}(X)=\int_{ \omega^{\prime}}E_{\theta}\varphi_{\Lambda,\Lambda^{\prime}}(X)\,d\Lambda^{ \prime}(\theta)=\beta_{\Lambda,\Lambda^{\prime}}.\]

Condition (8.4) is thus satisfied and Theorem 8.1.1 applies. \(\blacksquare\)

The following remark is often useful in applying the theorem. Suppose \(\varphi_{\Lambda,\Lambda^{\prime}}\) satisfies: its power function is constant and smallest over \(\omega^{\prime}\) on \(\omega^{\prime}_{0}\)\(\equiv\) the support of \(\Lambda^{\prime}\). Then, the condition

\[\beta_{\Lambda,\Lambda^{\prime}}=\inf_{\theta\in\omega^{\prime}}E_{\theta} \varphi_{\Lambda,\Lambda^{\prime}}\]

holds. To see why, note that

\[\inf_{\theta\in\omega^{\prime}}E_{\theta}\varphi_{\Lambda,\Lambda^{\prime}}= \inf_{\theta\in\omega^{\prime}_{0}}E_{\theta}\varphi_{\Lambda,\Lambda^{\prime }}=\int_{\theta\in\omega^{\prime}}E_{\theta}\varphi_{\Lambda,\Lambda^{\prime} }d\Lambda^{\prime}(\theta)\]

\[=\int\int\varphi_{\Lambda,\Lambda^{\prime}}p_{\theta}(x)d\Lambda^{\prime}( \theta)\mu(dx)=\int\varphi_{\Lambda,\Lambda^{\prime}}h^{\prime}(x)\mu(dx)= \beta_{\Lambda,\Lambda^{\prime}}\.\]

**Example 8.1.1**: **(Simple example)** Suppose \(X\sim N(\xi,1)\). Test \(H:\xi=0\) versus \(H^{\prime}:|\xi|\geq\epsilon\), where \(\epsilon>0\) is fixed. Let \(\Lambda^{\prime}\) put equal mass at \(\pm\epsilon\). To see why this works, calculate the Neyman-Pearson test \(\varphi_{\Lambda,\Lambda^{\prime}}\) given (8.5); it rejects for large values of

\[\frac{\exp[-\frac{1}{2}(X-\epsilon)^{2}]+\exp[-\frac{1}{2}(X+\epsilon)^{2}]}{ \exp(-\frac{1}{2}X^{2})}\propto[\exp(\epsilon\,X)+\exp(-\epsilon\,X)]\.\]

The last expression is clearly a function of \(|X|\) and it is easy to check that it is an increasing function of \(|X|\). So, the test rejects for large \(|X|\), i.e., when \(|X|\geq z_{1-\frac{\alpha}{2}}\). Its minimum power occurs when \(|\xi|=\epsilon\) (because the family of distributions of \(|\dot{X}|\) has monotone likelihood ratio), and its power is the same at \(\epsilon\) and \(-\epsilon\). So, it is maximin. It is also UMPI. However, it is not UMPU for the alternatives considered (Problem 8.2). \(\blacksquare\)

**Example 8.1.2**: **(Many normal means)** Suppose \(X_{1}\),..., \(X_{n}\) are independent with \(X_{i}\sim N(\xi_{i},1)\). The null hypothesis specifies \(H:\ \xi_{1}=\cdots\xi_{n}=0\) while the alternative specifies exactly one of the \(\xi_{i}=\xi\), where \(\xi>0\) is assumed known. The least favorable distribution \(\Lambda^{\prime}\) is uniform on the \(n\) vectors

\[(\xi,0,\,\ldots,0),\ (0,\,\xi,0,\,\ldots,0),\,\ldots,\,(0,\,\ldots,0,\,\xi)\.\]The resulting test \(\varphi_{\Lambda,\,\Lambda^{\prime}}\) rejects for large values of \(T=\sum_{i=1}^{n}\exp(\xi\,X_{i})\) To see why, the average likelihood under the alternative divided by the likelihood under the null is given by:

\[\frac{\frac{1}{n}\sum_{i=1}^{n}\exp[-\frac{1}{2}(X_{i}-\xi)^{2}]\Pi_{j\neq i} \exp[-\frac{1}{2}X_{j}^{2}]}{\Pi_{i=1}^{n}\exp[-\frac{1}{2}X_{i}^{2}]}\,\]

which is equivalent to \(T\). By symmetry, its power is the same for all \(n\) alternative mean vectors, and so it is maximin. \(\blacksquare\)

**Example 8.1.3**: (Many normal means with different \(\omega^{\prime}\)): Under the setup of Example 8.1.2, suppose the alternative parameter space is specified by

\[\omega^{\prime}=\{(\xi_{1},\ldots,\xi_{n}):\ \ \sum_{i}\xi_{i}^{2}=\delta^{2}\}\,\]

where \(\delta>0\) is fixed. Now, let \(\Lambda^{\prime}\) be the uniform distribution on \(S_{n}\), the sphere of radius \(\delta\) centered at the origin. Letting \(U\) denote the uniform distribution on \(S_{n}\), the test \(\varphi_{\Lambda,\,\Lambda^{\prime}}\) rejects for large values of

\[\frac{\int_{S_{n}}\,\Pi_{i}\,\exp[-\frac{1}{2}(X_{i}-\xi_{i})^{2}]dU(\xi)}{ \Pi_{i}\,\exp[-\frac{1}{2}X_{i}^{2}]}\]

\[\propto\int_{S_{n}}\exp(\sum_{i}\xi_{i}X_{i})dU(\xi)=CE[\exp(\Xi^{\top}X)]\,\]

where \(\Xi=(\Xi_{1},\,\ldots,\,\Xi_{n})^{\top}\) is random and uniform on \(S_{n}\) and the above expectation is with respect to the distribution of the \(\Xi_{i}\) with the \(X_{i}\) fixed.

By symmetry, the test \(\varphi_{\Lambda,\,\Lambda^{\prime}}\) must be invariant with respect to orthogonal transformations. Indeed, \(E[\exp(\Xi^{\top}X)]\) is the same for \(X=(X_{1},\,\ldots,\,X_{n})^{\top}\) and \(O\,X\), if \(O\) is orthogonal. To see why, note that \(\Xi^{\top}O\,X=(O\,\Xi)^{\top}X\). Since the distribution of \(O\,\Xi\) and \(\Xi\) are the same, then the distribution of \(\Xi^{\top}O\,X\) is the same as that of \(\Xi^{\top}X\), giving the result.

Therefore, the maximin test is invariant, and so it cannot improve upon the UMPI test. Therefore, the UMPI test must be maximin. By Example 6.3.3, the UMPI test rejects for large values of \(T=\sum_{i}X_{i}^{2}\). If we did not know the UMPI test already, we just need to show that \(E[\exp(\Xi^{\top}X)]\) is an increasing function of \(T\); see Problem 8.3. \(\blacksquare\)

Suppose that the sets \(\Omega_{H}\), \(\Omega_{I}\), and \(\Omega_{K}\) are defined in terms of a nonnegative function \(d\), which is a measure of the distance of \(\theta\) from \(H\), by

\[\Omega_{H} =\{\theta:d(\theta)=0\},\ \ \ \ \ \Omega_{I}=\{\theta:0<d( \theta)<\Delta\},\] \[\Omega_{K} =\{0:d(\theta)\geq\Delta\}.\]Suppose also that the power function of any test is continuous in \(\theta\). In the limit as \(\Delta=0\), there is no indifference zone. Then \(\Omega_{K}\) becomes the set \(\{\theta:d(\theta)>0\}\), and the infimum of \(\beta(\theta)\) over \(\Omega_{K}\) is \(\leq\alpha\) for any level-\(\alpha\) test. This infimum is therefore maximized by any test satisfying \(\beta(\theta)\geq\alpha\) for all \(\theta\in\Omega_{K}\), that is, by any unbiased test, so that unbiasedness is seen to be a limiting form of the maximin criterion. A more useful limiting form, since it will typically lead to a unique test, is given by the following definition. A test \(\varphi_{0}\) is said to _maximize the minimum power locally1_ if, given any other test \(\varphi\), there exists \(\Delta_{0}\) such that

Footnote 1: A different definition of local minimaxity is given by Giri and Kiefer (1964).

\[\inf_{\omega_{\Delta}}\beta_{\varphi_{0}}(\theta)\geq\inf_{\omega_{\Delta}} \beta_{\varphi}(\theta)\quad\text{ for all }\ \ 0<\Delta<\Delta_{0}, \tag{8.7}\]

where \(\omega_{\Delta}\) is the set of \(\theta\)'s for which \(d(\theta)\geq\Delta\).

### Further Examples

In Chapter 3 it was shown for a family of probability densities depending on a real parameter \(\theta\) that a UMP test exists for testing \(H:\theta\leq\theta_{0}\) against \(\theta>\theta_{0}\) provided for all \(\theta<\theta^{\prime}\) the ratio \(p_{\theta^{\prime}}(x)/p_{\theta}(x)\) is a monotone function of some real-valued statistic. This assumption, although satisfied for a one-parameter exponential family, is quite restrictive, and a UMP test of \(H\) will in fact exist only rarely. A more general approach is furnished by the formulation of the preceding section. If the indifference zone is the set of \(\theta\)'s with \(\theta_{0}<\theta<\theta_{1}\), the problem becomes that of maximizing the minimum power over the class of alternatives \(\omega^{\prime}:\theta\geq\theta_{1}\). Under appropriate assumptions, one would expect the least favorable distributions \(\Lambda\) and \(\Lambda^{\prime}\) of Theorem 8.1.1 to assign probability 1 to the points \(\theta_{0}\) and \(\theta_{1}\), and hence the maximin test to be given by the rejection region \(p_{\theta_{1}}(x)/p_{\theta_{0}}(x)>C\). The following lemma gives sufficient conditions for this to be the case.

**Lemma 8.2.1**: _Let \(X_{1},\ldots,\)\(X_{n}\) be identically and independently distributed with probability density \(f_{\theta}(x)\), where \(\theta\) and \(x\) are real-valued, and suppose that for any \(\theta<\theta^{\prime}\) the ratio \(f_{\theta^{\prime}}(x)/f_{\theta}(x)\) is a nondecreasing function of \(x\). Then the level-\(\alpha\) test \(\varphi\) of \(H\) which maximizes the minimum power over \(\omega^{\prime}\) is given by_

\[\varphi(x_{1},\ldots,x_{1})=\left\{\begin{array}{ll}1&\text{if }\ r(x_{1}, \ldots,x_{n})>C,\\ \gamma&\text{if }\ r(x_{1},\ldots,x_{n})=C,\\ 0&\text{if }\ r(x_{1},\ldots,x_{n})<C,\end{array}\right. \tag{8.8}\]

_where \(r(x_{1},\ldots,x_{n})=f_{\theta_{1}}(x_{1})\ldots f_{\theta_{1}}(x_{n})/f_{ \theta_{0}}(x_{1})\ldots f_{\theta_{0}}(x_{n})\) and where \(C\) and \(\gamma\) are determined by_

\[E_{\theta_{0}}\varphi(X_{1},\ldots,X_{n})=\alpha. \tag{8.9}\]Proof. The function \(\varphi(x_{1},\ldots,x_{n})\) is nondecreasing in each of its arguments, so that by Lemma 3.4.2,

\[E_{\theta}\varphi(X_{1},\ldots,X_{n})\leq E_{\theta^{\prime}}\varphi(X_{1},\ldots,X_{n})\]

when \(\theta<\theta^{\prime}\). Hence the power function of \(\varphi\) is monotone and \(\varphi\) is a level-\(\alpha\) test. Since \(\varphi=\varphi_{\Lambda,\Lambda^{\prime}}\), where \(\Lambda\) and \(\Lambda^{\prime}\) are the distributions assigning probability 1 to the points \(\theta_{0}\) and \(\theta_{1}\), Condition (8.4) is satisfied, which proves the desired result as well as the fact that the pair of distributions \((\Lambda,\,\Lambda^{\prime})\) is least favorable. \(\blacksquare\)

**Example 8.2.1**: Let \(\theta\) be a location parameter, so that \(f_{\theta}(x)=g(x-\theta)\), and suppose for simplicity that \(g(x)>0\) for all \(x\). We will show that a necessary and sufficient condition for \(f_{\theta}(x)\) to have monotone likelihood ratio in \(x\) is that \(-\log g\) is convex. The condition of monotone likelihood ratio in \(x\),

\[\frac{g(x-\theta^{\prime})}{g(x-\theta)}\leq\frac{g(x^{\prime}-\theta^{\prime} )}{g(x^{\prime}-\theta)}\qquad\text{for all}\quad x<x^{\prime},\quad\theta< \theta^{\prime},\]

is equivalent to

\[\log g(x^{\prime}-\theta)+\log g(x-\theta^{\prime})\leq\log g(x-\theta)+\log g (x^{\prime}-\theta^{\prime}).\]

Since \(x-\theta=t(x-\theta^{\prime})+(1-t)(x^{\prime}-\theta)\) and \(x^{\prime}-\theta^{\prime}=(1-t)(x-\theta^{\prime})+t(x^{\prime}-\theta)\), where \(t=(x^{\prime}-x)/(x^{\prime}-x+\theta^{\prime}-\theta)\), a sufficient condition for this to hold is that the function \(-\log g\) is convex. To see that this condition is also necessary, let \(a<b\) be any real numbers, and let \(x-\theta^{\prime}=a\), \(x^{\prime}-\theta=b\), and \(x^{\prime}-\theta^{\prime}=x-\theta\). Then \(x-\theta=\frac{1}{2}(x^{\prime}-\theta+x-\theta^{\prime})=\frac{1}{2}(a+b)\), and the condition of monotone likelihood ratio implies

\[\tfrac{1}{2}[\log g(a)+\log g(b)]\leq\log g\big{[}\tfrac{1}{2}(a+b)\big{]}.\]

Since \(\log g\) is measurable, this in turn implies that \(-\log g\) is convex.2

Footnote 2: See Sierpinski (1920).

A density \(g\) for which \(-\log g\) is convex is called _strongly unimodal_. Basic properties of such densities were obtained by Ibragimov (1956). Strong unimodality is a special case of total positivity. A density of the form \(g(x-\theta)\) which is totally positive of order \(r\) is said to be a Polya frequency function of order \(r\). It follows from Example 8.2.1 that \(g(x-\theta)\) is a Polya frequency function of order 2 if and only if it is strongly unimodal. [For further results concerning Polya frequency functions and strongly unimodal densities, see Karlin (1968), Marshall and Olkin (1979), Huang and Ghosh (1982), and Loh (1984a, b).]

Two distributions which satisfy the above condition [besides the normal distribution, for which the resulting densities \(p_{\theta}(x_{1},\ldots,x_{n})\) form an exponential family] are the _double-exponential distribution_ with

\[g(x)=\tfrac{1}{2}e^{-|x|}\]and the _logistic distribution_, whose cumulative distribution function is

\[G(x)=\frac{1}{1+e^{-x}},\]

so that the density is \(g(x)=e^{-x}/(1+e^{-x})^{2}\). \(\blacksquare\)

**Example 8.2.2**: To consider the corresponding problem for a scale parameter, let \(f_{\theta}(x)=\theta^{-1}h(x/\theta)\) where \(h\) is an even function. Without loss of generality one may then restrict \(x\) to be nonnegative, since the absolute values \(|X_{1}|\),..., \(|X_{n}|\) form a set of sufficient statistics for \(\theta\). If \(Y_{i}=\log X_{i}\) and \(\eta=\log\theta\), the density of \(Y_{i}\) is

\[h(e^{y-\eta})e^{y-\eta}.\]

By Example 8.2.1, if \(h(x)>0\) for all \(x\geq 0\), a necessary and sufficient condition for \(f_{\theta^{\prime}}(x)/f_{\theta}(x)\) to be a nondecreasing function of \(x\) for all \(\theta\,<\theta^{\prime}\) is that \(-\log[e^{y}h(e^{y})]\) or equivalently \(-\log h(e^{y})\) is a convex function of \(y\). An example in which this holds--in addition to the normal and double-exponential distributions, where the resulting densities form an exponential family--is the _Cauchy distribution_ with

\[h(x)=\frac{1}{\pi}\,\frac{1}{1+x^{2}}.\]

Since the convexity of \(-\log h(y)\) implies that of \(-\log h(e^{y})\), it follows that if \(h\) is an even function and \(h(x-\theta)\) has monotone likelihood ratio, so does \(h(x/\theta)\). When \(h\) is the normal or double-exponential distribution, this property of \(h(x/\theta)\) also follows from Example 8.2.1. That monotone likelihood ratio for the scale-parameter family does not conversely imply the same property for the associated location parameter family is illustrated by the Cauchy distribution. The condition is therefore more restrictive for a location than for a scale parameter. \(\blacksquare\)

The chief difficulty in the application of Theorem 8.1.1 to specific problems is the necessity of knowing, or at least being able to guess correctly, a pair of least favorable distributions (\(\Lambda\), \(\Lambda^{\prime}\)). Guidance for obtaining these distributions is sometimes provided by invariance considerations. If there exists a group \(G\) of transformations of \(X\) such that the induced group \(\tilde{G}\) leaves both \(\omega\) and \(\omega^{\prime}\) invariant, the problem is symmetric in the various \(\theta\)'s that can be transformed into each other under \(\tilde{G}\). It then seems plausible that unless \(\Lambda\) and \(\Lambda^{\prime}\) exhibit the same symmetries, they will make the statistician's task easier, and hence will not be least favorable.

**Example 8.2.3**: In the problem of paired comparisons considered in Example 6.3.6, the observations \(X_{i}\) (\(i=1\),..., \(n\)) are independent variables taking on the values \(1\) and \(0\) with probabilities \(p_{i}\) and \(q_{i}=1-p_{i}\). The hypothesis \(H\) to be tested specifies the set \(\omega:\max p_{i}\leq\frac{1}{2}\). Only alternatives with \(p_{i}\geq\frac{1}{2}\) for all \(i\) are considered, and as \(\omega^{\prime}\) we take the subset of those alternatives for which \(\max p_{i}\geq\frac{1}{2}+\delta\). One would expect \(\Lambda\) to assign probability 1 to the point \(p_{1}=\cdots p_{n}=\frac{1}{2}\), and \(\Lambda^{\prime}\) to assign positive probability only to the \(n\) points (\(p_{1}\),..., \(p_{n}\)) which have \(n-1\) coordinates equal to \(\frac{1}{2}\) and the remaining coordinate equal to \(\frac{1}{2}+\delta\). Because of the symmetry with regard to the \(n\) variables, it seems plausible that \(\Lambda^{\prime}\) should assign equal probability \(1/n\) to each of these \(n\) points. With these choices, the test \(\varphi_{\Lambda,\Lambda^{\prime}}\) rejects when

\[\sum_{i=1}^{n}\left(\frac{\frac{1}{2}+\delta}{\frac{1}{2}}\right)^{x_{i}}>C.\]

This is equivalent to \(\sum_{i=1}^{n}x_{i}>C\), which had previously been seen to be UMP invariant for this problem. Since the critical function \(\varphi_{\Lambda,\Lambda^{\prime}}(x_{1},\ldots,x_{n})\) is nondecreasing in each of its arguments, it follows from Lemma 3.4.2 that \(p_{i}\leq p_{i}^{\prime}\) for \(i=1,\ldots,n\) implies

\[E_{p_{1},\ldots,p_{n}}\varphi_{\Lambda,\Lambda^{\prime}}(X_{1},\ldots,X_{n}) \leq E_{p_{1}^{{}^{\prime}},\ldots,p_{n}^{\prime}}\varphi_{\Lambda,\Lambda^{ \prime}}(X_{1},\ldots,X_{n})\]

and hence the conditions of Theorem 8.1.1 are satisfied.

**Example 8.2.4**: Let \(X=(X_{1},\ldots,X_{n})\) be a sample from \(N(\xi,\sigma^{2})\), and consider the problem of testing \(H:\sigma=\sigma_{0}\) against the set of alternatives \(\omega^{\prime}:\sigma\leq\sigma_{1}\) or \(\sigma\geq\sigma_{2}\) (\(\sigma_{1}<\sigma_{0}<\sigma_{2}\)). This problem remains invariant under the transformations \(X_{i}^{\prime}=X_{i}+c\), which in the parameter space induce the group \(\bar{G}\) of transformations \(\xi^{\prime}=\xi+c\), \(\sigma^{\prime}=\sigma\). One would therefore expect the least favorable distribution \(\Lambda\) over the line \(\omega:-\infty<\xi<\infty\), \(\sigma=\sigma_{0}\), to be invariant under \(\bar{G}\). Such invariance implies that \(\Lambda\) assigns to any interval a measure proportional to the length of the interval. Hence \(\Lambda\) cannot be a probability measure and Theorem 8.1.1 is not directly applicable. The difficulty can be avoided by approximating \(\Lambda\) by a sequence of probability distributions, in the present case, for example, by the sequence of normal distributions \(N(0,k)\), \(k=1\), \(2\),....

In the particular problem under consideration, it happens that there also exist least favorable distributions \(\Lambda\) and \(\Lambda^{\prime}\), which are true probability distributions and therefore not invariant. These distributions can be obtained by an examination of the corresponding one-sided problem in Section 3.9, as follows. On \(\omega\), where the only variable is \(\xi\), the distribution \(\Lambda\) of \(\xi\) is taken as the normal distribution with an arbitrary mean \(\xi_{1}\) and with variance \((\sigma_{2}^{2}-\sigma_{0}^{2})/n\). Under \(\Lambda^{\prime}\) all probability should be concentrated on the two lines \(\sigma=\sigma_{1}\) and \(\sigma=\sigma_{2}\) in the (\(\xi\), \(\sigma\)) plane, and we put \(\Lambda^{\prime}=p\Lambda^{\prime}_{1}+q\Lambda^{\prime}_{2}\), where \(\Lambda^{\prime}_{1}\) is the normal distribution with mean \(\xi_{1}\) and variance \((\sigma_{2}^{2}-\sigma_{1}^{2})/n\), while \(\Lambda^{\prime}_{2}\) assigns probability 1 to the point \((\xi_{1},\sigma_{2})\). A computation analogous to that carried out in Section 3.9 then shows the acceptance region to be given by \[\frac{p}{\sigma_{1}^{n-1}\sigma_{2}}\exp\left[\frac{-1}{2\sigma_{1}^{2}}\sum(x_{i}- \bar{x})^{2}-\frac{n}{2\sigma_{2}^{2}}(\bar{x}-\xi_{1})^{2}\right]\]

\[\begin{array}{c}+\frac{q}{\sigma_{2}^{n}}\exp\left[\frac{-1}{2\sigma_{2}^{2}} \left\{\sum(x_{i}-\bar{x})^{2}+n(\bar{x}-\xi_{1})^{2}\right\}\right]\\ \frac{1}{\sigma_{0}^{n-1}\sigma_{2}}\exp\left[\frac{-1}{2\sigma_{0}^{2}}\sum(x_ {i}-\bar{x})^{2}-\frac{n}{2\sigma_{2}^{2}}(\bar{x}-\xi_{1})^{2}\right]\end{array} \right.<C\,\]

which is equivalent to

\[C_{1}\leq\sum(x_{i}-\bar{x})^{2}\leq C_{2}.\]

The probability of this inequality is independent of \(\xi\), and hence \(C_{1}\) and \(C_{2}\) can be determined so that the probability of acceptance is \(1-\alpha\) when \(\sigma=\sigma_{0}\), and is equal for the two values \(\sigma=\sigma_{1}\) and \(\sigma=\sigma_{2}\).

It follows from Section 3.7 that there exist \(p\) and \(C\) which lead to these values of \(C_{1}\) and \(C_{2}\) and that the above test satisfies the conditions of Corollary 8.1.1 with \(\omega_{0}=\omega\), and with \(\omega_{0}^{\prime}\) consisting of the two lines \(\sigma=\sigma_{1}\) and \(\sigma=\sigma_{2}\). \(\blacksquare\)

### Comparing Two Approximate Hypotheses

As in Section 3.2, let \(P_{0}\neq P_{1}\) be two distributions possessing densities \(p_{0}\) and \(p_{1}\) with respect to a measure \(\mu\). Since distributions even at best are known only approximately, let us assume that the true distributions are approximately \(P_{0}\) or \(P_{1}\) in the sense that they lie in one of the families

\[\mathcal{P}_{i}=\{Q:Q=(1-\epsilon_{i})P_{i}+\epsilon_{i}G_{i}\},\ \ \ \ \ i=0,1, \tag{8.10}\]

with \(\epsilon_{0}\), \(\epsilon_{1}\) given and the \(G_{i}\) arbitrary unknown distributions. We wish to find the level-\(\alpha\) test of the hypothesis \(H\) that the true distribution lies in \(\mathcal{P}_{0}\), which maximizes the minimum power over \(\mathcal{P}_{1}\). This is the problem considered in Section 8.1 with \(\theta\) indicating the true distribution, \(\Omega_{H}=\mathcal{P}_{0}\), and \(\Omega_{K}=\mathcal{P}_{1}\).

The following theorem shows the existence of a pair of least favorable distributions \(\Lambda\) and \(\Lambda^{\prime}\) satisfying the conditions of Theorem 8.1.1, each assigning probability 1 to a single distribution, \(\Lambda\) to \(Q_{0}\in\mathcal{P}_{0}\) and \(\Lambda^{\prime}\) to \(Q_{1}\in\mathcal{P}_{1}\), and exhibits the \(Q_{i}\) explicitly.

**Theorem 8.3.1**: _Let_

\[q_{0}(x)=\left\{\begin{array}{ccc}(1-\epsilon_{0})p_{0}(x)&\mbox{if}&\frac{ p_{1}(x)}{p_{0}(x)}<b,\\ \frac{(1-\epsilon_{0})p_{1}(x)}{b}&\mbox{if}&\frac{p_{1}(x)}{p_{0}(x)}\geq b, \end{array}\right. \tag{8.11}\]

\[q_{1}(x)=\left\{\begin{array}{ccc}(1-\epsilon_{1})p_{1}(x)&\mbox{if}&\frac {p_{1}(x)}{p_{0}(x)}>a,\\ a(1-\epsilon_{1})p_{0}(x)&\mbox{if}&\frac{p_{1}(x)}{p_{0}(x)}\leq a.\end{array}\right.\]

[MISSING_PAGE_FAIL:386]

\[\gamma(c+\Delta)-\gamma(c)\geq\Delta\int_{r(x)<c}p_{0}(x)\,d\mu(x) \tag{8.16}\]

and that \(\gamma(c)=1\) for all \(c\) for which

\[P_{i}[r(x)\leq c]=0\ \ \ \ \ (i=0,1). \tag{8.17}\]

If \(c_{0}\) is the supremum of the values for which (8.17) holds, (8.16) shows that \(\gamma\) is strictly increasing for \(c>c_{0}\) and this proves uniqueness. The proof for \(b\) is exactly analogous (Problem 8.16).

(ii): As \(\epsilon_{1}\to 0\), the solution \(a\) of (8.13) tends to \(c_{0}\). Analogously, as \(\epsilon_{1}\to 0\), \(b\to\infty\) (Problem 8.16).

(iii): This will follow from the following facts:

1. When \(X\) is distributed according to a distribution in \(\mathcal{P}_{0}\), the statistic \(r^{*}(X)\) is stochastically largest when the distribution of \(X\) is \(Q_{0}\).
2. When \(X\) is distributed according to a distribution in \(\mathcal{P}_{1}\), \(r^{*}(X)\) is stochastically smallest for \(Q_{1}\).
3. \(r^{*}(X)\) is stochastically larger when the distribution of \(X\) is \(Q_{1}\) than when it is \(Q_{0}\).

These statements are summarized in the inequalities

\[Q^{\prime}_{0}[r^{*}(X)<t]\geq Q_{0}[r^{*}(X)<t]\geq Q_{1}[r^{*}(X)<t]\geq Q^{ \prime}_{1}[r^{*}(X)<t] \tag{8.18}\]

for all \(t\) and all \(Q^{\prime}_{i}\in\mathcal{P}_{i}\).

From (8.12), it is seen that (8.18) is obvious when \(t\leq ka\) or \(t>kb\). Suppose therefore that \(ak<t\leq bk\), and denote the event \(r^{*}(X)<t\) by \(E\). Then \(Q^{\prime}_{0}(E)\geq(1-\epsilon_{0})P_{0}(E)\) by (8.10). But \(r^{*}(x)<t<kb\) implies \(r(X)<b\) and hence \(Q_{0}(E)=(1-\epsilon)P_{0}(E)\). Thus \(Q^{\prime}_{0}(E)\geq Q_{0}(E)\), and analogously \(Q^{\prime}_{1}(E)\leq Q_{1}(E)\). Finally, the middle inequality of (8.18) follows from Corollary 3.2.1.

If the \(\epsilon\)'s are sufficiently small so that \(Q_{0}\neq Q_{1}\), it follows from (a)-(c) that \(\mathcal{P}_{0}\) and \(\mathcal{P}_{1}\) are nonoverlapping.

That (\(Q_{0}\), \(Q_{1}\)) is least favorable and the associated test \(\varphi\) is maximin now follows from Theorem 8.1.1, since the most powerful test \(\varphi\) for testing \(Q_{0}\) against \(Q_{1}\) is a nondecreasing function of \(q_{1}(X)/q_{0}(X)\). This shows that \(E\varphi(X)\) takes on its sup over \(\mathcal{P}_{0}\) at \(Q_{0}\) and its inf over \(\mathcal{P}_{1}\) at \(Q_{1}\), and this completes the proof. \(\blacksquare\)

Generalizations of this theorem are given by Huber and Strassen (1973,1974). See also Rieder (1977) and Bednarski (1984). An optimum permutation test, with generalizations to the case of unknown location and scale parameters, is discussed by Lambert (1985).

When the data consist of \(n\) identically, independently distributed random variables \(X_{1}\),..., \(X_{n}\), the neighborhoods (8.10) may not be appropriate, since they do not preserve the assumption of independence. If \(P_{i}\) has density

\[p_{i}(x_{1},\ldots,x_{n})=f_{i}(x_{1})\ldots f_{i}(x_{n})\ \ \ \ \ (i=0,1), \tag{8.19}\]a more appropriate model approximating (8.19) may then assign to \(X=(X_{1},\ldots,X_{n})\) the family \({\cal P}_{i}^{*}\) of distributions according to which the \(X_{j}\) are independently distributed, each with distribution

\[(1-\epsilon_{i})F_{i}(x_{j})+\epsilon_{i}G_{i}(x_{j}), \tag{8.20}\]

where \(F_{i}\) has density \(f_{i}\) and where as before the \(G_{i}\) are arbitrary.

**Corollary 8.3.1**: _Suppose \(q_{0}\) and \(q_{1}\) defined by (8.11) with \(x=x_{j}\) satisfy (8.18) and hence are a least favorable pair for testing \({\cal P}_{0}\) against \({\cal P}_{1}\) on the basis of the single observation \(X_{j}\). Then the pair of distributions with densities \(q_{i}(x_{1})\ldots q_{i}(x_{n})\) (\(i=0,1\)) is least favorable for testing \({\cal P}_{0}^{*}\) against \({\cal P}_{1}^{*}\), so that the maximin test is given by_

\[\varphi(x_{1},\ldots,x_{n})=\left\{\begin{array}{ll}1&\\ \gamma&\\ 0&\end{array}\right.\mbox{if }\prod_{j=1}^{n}\left[\frac{q_{1}(x_{j})}{q_{0}(x_{ j})}\right]\mathop{\simeq}\limits_{\approx}c. \tag{8.21}\]

Proof. By assumption, the random variables \(Y_{j}=q_{1}(X_{j})/q_{0}(X_{j})\) are stochastically increasing as one moves successively from \(Q_{0}^{\prime}\in{\cal P}_{0}\) to \(Q_{0}\) to \(Q_{1}\) to \(Q_{1}^{\prime}\in{\cal P}_{1}\). The same is then true of any function \(\psi\left(Y_{1},\ldots,Y_{n}\right)\) which is nondecreasing in each of its arguments by Lemma 3.4.1, and hence of \(\varphi\) defined by (8.21). The proof now follows from Theorem 8.3.1.

Instead of the problem of testing \(P_{0}\) against \(P_{1}\), consider now the situation of Lemma 8.2.1 where \(H:\theta\leq\theta_{0}\) is to be tested against \(\theta\geq\theta_{1}\) (\(\theta_{0}<\theta_{1}\)) on the basis of \(n\) independent observations \(X_{j}\), each distributed according to a distribution \(F_{\theta}(x_{j})\) whose density \(f_{\theta}(x_{j})\) is assumed to have monotone likelihood ratio in \(x_{j}\).

A robust version of this problem is obtained by replacing \(F_{\theta}\) with

\[(1-\epsilon)F_{\theta}(x_{j})+\epsilon G(x_{j}),\qquad j=1,\ldots,n, \tag{8.22}\]

where \(\epsilon\) is given and for each \(\theta\) the distribution \(G\) is arbitrary. Let \({\cal P}_{0}^{**}\) and \({\cal P}_{1}^{**}\) be the classes of distributions (8.22) with \(\theta\leq\theta_{0}\) and \(\theta\geq\theta_{1}\), respectively; and let \({\cal P}_{0}^{*}\) and \({\cal P}_{1}^{*}\) be defined as in Corollary 8.3.1 with \(f_{\theta_{i}}\) in place of \(f_{i}\). Then the maximin test (8.21) of \({\cal P}_{0}^{*}\) against \({\cal P}_{1}^{*}\) retains this property for testing \({\cal P}_{0}^{**}\) against \({\cal P}_{1}^{**}\).

This is proved in the same way as Corollary 8.3.1, using the additional fact that if \(F_{\theta^{\prime}}\) is stochastically larger than \(F_{\theta}\), then \((1-\epsilon)F_{\theta^{\prime}}+\epsilon G\) is stochastically larger than \((1-\epsilon)F_{\theta}+\epsilon G\).

### Maximin Tests and Invariance

When the problem of testing \(\Omega_{H}\) against \(\Omega_{K}\) remains invariant under a certain group of transformations, it seems reasonable to expect the existence of an invariant pair of least favorable distributions (or at least of sequences of distributions which in some sense are least favorable and invariant in the limit), and hence also of a maximin test which is invariant. This suggests the possibility of bypassing the somewhat cumbersome approach of the preceding sections. If it could be proved that for an invariant problem there always exists an invariant test that maximizes the minimum power over \(\Omega_{K}\), attention could be restricted to invariant tests; in particular, a UMP invariant test would then automatically have the desired maximin property (although it would not necessarily be admissible). These speculations turn out to be correct for an important class of problems, although unfortunately not in general. To find out under what conditions they hold, it is convenient first to separate out the statistical aspects of the problem from the group-theoretic ones by means of the following lemma.

Let \({\cal P}=\{P_{\theta},\theta\in\Omega\}\) be a dominated family of distributions on \(({\cal X},{\cal A})\), and let \(G\) be a group of transformations of \(({\cal X},{\cal A})\), such that the induced group \(\bar{G}\) leaves the two subsets \(\Omega_{H}\) and \(\Omega_{K}\) of \(\Omega\) invariant. Suppose that for any critical function \(\varphi\) there exists an (almost) invariant critical function \(\psi\) satisfying

\[\inf_{\bar{G}}E_{\bar{g}\theta}\varphi(X)\leq E_{\theta}\psi\left(X\right)\leq \sup_{\bar{G}}E_{\bar{g}\theta}\varphi(X) \tag{8.23}\]

for all \(\theta\in\Omega\). Then if there exists a level-\(\alpha\) test \(\varphi_{0}\) maximizing \(\inf_{\Omega_{k}}E_{\theta}\varphi(X)\), there also exists an (almost) invariant test with this property.

Proof. Let \(\inf_{\Omega_{K}}E_{\theta}\varphi_{0}(X)=\beta\), and let \(\psi_{0}\) be an (almost) invariant test such that (8.23) holds with \(\varphi=\varphi_{0}\), \(\psi=\psi_{0}\). Then

\[E_{\theta}\psi_{0}(X)\leq\sup_{\bar{G}}E_{\bar{g}\theta}\varphi_{0}(X)\leq \alpha\quad\quad\mbox{for all}\quad\theta\in\Omega_{H}\]

and

\[E_{\theta}\psi_{0}(X)\geq\inf_{\bar{G}}E_{\bar{g}\theta}\varphi_{0}(X)\geq \beta\quad\quad\mbox{for all}\quad\theta\in\Omega_{K},\]

as was to be proved.

To determine conditions under which there exists an invariant or almost invariant test \(\psi\) satisfying (8.23), consider first the simplest case that \(G\) is a finite group, \(G=\{g_{1},\ldots,g_{N}\}\) say. If \(\psi\) is then defined by

\[\psi\left(x\right)=\frac{1}{N}\sum_{i=1}^{N}\varphi(g_{i}x), \tag{8.24}\]

it is clear that \(\psi\) is again a critical function, and that it is invariant under \(G\). It also satisfies (8.23), since \(E_{\theta}\varphi(gX)=E_{\bar{g}\theta}\varphi(X)\) so that \(E_{\theta}\psi(X)\) is the average of a number of terms of which the first and last member of (8.23) are the minimum and maximum, respectively.

An illustration of the finite case is furnished by Example 8.2.3. Here the problem remains invariant under the \(n!\) permutations of the variables \((X_{1},\ldots,X_{n})\)Lemma 8.4.1 is applicable and shows that there exists an invariant test maximizing \(\inf_{\Omega_{K}}E_{\theta}\varphi(X)\). Thus in particular the UMP invariant test obtained in Example 6.3.6 has this maximin property and therefore constitutes a solution of the problem.

It also follows that, under the setting of Theorem 6.3.1, the UMPI test given by (6.10) is maximin.

The definition (8.24) suggests the possibility of obtaining \(\psi(x)\) also in other cases by averaging the values of \(\varphi(gx)\) with respect to a suitable probability distribution over the group \(G\). To see what conditions would be required of this distribution, let \(\mathcal{B}\) be a \(\sigma\)-field of subsets of \(G\) and \(v\) a probability distribution over \((G,\mathcal{B})\). Disregarding measurability problems for the moment, let \(\psi\) be defined by

\[\psi(x)=\int\varphi(gx)\,dv(g). \tag{8.25}\]

Then \(0\leq\psi\leq 1\), and (8.23) is seen to hold by applying Fubini's Theorem (Theorem 2.2.4) to the integral of \(\psi\) with respect to the distribution \(P_{\theta}\). For any \(g_{0}\in G\),

\[\psi(g_{0}x)=\int\varphi(gg_{0}x)\,dv(g)=\int\varphi(hx)\,dv^{*}(h)\,\]

where \(h=gg_{0}\) and where \(v^{*}\) is the measure defined by

\[v^{*}(B)=v(Bg_{0}^{-1})\quad\quad\text{for all}\quad B\in\mathcal{B},\]

into which \(v\) is transformed by the transformation \(h=gg_{0}\). Thus \(\psi\) will have the desired invariance property, \(\psi(g_{0}x)=\psi(x)\) for all \(g_{0}\in G\), if \(v\) is _right invariant_, that is, if it satisfies

\[v(Bg)=v(B)\quad\quad\text{for all}\quad B\in\mathcal{B},\quad g\in G. \tag{8.26}\]

Such a condition was previously used in (6.18).

The measurability assumptions required for the above argument are: (i) For any \(A\in\mathcal{A}\), the set of pairs \((x,\,g)\) with \(gx\in A\) is measurable (\(\mathcal{A}\times\mathcal{B}\)). This insures that the function \(\psi\) defined by (8.25) is again measurable. (ii) For any \(B\in\mathcal{B}\), \(g\in G\), the set \(Bg\) belongs to \(\mathcal{B}\).

**Example 8.4.1**: If \(G\) is a finite group with elements \(g_{1}\),..., \(g_{N}\), let \(\mathcal{B}\) be the class of all subsets of \(G\) and \(v\) the probability measure assigning probability \(1/N\) to each of the \(N\) elements. Condition (8.26) is then satisfied, and the definition (8.25) of \(\psi\) in this case reduces to (8.24). \(\blacksquare\)

**Example 8.4.2**: Consider the group \(G\) of orthogonal \(n\times n\) matrices \(\Gamma\), with the group product \(\Gamma_{1}\Gamma_{2}\) defined as the corresponding matrix product. Each matrix can be interpreted as the point in \(n^{2}\)-dimensional Euclidean space whose coordinates are the \(n^{2}\) elements of the matrix. The group then defines a subset of this space; the Borel subsets of \(G\) will be taken as the \(\sigma\)-field \(\mathcal{B}\). To prove the existence of a right invariant probability measure over \((G,\)\(\Cal{B})\), we shall define a random orthogonal matrix whose probability distribution satisfies (8.26) and is therefore the required measure. With any nonsingular matrix \(x=(x_{ij})\), associate the orthogonal matrix \(y=f(x)\) obtained by applying the following Gram-Schmidt orthogonalization process to the \(n\) row vectors \(x_{i}=(x_{i1},\ldots,x_{in})\) of \(x:y_{1}\) is the unit vector in the direction of \(x_{1}\); \(y_{2}\) the unit vector in the plane spanned by \(x_{1}\) and \(x_{2}\) which is orthogonal to \(y_{1}\) and forms an acute angle with \(x_{2}\); and so on. Let \(y=(y_{ij})\) be the matrix whose \(i\)th row is \(y_{i}\).

Suppose now that the variables \(X_{ij}\) (\(i,\)\(j=1,\ldots,n\)) are independently distributed as \(N(0,\)\(1)\), let \(X\) denote the random matrix \((X_{ij})\), and let \(Y=f(X).\) To show that the distribution of the random orthogonal matrix \(Y\) satisfies (8.26), consider any fixed orthogonal matrix \(\Gamma\) and any fixed set \(B\in\Cal{B}.\) Then \(P\{Y\in B\Gamma\}=P\{Y\Gamma^{\prime}\in B\}\) and from the definition of \(f\) it is seen that \(Y\Gamma^{\prime}=f(X\Gamma^{\prime})\). Since the \(n^{2}\) elements of the matrix \(X\Gamma^{\prime}\) have the same joint distribution as those of the matrix \(X\), the matrices \(f(X\Gamma^{\prime})\) and \(f(X)\) also have the same distribution, as was to be proved. \(\blacksquare\)

Examples 8.4.1 and 8.4.2 are sufficient for the applications to be made here. General conditions for the existence of an invariant probability measure, of which these examples are simple special cases, are given in the theory of Haar measure. [This is treated, for example, in the books by Halmos (1974), Loomis (1953), and Nachbin (1965). For a discussion in a statistical setting, see Eaton (1983, 1989), Farrell (1998a), and Wijsman (1990), and for a more elementary treatment Berger (1985a).]

### The Hunt-Stein Theorem

Invariant measures exist (and are essentially unique) for a large class of groups, but unfortunately they are frequently not finite and hence cannot be taken to be probability measures. The situation is similar and related to that of the nonexistence of a least favorable pair of distributions in Theorem 8.1.1. There it is usually possible to overcome the difficulty by considering instead a sequence of distributions which has the desired property in the limit. Analogously we shall now generalize the construction of \(\psi\) as an average with respect to a right-invariant probability distribution, by considering a sequence of distributions over \(G\) which are approximately right invariant for \(n\) sufficiently large.

Let \(\Cal{P}=\{P_{\theta},\)\(\theta\in\Omega\}\) be a family of distributions over a Euclidean space \((\Cal{X},\Cal{A})\) dominated by a \(\sigma\)-finite measure \(\mu\), and let \(G\) be a group of transformations of \((\Cal{X},\Cal{A})\) such that the induced group \(\bar{G}\) leaves \(\Omega\) invariant.

Theorem 8.5.1 (Hunt-Stein.): Let \(\Cal{B}\) be a \(\sigma\)-field of subsets of \(G\) such that for any \(A\in\Cal{A}\) the set of pairs \((x,\)\(g)\) with \(gx\in A\) is in \(\Cal{A}\times\Cal{B}\) and for any \(B\in\Cal{B}\) and \(g\in G\) the set \(Bg\) is in \(\Cal{B}\). Suppose that there exists a sequence of probability distributions \(v_{n}\) over \((G,\)\(\Cal{B})\) which is asymptotically right invariant in the sense that for any \(g\in G\), \(B\in\Cal{B}\),\[\lim_{n\to\infty}|v_{n}(Bg)-v_{n}(B)|=0. \tag{8.27}\]

_Then given any critical function \(\varphi\), there exists a critical function \(\psi\) which is almost invariant and satisfies (8.23)._

Proof. Let

\[\psi_{n}(x)=\int\varphi(gx)\,dv_{n}(g),\]

which as before is measurable and between \(0\) and \(1\). By the weak compactness theorem (Theorem A.5.1 of the Appendix) there exists a subsequence \(\{\psi_{n_{i}}\}\) and a measurable function \(\psi\) between \(0\) and \(1\) satisfying

\[\lim_{i\to\infty}\int\psi_{n_{i}}p\,d\mu=\int\psi p\,d\mu\]

for all \(\mu\)-integrable functions \(p\), so that in particular

\[\lim_{i\to\infty}E_{\theta}\psi_{n_{i}}(X)=E_{\theta}\psi(X)\]

for all \(\theta\in\Omega\). By Fubini's Theorem,

\[E_{\theta}\psi_{n_{i}}(X)=\int[E_{\theta}\varphi(gX)]\,dv_{n_{i}}(g)=\int E_{ \tilde{g}\tilde{\theta}}\varphi(X)\,dv_{n_{i}}(g)\,\]

so that

\[\inf_{\tilde{G}}E_{\tilde{g}\tilde{\theta}}\varphi(X)\leq E_{\theta}\psi_{n_{ i}}(X)\leq\sup_{\tilde{G}}E_{\tilde{g}\tilde{\theta}}\varphi(X),\]

and \(\psi\) satisfies (8.23).

In order to prove that \(\psi\) is almost invariant we shall show below that for all \(x\) and \(g\),

\[\psi_{n_{i}}(gx)-\psi_{n_{i}}(x)\to 0. \tag{8.28}\]

Let \(I_{A}(x)\) denote the indicator function of a set \(A\in\mathcal{A}\). Using the fact that \(I_{gA}(gx)=I_{A}(x)\), we see that (8.28) implies

\[\int_{A}\psi(x)\,d\,P_{\theta}(x) = \lim_{i\to\infty}\int\psi_{n_{i}}(x)I_{A}(x)\,dP_{\theta}(x)\] \[= \lim_{i\to\infty}\int\psi_{n_{i}}(g\,X)I_{gA}(gx)\,dP_{\theta}(x)\] \[= \int\psi(x)I_{gA}(x)\,dP_{\tilde{g}\tilde{\theta}}(x)=\int_{A} \psi(gx)\,dP_{\theta}(x)\,\]

and hence \(\psi(gx)=\psi(x)\) (a.e. \(\mathcal{P}\)), as was to be proved.

To prove (8.28), consider any fixed \(x\) and any integer \(m\), and let \(G\) be partitioned into the mutually exclusive sets

\[B_{k}=\left\{h\in G:a_{k}<\varphi(hx)\leq a_{k}+\frac{1}{m}\right\},\ \ \ \ \ \ k=0,\ldots,m,\]

where \(a_{k}=(k-1)/m\). In particular, \(B_{0}\) is the set \(\{h\in G:\varphi(hx)=0\}\). It is seen from the definition of the sets \(B_{k}\) that

\[\sum_{k=0}^{m}a_{k}v_{n_{i}}(B_{k})\leq\sum_{k=0}^{m}\int_{B_{k}} \varphi(hx)\,dv_{n_{i}}(h) \leq \sum_{k=0}^{m}\left(a_{k}+\frac{1}{m}\right)v_{n_{i}}(B_{k})\] \[\leq \sum_{k=0}^{m}a_{k}v_{n_{i}}(B_{k})+\frac{1}{m}\,\]

and analogously that

\[\left|\sum_{k=0}^{m}\int_{B_{k}g^{-1}}\varphi(hgx)\,dv_{n_{i}}(h)-\sum_{k=0}^{ m}a_{k}v_{n_{i}}(B_{k}g^{-1})\right|\leq\frac{1}{m},\]

from which it follows that

\[\psi_{n_{i}}(gx)-\psi_{n_{i}}(x):\leq\sum|a_{k}|\cdot|v_{n_{i}}(B_{k}g^{-1})-v _{n_{i}}(B_{k})|+\frac{2}{m}.\]

By (8.27) the first term of the right-hand side tends to zero as \(i\) tends to infinity, and this completes the proof. \(\blacksquare\)

When there exist a right-invariant measure \(\upsilon\) over \(G\) and a sequence of subsets \(G_{n}\) of \(G\) with \(G_{n}\subseteq G_{n+1}\), \(\cup G_{n}=G\), and \(\upsilon(G_{n})=c_{n}<\infty\), it is suggestive to take for the probability measures \(v_{n}\) of Theorem 8.5.1 the measures \(\upsilon/c_{n}\) truncated on \(G_{n}\). This leads to the desired result in the example below. On the other hand, there are cases in which there exists such a sequence of subsets of \(G_{n}\) but no invariant test satisfying (8.23) and hence no sequence \(v_{n}\) satisfying (8.27).

**Example 8.5.1** Let \(x=(x_{1},\ldots,x_{n})\), \(\mathcal{A}\) be the class of Borel sets in \(n\)-space, and \(G\) the group of translations \((x_{1}+g,\ldots,x_{n}+g)\), \(-\infty<g<\infty\). The elements of \(G\) can be represented by the real numbers, and the group product \(gg^{\prime}\) is then the sum \(g+g^{\prime}\). If \(\mathcal{B}\) is the class of Borel sets on the real line, the measurability assumptions of Theorem 8.5.1 are satisfied. Let \(\upsilon\) be Lebesgue measure, which is clearly invariant under \(G\), and define \(v_{n}\) to be the uniform distribution on the interval \(I(-n,n)=\{g:-n\leq g\leq n\}\). Then for all \(B\in\mathcal{B}\), \(g\in G\),

\[|\upsilon_{n}(B)-\upsilon_{n}(Bg)|=\frac{1}{2n}|\upsilon[B\cap I(-n,n)]- \upsilon[B\cap I(-n-g,n-g)]|\leq\frac{|g|}{2n},\]

so that (8.27) is satisfied.

This argument also covers the group of scale transformations \((ax_{1},\ldots,ax_{n}),0<a<\infty\), which can be transformed into the translation group by taking logarithms. \(\blacksquare\)

When applying the Hunt-Stein Theorem to obtain invariant minimax tests, it is frequently convenient to carry out the calculation in steps, as was done in Theorem 6.6.1. Suppose that the problem remains invariant under two groups \(D\) and \(E\), and denote by \(y=s(x)\) a maximal invariant with respect to \(D\) and by \(E^{*}\) the group defined in Theorem 6.2.2, which \(E\) induces in \(y\)-space. If \(D\) and \(E^{*}\) satisfy the conditions of the Hunt-Stein Theorem, it follows first that there exists a maximin test depending only on \(y=s(x)\), and then that there exists a maximin test depending only on a maximal invariant \(z=t(y)\) under \(E^{*}\).

**Example 8.5.2**: Consider a univariate linear hypothesis in the canonical form in which \(Y_{1},\ldots,Y_{n}\) are independently distributed as \(N(\eta_{i},\sigma^{2})\), where it is given that \(\eta_{s+1}=\cdots=\eta_{n}=0\), and where the hypothesis to be tested is \(\eta_{1}=\cdots=\eta_{r}=0\). It was shown in Section 7.1 that this problem remains invariant under certain groups of transformations and that with respect to these groups there exists a UMP invariant test. The groups involved are the group of orthogonal transformations, translation groups of the kind considered in Example 8.5.1, and a group of scale changes. Since each of these satisfies the assumptions of the Hunt-Stein Theorem, and since they leave invariant the problem of maximizing the minimum power over the set of alternatives

\[\sum_{i=1}^{r}\frac{\eta_{i}^{2}}{\sigma^{2}}\geq\psi_{1}^{2}\ \ \ \ \ (\psi_{1}>0), \tag{8.29}\]

it follows that the UMP invariant test of Chapter 7 is also the solution of this maximin problem. It is also seen slightly more generally that the test which is UMP invariant under the same groups for testing

\[\sum_{i=1}^{r}\frac{\eta_{i}^{2}}{\sigma^{2}}\leq\psi_{0}^{2}\]

(Problem 7.4) maximizes the minimum power over the alternatives (8.29) for \(\psi_{0}<\psi_{1}\). \(\blacksquare\)

**Example 8.5.3**: **(Stein)**  Let \(G\) be the group of all nonsingular linear transformations of \(p\)-space. That for \(p>1\) this does not satisfy the conditions of Theorem 8.5.1 is shown by the following problem, which is invariant under \(G\) but for which the UMP invariant test does not maximize the minimum power. Generalizing Example 6.2.1, let \(X=(X_{1},\ldots,X_{p})\), \(Y=(Y_{1},\ldots,Y_{p})\) be independently distributed according to \(p\)-variate normal distributions with zero means and nonsingular covariance matrices \(E(X_{i}X_{j})=\sigma_{ij}\) and \(E(Y_{i}Y_{j})=\Delta\sigma_{ij}\), and let \(H:\Delta\leq\Delta_{0}\) be tested against \(\Delta\geq\Delta_{1}\) (\(\Delta_{0}<\Delta_{1}\)), the \(\sigma_{ij}\) being unknown.

This problem remains invariant if the two vectors are subjected to any common nonsingular transformation, and since with probability 1 this group is transitive over the sample space, the UMP invariant test is trivially \(\varphi(x,\,y)\equiv\alpha\). The maximin power against the alternatives \(\Delta\geq\Delta_{1}\) that can be achieved by invariant tests is therefore \(\alpha\). On the other hand, the test with rejection region \(Y_{1}^{2}/X_{1}^{2}>C\) has a strictly increasing power function \(\beta(\Delta)\), whose minimum over the set of alternatives \(\Delta\geq\Delta_{1}\) is \(\beta(\Delta_{1})>\beta(\Delta_{0})=\alpha\). \(\blacksquare\)

It is a remarkable feature of Theorem 8.5.1 that its assumptions concern only the group \(G\) and not the distributions \(P_{\theta}\).3 When these assumptions hold for a certain \(G\) it follows from (8.23) as in the proof of Lemma 8.4.1 that for any testing problem which remains invariant under \(G\) and possesses a UMP invariant test, this test maximizes the minimum power over any invariant class of alternatives. Suppose conversely that a UMP invariant test under \(G\) has been shown in a particular problem not to maximize the minimum power, as was the case for the group of linear transformations in Example 8.5.3. Then the assumptions of Theorem 8.5.1 cannot be satisfied. However, this does not rule out the possibility that for another problem remaining invariant under \(G\), the UMP invariant test may maximize the minimum power. Whether or not it does is no longer a property of the group alone but will in general depend also on the particular distributions.

Footnote 3: These assumptions are essentially equivalent to the condition that the group \(G\) is _amenable_. Amenability and its relationship to the Hunt–Stein Theorem are discussed by Bondar and Milnes (1982) and (with a different terminology) by Stone and von Randow (1968).

Consider in particular the problem of testing \(H:\xi_{1}=\cdots=\xi_{p}=0\) on the basis of a sample \((X_{\alpha 1},\,\ldots,\,X_{\alpha p})\), \(\alpha=1,\ldots,\,n\), from a \(p\)-variate normal distribution with mean \(E(X_{\alpha i})=\xi_{i}\) and common covariance matrix \((\sigma_{ij})=(a_{ij})^{-1}\). This problem remains invariant under a number of groups, including that of all nonsingular linear transformations of \(p\)-space, and a UMP invariant test exists. An invariant class of alternatives under these groups is

\[\sum\sum\frac{a_{ij}\xi_{i}\xi_{j}}{\sigma^{2}}\geq\psi_{1}^{2}. \tag{8.30}\]

Here, Theorem 8.5.1 is not applicable, and the question of whether the \(T^{2}\)-test of \(H:\psi=0\) maximizes the minimum power over the alternatives

\[\sum\sum a_{ij}\xi_{i}\xi_{j}=\psi_{1}^{2} \tag{8.31}\]

[and hence a fortiori over the alternatives (8.30)] presents formidable difficulties. The minimax property was proved for the case \(p=2\), \(n=3\) by Giri, Kiefer, and Stein (1963), for the case \(p=2\), \(n=4\) by Linnik, Pliss, and Salaevskii (1968), and for \(p=2\) and all \(n\geq 3\) by Salaevskii (1971). The proof is effected by first reducing the problem through invariance under the group \(G_{1}\) of Example 6.6.2, to which Theorem 8.5.1 is applicable, and then applying Theorem 8.1.1 to the reduced problem. It is a consequence of this approach that it also establishes the admissibility of \(T^{2}\) as a test of \(H\) against the alternatives (8.31). In view of the inadmissibility results for point estimation when \(p\geq 3\) (see Lehmann and Casella (1998), Sections 5.4, 5.5, it seems unlikely that \(T^{2}\) is admissible for \(p\geq 3\), and hence that the same method can be used to prove the minimax property in this situation.

The problem becomes much easier when the minimax property is considered against local or distant alternatives rather than against (8.31). Precise definitions and proofs of the fact that \(T^{2}\) possesses these properties for all \(p\) and \(n\) are provided by Giri and Kiefer (1964) and in the references given in Section 7.9.

The theory of this and the preceding section can be extended to confidence sets if the accuracy of a confidence set at level \(1-\alpha\) is assessed by its volume or some other appropriate measure of its size. Suppose that the distribution of \(X\) depends on the parameters \(\theta\) to be estimated and on nuisance parameters \(\vartheta\), and that \(\mu\) is a \(\sigma\)-finite measure over the parameter set \(\omega=\{\theta:(\theta,\vartheta)\in\Omega\}\), with \(\omega\) assumed to be independent of \(\vartheta\). Then the confidence sets \(S(X)\) for \(\theta\) are minimax with respect to \(\mu\) at level \(1-\alpha\) if they minimize

\[\sup E_{\theta,\vartheta}\mu[S(X)]\]

among all confidence sets at the given level.

The problem of minimizing \(E\mu[S(X)]\) is related to that of minimizing the probability of covering false values (the criterion for accuracy used so far) by the relation (Problem 8.39)

\[E_{\theta_{0},\vartheta}\mu[S(X)]=\int_{\theta\neq\theta_{0}}P_{\theta_{0}, \vartheta}[\theta\in S(X)]\,d\mu(\theta), \tag{8.32}\]

which holds provided \(\mu\) assigns measure zero to the set \(\{\theta=\theta_{0}\}\). (For the special case that \(\theta\) is real-valued and \(\mu\) Lebesgue measure, see Problem 5.26.)

Suppose now that the problem of estimating \(\theta\) is invariant under a group \(G\) in the sense of Section 6.11 and that it satisfies the invariance condition

\[\mu[S(gx)]=\mu[S(x)]. \tag{8.33}\]

If uniformly most accurate equivariant confidence sets exist, they minimize (8.32) among all equivariant confidence sets at the given level, and one may hope that under the assumptions of the Hunt-Stein Theorem, they will also be minimax with respect to \(\mu\) among the class of all (not necessarily equivariant) confidence sets at the given level. Such a result does hold and can be used to show for example that the most accurate equivariant confidence sets of Examples 6.11.2 and 6.11.3 minimize their maximum expected Lebesgue measure. A more general class of examples is provided by the confidence intervals derived from the UMP invariant tests of univariate linear hypotheses such as the confidence spheres for \(\theta_{i}=\mu+\alpha_{i}\) or for \(\alpha_{i}\) given in Section 7.4.

Minimax confidence sets \(S(x)\) are not necessarily admissible; that is, there may exist sets \(S^{\prime}(x)\) having the same confidence level but such that

\[E_{\theta,\vartheta}\mu[S^{\prime}(X)]\leq E_{\theta,\vartheta}\mu[S(X)]\quad \quad\text{for all}\quad\theta,\vartheta\]

with strict inequality holding for at least some \((\theta,\vartheta)\).

**Example 8.5.4**: Let \(X_{i}\) (\(i=1,\ldots,s\)) be independently normally distributed with mean \(E(X_{i})=\theta_{i}\) and variance 1, and let \(G\) be the group generated by translations \(X_{i}+c_{i}\) (\(i=1,\ldots,s\)) and orthogonal transformations of \((X_{1},\ldots,X_{s})\). (\(G\) is the Euclidean group of rigid motions in \(s\)-space.) In Example 6.12.2, it was argued that the confidence sets

\[C_{0}=\{(\theta_{1},\ldots,\theta_{s}):\ \sum(\theta_{i}-X_{i})^{2}\leq c\} \tag{8.34}\]

are uniformly most accurate equivariant. The volume \(\mu[S(X)]\) of any confidence set \(S(X)\) remains invariant under the transformations \(g\in G\), and it follows from the results of Problems 8.31 and 8.7 and Examples 8.5.1 and 8.5.2 that the confidence sets (8.34) minimize the maximum expected volume.

However, very surprisingly, they are not admissible unless \(s=1\) or \(2\). In the case \(s\geq 3\), Stein (1962) suggested the region (8.34) can be improved by recentered regions of the form

\[C_{1}=\{(\theta_{1},\ldots,\theta_{s}):\ (\theta_{i}-\hat{b}X_{i})^{2}\leq c \}\, \tag{8.35}\]

where \(\hat{b}=\max(0,1-(s-2)/\sum_{i}X_{i}^{2})\). In fact, Brown (1966) proved that, for \(s\geq 3\),

\[P_{\theta}\{\theta\in C_{1}\}>P_{\theta}\{\theta\in C_{0}\}\]

for all \(\theta\). This result, which will not be proved here, is closely related to the inadmissibility of \(X_{1},\ldots,X_{s}\) as a point estimator of \((\theta_{1},\ldots,\theta_{s})\) for a wide variety of loss functions. The work on point estimation, which is discussed in Lehmann and Casella (1998), Sections 5.4-5.6, for squared error loss, provides easier access to these ideas than the present setting. Further entries into the literature on admissibility are Stein (1981), Hwang and Casella (1982), and Tseng and Brown (1997); additional references are provided in Lehmann and Casella (1998), p.423.

The inadmissibility of the confidence sets (8.34) is particularly surprising in that the associated UMP invariant tests of the hypotheses \(H:\theta_{i}=\theta_{i_{0}}\) (\(i=1,\ldots,s\)) are admissible (Problems 8.29, 8.30). \(\blacksquare\)

### Most Stringent Tests

One of the practical difficulties in the consideration of tests that maximize the minimum power over a class \(\Omega_{K}\) of alternatives is the determination of an appropriate \(\Omega_{K}\). If no information is available on which to base the choice of this set, and if a natural definition is not imposed by invariance arguments, a frequently reasonable definition can be given in terms of the power that can be achieved against the various alternatives. The _envelope power function_\(\beta_{\alpha}^{*}\) was defined in Problem 6.27 by

\[\beta_{\alpha}^{*}(\theta)=\sup\beta_{\varphi}(\theta),\]

where \(\beta_{\varphi}\) denotes the power of a test \(\varphi\) and where the supremum is taken over all level-\(\alpha\) tests of \(H\). Thus \(\beta_{\alpha}^{*}(\theta)\) is the maximum power that can be attained at level \(\alpha\) against the alternative \(\theta\). (That it can be attained follows under mild restrictions from Theorem A.5.1 of the Appendix.) If

\[S_{\Delta}^{*}=\{\theta:\beta_{\alpha}^{*}(\theta)=\Delta\},\]

then of two alternatives \(\theta_{1}\in S_{\Delta_{1}}^{*},\theta_{2}\in S_{\Delta_{2}}^{*},\theta_{1}\) can be considered closer to \(H\), equidistant, or further away than \(\theta_{2}\) as \(\Delta_{1}\) is \(<\), =, or \(>\Delta_{2}\).

The idea of measuring the distance of an alternative from \(H\) in terms of the available information has been encountered before. If, for example, \(X_{1}\),..., \(X_{n}\) is a sample from \(N(\xi,\,\sigma^{2})\), the problem of testing \(H:\xi\leq 0\) was discussed (Section 5.2) both when the alternatives \(\xi\) are measured in absolute units and when they are measured in \(\sigma\)-units. The latter possibility corresponds to the present proposal, since it follows from invariance considerations (Problem 6.27) that \(\beta_{\alpha}^{*}(\xi,\sigma)\) is constant on the lines \(\xi/\sigma=\) constant.

Fixing a value of \(\Delta\) and taking as \(\Omega_{K}\) the class of alternatives \(\theta\) for which \(\beta_{\alpha}^{*}(\theta)\geq\Delta\), one can determine the test that maximizes the minimum power over \(\Omega_{K}\). Another possibility, which eliminates the need of selecting a value of \(\Delta\), is to consider for any test \(\varphi\) the difference \(\beta_{\alpha}^{*}(\theta)-\beta_{\varphi}(\theta)\). This difference measures the amount by which the actual power \(\beta_{\varphi}(\theta)\) falls short of the maximum power attainable. A test that minimizes

\[\sup_{\Omega-\omega}[\beta_{\alpha}^{*}(\theta)-\beta_{\varphi}(\theta)] \tag{8.36}\]

is said to be _most stringent_. Thus a test is most stringent if it minimizes its maximum shortcoming.

Let \(\varphi_{\Delta}\) be a test that maximizes the minimum power over \(S_{\Delta}^{*}\), and hence minimizes the maximum difference between \(\beta_{\alpha}^{*}(\theta)\) and \(\beta_{\varphi}(\theta)\) over \(S_{\Delta}^{*}\). If \(\varphi_{\Delta}\) happens to be independent of \(\Delta\), it is most stringent. This remark makes it possible to apply the results of the preceding sections to the determination of most stringent tests. Suppose that the problem of testing \(H:\theta\in\omega\) against the alternatives \(\theta\in\Omega-\omega\) remains invariant under a group \(G\), that there exists a UMP almost invariant test \(\varphi_{0}\) with respect to \(G\), and that the assumptions of Theorem 8.5.1 hold. Since \(\beta_{\alpha}^{*}(\theta)\) andhence the set \(S_{\Delta}^{*}\) is invariant under \(\bar{G}\) (Problem 6.27), it follows that \(\varphi_{0}\) maximizes the minimum power over \(S_{\Delta}^{*}\) for each \(\Delta\), and \(\varphi_{0}\) is therefore most stringent.

As an example of this method consider the problem of testing \(H:\,p_{1},\,\ldots,\,p_{n}\leq\frac{1}{2}\) against the alternative \(K:\,p_{i}>\frac{1}{2}\) for all \(i\), where \(p_{i}\) is the probability of success in the \(i\)th trial of a sequence of \(n\) independent trials. If \(X_{i}\) is 1 or 0 as the \(i\)th trial is a success or failure, then the problem remains invariant under permutations of the \(X\)'s, and the UMP invariant test rejects (Example 6.3.6) when \(\sum X_{i}>C\). It now follows from the remarks above that this test is also most stringent.

Another illustration is furnished by the general univariate linear hypothesis. Here it follows from the discussion in Example 8.5.2 that the standard test for testing \(H:\,\eta_{1}=\cdots=\eta_{r}=0\) or \(H^{\prime}:\,\sum_{i=1}^{r}\eta_{i}^{2}/\sigma^{2}\leq\psi_{0}^{2}\) is most stringent.

When the invariance approach is not applicable, the explicit determination of most stringent tests typically is difficult. The following is a class of problems for which they are easily obtained by a direct approach. Let the distributions of \(X\) constitute a one-parameter exponential family, the density of which is given by (3.19), and consider the hypothesis \(H:\theta=\theta_{0}\). Then according as \(\theta>\theta_{0}\) or \(\theta<\theta_{0}\), the envelope power \(\beta_{\alpha}^{*}(\theta)\) is the power of the UMP one-sided test for testing \(H\) against \(\theta>\theta_{0}\) or \(\theta<\theta_{0}\). Suppose that there exists a two-sided test \(\varphi_{0}\) given by (4.3), such that

\[\sup_{\theta<\theta_{0}}[\beta_{\alpha}^{*}(\theta)-\beta_{\varphi_{0}}(\theta) ]=\sup_{\theta>\theta_{0}}[\beta_{\alpha}^{*}(\theta)-\beta_{\varphi_{0}}( \theta)], \tag{8.37}\]

and that the supremum is attained on both sides, say at points \(\theta_{1}<\theta_{0}<\theta_{2}\). If \(\beta_{\varphi_{0}}(\theta_{i})=\beta_{i},i=1,2\), an application of the fundamental lemma [Theorem 3.6.1(iii)] to the three points \(\theta_{1}\), \(\theta_{2}\), \(\theta_{0}\) shows that among all tests \(\varphi\) with \(\beta_{\varphi}(\theta_{1})\geq\beta_{1}\) and \(\beta_{\varphi}(\theta_{2})\geq\beta_{2}\), only \(\varphi_{0}\) satisfies \(\beta_{\varphi}(\theta_{0})\leq\alpha\). For any other level-\(\alpha\) test, therefore, either \(\beta_{\varphi}(\theta_{1})<\beta_{1}\) or \(\beta_{\varphi}(\theta_{2})<\beta_{2}\), and it follows that \(\varphi_{0}\) is the unique most stringent test. The existence of a test satisfying (8.37) can be proved by a continuity consideration [with respect to variation of the constants \(C_{i}\) and \(\gamma_{i}\) which define the boundary of the test (4.3)] from the fact that for the UMP one-sided test against the alternatives \(\theta>\theta_{0}\) the right-hand side of (8.37) is zero and the left-hand side positive, while the situation is reversed for the other one-sided test.

### Monotone Tests

In some testing problems, it may be reasonable to restrict attention to tests that are monotone in an appropriate sense. We begin with a motivating example.

**Example 8.7.1**: (**Testing For Superiority**) Suppose \((X_{1},\,\ldots,\,X_{s})^{\top}\) is multivariate normal with unknown mean \(\theta=(\theta_{1},\,\ldots,\theta_{s})^{\top}\) and known covariance matrix \(\Sigma\). Assume \(\Sigma=I_{s}\), the identity. The null hypothesis \(H_{0}:\theta\in\Omega_{0}\) specifies not all \(\theta_{i}>0\) so that

\[\Omega_{0}=\{\theta:\,\theta_{i}\leq 0\ \ \mbox{ for some }i\}\,\]and the alternative specifies all \(\theta_{i}>0\), or \(\Omega_{1}=\Omega_{0}^{c}\). By Problem 4.8, the only unbiased test is \(\phi\equiv\alpha\), and so trivially it is UMPU. However, there are reasonable tests for this problem. The likelihood ratio test rejects for large values of \(T=\min(X_{1},\,\ldots,\,X_{s})\) (Problem 8.41), say when \(T\) exceeds an appropriate threshold \(c\). In order to determine the critical value \(c\) so that the level of the test is controlled, we must ensure that

\[P_{\theta}\{T>c\}\leq\alpha\,\quad\text{for all }\theta\in\Omega_{0}\.\]

As a function of \(\theta\), this rejection probability is increasing in each of the \(\theta_{i}\). Hence, this probability is maximized over \(\theta\in\Omega_{0}\) when one of the \(\theta_{i}\) is zero and the remaining are tending to \(\infty\). By symmetry, let \(\theta_{1}=0\). Then, for \(\theta\in\Omega_{0}\),

\[P_{\theta}\{T>c\}\leq P_{\theta_{1}=0}\{X_{1}>c\}\]

and

\[\sup_{\theta\in\Omega_{0}}P_{\theta}\{T>c\}=P_{\theta_{1}=0}\{X_{1}>c\}\,\]

because for any \(c\) and \(i>1\), \(P_{\theta_{i}}\{X_{i}>c\}\to 0\) as \(\theta_{i}\to\infty\). Hence, the test that rejects when \(T>z_{1-\alpha}\) has size \(\alpha\). Such a test seems intuitively reasonable, for in order to claim that all \(\theta_{i}\) are positive, large values of the smallest \(X_{i}\) support this claim. However, we would like to know if there is a more principled reason to support this test. Note that when all \(\theta_{i}=0\), the rejection probability becomes

\[P_{0}\{\min(X_{1},\,\ldots,\,X_{s})>z_{1-\alpha}\}=\alpha^{s}\,\]

which is \(<\alpha\). Hence, the likelihood ratio test is biased (since the power near the origin will be \(<\alpha\)). Also notice that the likelihood ratio test is monotone in the following sense. If \(x=(x_{1},\,\ldots,\,x_{s})^{\top}\) is in the rejection region and \(x_{i}^{\prime}\geq x_{i}\) for all \(i\), then \(x^{\prime}=(x_{1}^{\prime},\,\ldots,\,x_{s}^{\prime})^{\top}\) is also in the rejection region. Intuitively, if you are willing to reject based on \(X\), you should be willing to reject based on \(X^{\prime}\) if all the components of \(X^{\prime}\) are at least as large as the corresponding components of \(X\). We will return to this example shortly. \(\blacksquare\)

The principle of monotonicity is based on the following idea. Suppose \((X_{1},\,\ldots,\,X_{s})^{\top}\) is distributed according to \(P_{\theta}\), where \(\theta=(\theta_{1},\,\ldots,\,\theta_{s})^{\top}\). Let \(\theta^{\prime}=(\theta_{1}^{\prime},\,\ldots,\,\theta_{s}^{\prime})^{\top}\). If \(\theta\leq\theta^{\prime}\) (meaning \(\theta_{i}\leq\theta_{i}^{\prime}\) for all \(i\)), then we will assume that \(\theta^{\prime}\) typically produces larger values of \(X\) than does \(\theta\). In order to formalize the sense in which the \(P_{\theta}\) are appropriately stochastically increasing, we will generalize the univariate definition in Section 3.4 to \(s\) dimensions.

A set \(\omega\in\,\mathbb{R}^{s}\) is said to be monotone increasing if

\[x=(x_{1},\,\ldots,\,x_{s})^{\top}\in\omega\ \text{ and }x_{i}\leq x_{i}^{ \prime}\text{ for all }i\text{ implies }x^{\prime}\in\omega. \tag{8.38}\]

(Similarly, \(\omega\) is monotone decreasing if \(-\omega=\{x:-x\in\omega\}\) is monotone increasing.) A nonrandomized test is called monotone increasing, or just monotone, if its rejection region is monotone increasing in the sense (8.38). In this section, we will restrict attention to nonrandomized tests. (More generally, one may call a test \(\phi=\phi(X_{1},\ldots,X_{n})\) monotone if it is nondecreasing in each of its arguments.) The restriction to monotone rejection regions may be appropriate when the distributions \(P_{\theta}\) are stochastically increasing; that is, \(\theta_{i}\leq\theta_{i}^{\prime}\) for all \(i\) implies

\[\int_{\omega}d\,P_{\theta}\leq\int_{\omega}d\,P_{\theta^{\prime}} \tag{8.39}\]

for every monotone increasing set \(\omega\).

We now consider sufficient conditions to verify (8.39). Let \(\theta\leq\theta^{\prime}\) (componentwise) and \(X\sim P_{\theta}\). Suppose we can find a function \(h\) such that \(X^{\prime}=h(X)\), with \(X\leq X^{\prime}\) and \(X^{\prime}\sim P_{\theta^{\prime}}\). Another possibility is to assume that, for some random vector \(Z\), we may write \(X=f(Z)\) and \(X^{\prime}=f^{\prime}(Z)\), where \(f\) and \(f^{\prime}\) are such that \(X\leq X^{\prime}\), \(X\sim P_{\theta}\), and \(X^{\prime}\sim P_{\theta^{\prime}}\). Then (8.39) holds in either case because

\[\int_{\omega}d\,P_{\theta}=P\{X\in\omega\}\leq P\{X^{\prime}\in\omega\}=\int_{ \omega}dP_{\theta^{\prime}}\.\]

For example, in a general multivariate location model, \(P_{\theta}\) is the distribution of \(Z+\theta\) where \(Z\) is a random vector with any fixed known distribution. If we take \(f(Z)=Z+\theta\) and \(f^{\prime}(Z)=Z+\theta^{\prime}\), then the conditions are satisfied. Thus, Condition (8.39) holds in Example 8.7.1.

Assume, as in Example 8.7.1, that the alternative parameter space \(\Omega_{1}\) is monotone increasing. Then, the case for the restriction to monotone tests is particularly compelling when, as in the above sufficient conditions for (8.39), we can write \(X=f(Z)\), \(X^{\prime}=f^{\prime}(Z)\) and \(X\leq X^{\prime}\), where \(X\sim P_{\theta}\) and \(X^{\prime}\sim P_{\theta^{\prime}}\) with \(\theta\leq\theta^{\prime}\). Consider the following (unrealistic) situation. Michael is testing \(H_{0}\) on the basis of \(X=f(Z)\) and Azeem is testing \(H_{0}\) on the basis of \(X^{\prime}=f^{\prime}(Z)\) (for the same \(Z\), so that \(X\leq X^{\prime}\)). Suppose Michael rejects \(H_{0}\) and claims \(\theta\in\Omega_{1}\). Michael would also conclude \(\theta^{\prime}\in\Omega_{1}\) for any \(\theta^{\prime}\geq\theta\), and so Azeem should reject \(H_{0}\) as well. Thus, if Michael and Azeem are using the same test \(\phi\), then when Michael rejects \(H_{0}\) on the basis of \(X\), so must Azeem on the basis of \(X^{\prime}\). Since \(X\leq X^{\prime}\), the test must be monotone. Finally, even if Michael's and Azeem's testing problems are not coupled, Michael should still want to apply the same test \(\phi\) when faced with the problem of testing \(H_{0}\) based on \(X\).

The restriction to monotone tests sometimes allows one to find an optimal procedure, as we now show.

**Example 8.7.2**: **(Continuation of Example 8.7.1)** We will show that, among monotone level \(\alpha\) tests, the likelihood ratio test is UMP. Recall that the rejection region is the set

\[E=\{(x_{1},\ldots,x_{s})^{\top}:\ \min(x_{1},\ldots,x_{s})\geq z_{1-\alpha}\}\.\]

Suppose \(E^{\prime}\) is any other monotone increasing rejection region and that \(E^{\prime}\) includes some point \(y\) that is not in \(E\). Therefore, \(y_{i}<z_{1-\alpha}\) for some \(i\). Without loss of generality, assume \(y_{1}<z_{1-\alpha}\). Then, since \(E^{\prime}\) is monotone, \(y\in E^{\prime}\) implies the set \(R_{y}\subset E^{\prime}\), where

\[R_{y}=\{(x_{1},\ldots,x_{s})^{\top}:\ x_{i}\geq y_{i}\ \ \ \mbox{for all }i\}\.\]

Then, the size of the test with rejection region \(E^{\prime}\) is at least

\[\sup_{\theta\in\Omega_{0}}P_{\theta}\{R_{y}\}\geq\sup_{\theta_{1}=0,\theta_{i} \rightarrow\infty}P_{\theta}\{X_{i}\geq y_{i},\ i=1,\ldots,s\}\]

\[=P_{0}\{X_{1}\geq y_{1}\}>P_{0}\{X_{1}>z_{1-\alpha}\}=\alpha\.\]

Hence, such a test is not level \(\alpha\). Therefore, any other level \(\alpha\) test with rejection region \(E^{\prime}\) must satisfy \(E^{\prime}\subseteq E\). If \(E^{\prime}\) is a strict subset of \(E\) with the set difference \(E\setminus E^{\prime}\) having positive Lebesgue measure, then the power against any alternative of the test with rejection region \(E\) is strictly bigger than that of \(E^{\prime}\), as claimed.

Despite the likelihood ratio test being UMP among monotone tests, it is not \(\alpha\)-admissible. In order to construct a level \(\alpha\) test with better power, one can enlarge the rejection region \(E\) of the likelihood ratio test in such a way so that the new rejection region is still level \(\alpha\). Since the rejection probability of the likelihood ratio test is \(\alpha^{s}\) at the origin, one can add a region \(F\) to \(E\) where \(F\) is some region in the quadrant where all components are negative. For example, one may consider

\[F=\{x:\ \max_{i}x_{i}\leq d\}\]

for some \(d<0\). If \(d\) is chosen far enough away from \(0\) so that

\[P_{0}\{F\}<\alpha-\alpha^{s}\,\]

then the test that has rejection region \(E\cup F\) has probability of a Type 1 error under \(\theta=0\) equal to

\[P_{0}\{E\cup F\}=P_{0}\{E\}+P_{0}\{F\}=\alpha^{s}+P_{0}\{F\}<\alpha\.\]

One just needs to make sure that the level is controlled for all \(\theta\in\Omega_{0}\) (Problem 8.43). Such a test is clearly nonsensical because \(X\in F\) means that all components of \(X\) are negative, which is not evidence against the null hypothesis. While the test with rejection region \(E\cup F\) increases power, it does so at the expense of increasing Type 1 error, i.e., the rejection probability under \(\theta\in\Omega_{0}\). Although the likelihood ratio test is not \(\alpha\)-admissible, it is in fact \(d\)-admissible; see Cohen et al. (1983) and Nomakuchi and Sakata (1987). Perlman and Wu (1999) advocate for the use of the likelihood ratio test and question the utility of the notion of \(\alpha\)-admissibility.

Monotonicity does not always reduce the problem sufficiently far so that a UMP monotone test exists. However, as the next example shows, we can obtain the maximin monotone level \(\alpha\) test, i.e., the test that maximizes minimum power among monotone level \(\alpha\) tests.

**Example 8.7.3** (**Moment Inequalities**): Assume \(X=(X_{1},\ldots,X_{s})^{\top}\) is multivariate normal with unknown mean \(\theta=(\theta_{1},\ldots,\theta_{s})^{\top}\) and known invertible covariance matrix \(\Sigma\). The problem now is to test the null hypothesis \(H_{0}:\theta\in\Omega_{0}\), where

\[\Omega_{0}=\{\theta:\ \theta_{i}\leq 0\ \ \mbox{for all}\ i=1,\ldots s\}\.\]

Note that \(\Omega_{1}=\Omega_{0}^{c}\) is a monotone increasing set. As in Example 8.7.1, the UMPU level \(\alpha\) test is the trivial test \(\phi\equiv\alpha\), but monotonicity considerations apply.4

Footnote 4: When the null hypothesis parameter space is described as a number of inequalities about means being satisfied, the problem is known in econometrics as testing moment inequalities; for a review, see Canay and Shaikh (2017).

First, let's consider the likelihood ratio test. It rejects for large values of \(T\) given by

\[T=\inf_{\theta\in\Omega_{0}}(X-\theta)^{\top}\Sigma^{-1}(X-\theta). \tag{8.40}\]

One can check that \(T\) is monotone increasing in each of its arguments, and therefore the test that rejects when \(T>c\) is monotone. For this problem, any monotone test has its largest probability of rejection under \(\theta\in\Omega_{0}\) occur at \(\theta=0\). Therefore, the constant \(c=c_{1-\alpha}\) should be determined so that

\[P_{0}\{T\geq c_{1-\alpha}\}=\alpha\.\]

In the special case that \(\Sigma\) is the identity matrix, the test statistic \(T\) reduces to \(T=\sum_{i}\mbox{max}^{2}(X_{i},0)\).

In order to determine the maximin monotone test, let us assume that \(\Sigma\) has diagonal elements equal to one (or simply divide each \(X_{i}\) by its standard deviation). Consider the region in the alternative parameter space \(\omega(\epsilon)\) defined, for some fixed \(\epsilon>0\), by

\[\omega(\epsilon)=\{\theta:\ \theta_{i}\geq\epsilon\ \ \mbox{ for some}\ i\}\.\]

The goal is to maximize

\[\inf_{\theta\in\omega(\epsilon)}P_{\theta}\{\mbox{reject}\ H_{0}\}\]

among level \(\alpha\) monotone tests. Letting \(M=\mbox{max}(X_{1},\ldots,X_{s})\), consider the test that rejects \(H_{0}\) if \(M>d\) for some constant \(d\). Such a test is clearly monotone and \(d=d_{1-\alpha}\) can be determined so that

\[P_{0}\{M\geq d_{1-\alpha}\}=\alpha\.\]Of course, if \(\Sigma\) is the identity matrix, then

\[d_{1-\alpha}=\Phi^{-1}\left[(1-\alpha)^{1/s}\right]. \tag{8.41}\]

We now argue that the test that rejects when \(M\geq d_{1-\alpha}\) is the maximin monotone test. The worst-case power of this test over the region \(\omega(\epsilon)\) is

\[\inf_{\theta\in\omega(\epsilon)}P_{\theta}\{M\geq d_{1-\alpha}\}\,\]

which occurs when some \(\theta_{i}=\epsilon\) and the remaining \(\theta_{j}\) satisfy \(\theta_{j}\to-\infty\). In such a case, \(M=X_{i}\) with probability tending to one, and the worse case power becomes

\[P_{\theta_{i}=\epsilon}\{X_{i}\geq d_{1-\alpha}\}=1-\Phi(d_{1-\alpha}-\epsilon). \tag{8.42}\]

Assume \(\phi\) is another monotone level \(\alpha\) test with acceptance region \(A\) and rejection region \(R=A^{c}\). Suppose \(\phi\) includes some point \(x\in A\) that falls in the interior of the rejection region of the test based on \(M\), so that \(x_{i}>d_{1-\alpha}\) for some \(i\). Without loss of generality, assume \(x_{1}>d_{1-\alpha}\). Let

\[A_{x}=\{y:y_{i}\leq x_{i}\ \ \ \mbox{for all}\ i\}\ \.\]

Then, \(A_{x}\) is a monotone decreasing set and \(x\in A\) implies \(A_{x}\subseteq A\). Therefore, \(P_{\theta}\{A\}\geq P_{\theta}\{A_{x}\}\) and so

\[\sup_{\theta\in\omega(\epsilon)}P_{\theta}\{A\}\geq\sup_{\theta\in\omega( \epsilon)}P_{\theta}\{X_{1}\leq x_{1},\ldots,X_{s}\leq x_{s}\}\.\]

But as \(\theta_{j}\to-\infty\), the event \(\{X_{j}\leq x_{j}\}\) has probability tending to one. So,

\[\sup_{\theta\in\omega(\epsilon)}P_{\theta}\{A\}\geq\lim_{\theta_{1}=\epsilon, \theta_{j}\to-\infty,j>1}P_{\theta}\{X_{1}\leq x_{1},\ldots,X_{s}\leq x_{s}\}=\]

\[P_{\theta_{1}=\epsilon}\{X_{1}\leq x_{1}\}=\Phi(x_{1}-\epsilon)\.\]

Therefore,

\[\inf_{\theta\in\omega(\epsilon)}P_{\theta}(R)\leq 1-\Phi(x_{1}-\epsilon)<1- \Phi(d_{1-\alpha}-\epsilon)\,\]

since \(x_{1}>d_{1-\alpha}\). Therefore, the worst-case power of \(\phi\) is worse than that of the claimed optimal test, by (8.42). The assumption that \(x_{1}>d_{1-\alpha}\) cannot hold in order for \(\phi\) to be optimal. Hence, the rejection region \(R\) of \(\phi\) must be contained in the rejection region \(\{M\geq d_{1-\alpha}\}\). But then it cannot have better power than the test based on \(M\). Hence, the test that rejects for large \(M\) is maximin among monotone tests. Note that, by taking \(\epsilon=0\) in the argument, the test is also the monotone test with smallest worst-case bias; that is,\[\alpha - \inf_{\theta\in\Omega_{0}^{c}}P_{\theta}\{\text{reject H}_{0}\}\]

is minimized among monotone tests.

In this example, the case for tests that are not monotone is motivated by the following observations. First, the critical value of the test based on \(M\) increases with the dimension \(s\); see (8.41) in the case where \(\Sigma\) is the identity. Therefore, one strategy is to reduce the dimension so that a reduced critical value makes it easier to reject \(H_{0}\). For example, if one observes a large negative \(X_{j}\), one might feel confident that \(\theta_{j}\leq 0\) and remove such an index \(j\) from consideration. That is, for some threshold \(t<0\), let \(J=\{j:X_{j}>t\}\), and only test \(\theta_{j}\) for \(j\in J\). For example, one might use the statistic \(\max_{j\in J}X_{j}\) with critical value determined when all corresponding \(\theta_{j}=0\). However, it is not clear that such a procedure controls the size of the test. Such procedures are known in the econometrics literature as moment selection procedures. Note that such a procedure is not monotone. As such, the probability of a Type 1 error as a function of \(\theta\in\Omega\) need not be maximized at the origin, and one must take into account the selection step. Moment selection methods that provide error control are provided in Andrews and Barwick (2012) and Romano et al. (2014), who also provide references to this growing literature. \(\blacksquare\)

Unfortunately, the test that is maximin among monotone tests need not be maximin without the monotonicity restriction. Such is the case in Example 8.7.3, but we next show it in another example.

**Example 8.7.4**: (**Cauchy Location Model**) Let \(X\) be an observation from the Cauchy location model with center \(\theta\); \(X\) has density \(f(x-\theta)\), where

\[f(x)=\frac{1}{\pi(1+x^{2})}\enspace.\]

Consider the problem of testing \(H_{0}:\theta=0\) against \(\theta>0\). By Problem 3.33, no UMP test exists. The only monotone tests are those with rejection regions \(\{X\geq c\}\) (or \(\{X>c\}\)) for some \(c\). Hence, if \(c=c_{1-\alpha}\) is the \(1-\alpha\) quantile of the Cauchy distributed centered at \(0\), then the test that rejects if \(X\geq c_{1-\alpha}\) is trivially UMP among monotone tests.

Next, consider the problem of finding a maximin test over the region \(\theta\geq a\) for some fixed \(a>0\), without the restriction to monotone tests. We now argue that the UMP monotone test is generally not maximin, depending on the value of \(a\). First, note that the power of the UMP monotone test over \(\theta\geq a\) attains its minimum at \(\theta=a\). Also, for testing \(\theta=0\) against \(\theta=a\), the likelihood ratio is

\[\frac{f(X-a)}{f(X)}=\frac{1+X^{2}}{1+(X-a)^{2}}\enspace, \tag{8.43}\]

which is \(\geq 1\) if and only if \(X\geq a/2\). Therefore, by the Neyman-Pearson Lemma, the test that rejects if \(X\geq a/2\) is most powerful at level \(P_{0}\{X\geq a/2\}\). If \(a/2=c_{1-\alpha}\)then this MP level \(\alpha\) test is in fact the UMP monotone level \(\alpha\) test. It follows easily that, in this case, the test that rejects if \(X\geq a/2\) is maximin among all level \(\alpha\) tests. Indeed, if it were not, there would be another test whose power at \(\theta=a\) would be less than that of the UMP monotone test, a contradiction since the UMP monotone test maximizes power at \(a\).

On the other hand, consider the case where \(a/2<c_{1-\alpha}\). Then, the UMP monotone test is no longer maximin. To appreciate why, again consider the likelihood ratio for testing \(\theta=0\) against \(\theta=a\). The likelihood ratio at \(a/2\) is 1, it exceeds 1 for \(X>a/2\) and it tends to 1 as \(X\to\infty\). The likelihood ratio (8.43) is plotted in Figure 8.1 as a function of \(X\). Since \(a/2<c_{1-\alpha}\), we may therefore modify the rejection region \(\{X\geq c_{1-\alpha}\}\) by removing a small interval of very large \(X\) values and including values of \(X\) near \(c_{1-\alpha}\). The new rejection region now includes an interval \([c_{1-\alpha}-\epsilon,\,c_{1-\alpha})\) for some small enough \(\epsilon\). The result is an increase in power at \(\theta=a\), and it can be done in such a way that the worst-case power is not decreased (Problem 8.48).

By a similar argument, it follows that, again in the case \(a/2<c_{1-\alpha}\), the rejection region of the maximin test cannot include any semi-infinite interval (\(b,\,\infty\)). On the other hand, the rejection region of the maximin test cannot be a bounded set, because the probability of any bounded set tends to 0 as \(\theta\to\infty\). It follows that the rejection region of the maximin test is unbounded but does not include a semi-infinite interval. That is, to the right of any point \(b\), both the rejection region and the acceptance region contains subsets with positive measure. A conjecture is that it is an infinite sequence of intervals, though it is clearly not monotone.

Finally, in the case \(a/2>c_{1-\alpha}\), similar arguments show that the maximin monotone level \(\alpha\) test is not maximin among all level \(\alpha\) tests. \(\blacksquare\)

Figure 8.1: Cauchy likelihood ratio

### Problems

#### Section 8.1

**Problem 8.1**: _Existence of maximin tests.5 Let \((\mathcal{X},\mathcal{A})\) be a Euclidean sample space, and let the distributions \(P_{\theta},\theta\in\Omega\), be dominated by a \(\sigma\)-finite measure over \((\mathcal{X},\mathcal{A})\). For any mutually exclusive subsets \(\Omega_{H}\), \(\Omega_{K}\) of \(\Omega\) there exists a level-\(\alpha\) test maximizing (8.2). [Let \(\beta=\sup[\inf_{\Omega_{k}}E_{\theta}\varphi(X)]\), where the supremum is taken over all level-\(\alpha\) tests of \(H:\theta\in\Omega_{H}\). Let \(\varphi_{n}\) be a sequence of level-\(\alpha\) tests such that \(\inf_{\Omega_{K}}E_{\theta}\varphi_{n}(X)\) tends to \(\beta\). If \(\varphi_{n_{i}}\) is a subsequence and \(\varphi\) a test (guaranteed by Theorem A.5.1 of the Appendix) such that \(E_{\theta}\varphi_{n_{i}}(X)\) tends to \(E_{\theta}\varphi(X)\) for all \(\theta\in\Omega\), then \(\varphi\) is a level-\(\alpha\) test and \(\inf_{\Omega_{k}}E_{\theta}\varphi(X)=\beta\).]_

Footnote 5: The existence of maximin tests is established in considerable generality in Cvitanic and Karatzas (2001).

**Problem 8.2**: In Example 8.1.1, explain why the maximin test is not UMPU for the alternatives considered.

**Problem 8.3**: In Example 8.1.3, complete the argument using Corollary 8.1.1 to find the maximin test without assuming you already know the UMPI test. What if the alternative specifies \(\sum_{i=1}^{n}\xi_{i}^{2}\geq\delta^{2}\)?

**Problem 8.4**: _Locally most powerful tests. 6 Let \(d\) be a measure of the distance of an alternative \(\theta\) from a given hypothesis \(H\). A level-\(\alpha\) test \(\varphi_{0}\) is said to be locally most powerful (LMP) if, given any other level-\(\alpha\) test \(\varphi\), there exists \(\Delta\) such that_

Footnote 6: Locally optimal tests for multiparameter hypotheses are given in Gupta and Vermeire (1986).

\[\beta_{\varphi_{0}}(\theta)\geq\beta_{\varphi}(\theta)\ \ \ \mbox{for all}\ \theta\ \mbox{with}\ 0<d(\theta)<\Delta\,. \tag{8.44}\]

_Suppose that \(\theta\) is real-valued and that the power function of every test is continuously differentiable at \(\theta_{0}\)._

1. _If there exists a unique level-_\(\alpha\) _test_ \(\varphi_{0}\) _of_ \(H:\theta=\theta_{0}\)_, maximizing_ \(\beta_{\varphi}^{\prime}(\theta_{0})\)_, then_ \(\varphi_{0}\) _is the unique LMP level-_\(\alpha\) _test of_ \(H\) _against_ \(\theta>\theta_{0}\) _for_ \(d(\theta)=\theta-\theta_{0}\)_._
2. _To see that (i) is not correct without the uniqueness assumption, let_ \(X\) _take on the values 0 and 1 with probabilities_ \(P_{\theta}(0)=\frac{1}{2}-\theta^{3}\)_,_ \(P_{\theta}(1)=\frac{1}{2}+\theta^{3}\)_,_ \(-\frac{1}{2}<\theta^{3}<\frac{1}{2}\)_, and consider testing_ \(H:\theta=0\) _against_ \(K:\theta>0\)_. Then every test_ \(\varphi\) _of size_ \(\alpha\) _maximizes_ \(\beta_{\varphi}^{\prime}(0)\)_, but not every such test is LMP._ _[_Kallenberg et al._ _(_1984_)_.]_
3. _The following_7 _is another counterexample to (i) without uniqueness, in which in fact no LMP test exists. Let_ \(X\) _take on the values 0, 1, 2 with probabilities_\[P_{\theta}(x) = \alpha + \epsilon\left[\theta + \theta^{2}\sin\left(\frac{x}{\theta}\right)\right]\quad\quad\mbox{for}\quad x = 1,2,\] \[P_{\theta}(0) = 1 - p_{\theta}(1) - p_{\theta}(2),\] where \(-1\leq\theta\leq 1\) and \(\epsilon\) is a sufficiently small number. Then a test \(\varphi\) at level \(\alpha\) maximizes \(\beta^{\prime}(0)\) provided \[\varphi(1) + \varphi(2) = 1\,\] but no LMP test exists.
* A unique LMP test maximizes the minimum power locally provided its power function is bounded away from \(\alpha\) for every set of alternatives which is bounded away from \(H\).
* Let \(X_{1},\ldots,X_{n}\) be a sample from a Cauchy distribution with unknown location parameter \(\theta\), so that the joint density of the \(X\)'s is \(\pi^{-n}\prod_{i=1}^{n}[1+(x_{i}-\theta)^{2}]^{-1}\). The LMP test for testing \(\theta=0\) against \(\theta>0\) at level \(\alpha<\frac{1}{2}\) is not unbiased and hence does not maximize the minimum power locally. [(iii): The unique most powerful test against \(\theta\) is \[\left\{\begin{array}{ll}\varphi(1)\\ \varphi(2)\end{array}\right.=1\quad\mbox{if}\,\sin\left(\frac{1}{\theta}\right) \geqq\sin\left(\frac{2}{\theta}\right),\] and each of these inequalities holds at values of \(\theta\) arbitrarily close to \(0\). (v): There exists \(M\) so large that any point with \(x_{i}\geq M\) for all \(i=1,\ldots,n\) lies in the acceptance region of the LMP test. Hence the power of the test tends to zero as \(\theta\) tends to infinity.]

**Problem 8.5**: Under the setting of Problem 3.35, determine the locally most powerful test.

**Problem 8.6**: A level-\(\alpha\) test \(\varphi_{0}\) is locally unbiased (loc. unb.) if there exists \(\Delta_{0}>0\) such that \(\beta_{\varphi_{0}}(\theta)\geq\alpha\) for all \(\theta\) with \(0<d(\theta)<\Delta_{0}\); it is LMP loc. unb. if it is loc. unb. and if, given any other loc. unb. level-\(\alpha\) test \(\varphi\), there exists \(\Delta\) such that (8.44) holds. Suppose that \(\theta\) is real-valued and that \(d(\theta)=|\theta-\theta_{0}|\), and that the power function of every test is twice continuously differentiable at \(\theta=\theta_{0}\).

* If there exists a unique test \(\varphi_{0}\) of \(H:\theta=\theta_{0}\) against \(K:\theta\neq\theta_{0}\) which among all loc. unb. tests maximizes \(\beta^{\prime\prime}(\theta_{0})\), then \(\varphi_{0}\) is the unique LMP loc. unb. level-\(\alpha\) test of \(H\) against \(K\).
* The test of part (i) maximizes the minimum power locally provided its power function is bounded away from \(\alpha\) for every set of alternatives that is bounded away from \(H\).

[(ii): A necessary condition for a test to be locally minimax is that it is loc. unb.]

**Problem 8.7**: _Locally uniformly most powerful tests_. If the sample space is finite and independent of \(\theta\), the test \(\varphi_{0}\) of Problem 8.4(i) is not only LMP but also locally 

[MISSING_PAGE_FAIL:409]

among all similar (and hence all locally unbiased) tests where \(S=\{(\eta_{1},\ldots,\eta_{r}):\sum_{i=1}^{r}\eta_{i}^{2}=\rho^{2}\sigma^{2}\}\). Letting \(\rho\) tend to zero and utilizing the conditions

\[\beta_{\varphi}^{i}(\vartheta)=0,\ \ \ \ \ \int_{S}\eta_{i}\eta_{j}\,dA=0\ \ \ {\rm for}\,i\neq j,\ \ \ \ \ \int_{S}\eta_{i}^{2}\,dA=k(\rho\sigma),\]

one finds that \(\varphi_{0}\) maximizes \(\sum_{i=1}^{r}\beta_{\varphi}^{ii}(\eta,\sigma^{2})\) among all locally unbiased tests. Since for any positive definite matrix, \(|(\beta_{\varphi}^{ij})|\leq\prod\beta_{\varphi}^{ii}\), it follows that for any locally strictly unbiased test \(\varphi\),

\[|(\beta_{\varphi}^{ij})|\leq\prod\beta_{\varphi}^{ii}\leq\left[\frac{\Sigma \beta_{\varphi}^{ii}}{r}\right]^{r}\leq\left[\frac{\Sigma\beta_{\varphi_{0}}^{ ii}}{r}\right]^{r}=[\beta_{\varphi_{0}}^{11}]^{r}=|(\beta_{\varphi_{0}}^{ij})|.]\]

**Problem 8.10**: Let \(Z_{1},\ldots,Z_{n}\) be identically independently distributed according to a continuous distribution \(D\), of which it is assumed only that it is symmetric about some (unknown) point. For testing the hypothesis \(H:D(0)=\frac{1}{2}\), the sign test maximizes the minimum power against the alternatives \(K:D(0)\leq q(q<\frac{1}{2})\). [A pair of least favorable distributions assign probability 1, respectively, to the distributions \(F\in H\), \(G\in K\) with densities

\[f(x)=\frac{1-2q}{2(1-q)}\left(\frac{q}{1-q}\right)^{|x|},\ \ \ \ \ g(x)=(1-2q)\left(\frac{q}{1-q}\right)^{|x|}\]

where for all \(x\) (positive, negative, or zero) [\(x\)] denotes the largest integer \(\leq x\).]

**Problem 8.11**: Let \(f_{\theta}(x)=\theta g(x)+(1-\theta)h(x)\) with \(0\leq\theta\leq 1\). Then \(f_{\theta}(x)\) satisfies the assumptions of Lemma 8.2.1 provided \(g(x)/h(x)\) is a nondecreasing function of \(x\).

**Problem 8.12**: Let \(x=(x_{1},\ldots,x_{n})\), and let \(g_{\theta}(x,\xi)\) be a family of probability densities depending on \(\theta=(\theta_{1},\ldots,\theta_{r})\) and the real parameter \(\xi\), and jointly measurable in \(x\) and \(\xi\). For each \(\theta\), let \(h_{\theta}(\xi)\) be a probability density with respect to a \(\sigma\)-finite measure \(\nu\) such that \(p_{\theta}(x)=\int g_{\theta}(x,\xi)h_{\theta}(\xi)\,d\nu(\xi)\) exists. We shall say that a function \(f\) of two arguments \(u=(u_{1},\ldots,u_{r})\), \(v=(v_{1},\ldots,v_{s})\) is nondecreasing in \((u,\,v)\) if \(f(u^{\prime},\,v)/f(u,\,v)\leq f(u^{\prime},\,v^{\prime})/f(u,\,v^{\prime})\) for all \((u,\,v)\) satisfying \(u_{i}\leq u^{\prime}_{i}\), \(v_{j}\leq v^{\prime}_{j}\) (\(i=1,\ldots,r\); \(j=1,\ldots,s\)). Then \(p_{\theta}(x)\) is nondecreasing in \((x,\theta)\) provided the product \(g_{\theta}(x,\xi)h_{\theta}(\xi)\) is (a) nondecreasing in \((x,\theta)\) for each fixed \(\xi\); (b) nondecreasing in \((\theta,\,\xi)\) for each fixed \(x\); (c) nondecreasing in \((x,\xi)\) for each fixed \(\theta\).

[Interpreting \(g_{\theta}(x,\xi)\) as the conditional density of \(x\) given \(\xi\), and \(h_{\theta}(\xi)\) as the a priori density of \(\xi\), let \(\rho(\xi)\) denote the a posteriori density of \(\xi\) given \(x\), and let \(\rho^{\prime}(\xi)\) be defined analogously with \(\theta^{\prime}\) in place of \(\theta\). That \(p_{\theta}(x)\) is nondecreasing in its two arguments is equivalent to \[\int\frac{g_{\theta}(x^{\prime},\xi)}{g_{\theta}(x,\xi)}\rho(\xi)\,d\upsilon(\xi) \leq\int\frac{g_{\theta^{\prime}}(x^{\prime},\xi)}{g_{\theta^{\prime}}(x,\xi)} \rho^{\prime}(\xi)\,d\upsilon(\xi).\]

By (a) it is enough to prove that

\[D=\int\frac{g_{\theta}(x^{\prime},\xi)}{g_{\theta}(x,\xi)}[\rho^{\prime}(\xi)- \rho(\xi)]\,d\upsilon(\xi)\geq 0.\]

Let \(S_{-}=\{\xi:\rho^{\prime}(\xi)/\rho(\xi)<1\}\) and \(S_{+}=\{\xi:\rho(\xi)/\rho(\xi)\geq 1\}\). By (b) the set \(S_{-}\) lies entirely to the left of \(S_{+}\). It follows from (c) that there exists \(a\leq b\) such that

\[D=a\int_{S_{-}}[\rho^{\prime}(\xi)-\rho(\xi)]\,d\upsilon(\xi)+b\int_{S_{+}}[ \rho^{\prime}(\xi)-\rho(\xi)]\,d\upsilon(\xi),\]

and hence that \(D=(b-a)\int_{S_{+}}[\rho^{\prime}(\xi)-\rho(\xi)]\,d\upsilon(\xi)\geq 0.]

**Problem 8.13**:
* Let \(X\) have binomial distribution \(b(p,n)\), and consider testing \(H:p=p_{0}\) at level \(\alpha\) against the alternatives \(\Omega_{K}:p/q\leq\frac{1}{2}p_{0}/q_{0}\) or \(\geq 2\,p_{0}/q_{0}\). For \(\alpha=.05\) determine the smallest sample size for which there exists a test with power \(\geq.8\) against \(\Omega_{K}\) if \(p_{0}=.1,.2,.3,.4,.5\).
* Let \(X_{1}\),..., \(X_{n}\) be independently distributed as \(N(\xi,\sigma^{2})\). For testing \(\sigma=1\) at level \(\alpha=.05\), determine the smallest sample size for which there exists a test with power \(\geq.9\) against the alternatives \(\sigma^{2}\leq\frac{1}{2}\) and \(\sigma^{2}\geq 2\). [See Problem 4.5.]

**Problem 8.14**: _Double-exponential distribution._ Let \(X_{1}\),..., \(X_{n}\) be a sample from the double-exponential distribution with density \(\frac{1}{2}e^{-|x-\theta|}\). The LMP test for testing \(\theta\leq 0\) against \(\theta>0\) is the sign test, provided the level is of the form

\[\alpha=\frac{1}{2^{n}}\sum_{k=0}^{m}{n\choose k},\]

so that the level-\(\alpha\) sign test is nonrandomized.

[Let \(R_{k}\) (\(k=0,\ldots,n\)) be the subset of the sample space in which \(k\) of the \(X\)'s are positive and \(n-k\) are negative. Let \(0\leq k<l<n\), and let \(S_{k}\), \(S_{l}\) be subsets of \(R_{k}\), \(R_{l}\) such that \(P_{0}(S_{k})=P_{0}(S_{l})\neq 0\). Then it follows from a consideration of \(P_{\theta}(S_{k})\) and \(P_{0}(S_{l})\) for small \(\theta\) that there exists \(\Delta\) such that \(P_{\theta}(S_{k})<P_{\theta}(S_{l})\) for \(0<\theta<\Delta\). Suppose now that the rejection region of a nonrandomized test of \(\theta=0\) against \(\theta>0\) does not consist of the upper tail of a sign test. Then it can be converted into a sign test of the same size by a finite number of steps, each of which consists in replacing an \(S_{k}\) by an \(S_{l}\) with \(k<l\), and each of which therefore increases the power for \(\theta\) sufficiently small.]

### _Section 8.3_

**Problem 8.15**: If (8.13) holds, show that \(q_{1}\) defined by (8.11) belongs to \({\cal P}_{1}\).

**Problem 8.16**: Show that there exists a unique constant \(b\) for which \(q_{0}\) defined by (8.11) is a probability density with respect to \(\mu\), that the resulting \(q_{0}\) belongs to \({\cal P}_{0}\), and that \(b\to\infty\) as \(\epsilon_{0}\to 0\).

**Problem 8.17**: Prove the formula (8.15).

**Problem 8.18**: Show that if \({\cal P}_{0}\neq{\cal P}_{1}\) and \(\epsilon_{0}\), \(\epsilon_{1}\) are sufficiently small, then \(Q_{0}\neq Q_{1}\).

**Problem 8.19**: Evaluate the test (8.21) explicitly for the case that \(P_{i}\) is the normal distribution with mean \(\xi_{i}\) and known variance \(\sigma^{2}\), and when \(\epsilon_{0}=\epsilon_{1}\).

**Problem 8.20**: Determine whether (8.21) remains the maximin test if in the model (8.20) \(G_{i}\) is replaced by \(G_{ij}\).

**Problem 8.21**: Write out a formal proof of the maximin property outlined in the last paragraph of Section 8.3.

### _Section 8.4_

**Problem 8.22**: Let \(X_{1}\),..., \(X_{n}\) be independent and normally distributed with means \(E(X_{i})=\mu_{i}\) and variance 1. The test of \(H:\mu_{1}=\cdots=\mu_{n}=0\) that maximizes the minimum power over \(\omega^{\prime}:\sum\mu_{i}\geq d\) rejects when \(\sum X_{i}\geq C\).

[If the least favorable distribution assigns probability 1 to a single point, invariance under permutations suggests that this point will be \(\mu_{1}=\cdots=\mu_{n}=d/n\)].

**Problem 8.239**: (i) In the preceding problem determine the maximin test if \(\omega^{\prime}\) is replaced by \(\sum a_{i}\mu_{i}\geq d\), where the \(a\)'s are given positive constants.

(ii) Solve part (i) with \(Var(X_{i})=1\) replaced by \(Var(X_{i})=\sigma_{i}^{2}\) (known).

[(i): Determine the point \((\mu_{1}^{*},\ldots,\mu_{n}^{*})\) in \(\omega^{\prime}\) for which the MP test of \(H\) against \(K:(\mu_{1}^{*},\ldots,\mu_{n}^{*})\) has the smallest power, and show that the MP test of \(H\) against \(K\) is a maximin solution.]

**Problem 8.24**: Let \(X_{1}\),..., \(X_{n}\) be independent normal variables with variance 1 and means \(\xi_{1}\),..., \(\xi_{n}\), and consider the problem of testing \(H:\xi_{1}=\cdots=\xi_{n}=0\) against the alternatives \(K=\{K_{1},\ldots,K_{n}\}\), where \(K_{i}:\xi_{j}=0\) for \(j\neq i\), \(\xi_{i}=\xi\) (known and positive). Show that the problem remains invariant under permutation of the \(X\)'s and that there exists a UMP invariant test \(\phi_{0}\) which rejects when \(\sum e^{\xi X_{i}}>C\), by the following two methods.

(i) The order statistics \(X_{(1)}<\cdots<X_{(n)}\) constitute a maximal invariant.

2. Let \(f_{0}\) and \(f_{i}\) denote the densities under \(H\) and \(K_{i}\) respectively. Then the level-\(\alpha\) test \(\phi_{0}\) of \(H\) versus \(K^{\prime}:f=(1/n)\sum f_{i}\) is UMP invariant for testing \(H\) versus \(K\).

[(ii): If \(\phi_{0}\) is not UMP invariant for \(H\) versus \(K\), there exists an invariant test \(\phi_{1}\) whose (constant) power against \(K\) exceeds that of \(\phi_{0}\). Then \(\phi_{1}\) is also more powerful against \(K^{\prime}\).]

**Problem 8.25**: The UMP invariant test \(\phi_{0}\) of Problem 8.24

1. maximizes the minimum power over \(K\);
2. is admissible.
3. For testing the hypothesis \(H\) of Problem 8.24 against the alternatives \(K^{\prime}=\{K_{1},\ldots,K_{n},\,K^{\prime}_{1},\ldots,\,K^{\prime}_{n}\}\), where under \(K^{\prime}_{i}:\xi_{j}=0\) for all \(j\neq i\), \(\xi_{i}=-\xi\), determine the UMP test under a suitable group \(G^{\prime}\), and show that it is both maximin and invariant.

[ii): Suppose \(\phi^{\prime}\) is uniformly at least as powerful as \(\phi_{0}\), and more powerful for at least one \(K_{i}\), and let

\[\phi^{*}(x_{1},\ldots,x_{n})=\frac{\sum\phi^{\prime}(x_{i_{1}},\ldots,x_{i_{n} })}{n!},\]

where the summation extends over all permutations. Then \(\phi^{*}\) is invariant, and its power is independent of \(i\) and exceeds that of \(\phi_{0}\).]

**Problem 8.26**: Suppose Problems 8.24-8.25 are modified so that the one nonzero mean may \(\xi\) or \(-\xi\). How do the results change?

**Problem 8.27**: Suppose \(X_{1},\ldots,X_{n}\) are independent normal variables with \(X_{i}\sim N(\xi_{i},\,1)\). The null hypothesis specifies all \(\xi_{i}=0\). Fix an integer \(k\geq 1\). Suppose \(\omega^{\prime}\) specifies that at least \(k\) of the \(X_{i}\) have mean at least \(\xi\), where \(\xi\) is known and positive. Determine a maximin test as explicitly as possible.

**Problem 8.28**: For testing \(H:f_{0}\) against \(K:\{f_{1},\ldots,f_{s}\}\), suppose there exists a finite group \(G=\{g_{1},\ldots,g_{N}\}\) which leaves \(H\) and \(K\) invariant and which is transitive in the sense that given \(f_{j}\), \(f_{j^{\prime}}(1\leq j,\,j^{\prime})\) there exists \(g\in G\) such that \(\bar{g}\,f_{j}=f_{j^{\prime}}\). In generalization of Problems 8.24, 8.25, determine a UMP invariant test, and show that it is both maximin against \(K\) and admissible.

**Problem 8.29**: To generalize the results of the preceding problem to the testing of \(H:f\) versus \(K:\{f_{\theta},\,\theta\in\omega\}\), assume:

1. There exists a group \(G\) that leaves \(H\) and \(K\) invariant.
2. \(\tilde{G}\) is transitive over \(\omega\).
3. There exists a probability distribution \(Q\) over \(G\) which is right invariant in the sense of Section 8.4.

Determine a UMP invariant test, and show that it is both maximin against \(K\) and admissible.

**Problem 8.30**: Let \(X_{1}\),..., \(X_{n}\) be independent normal with means \(\theta_{1}\),..., \(\theta_{n}\) and variance 1.

(i) Apply the results of the preceding problem to the testing of \(H:\theta_{1}=\cdots=\theta_{n}=0\) against \(K:\sum\theta_{i}^{2}=r^{2}\), for any fixed \(r>0\). (ii) Show that the results of (i) remain valid if \(H\) and \(K\) are replaced by \(H^{\prime}:\sum\theta_{i}^{2}\leq r_{0}^{2}\), \(K^{\prime}:\sum\theta_{i}^{2}\geq r_{1}^{2}\) (\(r_{0}<r_{1}\)).

**Problem 8.31**: Suppose in Problem 8.30(i) the variance \(\sigma^{2}\) is unknown and that the data consist of \(X_{1}\),..., \(X_{n}\) together with an independent random variable \(S^{2}\) for which \(S^{2}/\sigma^{2}\) has a \(\chi^{2}\)-distribution. If \(K\) is replaced by \(\sum\theta_{i}^{2}/\sigma^{2}=r^{2}\), then

(i) the confidence sets \(\sum(\theta_{i}-X_{i})^{2}/S^{2}\leq C\) are uniformly most accurate equivariant under the group generated by the \(n\)-dimensional generalization of the group \(G_{0}\) of Example 6.11.2, and the scale changes \(X_{i}^{\prime}=cX_{i}\), \(S^{\prime 2}=c^{2}S^{2}\). (ii) The confidence sets of (i) are minimax with respect to the measure \(\mu\) given by

\[\mu[C(X,S^{2})]=\frac{1}{\sigma^{2}}[\mbox{ volume of }C(X,S^{2})].\]

[Use polar coordinates with \(|\theta^{2}|=\sum\theta_{i}^{2}\).]

**Section 8.5**: Let \(X=(X_{1}\),..., \(X_{p})\) and \(Y=(Y_{1}\),..., \(Y_{p})\) be independently distributed according to \(p\)-variate normal distributions with zero means and covariance matrices \(E(X_{i}X_{j})=\sigma_{ij}\) and \(E(Y_{i}Y_{j})=\Delta\sigma_{ij}\). (i) The problem of testing \(H:\Delta\leq\Delta_{0}\) remains invariant under the group \(G\) of transformations \(X^{*}=XA,Y^{*}=YA\), where \(A=(a_{ij})\) is any nonsingular \(p\times p\) matrix with \(a_{ij}=0\) for \(i>j\), and there exists a UMP invariant test under \(G\) with rejection region \(Y_{1}^{2}/X_{1}^{2}>C\). (ii) The test with rejection region \(Y_{1}^{2}/X_{1}^{2}>C\) maximizes the minimum power for testing \(\Delta\leq\Delta_{0}\) against \(\Delta\geq\Delta_{1}\) (\(\Delta_{0}<\Delta_{1}\)). [(ii): That the Hunt-Stein Theorem is applicable to \(G\) can be proved in steps by considering the group \(G_{q}\) of transformations \(X_{q}^{\prime}=\alpha_{1}X_{1}+\cdots+\alpha_{q}X_{q}\), \(X_{i}^{\prime}=X_{i}\) for \(i=1\),..., \(q-1\), \(q+1\),..., \(p\), successively for \(q=1\),..., \(p-1\). Here \(\alpha_{q}\neq 0\), since the matrix \(A\) is nonsingular if and only if \(a_{ii}\neq 0\) for all \(i\). The group product (\(\gamma_{1}\),..., \(\gamma_{q}\)) of two such transformations (\(\alpha_{1}\),..., \(\alpha_{q}\)) and (\(\beta_{1}\),..., \(\beta_{q}\)) is given by \(\gamma_{1}=\alpha_{q}+\beta_{1}\), \(\gamma_{2}=a_{2}\beta_{q}+\beta_{2}\),..., \(\gamma_{q-1}=\alpha_{q-1}\beta_{q}+\beta_{q-1}\), \(\gamma_{q}=\alpha_{q}\), \(\beta_{q}\), which shows \(G_{q}\) to be isomorphic to a group of scale changes (multiplication of all components by \(\beta_{q}\)) and translations [addition of (\(\beta_{1}\),..., \(\beta_{q-1}\), 0)].

The result now follows from the Hunt-Stein Theorem and Example 8.5.1, since the assumptions of the Hunt-Stein Theorem, except for the easily verifiable measurability conditions, concern only the abstract structure (\(G\), \(\mathcal{B}\)), and not the specific realization of the elements of \(G\) as transformations of some space.]

**Problem 8.33**: Suppose that the problem of testing \(\theta\in\Omega_{H}\) against \(\theta\in\Omega_{K}\) remains invariant under \(G\), that there exists a UMP almost invariant test \(\varphi_{0}\) with respect to \(G\), and that the assumptions of Theorem 8.5.1 hold. Then \(\varphi_{0}\) maximizes \(\inf_{\Omega_{K}}[w(\theta)E_{\theta}\varphi(X)+u(\theta)]\) for any weight functions \(w(\theta)\geq 0\), \(u(\theta)\) that are invariant under \(\tilde{G}\).

**Problem 8.34**: Suppose \(X\) has the multivariate normal distribution in \(\mathbf{R}^{k}\) with unknown mean vector \(h\) and known positive definite covariance matrix \(C^{-1}\). Consider testing \(h=0\) versus \(|C^{1/2}h|\geq b\) for some \(b>0\), where \(|\cdot|\) denotes the Euclidean norm.

(i) Show the test that rejects when \(|C^{1/2}X|^{2}>c_{k,\,1-\alpha}\) is maximin, where \(c_{k,\,1-\alpha}\) denotes the \(1-\alpha\) quantile of the Chi-squared distribution with \(k\) degrees of freedom.

(ii) Show that the maximin power of the above test is given \(P\{\chi_{k}^{2}(b^{2})>c_{k,\,1-\alpha}\}\), where \(\chi_{k}^{2}(b^{2})\) denotes a random variable that has the noncentral Chi-squared distribution with \(k\) degrees of freedom and noncentrality parameter \(b^{2}\).

**Problem 8.35**: Suppose \(X_{1}\),..., \(X_{k}\) are independent, with \(X_{i}\sim N(\theta_{i}\), \(1)\). Consider testing the null hypothesis \(\theta_{1}=\cdots=\theta_{k}=0\) against \(\max|\theta_{i}|\geq\delta\), for some \(\delta>0\). Find a maximin level \(\alpha\) test as explicitly as possible. Compare this test with the maximin test if the alternative parameter space were \(\sum_{i}\theta_{i}^{2}\geq\delta^{2}\). Argue they are quite similar for small \(\delta\). Specifically, consider the power of each test against (\(\delta\), \(0\),..., \(0\)) and show that it is equal to \(\alpha+C_{\alpha}\delta^{2}+o(\delta^{2})\) as \(\delta\to 0\), and the constant \(C_{\alpha}\) is the same for both tests.

### _Section 8.6_

**Problem 8.36**: _Existence of most stringent tests_. Under the assumptions of Problem 8.1 there exists a most stringent test for testing \(\theta\in\Omega_{H}\) against \(\theta\in\Omega-\Omega_{H}\).

**Problem 8.37**: Let \(\{\Omega_{\Delta}\}\) be a class of mutually exclusive sets of alternatives such that the envelope power function is constant over each \(\Omega_{\Delta}\) and that \(\cup\Omega_{\Delta}=\Omega-\Omega_{H}\), and let \(\varphi_{\Delta}\) maximize the minimum power over \(\Omega_{\Delta}\). If \(\varphi_{\Delta}=\varphi\) is independent of \(\Delta\), then \(\varphi\) is most stringent for testing \(\theta\in\Omega_{H}\).

**Problem 8.38**: Let \((Z_{1},\,\ldots,\,Z_{N})=(X_{1},\,\ldots,\,X_{m},\,Y_{1},\,\ldots,\,Y_{n})\) be distributed according to the joint density (5.55), and consider the problem of testing \(H:\eta=\xi\) against the alternatives that the \(X\)'s and \(Y\)'s are independently normally distributed with common variance \(\sigma^{2}\) and means \(\eta\neq\xi\). Then the permutation test with rejection region \(|\tilde{Y}-\tilde{X}|>C[T(Z)]\), the two-sided version of the test (5.54), is most stringent.

[Apply Problem 8.37 with each of the sets \(\Omega_{\Delta}\) consisting of two points (\(\xi_{1}\), \(\eta_{1}\), \(\sigma\)), (\(\xi_{2}\), \(\eta_{2}\), \(\sigma\)) such that

\[\xi_{1} = \zeta\,-\,\frac{n}{m+n}\delta\mbox{,}\qquad\eta_{1}=\zeta\,+\,\frac {m}{m+n}\delta\mbox{;}\] \[\xi_{2} = \zeta\,+\,\frac{n}{m+n}\delta\mbox{,}\qquad\eta_{2}=\zeta\,-\, \frac{m}{m+n}\delta\]

for some \(\zeta\) and \(\delta\).]

**Problem 8.39**: Show that the UMP invariant test of Problem 8.24 is most stringent.

**Problem 8.40**: Show that a region \(\omega\) is monotone increasing if and only if its complement is monotone decreasing. In the plane, how would you characterize the class of all monotone increasing regions?

**Problem 8.41**: In Example 8.7.1, determine the likelihood ratio test for general \(\Sigma\), and show that it reduces to the test that rejects for large values of \(\min(X_{1}\),..., \(X_{s})\) when \(\Sigma\) is the identity matrix. How do you calculate the critical value for general \(\Sigma\)? Is the resulting test monotone?

**Problem 8.42**: Suppose \(X=(X_{1}\),..., \(X_{s})^{\top}\sim P_{\theta}\), where the \(P_{\theta}\) form a multivariate location model. So \(P_{\theta}\) is the distribution of \(Z+\theta\), where \(Z\) has a fixed (known) distribution in \({\rm I\kern-1.8ptR}\). For testing superiority as in Example 8.7.1, determine the UMP monotone level \(\alpha\) test as explicitly as possible.

**Problem 8.43**: In Example 8.7.1, show how one may add a region \(F\) to the rejection region \(E\) of the likelihood ratio test and still maintain the size of the test.

**Problem 8.44**: Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\) with both parameters unknown. Show that, for testing \(\mu\leq 0\) against \(\mu>0\), the one-sided \(t\)-test is _not_ monotone increasing. Does the assumption (8.39) hold for the parametrization (\(\theta_{1}\), \(\theta_{2}\)) = (\(\mu\), \(\sigma\)) or perhaps (\(\mu\), \(1/\sigma\))? [Consider the monotone sets \(\{X_{1}>c\}\) when \(c\) is both positive and negative.]

**Problem 8.45**: In Example 8.7.3, find the most powerful test for testing \(\theta\in\Omega_{0}\) against a fixed alternative \(\theta=a\) and compute the power of this test. [The least favorable distribution puts mass one at the point \(\theta_{a}\), where \(\theta_{a}\) minimizes \((\theta\,-\,a)^{\top}\Sigma^{-1}(\theta\,-\,a)\) over \(\theta\).]

**Problem 8.46**: In Example 8.7.3, assume \(\Sigma\) is the identity matrix. Calculate the minimum power of the likelihood ratio test over the region \(\omega(\epsilon)\) and compare it to the maximin monotone test.

**Problem 8.47**: Find the maximin monotone level \(\alpha\) test in Example 8.7.3 for general \(\Sigma\). Also allow the region \(\omega(\epsilon)\) to be generalized and have the form \(\{\theta:\theta_{i}\geq\epsilon_{i}\ \ \text{for some}\ i\}\), where the \(\epsilon_{i}\) may vary with \(i\).

**Problem 8.48**: Provide the missing details in Example 8.7.4. What happens in the case \(a>2c_{1-\alpha}\)?

### Notes

The concepts and results of Section 8.1 are essentially contained in the minimax theory developed by Wald for general decision problems. An exposition of this theory and some of its applications is given in Wald's book (1950). For more recent assessments of the important role of the minimax approach, see Brown (1994, 2000). The ideas of Section 8.3, and in particular Theorem 8.3.1, are due to Huber (1965) and form the core of his theory of robust tests [Huber (1981, Chapter 10)]. The material of Sections 8.4 and 8.5, including Lemma 8.4.1, Theorem 8.5.1, and Example 8.5.2, constitutes the main part of an unpublished paper of Hunt and Stein (1946).

Section 8.7 was inspired by Lehmann (1952a). Problem 8.45 is taken from Romano, Shaikh, and Wolf (2014). The \(\alpha\)-inadmissibility of the likelihood ratio test in Example 8.7.1 has been studied in Berger (1989), Liu and Berger (1995) and McDermott and Wang (2002). Perlman and Wu (1999) advocate the use of likehood ratio tests and reject the utility of \(\alpha\)-admissibility; they provide numerous examples and references.

## Chapter 9 Multiple Testing and Simultaneous Inference

### 9.1 Introduction and the FWER

#### Basic Framework

Virtually any scientific experiment sets out to answer questions about the process under investigation, which often can be formally translated into a set of hypotheses. It is the exception that a single hypothesis is considered. In fact, the analysis of data or "data snooping" or "data mining" often leads to further questions as well. The statistician is then faced with the challenge of accounting for all possible errors resulting from a complex data analysis, so that any resulting inferences or interesting conclusions can reliably be viewed as real structure rather than artifacts of random data.

For example, when testing more than one parameter, say

\[H\colon\theta_{1}=\cdots=\theta_{s}=0 \tag{9.1}\]

against the alternatives that one or more of the \(\theta\)'s are positive, it is typically not enough simply to accept or reject \(H\). If \(H\) is not rejected, nothing more is required: the finding is that none of the parameter values are significant. However, when \(H\) is rejected, one will in most cases want to know just which of the parameters \(\theta\) are significant. And when \(H\) is tested against the two-sided alternatives that one or more of the \(\theta\)'s are different from 0, one would in case of rejection usually want to know the signs of the significant \(\theta\)'s.

**Example 9.1.1** (Normal one-sample problem): Suppose that \(X_{1}\),..., \(X_{n}\) is a sample from \(N(\xi,\sigma^{2})\) and consider the hypothesis \(H\): \(\xi\leq\xi_{0}\), \(\sigma\leq\sigma_{0}\). In case of rejection one would want to know whether it is the mean or the variance that is rejected, or perhaps both.

Example 9.1.2 (Comparing several treatments with a control)When testing several treatments against a control, the overall null hypothesis states that none of the treatments is an improvement over, or differs from, the control. In case of rejection one will wish to know just which of the treatments show a significant improvement, or difference.

Example 9.1.3 (Testing equality of several treatments)Instead of comparing several treatments with a control, one may wish to compare a number of possible alternative situations with each other. If the quality of the \(i\)th of \(s\) alternatives is measured by a parameter \(\theta_{i}\), the overall null hypothesis may be expressed as

\[H\text{: }\theta_{1}=\cdots=\theta_{s}. \tag{9.2}\]

Rather than considering \(H\), one may instead consider the family of \(\binom{s}{2}\) null hypotheses \(H_{i,j}\) specified by

\[H_{i,j}\text{ : }\theta_{i}=\theta_{j}\ \ \ i\neq j\.\]

Since many multiple testing problems, like those in Examples 9.1.2 and 9.1.3, are concerned with _multiple comparisons_, the whole subject of multiple testing has frequently, though somewhat inaccurately, been called multiple comparisons.

When comparing several medical, agricultural, or industrial treatments, the number of treatments may be fairly small, say, in the single digits. Even so, treatments may be administered at different dosages, with further comparisons made among various subgroups of the populations. Larger numbers occur in some educational studies, where for example it may be desired to compare performance in the 50 of the U.S. states, of which there are 1225 pairwise comparisons. And with over 3100 counties in the U.S., there are over 4.8 million pairwise comparisons.

More modern applications of multiple comparison methodology occur in microarrays where thousands or even tens of thousands of genes are tested simultaneously. Each microarray corresponds to one unit (plant, animal or person) and in these experiments the sample size (the number of such units) is typically of a much smaller order of magnitude (in the tens) than the number of comparisons being tested. In genome-wide association studies, differences between those with and without a particular disease are compared across their genomes by identifying genetic variations. Such studies require scanning millions of single-nucleotide polymorphisms (SNPs) in order to determine subsets of them that are associated with the disease.

Let us now consider the general problem of simultaneously testing a finite numbers of hypotheses \(H_{i}\) (\(i=1,\ldots,s\)). We shall assume that tests for the individual hypotheses are available and the problem is how to combine them into a simultaneous test procedure.

The easiest approach is to disregard the multiplicity and simply test each hypothesis at level \(\alpha\). However, with such a procedure the probability of one or more false rejections rapidly increases with \(s\). When the number of true hypotheses is large, we shall be nearly certain to reject some of them. To get a numerical idea of this phenomenon, the following Table shows (to 2 decimals) the probability of one or more false rejections when all of the hypotheses \(H_{1},\ldots,\)\(H_{s}\) are true, when the test statistics used for testing \(H_{1},\ldots,\)\(H_{s}\) are independent, and when the level at which each of the \(s\) hypotheses is tested is \(\alpha=0.05\).

\begin{tabular}{l c c c c c} \(s\) & 1 & 2 & 5 & 10 & 50 \\ \hline \(P\)(at least one false rejection) & 0.05 & 0.10 & 0.23 & 0.40 & 0.92 \\ \end{tabular} In this sense the claim that the procedure controls the probability of false rejections at level 0.05 is clearly very misleading.

We shall therefore begin by replacing the usual condition for testing a single hypothesis, that the probability of a false rejection not exceed \(\alpha\), by the requirement, when testing several hypotheses, that the probability of one or more false rejections not exceed a given level. This probability is called the familywise error rate (FWER). Here the term "family" refers to the collection of hypotheses \(H_{1},\ldots,H_{s}\) that is being considered for joint testing. In a laboratory testing blood samples, this might be all the tests performed in a day, or those performed in a day by a given tester. Alternatively, the tests given in the morning and afternoon might be considered as separate families, and so on. Which tests are to be treated jointly as a family depends on the situation.

We now more formally consider the general problem of simultaneously testing null hypotheses \(H_{i}\) (\(i=1,\ldots,s\)). Suppose data \(X\) is available from some model \(P\in\Omega\). A general hypothesis \(H\) can be viewed as a subset \(\omega\) of \(\Omega\). The problem is to simultaneously test the null hypotheses

\[H_{i}:\ P\in\omega_{i}\quad i=1,\ldots s\.\]

The number \(s\) of null hypotheses is finite for now, but may be infinite in general. Let \(I=I(P)\) denote the set of true null hypotheses when \(P\) is the true probability distribution; that is, \(i\in I(P)\) if and only if \(P\in\omega_{i}\). We shall assume that tests for the individual hypotheses are available and the problem is how to combine them so that decisions can be made about each of the individual hypotheses.

Once the family has been defined, control of the FWER (at joint level \(\alpha\)) requires that

\[\mbox{FWER}\leq\alpha\, \tag{9.3}\]

where

\[\mbox{FWER}=\mbox{FWER}_{\mbox{\scriptsize P}}=P\{\mbox{reject any }H_{i}\mbox{ with }i\in I(P)\}. \tag{9.4}\]

Note that we will require \(\mbox{FWER}_{P}\leq\alpha\) for all \(P\in\Omega\). Control of the familywise error rate in this sense is called _strong_ control of the FWER to distinguish it from _weak_ control, where \(\mbox{FWER}_{P}\leq\alpha\) is only required to hold when all null hypotheses are true, i.e., when \(P\in\bigcap_{i}\omega_{i}\). Weak control is typically not very meaningful, and so when we refer to control of the FWER, we mean strong control.

Safeguards against false rejections are of course not the only concern of multiple testing procedures. Corresponding to the power of a single test one must also consider the ability of a procedure to detect departures from the null hypotheses when they do occur. When the number of tests is in the tens or hundreds of thousands, control of the FWER at conventional levels becomes so stringent that individual departures from the hypothesis have little chance of being detected. For this reason, we shall later also consider alternatives to the FWER that control false rejections less severely and consequently provide better "power". For multiple testing, several notions of power can be considered and will be discussed later. For now, better power just roughly means an increased ability to detect false null hypotheses.

Methods that control the FWER are often described by the \(p\)-values of the individual tests, which were introduced in Section 3.2. (Alternatively, some methods may begin with suitable test statistics \(T_{i}\) for testing \(H_{i}\).) We now present some simple methods that control the FWER which can be stated easily in terms of \(p\)-values. Assume that \(\hat{p}_{i}\) is a \(p\)-value for testing \(H_{i}\); specifically, we assume

\[P\{\hat{p}_{i}\leq u\}\leq u \tag{9.5}\]

for any \(u\in(0,\,1)\) and any \(P\in\omega_{i}\). Note that it is not required that the distribution of \(\hat{p}_{i}\) be uniform on \((0,\,1)\) whenever \(H_{i}\) is true. (For example, if \(H_{i}\) corresponds to testing \(\theta_{i}\leq 0\) but the true \(\theta_{i}\) is \(<0\), exact uniformity is too strong. Also, even if the null hypothesis is simple, the \(p\)-value may have a discrete distribution.) The requirement (9.5) means a true null \(p\)-value has a distribution that is stochastically larger than that of a \(U(0,\,1)\) variable.

#### Single-Step Methods

In single-step methods, individual \(p\)-values (or test statistics) are compared to their critical values simultaneously, upon which decisions about rejections of any of the \(H_{i}\) are made, and the multiple testing procedure stops. This is only a rough description, but the following methods convey the idea.

Bonferroni Method

**Theorem 9.1.1**: _(Bonferroni Method) Assume each of the \(p\)-values \(\hat{p}_{i},\ i=1,\,\ldots,\,s\), satisfies (9.5). (i) If, for \(i=1,\,\ldots,\,s\), hypothesis \(H_{i}\) is rejected when \(\hat{p}_{i}\leq\alpha/s\), then the FWER for the simultaneous testing of \(H_{1},\,\ldots,\,H_{s}\) satisfies (9.3). (ii) Moreover, there exists a joint distribution for \((\hat{p}_{1},\,\ldots,\,\hat{p}_{s})\) for which each \(\hat{p}_{i}\) is distributed as \(U(0,\,1)\) and the familywise error rate is exactly \(\alpha\). Therefore, increasing the critical value from \(\alpha/s\) to any bigger constant will violate control of the FWER for some distribution of \(p\)-values._

Proof. To prove (i), suppose hypotheses \(H_{i}\) with \(i\in I=I(P)\) are true and the remainder false, with \(|I|\) denoting the cardinality of \(I\). From the Bonferroni inequality it follows that \[\text{FWER} = P\{\text{reject any }H_{i}\text{ with }i\in I\}\leq\sum_{i\in I}P\{\text{reject }H_{i}\}\] \[= \sum_{i\in I}P\left\{\hat{p}_{i}\leq\frac{\alpha}{s}\right\}\leq \sum_{i\in I}\frac{\alpha}{s}\leq\frac{|I|\alpha}{s}\leq\alpha\;.\]

The proof of (ii) is left as Problem 9.1.

Because of the multiplicity of tests, rejection of \(H_{i}\) will ordinarily entail its \(p\)-value \(\hat{p}_{i}\) to be much smaller than the usual significance level \(\alpha\). An instance of this is the Bonferroni method. As \(p\)-values for individual tests avoid the arbitrariness in choice of significance level, it is useful to define _adjusted_\(p\)-values.

**Definition 9.1.1**: For a given multiple testing method (or really a family of multiple testing methods indexed by \(\alpha\)), the FWER-adjusted \(p\)-value corresponding to \(H_{i}\) is given by

\[\tilde{p}_{i}=\inf\{\alpha:\ H_{i}\text{ is rejected when FWER is controlled at level }\alpha\}\;.\]

The Bonferroni adjusted \(p\)-value for \(H_{i}\) is given by

\[\tilde{p}_{i}=\min(s\hat{p}_{i},1)\;.\]

#### Weighted Bonferroni Method

Let \(w_{i}\geq 0\) satisfy \(\sum_{i=1}^{s}w_{i}=1\). Then, the weighted Bonferroni method rejects \(H_{i}\) if \(\hat{p}_{i}\leq w_{i}\alpha\). Properties of the weighted Bonferroni method are given in Problem 9.2.

#### Sidak Method

The Bonferroni procedure can be improved if one can incorporate knowledge of the joint distribution of \(p\)-values.

A simple improvement can be obtained if it is known that the \(p\)-values are mutually independent. The Sidak procedure rejects any \(H_{i}\) whose corresponding \(p\)-value satisfies

\[\hat{p}_{i}\leq 1-(1-\alpha)^{1/s}\;.\]

Note that the cutoff now is bigger than the Bonferroni cutoff of \(\alpha/s\). The corresponding adjusted \(p\)-values are given by (Problem 9.3)

\[\tilde{p}_{i}=1-(1-\hat{p}_{i})^{s}\;.\]

#### Tukey's Classical Method for Pairwise Comparisons

In multiple testing problems, _p_-values or test statistics are typically not independent. However, it is still possible to exploit their dependence structure in order to construct a procedure that controls the FWER. An illustration of how to account for this dependence is Tukey's method in an ANOVA setup, as described below. (Later, we will see that, even if the joint distribution of _p_-values is unknown, we can use resampling methods that implicitly estimate the dependence structure.)

##### (One-way Normal Balanced ANOVA)

Assume

\[X_{i,j}=\theta_{i}+\epsilon_{i,j}\,\]

where the \(\epsilon_{i,j}\) are i.i.d. \(N(0,\sigma^{2})\), \(i=1,\ldots,k\), \(j=1,\ldots,n\). Balanced here refers to equal sample sizes of \(n\) in each of the \(k\) groups. The underlying \(P\) can by indexed as \(P_{\theta,\sigma}\), where \(\theta=(\theta_{1},\ldots,\theta_{k})\). The usual estimator for \(\theta_{i}\) is

\[\bar{X}_{i}=\frac{1}{n}\sum_{j=1}^{n}X_{i,j}\]

and the estimator for \(\sigma^{2}\) is

\[\hat{\sigma}^{2}=\frac{\sum_{i=1}^{k}\sum_{j=1}^{n}(X_{i,j}-\bar{X}_{i})^{2}}{ k(n-1)}. \tag{9.6}\]

We are interested in the family of all pairwise comparisons: for any \(i\) and \(j\), consider testing

\[H_{i,j}:\ \theta_{i}=\theta_{j}\ \ \text{versus}\ \ H_{i,j}^{\prime}:\ \theta_{i}\neq\theta_{j}\.\]

Note that it is now natural to index the family of hypotheses by \((i,j)\). The number of tests is \(s=\binom{k}{2}\).

The usual test of \(H_{i,j}\) (ignoring the multiplicity) is based on the statistic

\[T_{i,j}=(n/2)^{1/2}(\bar{X}_{i}-\bar{X}_{j})/\hat{\sigma}\, \tag{9.7}\]

which under the null hypothesis \(H_{i,j}\) has the \(t\)-distribution with \(k(n-1)\) degrees of freedom. Indeed the numerator in (9.7) has the \(N(0,\sigma^{2})\) distribution and the denominator is the square root of a \(\chi^{2}\) variable with \(k(n-1)\) degrees of freedom divided by \(k(n-1)\), and the numerator and denominator are independent.

If we ignore the multiplicity of tests, the usual test rejects \(H_{i,j}\) if

\[|T_{i,j}|>t_{k(n-1)}(1-\frac{\alpha}{2})\,\]

where \(t_{d}(\alpha)\) is the \(\alpha\)-quantile of the \(t\)-distribution with \(d\) degrees of freedom. To control the FWER, the Bonferroni method rejects \(H_{i,j}\) if\[|T_{i,j}|>t_{k(n-1)}(1-\frac{\alpha^{\prime}}{2})\,\]

where \(\alpha^{\prime}=\alpha/\binom{k}{2}\).

To improve upon Bonferroni, we can determine a critical value \(c_{1-\alpha}\) so that

\[P_{\mu=0,\sigma}\{\max_{i,j}|T_{i,j}|>c_{1-\alpha}\}=\alpha. \tag{9.8}\]

To determine \(c_{1-\alpha}\), note that when all null hypotheses are true, i.e., \(\theta_{1}=\cdots=\theta_{k}\), the distribution of \(\max|T_{i,j}|\) does not depend on any unknown parameters. At this point, one could simulate its distribution to high accuracy. In fact, it has the studentized range distribution.

If \(Z_{1},\ldots,Z_{k}\) are i.i.d. \(N(0,1)\) and, independently, \(V\sim\chi_{d}^{2}\), then the distribution of

\[\max_{i,j}|Z_{i}-Z_{j}|/\sqrt{V/d}\]

has the studentized range distribution with parameters \(k\) and \(d\). We will denote its \(1-\alpha\) quantile by \(q_{k,d}(1-\alpha)\).

Critical values for this distribution have been tabled. The importance of this distribution is largely diminished by simulation and the use of resampling methods to be considered later; the latter will apply under greatly weakened conditions without having to invoke strong parametric assumptions. The point of the example is largely instructional and historical.

In order to satisfy (9.8), set

\[c_{1-\alpha}=\frac{q_{k,k(n-1)}(1-\alpha)}{\sqrt{2}}\.\]

This leads to Tukey's method.

**Tukey's method**: Reject any \(H_{i,j}\) for which \(|T_{i,j}|>q_{k,k(n-1)}(1-\alpha)/\sqrt{2}\).

Tukey's method clearly provides weak control of the FWER, as the critical value was determined when all null hypotheses are true. It seems reasonable that control of the FWER is most demanding when all null hypotheses are true, and hence strong control should hold as well. In order to demonstrate strong control, let

\[I=I(P_{\theta,\sigma})=\{(i,j):\ \theta_{i}=\theta_{j}\}\.\]

Then,

\[\max_{(i,j)\in I}|T_{i,j}|\leq\max_{i\neq j}\sqrt{\frac{n}{2}}|(\tilde{X}_{i}- \theta_{i})-(\tilde{X}_{j}-\theta_{j})|/\hat{\sigma}\, \tag{9.9}\]

and the distribution under \(P_{\mu,\sigma}\) of the right side of (9.9) is the studentized range distribution with parameters \(k\) and \(k(n-1)\). It follows that \[\text{FWER}_{P}=P\{\text{reject }H_{i,j}\text{ for any }(i,j)\in I\}\]

\[=P\{\max_{(i,j)\in I}|T_{i,j}|>q_{k,k(n-1)}(1-\alpha)/\sqrt{2}\}\]

\[\leq P\{\max_{i\neq j}\sqrt{\frac{n}{2}}|(\tilde{X}_{i}-\theta_{i})-(\tilde{X} _{j}-\theta_{j})|\widehat{\sigma}>q_{k,k(n-1)}(1-\alpha)\}=\alpha\.\]

Some points are worth noting.
* _Relationship to methods based on \(p\)-values._ Based on the individual tests, the (marginal) \(p\)-value for testing \(H_{i,j}\) is given by \[\hat{p}_{i,j}=2\bar{F}_{k(n-1)}(|T_{i,j}|)\,\] where \(F_{d}\) is the c.d.f. of the \(t\)-distribution with \(d\) degrees of freedom, and \(\bar{F}_{k(n-1)}=1-F_{k(n-1)}\). Note that, under \(H_{i,j}\), \(\hat{p}_{i,j}\sim U(0,1)\); however, note that these \(p\)-values are not mutually independent (or we could have just used Sidak's method). We now express Tukey's method as a single-step method that compares the \(p\)-values to a common cutoff, i.e., reject \(H_{i,j}\) if \(\hat{p}_{i,j}\leq d(\alpha)\) for an appropriate critical value \(d(\alpha)\). To see how \(d(\alpha)\) should be determined, note that \[P_{\mu,\sigma}\{\min_{(i,j)\in I}\hat{p}_{i,j}\leq d(\alpha)\}=P_{\mu,\sigma} \{\max_{(i,j)\in I}|T_{i,j}|\geq\bar{F}_{k(n-1)}^{-1}(d(\alpha)/2)\}\.\] In order to have strong control, \(d(\alpha)\) must satisfy \[\bar{F}_{k(n-1)}^{-1}(d(\alpha)/2)=q_{k,k(n-1)}(1-\alpha)/\sqrt{2}\ ;\] that is, comparing \(p\)-values with a common cutoff results in the same procedure as comparing the test statistics with a common cutoff. By working directly with the distribution of the maximum test statistic, one can avoid an initial step of having to compute marginal \(p\)-values. (This can be a time saver in other problems if each marginal \(p\)-value requires a computer-intensive method of calculation.) However, the equivalence between the approaches depends crucially on the fact that the marginal distribution of each test statistic \(T_{i,j}\) under \(H_{i,j}\) is identical; that is, it does not depend on \((i,j)\).
* _Adjusted \(p\)-values._ Let \(t_{i,j}\) be the observed value of \(T_{i,j}\). Then, the adjusted \(p\)-value for testing \(H_{i,j}\) is given by \[\bar{p}_{i,j}=P_{\theta=0,\sigma}\left\{\max_{k\neq l}|T_{k,l}|\geq t_{i,j} \right\}\.\] Equivalently, if \(\hat{p}_{i,j,\text{obs}}\) is the observed value of \(\hat{p}_{i,j}\), then, \[\bar{p}_{i,j}=P_{\theta=0,\sigma}\left\{\max_{k\neq l}|T_{k,l}|\geq t_{i,j} \right\}\.\]\[\tilde{p}_{i,j}=P_{\theta=0,\sigma}\left\{\min_{k\neq l}\,\hat{p}_{k,l}\leq\hat{p}_ {i,j,\mathrm{obs}}\right\}\]

* _Relationship to Simultaneous Confidence intervals._ Individual or marginal confidence intervals for \(\mu_{i}-\mu_{j}\) with nominal level \(1-\alpha\) can be contructed as \[\tilde{X}_{i}-\tilde{X}_{j}\pm\hat{\sigma}\sqrt{2/n}t_{k(n-1)}(1-\frac{\alpha }{2})\,\] (9.11) where \(t_{d}(\alpha)\) denotes the \(\alpha\)-quantile of the \(t\)-distribution with \(d\) degrees of freedom. That is, the marginal coverage probability that a particular interval contains the respective difference of means is \(1-\alpha\) with no multiplicity adjustment. The chance that all intervals simultaneously cover their respective difference in means can be much less than \(1-\alpha\). The Bonferroni approach replaces \(\alpha\) in (9.11) with \(\alpha^{\prime}=\alpha/\binom{k}{2}\). Then, all intervals jointly or simultaneously contain all the differences with probability at least \(1-\alpha\). One can get "smaller" joint intervals by replacing \(t_{k(n-1)}(1-\alpha^{\prime}/2)\) with \(q_{k,k(n-1)}(1-\alpha)/\sqrt{2}\). Indeed, the probability statement \[P_{\theta,\sigma}\left\{\max_{i\neq j}\sqrt{\frac{n}{2}}|(\tilde{X}_{i}-\tilde {X}_{j})-(\theta_{i}-\theta_{j})|/\hat{\sigma}\leq q_{k,k(n-1)}(1-\alpha)/ \sqrt{2}\right\}=1-\alpha\] can be _inverted_ to yield the simultaneous confidence intervals \[\tilde{X}_{i}-\tilde{X}_{j}\pm\hat{\sigma}/\sqrt{n}\cdot q_{k,k(n-1)}(1-\alpha )\.\] Having constructed simultaneous confidence intervals for \(\theta_{i}-\theta_{j}\), one can construct a multiple testing procedure by rejecting \(H_{i,j}\) if \(0\) is not in the interval for \(\theta_{i}-\theta_{j}\). This procedure controls the FWER. Of course, this is the same procedure as Tukey's method. We've merely stated the familiar duality between tests of a parameter (in this case, a multivariate parameter of length \(\binom{k}{2}\)) and confidence regions for that parameter). While both approaches lead to the same solution in this example, the approach based on constructing a null distribution and constructing confidence regions can lead to distinct solutions in more general problems.
* _Directional errors._ A _directional_ or _Type 3_ error results when the sign of an effect is misclassified. That is, when testing a real-valued parameter \(\theta=0\) against \(\theta\neq 0\), declaring \(\theta\) positive when it is in fact negative (or vice-a-versa) is a directional error. First, let us give some brief comments about directional errors when testing a single parameter \(\theta\). Suppose, a confidence interval \([L,\,U]\) for \(\theta\) can be constructed satisfying \[P\{L\leq\theta\leq U\}\geq 1-\alpha\.\]For testing \(\theta=0\) against \(\theta\neq 0\), consider the test that rejects \(\theta=0\) if \(0\notin[L,\,U]\); further, if \(\theta=0\) is rejected, declare \(\theta>0\) if \(L>0\) or declare \(\theta<0\) if \(U<0\). Then, \[P\{\text{commit a Type 1 or Type 3 error}\}\leq\alpha\.\] (9.12) Actually, if the confidence interval is equi-tailed, meaning \(P\{\theta<L\}\leq\alpha/2\) and \(P\{U<\theta\}\leq\alpha/2\), then, \[P\{\text{Type 3 error}\}\leq\alpha/2\.\] (9.13) Control of Type 1 or Type 3 errors carries over if tests are based on simultaneous confidence intervals for parameters \(\theta_{1},\,\ldots,\,\theta_{s}\). If \[P\{\theta_{i}\in[L_{i},\,U_{i}]\text{ simultaneously for all }i\}\geq 1-\alpha\,\] (9.14) then \[P\{\text{commit a Type 1 or Type 3 error for any }i\}\leq\alpha\.\] (9.15) Note that in (9.12), with only one parameter, one can only make a Type 1 error or a Type 3 error, but in (9.15), one can simultaneously make errors of both types (though the probability of making even one Type 1 or Type 3 error is controlled, i.e., it bounded by \(\alpha\)). The probability on the left side of (9.15) is sometimes known as the _mixed directional familywise error rate_ or mdFWER. Returning to the ANOVA setup, these considerations apply directly to inferences concerning the pairwise differences \(\theta_{i}-\theta_{j}\).
* _Unbalanced case._ Suppose the number of observations in group \(i\) is \(n_{i}\) (which varies with \(i\)). The natural test statistic for testing \(H_{i,j}:\ \theta_{i}=\theta_{j}\) is now generalized as \[T_{i,j}=(\bar{X}_{i}-\bar{X}_{j})/\hat{\sigma}\sqrt{\frac{1}{n_{i}}+\frac{1}{n _{j}}}\,\] where \(\bar{X}_{i}=\sum_{j=1}^{n_{i}}X_{i,j}/n_{i}\) and \[\hat{\sigma}^{2}=\frac{\sum_{i=1}^{k}(n_{i}-1)s_{i}^{2}}{\sum_{i=1}^{k}(n_{i} -1)}\] and \[s_{i}^{2}=\frac{\sum_{j=1}^{n_{i}}(X_{i,j}-\bar{X}_{i})^{2}}{n_{i}-1}\.\] Let \(N=\sum_{i}n_{i}\). Then \((N-k)\hat{\sigma}^{2}\) is distributed as Chi-squared with \(N-k\) degrees of freedom. Also, under \(H_{i,j}\), \(T_{i,j}\) has the \(t\)-distribution with \(N-k\) degrees of freedom. However,\[\max_{i\neq j}|\tilde{X}_{i}-\tilde{X}_{j}|/\hat{\sigma}\sqrt{\frac{1}{n_{i}}+\frac {1}{n_{j}}} \tag{9.16}\]

no longer has the studentized range distribution because the denominator depends on \((i,j)\). The Tukey-Kramer approximation replaces \(q_{k,k(n-1)}(1-\alpha)\) (the value used before when \(n_{i}=n\)) with \(q_{k,N-k}(1-\alpha)\). Hayter (1984) proved this procedure is conservative; that is, the resulting simultaneous confidence intervals cover all the respective differences with probability \(\geq 1-\alpha\), with equality only in the balanced case. This was seen as an important breakthrough in the field of multiple comparisons. One can use simulation to get a nearly exact method, with the approximation error due to simulation error (which is under our control). The main point is that simulation is possible because the distribution of (9.16) under all \(\theta_{i}=0\) does not depend on the unknown nuisance parameter \(\sigma\). In more complex problems, resampling methods offer viable approaches, and will be discussed in Chaps. 17 and 18. Here is a description of a Monte Carlo simulation method. For \(b=1\),..., \(B\), let \(Z_{i,j}^{(b)}\) be i.i.d. \(N(0,1)\), for \(i=1,\ldots,k\), \(j=1,\ldots,n_{i}\). Compute the statistic \[\max_{i,j}|T_{i,j}^{(b)}(Z)|\,\] i.e., the value of (9.16) for the data based on \(Z=\{Z_{i,j}\}\). The empirical distribution of these \(B\) values serves as a Monte Carlo approximation to the distribution of (9.16) under \(\mu=0\). One can then use the \(1-\alpha\) quantile of this empirical distribution as the single-step critical value.
* _Further issues._ More problematic than unequal sample sizes is unequal variances and the assumption of normality. Also, even with all these assumptions, the Tukey method can be improved. Under very weak assumptions, nonparametric large-sample methods based on resampling will be seen to handle the distributional challenges. But, the point of this example is that one can improve upon Bonferroni if one can incorporate knowledge of the joint distribution of test statistics into construction of critical values. In fact, Tukey's method can be improved by a stepwise method, which will be discussed later.

#### Duncan's Method

##### (Pairwise Comparisons of Treatments to a Control)

The setup is the same as all pairwise comparisons, except now we have a control group of observations \(X_{0,1},\ldots,X_{0,n}\), with \(X_{0,j}\sim N(\theta_{0},\sigma^{2})\). We are interested in testing \(s=k\) hypotheses \(H_{i}:\ \theta_{i}=\theta_{0}\), \(i=1,\ldots,k\). We wish to determine a critical value so that the procedure that rejects \(H_{i}\) if \(T_{i,0}=(n/2)^{1/2}|\tilde{X}_{i}-\tilde{X}_{0}|/\hat{\sigma}\) exceeds the critical value controls the FWER at level \(\alpha\).

**Definition 9.1.3**: If \(Z_{0}\),..., \(Z_{k}\) are i.i.d. \(N(0,\,1)\) and, independently of the \(Z\)s, \(V\sim\chi_{d}^{2}\), then

\[\max_{1\leq i\leq k}\sqrt{d/2}|Z_{i}-Z_{0}|/\sqrt{V}\]

has Dunnett's two-sided range distribution with parameters \(k\) and \(d\). We will denote its \(1-\alpha\) quantile by \(q_{k,d}^{D}(1-\alpha)\).

Duncan's method can then be stated as follows. Reject any \(H_{i}\) for which

\[T_{i,0}>q_{k,(k+1)(n-1)}^{D}(1-\alpha)\.\]

Similar to Tukey's method, Duncan's method controls the FWER at level \(\alpha\). Joint confidence intervals for the \(\theta_{i}-\theta_{0}\) can be constructed, and control of Type 1 or Type 3 errors holds as well (Problem 9.5).

We now close the discussion of single-step methods with an example of testing an infinite number of hypotheses.

**Example 9.1.6**: **(An Infinite Family)** Let \(X=(X_{1},\,X_{2})^{\top}\), where \(X_{1}\) and \(X_{2}\) are independent with \(X_{i}\sim N(\theta_{i},\,1)\). Fix \(\theta_{0}\in\mathbf{R}^{2}\). For a given vector \(a\in\mathbf{R}^{2}\), consider the null hypothesis \(H_{a}:\ a^{\top}\theta=a^{\top}\theta_{0}\). As \(a\) varies, we have an infinite family of hypotheses. The usual likelihood ratio test rejects for large values of

\[T_{a}=\frac{|a^{\top}(X-\theta_{0})|^{2}}{|a|^{2}}\.\]

Consider the single-step method which rejects any hypothesis \(H_{a}\) such that \(T_{a}>c\). In order to control the FWER at level \(\alpha\), \(c\) must be chosen appropriately. But, by the Cauchy-Schwarz inequality,

\[\frac{|a^{\top}(X-\theta_{0})|^{2}}{|a|^{2}}\leq|X-\theta_{0}|^{2}\,\]

with equality if and only if \(a\propto(X-\theta_{0})\). Hence,

\[P_{\theta_{0}}\{\sup_{a}T_{a}>c\}=P_{\theta_{0}}\{|X-\theta_{0}|^{2}>c\}\,\]

so that \(c\) should be taken to be \(c=c_{2}(1-\alpha)\), where \(c_{d}(1-\alpha)\) denotes the \(1-\alpha\) quantile of the Chi-squared distribution with \(d\) degrees of freedom. This procedure provides strong control of the FWER. Furthermore, one can invert the tests to get simultaneous confidence intervals for all the \(a^{\top}\theta\), yielding

\[\{a^{\top}\theta\in[a^{\top}X\pm|a|\sqrt{c_{2}(1-\alpha)}]\}\.\]

The ideas here form the basis for Scheffe's general projection method, discussed later in the chapter.

[MISSING_PAGE_FAIL:430]

hypotheses as long as their corresponding \(p\)-values are small. The Holm procedure uses

\[\alpha_{i}=\alpha/(s-i+1). \tag{9.19}\]

(Alternatively, if the rejection region of each test corresponds to large values of a test statistic, a stepdown procedure begins by determining whether or not the hypothesis corresponding to the largest test statistic should be rejected; see Procedure 9.1.1 below.)

On the other hand, _stepup_ procedures begin by looking at the least significant \(p\)-value (or the smallest value of a test statistic when the individual tests reject for large values). For a given set of constants (9.17), reject all hypotheses if \(\hat{p}_{(s)}\leq\alpha_{s}\). Otherwise, for \(r=s\),..., 1, reject hypotheses \(H_{(1)},\ldots,\)\(H_{(r)}\) if

\[\hat{p}_{(s)}>\alpha_{s},\ldots,\hat{p}_{(r+1)}>\alpha_{r+1}\ \ \mbox{but}\ \ \hat{p}_{(r)}\leq\alpha_{r}. \tag{9.20}\]

**Theorem 9.1.2**: _Assume each of the \(p\)-values \(\hat{p}_{i},\ i=1,\ldots,s\) satisfies (9.5). (i) The Holm stepdown procedure described in (9.18) with \(\alpha_{i}\) given by (9.19) controls the FWER; that is, (9.3) holds. (ii) One cannot increase even one of the \(\alpha_{i}\) without violating control of the FWER for some \(P\)._

Proof. Fix any \(P\) and let \(I(P)\) be the indices of the true null hypotheses. Assume \(|I(P)|\geq 1\) or there is nothing to prove. Order the \(p\)-values corresponding to the \(|I(P)|\) true null hypotheses; call them

\[\hat{q}_{(1)}\leq\cdots\leq\hat{q}_{|I(P)|}\.\]

Let \(j\) be the smallest (random) index satisfying \(\hat{p}_{(j)}=\hat{q}_{(1)}\), so

\[1\leq j\leq s-|I(P)|+1 \tag{9.21}\]

because the largest possible index \(j\) occurs when all the smallest \(p\)-values correspond to the \(s-|I(P)|\) false null hypotheses and the next \(|I(P)|\)\(p\)-values correspond to the true null hypotheses. So, \(\hat{p}_{(j)}=\hat{q}_{(1)}\). Then, the Holm procedure commits at least 1 false rejection if and only if

\[\hat{p}_{(1)}\leq\alpha_{1},\,\hat{p}_{(2)}\leq\alpha_{2},\ldots,\,\hat{p}_{( j)}\leq\alpha_{j}\,\]

which certainly implies that

\[\hat{q}_{(1)}=\hat{p}_{(j)}\leq\alpha_{j}=\frac{\alpha}{s+1-j}\.\]

But by (9.21),

\[\frac{\alpha}{s+1-j}\leq\frac{\alpha}{|I(P)|}\.\]So, the probability of at least 1 false rejection is bounded above by

\[P\left\{\hat{q}_{(1)}\leq\frac{\alpha}{|I(P)|}\right\}\.\]

By Bonferroni, the chance that the smallest among \(I(P)\)\(p\)-values is \(\leq\alpha/|I(P)|\) is \(\leq\alpha\).

To prove (ii), we want to show \(\alpha_{i}\) cannot be increased. Suppose \(i-1\) hypotheses are false and the corresponding \(p\)-values are identically 0 (or at least \(<\alpha/s\)). Then, a false rejection occurs if and only if

\[\hat{q}_{(1)}\leq\alpha_{i}=\frac{\alpha}{s-i+1}=\frac{\alpha}{|I(P)|}\.\]

Using the same construction for the joint distribution of the \(|I(P)|\)\(p\)-values where the FWER for Bonferroni is exactly \(\alpha\) shows the FWER is exactly \(\alpha\) for such a joint distribution.

If the \(p\)-values have discrete distributions, it is possible that there may be ties among them. However, the proof remains valid regardless of how tied \(p\)-values are ordered because monotonicity of the \(\alpha_{i}\) ensures that all hypotheses with a common tied \(p\)-value will be rejected if any of them are rejected. This comment applies to all stepwise methods developed in this chapter.

It follows that increasing even of the \(\alpha_{i}\) will lead to violation of the FWER in general.

The Holm adjusted \(p\)-values are seen to be given by:

\[\tilde{p}_{(1)}=sp_{(1)}\]

\[\tilde{p}_{(2)}=\max[\tilde{p}_{(1)},\,(s-1)\,\hat{p}_{(2)}]\]

\[\vdots\]

\[\tilde{p}_{(j)}=\max[\tilde{p}_{(j-1)},\,(s-j+1)\,\hat{p}_{(j)}]\]

\[\vdots\]

\[\tilde{p}_{(s)}=\max[\tilde{p}_{(s-1)},\,\hat{p}_{(s)}]\.\]

For certain parametric models, optimality results for some stepwise procedures will be developed in Section 9.4. For now, we show that it is possible to improve upon the Holm method by incorporating the dependence structure of the individual tests.

To see how, suppose that a test of the individual hypothesis \(H_{j}\) is based on a test statistic \(T_{n,j}\), with large values indicating evidence against \(H_{j}\). (The use of the subscript \(n\) in the test statistics will be for asymptotic purposes later on.)

If \(P\) is the true probability distribution generating the data, let

\[I(P)\subseteq\{1,\ldots,s\}\]

denote the indices of the set of true hypotheses; that is, \(i\in I(P)\) if and only if \(P\in\omega_{i}\). For \(K\subseteq\{1,\ldots,s\}\), let \(H_{K}\) denote the intersection hypothesis that all \(H_{i}\) with \(i\in K\) are true; that is, \(H_{K}\) is equivalent to \(P\in\bigcap_{i\in K}\omega_{i}\). In order to improve upon the Holm method, the basic idea is to use critical values that more accurately approximate the distribution of \(\max_{j\in K}T_{n,j}\) when testing \(H_{K}\), at least when \(H_{K}\) is in fact true. Let

\[T_{n,r_{1}}\geq T_{n,r_{2}}\geq\cdots\geq T_{n,r_{s}}\]

denote the observed ordered test statistics, and let \(H_{(1)}\), \(H_{(2)}\),..., \(H_{(s)}\) be the corresponding hypotheses. A stepdown procedure begins with the most significant test statistic. First, test the joint null hypothesis \(H_{\{1,\ldots,s\}}\) that all hypotheses are true. This hypothesis is rejected if \(T_{n,r_{1}}\) is large. If it is not large, accept all hypotheses; otherwise, reject the hypothesis corresponding to the largest test statistic. Once a hypothesis is rejected, remove it and test the remaining hypotheses by rejecting for large values of the maximum of the remaining test statistics, and so on.

To be specific, consider the following generic procedure, based on critical values \(\hat{c}_{n,K}(1-\alpha)\), where \(\hat{c}_{n,K}(1-\alpha)\) is designed for testing the intersection hypothesis \(H_{K}\) at nominal level \(\alpha\). Although we are not specifying the constants at this point, we note that they could be nonrandom or data-dependent.

**Procedure 9.1.1**: **(Generic Stepdown Method)**__

1. Let \(K_{1}=\{1,\ldots,s\}\). If \(T_{n,r_{1}}\leq\hat{c}_{n,K_{1}}(1-\alpha)\), then accept all hypotheses and stop; otherwise, reject \(H_{(1)}\) and continue.
2. Let \(K_{2}\) be the indices of the hypotheses not previously rejected. If \(T_{n,r_{2}}\leq\hat{c}_{n,K_{2}}(1-\alpha)\), then accept all remaining hypotheses and stop; otherwise, reject \(H_{(j)}\) and continue.

\[\vdots\]
2. Let \(K_{j}\) be the indices of the hypotheses not previously rejected. If \(T_{n,r_{j}}\leq\hat{c}_{n,K_{j}}(1-\alpha)\), then accept all remaining hypotheses and stop; otherwise, reject \(H_{(j)}\) and continue.

\[\vdots\]
3. If \(T_{n,r_{j}}\leq\hat{c}_{n,K_{s}}(1-\alpha)\), then accept \(H_{(s)}\); otherwise, reject \(H_{(s)}\).

The problem now is how to construct the \(\hat{c}_{n,K}(1-\alpha)\) so that the FWER is controlled. The following result reduces the multiple testing problem of controlling the FWER to that of constructing single tests that control the probability of a Type 1 error.

Note that such a procedure only seems reasonable if the test statistics are comparable or on the same scale, as in Example 9.1.7 below where each test statistic \(T_{n,i}\) has the standard normal distribution if \(H_{i}\) is true. In general, one may want to consider studentized or standardized statistics. If it is easy to convert a test based on some statistic to a \(p\)-value \(\hat{p}_{i}\), then another possibility is to use \(T_{n,i}=-\hat{p}_{i}\). Note that Theorem 9.1.3 below holds without any such assumptions, as long as the monotonicity requirement (9.22) holds.

**Theorem 9.1.3**: _Let \(P\) denote the true distribution generating the data. Consider Procedure 9.1.1 based on critical values \(\hat{c}_{n,K}(1-\alpha)\) which satisfy the monotonicity requirement: for any \(K\supset I(P)\),_

\[\hat{c}_{n,K}(1-\alpha)\geq\hat{c}_{n,I(P)}(1-\alpha)\;. \tag{9.22}\]

_(i) Then,_

\[FWER_{P}\leq P\{\max(T_{n,i}:\;i\in I(P))\geq\hat{c}_{n,I(P)}(1-\alpha)\}\;. \tag{9.23}\]

_(ii) Also suppose that if \(\hat{c}_{n,K}(1-\alpha)\) is used to test the intersection hypothesis \(H_{K}\), then it is level \(\alpha\) when \(K=I(P)\); that is,_

\[P\{\max(T_{n,i}:\;i\in I(P))>\hat{c}_{n,I(P)}(1-\alpha)\}\leq\alpha\;. \tag{9.24}\]

_Then \(FWER_{P}\leq\alpha\)._

Proof. Consider the event that a true hypothesis is rejected, so that for some \(i\in I(P)\), hypothesis \(H_{i}\) is rejected. Let \(\hat{j}\) be the smallest index \(j\) in the method where this occurs, so that

\[\max\{T_{n,i}:\;i\in I(P)\}>\hat{c}_{n,K_{j}}(1-\alpha)\;. \tag{9.25}\]

Since \(K_{j}\supseteq I(P)\), assumption (9.22) implies

\[\hat{c}_{n,K_{j}}(1-\alpha)\geq\hat{c}_{n,I(P)}(1-\alpha) \tag{9.26}\]

and so (i) follows. Part (ii) follows immediately from (i).

**Example 9.1.7**: **(Multivariate Normal Mean)** Suppose \(X=(X_{1},\ldots,X_{s})^{\top}\) is multivariate normal with unknown mean vector \(\mu=(\mu_{1},\ldots,\mu_{s})^{\top}\) and known covariance matrix \(\Sigma\) having (\(i\), \(j\)) component \(\sigma_{i,j}\). Consider testing

\[H_{i}:\mu_{i}\leq 0\quad\text{versus}\quad\mu_{i}>0\;.\]Let \(T_{n,i}=X_{i}/\sqrt{\sigma_{i,i}}\), since the test that rejects for large \(X_{i}/\sqrt{\sigma_{i,i}}\) is \(UM\,P\) for testing \(H_{i}\). One possible single-step procedure rejects any \(H_{i}\) for which \(T_{n,i}\geq c_{n,\{1,\ldots,s\}}\), where \(c_{n,\{1,\ldots,s\}}\) is determined by

\[P_{0}\left\{\max_{1\leq i\leq s}T_{n,i}\geq c_{n,\{1,\ldots,s\}}\right\}=\alpha\;.\]

In the special case when \(\sigma_{i,i}=\sigma^{2}\) is independent of \(i\) and \(\sigma_{i,j}\) has the product structure \(\sigma_{i,j}=\lambda_{i}\lambda_{j}\), then Appendix 3 of Hochberg and Tamhane (1987, p. 374) reduces the problem of determining the distribution of the maximum of a multivariate normal vector to a univariate integral. In general, one can resort to simulation to approximate the critical values; see Example 11.4.3.

The above method clearly controls the FWER at level \(\alpha\). In order to obtain a stepdown improvement by applying Theorem 9.1.3, let \(\hat{c}_{n,K}(1-\alpha)\) be the \(1-\alpha\) quantile of the distribution of \(\max(T_{n,i}:\;i\in K)\) when \(\mu=0\). Since

\[\max(T_{n,i}:\;i\in I)\leq\max(T_{n,i}:\;i\in K)\]

whenever \(I\subseteq K\), the monotonicity requirement (9.22) is satisfied. Moreover, the resulting test procedure rejects at least as many hypotheses as the Holm procedure (Problem 9.8).

**Example 9.1.8**: **(One-way Layout)**  Suppose for \(i=1,\ldots,s\) and \(j=1,\ldots,n_{i}\), \(X_{i,j}=\mu_{i}+\epsilon_{i,j}\), where the \(\epsilon_{i,j}\) are i.i.d. \(N(0,\sigma^{2})\); the vector \(\mu=(\mu_{1},\ldots,\mu_{s})^{\top}\) and \(\sigma^{2}\) are unknown. Consider testing \(H_{i}:\mu_{i}=0\) against \(\mu_{i}\neq 0\). Let \(t_{n,i}=n_{i}^{1/2}\tilde{X}_{i,\cdot}/\hat{\sigma}\), where

\[\tilde{X}_{i.}=n_{i}^{-1}\sum_{j=1}^{n_{i}}X_{i,j},\quad\hat{\sigma}^{2}=\sum_ {i=1}^{s}\sum_{j=1}^{n_{i}}(X_{i,j}-\tilde{X}_{i.})^{2}/\nu\;,\]

and \(\nu=\sum_{i}(n_{i}-1)\). Under \(H_{i}\), \(t_{n,i}\) has a \(t\)-distribution with \(\nu\) degrees of freedom. Let \(T_{n,i}=|t_{n,i}|\), and let \(\hat{c}_{n,K}(1-\alpha)\) denote the \(1-\alpha\) quantile of the distribution of \(\max(T_{n,i}:\;i\in K)\) when \(\mu=0\) and \(\sigma=1\). Since

\[\max(T_{n,i}:\;i\in I)\leq\max(T_{n,i}:\;i\in K)\;,\]

the monotonicity requirement (9.22) follows. Note that the joint distribution of \((t_{n,1},\ldots,t_{n,s})\) follows an \(s\)-variate multivariate \(t\)-distribution with \(\nu\) degrees of freedom; see Hochberg and Tamhane (1987, pp. 374-5).

Similarly, both Tukey's method in Example 9.1.4 and Duncan's method in Example 9.1.5 admit stepdown improvements (Problem 9.9).

### The Closure Method

#### The Basic Method and Some Examples

Consider the general problem of simultaneously testing \(s\) individual null hypotheses \(H_{1}\),..., \(H_{s}\). Data \(X\) with distribution \(P\in\Omega\) is available, where the parameter space \(\Omega\) may describe a parametric, semiparametric, or nonparametric model for \(P\). As before, a general hypothesis \(H\) can be viewed as a subset \(\omega\) of \(\Omega\). The problem is to test null hypotheses \(H_{i}:P\in\omega_{i}\) versus alternative hypotheses \(H_{i}^{\prime}:P\notin\omega_{i}\) simultaneously for \(i=1,\ldots,s\). Let \(I(P)\) denote the indices of the set of true null hypotheses when \(P\) is the true probability distribution, i.e., \(i\in I(P)\) if and only if \(P\in\omega_{i}\).

Typically, tests for the individual hypotheses are available and the problem is how to combine them into a suitable multiple testing procedure. In order to devise a procedure which controls the FWER, the closure method of Marcus et al. (1976) reduces the problem to constructing single tests that control the usual probability of a Type 1 error. Specifically, for a subset \(K\subseteq\{1,\ldots,s\}\), define the intersection hypothesis

\[H_{K}:P\in\omega_{K}\, \tag{9.27}\]

where

\[\omega_{K}=\bigcap_{i\in K}\omega_{i}\.\]

Of course, \(H_{i}=H_{\{i\}}\). Suppose \(\phi_{K}\) is a level \(\alpha\) test of \(H_{K}\), i.e.,

\[\sup_{P\in\omega_{K}}E_{P}[\phi_{K}]\leq\alpha. \tag{9.28}\]

The closure method rejects \(H_{i}\) if and only if \(H_{K}\) is rejected for all subsets \(K\) for which \(i\in K\). So, in order for \(H_{i}\) to be deemed significant, every intersection hypothesis that includes \(H_{i}\) must be deemed significant. Note that if a particular intersection hypothesis \(H_{K}\) corresponds to \(\omega_{K}\) being empty, then such a hypothesis is immediately considered to be rejected.

To see why the closure method controls the FWER, let \(A\) be the event that at least one \(H_{i}\) with \(i\in I(P)\) is rejected, and let \(B\) be the event that \(H_{I(P)}\) is rejected when applying the level \(\alpha\) test \(\phi_{I(P)}\). Then \(A\) implies \(B\) by construction. Hence,

\[\text{FWER}_{P}=P\{A\}\leq P\{B\}\leq\alpha\,\]

which is the desired result.

Define a family of hypotheses \(H_{1}\),..., \(H_{s}\) to be _closed_ if each intersection hypothesis \(H_{K}\) is a member of the family. The closure of a given family is the family of all intersection hypotheses induced by the given family. In some cases, there reallyis nothing to lose by assuming the given family is closed. One reason is that, when applying the closure method, one gets a rule that controls the FWER, not just for the original family, but for the (possibly) larger closed family.

**Example 9.2.1**: **(Disjoint hypotheses)** Suppose \(\omega_{i}\bigcap\omega_{j}=\emptyset\) if \(i\neq j\). Any intersection hypothesis is either empty or just an individual \(H_{j}\). So, each \(H_{i}\) can be tested at level \(\alpha\) and the FWER is controlled with no multiplicity adjustment. This example suggests partitioning the parameter space into disjoint regions and testing them separately, which leads to the partitioning principle of Finner and Strassburger (2002).

**Example 9.2.2**: **(Nested hypotheses)** Suppose

\[\omega_{1}\subset\omega_{2}\subset\cdots\subset\omega_{s}\.\]

Begin by testing \(H_{1}\) at level \(\alpha\). If \(H_{1}\) is accepted, stop; otherwise, next test \(H_{2}\) at level \(\alpha\). If \(H_{2}\) is accepted, there are no further rejections and stop. Otherwise, next test \(H_{3}\) at level \(\alpha\), and so on. This stepwise procedure controls the FWER at level \(\alpha\).

**Example 9.2.3**: **(Holm method)** Suppose the intersection hypotheses are tested using the Bonferroni method. That is, if \(H_{K}\) an intersection of \(m=|K|\) individual hypotheses, test \(H_{K}\) at level \(\alpha/m\). Applying the closure method yields the Holm method (Problem 9.11).

**Example 9.2.4**: **(Max-T Tests)** Suppose the test of \(H_{K}\) is reject \(H_{K}\) if \(\max_{i\in K}T_{n,i}>\hat{c}_{n,K}(1-\alpha)\). Assume each such test is level \(\alpha\), and the critical values are monotone, i.e.,

\[\hat{c}_{n,I}(1-\alpha)\leq\hat{c}_{n,K}(1-\alpha)\quad\text{ if }I\subseteq K\.\]

By Theorem 9.1.3, the stepdown method considered in Procedure 9.1.1 controls the FWER. Now, this method may be viewed as a special case of the closure method. Note the structure of the tests allows for a "shortcut" in its application, since at most \(s\) tests need to be carried out and not \(2^{s}-1\) tests.

#### 9.2.2 Simes' Identity and Hommel's Method

Consider the "global" intersection hypothesis \(H\) that all null hypotheses are true; that is, \(P\in\bigcap_{i=1}^{s}\omega_{i}\). Simes (1986) proposed the following test of \(H\):

\[\text{ reject }H\text{ if }\hat{p}_{(i)}\leq i\alpha/s\ \ \text{ for some }i. \tag{9.29}\]

The rationale is based on the following result due to Simes.

**Lemma 9.2.1**: **(Simes' Identity)** _Suppose \(\hat{p}_{1},\ldots,\hat{p}_{s}\) are i.i.d. \(U(0,1)\) variables with ordered values_\[\hat{p}_{(1)}\leq\cdots\leq\hat{p}_{(s)}\;.\]

_Let \(E_{s}=E_{s}(\alpha)\) be the event_

\[E_{s}(\alpha)=\bigcup_{i=1}^{s}\left\{\hat{p}_{(i)}\leq\frac{i\alpha}{s}\right\}\;. \tag{9.30}\]

_Then,_

\[P\left\{E_{s}(\alpha)\right\}=\alpha\;. \tag{9.31}\]

Proof. Let \(\gamma_{s}(\alpha)\) denote the left side of (9.31). Clearly, \(\gamma_{1}(\alpha)=\alpha\). The proof is by induction. Assume, \(\gamma_{s-1}(\alpha)\) for all \(\alpha\in[0,\,1]\). To prove the result for \(\gamma_{s}(\alpha)\), condition on \(\hat{p}_{(s)}=u\). Of course, if \(u\leq\alpha\), the event \(E_{s}(\alpha)\) has conditional probability one. Otherwise, if \(u>\alpha\), the conditional distribution of the other \(p\)-values is the distribution of \(s-1\) i.i.d. \(U(0,\,u)\) variables. Let \(U_{1},\,\ldots,\,U_{s-1}\) be i.i.d. \(U(0,\,1)\) with ordered values \(U_{(1)}\leq\cdots\leq U_{(s-1)}\). Then, for \(u>\alpha\),

\[P\{E_{s}(\alpha)|\hat{p}_{(s)}=u\}=P\left\{U_{(1)}\leq\frac{\alpha}{su}\right\} \cup\cdots\cup\{U_{(s-1)}\leq\frac{(s-1)\alpha}{su}\}\right\}\;.\]

Let \(\alpha^{\prime}=\alpha(s-1)/su\). Then, the last expression becomes

\[P\left\{U_{(1)}\leq\frac{\alpha^{\prime}}{s-1}\right\}\cup\{U_{(2)}\leq\frac{2 \alpha^{\prime}}{s-1}\}\cup\cdots\cup\{U_{(s-1)}\leq\alpha^{\prime}\}\right\}\;.\]

By the inductive hypothesis, the last expression is (conditionally) \(\alpha^{\prime}\). Averaging over values of \(u\) yields

\[\gamma_{s}(\alpha)=P\{\hat{p}_{(s)}\leq\alpha\}+E\left[\frac{(s-1)\alpha}{s \hat{p}_{(s)}}I\{\hat{p}_{(s)}>\alpha\}\right]\]

\[=\alpha^{s}+\frac{(s-1)\alpha}{s}\int_{\alpha}^{1}\frac{1}{u}su^{s-1}du= \alpha^{s}+\alpha[1-\alpha^{s-1}]=\alpha\;.\;\blacksquare\]

Note that Simes' method (9.29) is a test of the global null hypothesis, but it is not a multiple testing procedure in that it does not allow one to make inferences about individual hypotheses. Also, the independence assumption is strong, but it can be weakened to forms of positive dependence, in which case the equality in (9.31) is replaced by an inequality and \(\alpha\) is then an upper bound. In any case, the identity is interesting and important as it forms a basis for other multiple testing procedures discussed in Section 9.3.

**Example 9.2.5**: **(Hommel's Method)** _Hommel (1988) applied Simes' method for tests of the \(H_{K}\). When applying closure, one can use the following shortcut. Let \(\hat{j}\) be the number of hypotheses in the largest subset of hypotheses for which Simes' test of \(H_{K}\) is not significant. So,

\[\hat{j}=\max\{m\in\{1,\ldots,s\}:\ \hat{p}_{(s-m+k)}>\frac{k\alpha}{m}\text{ for all }k=1,\ldots,m\}\;.\]

If there are no nonsignificant Simes' tests, then reject all \(H_{1},\ldots,H_{s}\). Otherwise, reject \(H_{i}\) if \(\hat{p}_{i}\leq\alpha/\hat{j}\).

#### The Higher Criticism and Other Joint Tests

Suppose \(\hat{p}_{i}\) is available to test \(H_{i}\) for \(i=1,\ldots,s\). By applying the closure method, any test of the global null hypothesis that all hypotheses are true can be used to construct a multiple testing procedure that controls the FWER. Let \(\hat{F}_{n}\) be the empirical distribution of the \(p\)-values, defined by

\[\hat{F}_{s}(t)=\frac{1}{s}\sum_{i=1}^{s}I\{\hat{p}_{i}\leq t\}\;.\]

Assume the \(p\)-values are independent. Fix \(0<\beta<1\). Then \(s\hat{F}_{s}(\beta)\) is the number of \(p\)-values significant at level \(\beta\). A simple joint test can reject if \(s\hat{F}_{s}(\beta)\) is too large, where the cutoff value can be determined by the binomial distribution based on \(s\) trials and success probability \(\beta\). Such a "binomial" test dates back to Clopper and Pearson (1934). Equivalently, the test rejects for large values of

\[\frac{\sqrt{s}[\hat{F}_{s}(\beta)-\beta]}{\sqrt{\beta(1-\beta)}}\;,\]

and \(z_{1-\alpha}\) serves as an approximate critical value.

Rather than specifying a choice of \(\beta\), Tukey (1953) suggested the higher criticism statistic \(HC\) defined by

\[HC_{s}=\sup_{0<\beta<\beta_{0}}\left[\frac{\sqrt{s}[\hat{F}_{s}(\beta)-\beta]} {\sqrt{\beta(1-\beta)}}\right]\;,\]

where \(\beta_{0}\) is a tuning parameter. Donoho and Jin (2015) suggest using \(\beta_{0}=0.5\). They also discuss the ability of \(HC\) to adapt to spare settings where most of the \(p\)-values correspond to true null hypotheses; also see Section 13.5.4.

Another class of joint or intersection tests dates back to Fisher (1934a). The hypothesis that all \(H_{i}\) are true can be tested using

\[F_{s}=-2\sum_{i=1}^{s}\log(\hat{p}_{i})\;.\]If the \(\hat{p}_{i}\) are i.i.d. U(0,1), then \(F_{s}\) has the Chi-squared distribution with \(2s\) degrees of freedom (Problem 3.26).

Additional joint tests are reviewed in Hedges and Olkin (1985). Two concerns must be made regarding these joint tests. First, the assumption of independence is rarely applicable. Second, when used together with the closure method, the computational complexity of the procedure must be considered because in general one may have to carry out \(2^{s}-1\) such tests for all intersection hypotheses. However, in many cases, the order of the number of tests required is \(O(s^{2})\) (Problem 9.14).

#### Coherence and Cosonance

The closure method always guarantees that the resulting decision rule is _coherent_ in the following sense.

**Definition 9.2.1**: For testing hypotheses \(H_{1}\),..., \(H_{s}\), a multiple testing procedure is _coherent_ if, whenever \(H_{j}\) is rejected and \(\omega_{i}\subset\omega_{j}\), then \(H_{i}\) is rejected as well.

For example, suppose \(H_{1}\) specifies \(\theta_{1}=0\), \(H_{2}\) specifies \(\theta_{2}=0\) and \(H_{3}\) specifies \(\theta_{1}=\theta_{2}=0\). Then, if \(H_{1}\) is not true, then neither is \(H_{3}\), so that rejection of \(H_{1}\) should also be accompanied by rejection of \(H_{3}\). The requirement of coherence is reasonable because if \(H_{j}\) is established as being false, then \(H_{i}\) is then necessarily false as well if \(\omega_{i}\subset\omega_{j}\). As stated in Hochberg and Tamhane (1987), coherence "avoids the inconsistency of rejecting a hypothesis without also rejecting all hypotheses implying it". A method that is not coherent is then called incoherent.

The next result shows that any coherent multiple testing procedure that controls the FWER at level \(\alpha\) is the result of applying the closure method based on some family of tests of the intersection hypotheses.

**Theorem 9.2.1**: _For testing \(H_{1},\ldots,H_{s}\), suppose a coherent method controls the FWER at level \(\alpha\). Then, the method is the result of applying the closure method based on some family of tests \(\phi_{K}\) of \(H_{K}\) satisfying (9.28). Thus, all coherent decision rules are generated using the closure method._

Proof. Define tests of an arbitrary intersection hypothesis \(H_{K}\) as follows. If \(H_{K}\) is a member of the original family of hypotheses \(H_{1}\),..., \(H_{s}\), then \(H_{K}\) gets rejected according to whether the original rule rejects \(H_{K}\). Otherwise, if \(H_{K}\) is not in the original family, define a test of \(H_{K}\) by the rule that \(H_{K}\) is rejected if the original multiple testing method rejects any \(H_{i}\) with \(i\in K\). Then, applying the closure method based on this family of tests of intersection hypotheses results in the same multiple testing procedure as the original one. To see this, first note that any hypothesis that is not rejected by the original rule certainly cannot be rejected by the closure method. Moreover, any hypothesis that is rejected by the original given rule is also rejected by the closure method. Indeed, if \(H_{i}\) is rejected by the original method, then \(H_{K}\) must be rejected when \(i\in K\). This occurs by construction if \(H_{K}\) is not a member of the original family and by coherence otherwise.

Observe that the requirement of coherence does not restrict the multiple testing procedure unless any of the hypotheses imply any of the others. As a simple example, suppose \(X=(X_{1},\ldots,X_{s})\) is multivariate normal with unknown mean vector \((\theta_{1},\ldots,\theta_{s})\) and known covariance matrix \(\Sigma\). If \(H_{i}\) specifies \(\theta_{i}=0\), then no \(\omega_{i}\) is contained in any other \(\omega_{j}\). Therefore, Theorem 9.2.1 says that _all_ multiple testing procedures that control the FWER must be obtained by the closure method.

Theorem 9.2.1 implies that, in order to devise a multiple testing procedure that controls the FWER, one can restrict attention to procedures based on the closure method, at least if one is willing to rule out incoherent procedures. Moreover, the next result says there is no reason to consider incoherent procedures because one can always replace it by a coherent one which is at least as good. Taken together, these results show attention may be restricted to procedures based on the closure method. For convenience, we now assume that all tests are nonrandomized. This may be done without loss of generality by including an auxiliary independent uniform random variable in the definition of \(X\).

**Theorem 9.2.2**: _For testing \(H_{1},\ldots,H_{s}\), suppose a given incoherent multiple testing procedure controls the FWER at level \(\alpha\). Then, it can be replaced by a coherent procedure that controls the FWER at level \(\alpha\) and rejects at least as many hypotheses as the given incoherent one._

Proof. Suppose \(H_{i}\) is rejected based on a rejection region \(R_{i}\). Define a new procedure so that \(H_{i}\) is rejected based on the rejection region

\[R^{\prime}_{i}=\bigcup_{j:\omega_{j}\supseteq\omega_{i}}R_{j}. \tag{9.32}\]

Note that \(R_{i}\subseteq R^{\prime}_{i}\), so that the new procedure rejects \(H_{i}\) if the original one does. Moreover, the new procedure is coherent in the sense that if \(H_{j}\) is rejected, then so is any \(H_{i}\) for which \(\omega_{i}\subset\omega_{j}\). To see why, observe that (9.32) implies that \(R^{\prime}_{j}\subseteq R^{\prime}_{i}\) whenever \(\omega_{i}\subset\omega_{j}\).

If the original procedure controls the FWER, so does the new one. Indeed, suppose a false rejection is made by the new procedure, so that \(X\in R^{\prime}_{i}\) for some \(i\) with \(P\in\omega_{i}\). Then, it must be the case that \(X\in R_{j}\) for some \(j\) such that \(\omega_{j}\supseteq\omega_{i}\). Since \(P\in\omega_{i}\), it follows that \(X\in R_{j}\) for some \(j\) such that \(P\in\omega_{j}\). Thus, the original procedure also makes a false rejection. Control of the FWER therefore follows from the assumption that the original procedure controls the FWER.

Theorems 9.2.1 and 9.2.2 imply that we can restrict attention to the construction of multiple testing procedures by applying the closure method. However, not all methods generated by applying the closure method are _consonant_, as defined next.

**Definition 9.2.2**: For testing \(H_{1}\),..., \(H_{s}\), suppose that whenever a multiple testing procedure rejects an intersection hypothesis \(H_{K}\), it also rejects some \(H_{j}\) with \(j\in K\). Then, such a procedure is called _consonant_.

A method that is not consonant is called _dissonant_. It may be difficult to interpret results when using a dissonant method. Consider the following situation, where the use of a drug is considered for both lowering blood pressure as well as reducing the risk of heart attack. Suppose \(H_{1}\) specifies the drug is no more effective than the placebo for reduction of blood pressure and \(H_{2}\) postulates the drug is no more effective than the placebo for reduction of heart attack risk. Assume \(H_{3}=H_{\{1,2\}}\) is the joint intersection hypothesis. If \(H_{3}\) is rejected, but the statistician cannot reject either \(H_{1}\) or \(H_{2}\), then one cannot claim the drug is beneficial for a particular indication, even though one might believe there is some overall beneficial effect. Certainly, the lack of consonance makes interpretation awkward.

On its own, dissonance of a procedure is not logically inconsistent in the same way that incoherence is. Indeed, failure to reject a hypothesis should not be interpreted as its acceptance. However, dissonance is undesirable in that it may result in decreased ability to reject false null hypotheses.

**Example 9.2.6**: **(Two-sided Normal Means)**: Suppose \(X_{1}\) and \(X_{2}\) are independent with \(X_{i}\sim N(\theta_{i},1)\). The parameter space \(\Omega\) for \(\theta=(\theta_{1},\theta_{2})^{\top}\) is the entire plane. Suppose \(H_{i}\) specifies \(\theta_{i}=0\). To apply the closure method, we must specify tests of \(H_{1}\), \(H_{2}\) and the intersection hypothesis \(H_{\{1,2\}}\). For \(H_{i}\), use the uniformly most powerful unbiased (UMPU) level \(\alpha\) test, which rejects \(H_{i}\) if \(|X_{i}|>z_{1-\frac{\alpha}{2}}\). For \(H_{\{1,2\}}\), consider the following two choices.

(i) _The UMPI test._ This test rejects \(H_{\{1,2\}}\) if if (\(X_{1}\), \(X_{2}\)) falls in the rejection region \(R_{1,2}(\alpha)\) given by

\[R_{1,2}(\alpha)=\left\{(X_{1},X_{2}):\ X_{1}^{2}+X_{2}^{2}>c_{2}(1-\alpha) \right\}\,,\]

where \(c_{d}(1-\alpha)\) denotes the \(1-\alpha\) quantile of the Chi-squared distribution with \(d\) degrees of freedom.

(ii) _Stepdown test based on maximum._ Reject \(H_{\{1,2\}}\) if

\[\max\bigl{(}|X_{1}|,|X_{2}|\bigr{)}>m_{2}(1-\alpha)\, \tag{9.33}\]

where \(m_{s}(1-\alpha)\) is the \(1-\alpha\) quantile of the distribution of \(\max\bigl{(}|Z_{1}|,\ldots,|Z_{s}|\bigr{)}\) when the \(Z_{i}\) are i.i.d. \(N(0,1)\).

In both cases, the closure method begins by testing \(H_{\{1,2\}}\). If \(H_{\{1,2\}}\) is retained, then there are no rejections. However, if \(H_{\{1,2\}}\) is rejected, then \(H_{i}\) is rejected if \(|X_{i}|>z_{1-\frac{\alpha}{2}}\). Since \(m_{2}(1-\alpha)>z_{1-\frac{1}{2}}\), it follows that rejection of \(H_{\{1,2\}}\) using (ii) results in a consonant procedure. The same is not true when using (i). For example, suppose \(\alpha=0.05\), so that \(c_{2}(0.95)=5.99\). If \(X_{1}=1.9\) and \(X_{2}=-1.8\), then \(X_{1}^{2}+X_{2}^{2}=6.85\), so \(H_{\{1,2\}}\) is rejected but neither \(H_{1}\) nor \(H_{2}\) is rejected. The situation is depicted in Figure 9.1. The intersection test of \(H_{\{1,2\}}\) has rejection region outside the red circle; in order to claim a rejection of either \(H_{1}\) or \(H_{2}\), the observation must lie outside the blue square. The point in the lower right falls outside the red circle but inside the blue square, so there are no rejections.

One cannot conclude the intersection test based on (ii) is preferred merely because it is consonant. Note, however, that the procedure using (i) can be improved if the goal is to make correct decisions about \(H_{1}\) and \(H_{2}\). To see how, we may remove from procedure (i) points (of dissonance) in the rejection region for testing the intersection hypothesis \(H_{\{1,2\}}\) that do not allow for rejection of either \(H_{1}\) or \(H_{2}\). We can instead include other points in the rejection region that satisfy the constraint that the overall procedure is consonant, while still maintaining error control. In this example, this requires the test of the intersection hypothesis \(H_{\{1,2\}}\) to have a rejection region which lies entirely in

\[\left\{(X_{1},\,X_{2}):\ \max(|X_{1}|,|X_{2}|)>z_{1-\frac{\alpha}{2}}\right\}\,.\]

Indeed, any intersection test satisfying this constraint results in a consonant procedure when applying the closure method. A specific way to improve upon (i) is as follows. Consider a rejection region \(R^{\prime}_{1,2}(\alpha)\) of \(H_{\{1,2\}}\) of the form

\[R^{\prime}_{1,2}(\alpha)=\left\{(X_{1},\,X_{2}):\ X_{1}^{2}+X_{2}^{2}>c^{ \prime}_{2}(1-\alpha)\,\,\max\bigl{(}|X_{1}|,|X_{2}|\bigr{)}>z_{1-\frac{ \alpha}{2}}\right\}\,,\]

where the critical value \(c^{\prime}_{2}(1-\alpha)\) is chosen so that

\[P_{0,0}\left\{R^{\prime}_{1,2}(\alpha)\right\}=\alpha\.\]

Figure 9.1: Rejection regions of two intersections tests

Then, \(c_{2}^{\prime}(1-\alpha)<c_{2}(1-\alpha)\), and the resulting procedure is consonant. Furthermore, for any \(i\),

\[P_{\theta_{1},\theta_{2}}\left\{\text{reject }H_{i}\text{ using }R_{1,2}(\alpha)\right\}<P_{\theta_{1},\theta_{2}}\left\{\text{reject }H_{i}\text{ using }R_{1,2}^{\prime}(\alpha)\right\}\,.\]

Thus, the new consonant procedure has uniformly greater power at detecting a false null hypothesis \(H_{i}\). So, imposing consonance not only makes interpretation easier, but it also results in improved ability to detect false null hypotheses.

In the previous example, if the family of hypotheses includes not just \(H_{1}\) and \(H_{2}\), but also \(H_{\{1,2\}}\), then one could not claim that the new intersection test results in a uniform improvement over the original test. The improvement is only possible when intersection tests are used as a device in applying the closure method, and the hypotheses \(H_{1}\) and \(H_{2}\) are _elementary_ in the following sense. A hypothesis \(H_{i}\) is _elementary_ among a family of hypotheses if there exists no (other) \(H_{j}\) in the family with \(\omega_{i}\subset\omega_{j}\). For families of hypotheses, all of which are elementary, there is no need to consider dissonant procedures when applying the closure method because any dissonant procedure can be replaced by a consonant one that reaches the same decisions about the hypotheses in question. As in the example, the main idea is that when applying the closure method, one should construct intersection hypotheses to form the closed family in a consonant manner. In other words, the rejection region of an intersection hypotheses should be chosen so that points in the rejection region lead to the rejection of at least one elementary hypothesis.

**Theorem 9.2.3**: _Consider testing \(H_{1}\),..., \(H_{s}\) by a closed testing procedure. Assume all hypotheses are elementary. If the given procedure is dissonant among the closed family, it can be replaced by a consonant procedure that reaches the same decisions for the individual elementary hypotheses as the original procedure. Moreover, the new procedure controls the FWER if the original method does. Thus, there is no advantage in considering dissonant procedures._

A proof of Theorem 9.2.3 can be found in Romano et al. (2011). Note that the theorem claims that any dissonant procedure can be replaced with a consonant procedure that leads to the same decisions as the dissonant procedure. As in Example 9.2.6, however, one may strictly improve upon the dissonant procedure by removing points of dissonance from the rejection regions of the intersection hypotheses and adding to these rejection regions points that satisfy the constraint that the overall procedure be consonant.

### False Discovery Rate and Other Generalized Error Rates

#### Introduction to Various Error Rates

When the number of hypotheses tested is very large, control of the FWER at level \(\alpha\) becomes so stringent that individual departures from any null hypothesis have little chance of being detected, and it may be unreasonable to control the probability of even one false rejection. Several useful error rates that are commonly studied and used for large-scale multiple testing are presented in this section. These rates are referred to as Type 1 error rates in that they specifically are designed to offer protection against false discoveries. The term _false discovery_ refers to the rejection of a true null hypothesis. Analogously, the term _discovery_ refers to the rejection of any null hypothesis and the term _true discovery_ refers to the rejection of a false null hypothesis.

We next summarize some of these error rates and then develop some methods that are designed with these error rates in mind.

(i) The _generalized familywise error rate_ or _k-familywise error rate_ (_k_-FWER) is concerned with the probability of \(k\) or more false discoveries, where \(k\) is some positive integer:

\[k\text{-FWER}_{P} = P\left\{ {\text{reject at least }k\text{ hypotheses }H_{i}:i\in I(P)} \right\}\,.\]

Obviously, the special case \(k\) = 1 simplifies to the traditional FWER.

(ii) A related measure of error control is the average number of false discoveries, also known as the _per-family error rate_ (PFER). To this end, let \(F\) denote the number of false rejections made by a given multiple testing procedure. Then, \({\text{PFER}_{P}} = E_{P}(F)\), where the concern now is to ensure \({\text{PFER}_{P}} \leq \lambda\) for some \(\lambda > 0\).

(iii) Instead of error rates based only on the number of false discoveries, one can consider error rates based on the fraction of false discoveries (among all discoveries). Let \(R\) denote the total number of rejections. Then the _false discovery proportion_ (FDP) is defined as

\[{\text{FDP}} = (F/R) \cdot I\{R > 0\}\,\]

where _I_{*} denotes the indicator function. One then is concerned with the probability of the FDP exceeding a small, pre-specified proportion: _P_{FDP > _g_}, for some \(\gamma \in [0,1)\). The special choice of \(\gamma = 0\) simplifies to the traditional FWER.

(iv) Finally, the _false discovery rate_ (FDR) is given by the expected value of the FDP. Namely,

\[{\text{FDR}}_{P} = E_{P}({\text{FDP}})\,\]

where the concern now is to ensure \({\text{FDR}}_{P} \leq \gamma\) for some \(\gamma \in [0,1)\).

The \(k\)-FWER, PFER, FDP, and FDR can all be coined _generalized error rates_ in the sense that they relax and generalize the FWER. Although they are distinct, they share a common philosophy: by relaxing the FWER criterion and allowing for a small number (\(k\)-FWER), a small expected number (PFER), a small proportion (FDP), or a small expected proportion (FDR) of false discoveries, one is afforded greater power in return.

Having defined the various error rates, we next discuss what is meant by control of these error rates and what sort of conclusions one is afforded when applying corresponding multiple testing procedures to a set of data.

Control of the \(k\)-FWER means that, for a given significance level \(\alpha\),

\[k\mbox{-}\mbox{FWER}_{P}\leq\alpha\;\;\;\mbox{for any $P$}\;. \tag{9.34}\]

Control of the PFER means that, for a given \(\lambda>0\) (typically a positive integer),

\[\mbox{PFER}_{P}\leq\lambda\;\;\;\;\mbox{for any $P$}. \tag{9.35}\]

Control of the FDP means that, for a given significance level \(\alpha\) and for a given proportion \(\gamma\in[0,1)\),

\[P\{\mbox{FDP}\geq\gamma\}\leq\alpha\;\;\;\;\mbox{for any $P$}\;. \tag{9.36}\]

As a special case, if we take \(\alpha=0.5\), then FDP control reduces to the median of the distribution of the FDP being \(\leq\gamma\), or

\[\mbox{med}_{P}(\mbox{FDP})\leq\gamma\;.\]

Finally, control of the FDR means that, for a given proportion \(\gamma\in[0,1)\),

\[\mbox{FDR}_{P}\leq\gamma\;\;\;\;\mbox{for any $P$}\;. \tag{9.37}\]

In practice, one may settle for approximate error control based on large sample theory. In fact, individual \(p\)-values of marginal tests often rely on large sample arguments to be discussed later. Even so, what conclusions can be drawn when the various error rates are controlled?

Control of the \(k\)-FWER allows one to be \(100(1-\alpha)\)% confident that there are at most \(k-1\) false discoveries among the rejected hypotheses. In particular, for \(k=1\), one can be \(100(1-\alpha)\)% confident that there are no false discoveries at all.

On the other hand, control of the PFER does not really allow one to draw any meaningful conclusion about the realized value of \(F\) at all (except for some very crude bounds, based on Markov's inequality). The general reason is that, by controlling an expected value, one can conclude little about the realization of the underlying random variable.

Control of the FDP allows one to be \(100(1-\alpha)\)% confident that the proportion of false discoveries among all rejected hypotheses is at most \(\gamma\). Or, in other words, that the realized FDP is at most \(\gamma\).

On the other hand, control of the FDR does not really allow one to draw any meaningful conclusion about the realized FDP at all. The general reason is, again, that by controlling an expected value, one can conclude little about the realization of the underlying random variable. Unfortunately, this important point is not always appreciated by researchers when applying multiple testing procedures that control the FDR. Instead, by a law of large numbers, one might conclude that the _average_ realized FDP--when FDR control is repeatedly applied to large number of data sets--will be at most \(\gamma\) (plus some small \(\varepsilon\)).

The balance of this section is to develop some procedures for multiple testing problems under such error rate control restrictions.

#### False Discovery Rate

A radical weakening of the FWER was proposed by Benjamini and Hochberg (1995), who suggested using the _false discovery rate_ as a measure of Type 1 error control.

**Definition 9.3.1**: For a given multiple testing decision rule, let \(F\) denote the number of rejections of true null hypotheses and let \(R\) denote the total number of rejections. The _false discovery proportion_ or FDP is defined to be \(F/R\) if \(R>0\) and equal to 0 if \(R=0\). The _false discovery rate_ or FDR is defined to be

\[\text{FDR}=\text{FDR}_{P}=E_{P}(\text{FDP})\.\]

Now, the goal is to construct procedures that satisfy

\[\text{FDR}=\text{FDR}_{P}\leq\alpha\]

for a given level \(\alpha\) and for all \(P\) in a given family of probability distributions \(P\) generating the data.

Notice that, if all null hypotheses are true, then \(F\), the total number of false rejections or false discoveries is equal to \(R\). So, \(\text{FDP}=1\) if there are any rejections, in which case the FDR is the probability of any false rejections, which is just the FWER. However, if some null hypotheses are false, then \(\text{FDR}\leq\text{FWER}\) and usually \(\text{FDR}<\text{FWER}\). In general, the FDR is more liberal (in the sense of permitting more rejections) than the FWER.

The Benjamini-Hochberg (BH) procedure (1995) is a stepup procedure of the form described in Section 9.1.3, where the \(\alpha_{i}\) in (9.20) are specified by \(\alpha_{i}=\alpha i/s\). In the following result, FDR control of the BH procedure is proved if the _p_-values are assumed mutually independent. In the statement of the theorem, recall that \(I(P)\) denotes the indices of the true null hypotheses and \(|I(P)|\) denotes the number of true null hypotheses.

**Theorem 9.3.1**: **(FDR Control of the Benjamini-Hochberg procedure)** _Consider testing \(H_{1}\),..., \(H_{s}\) based on \(p\)-values \(\hat{p}_{1}\),..., \(\hat{p}_{s}\), where the \(p\)-values corresponding to true null hypotheses satisfy the requirement (9.5). Assume the \(p\)-values are mutually independent. Then, the stepup procedure based on critical values \(\alpha_{i}=\alpha i/s\) satisfies_

\[FDR_{P}\leq\frac{|I(P)|}{s}\alpha\leq\alpha. \tag{9.38}\]

_The first inequality is an equality if the \(p\)-values corresponding to true null hypotheses are exactly distributed as uniform on \([0,1]\)._

Proof. Let \(R\) denote the number of rejections of the BH procedure and \(F\) the number of false rejections. Also, for any \(1\leq i\leq s\), let \(R_{i}^{*}\) denote the number of rejections when applying the BH procedure to the \(p\)-values \(\hat{p}_{1}\),..., \(\hat{p}_{s}\) with \(\hat{p}_{i}\) replaced by zero. Note that

FDR \[= E\left\{\frac{F}{R\lor 1}\right\}=\sum_{i\in I}\sum_{r=1}^{s} \frac{1}{r}P\{\hat{p}_{i}\leq r\alpha/s,\,R=r\}\] (9.39) \[= \sum_{i\in I}\sum_{r=1}^{s}\frac{1}{r}P\{\hat{p}_{i}\leq r\alpha/ s,\,R_{i}^{*}=r\}\] \[= \sum_{i\in I}\sum_{r=1}^{s}\frac{1}{r}P\{\hat{p}_{i}\leq r\alpha/ s\}P\{R_{i}^{*}=r\}\] \[\leq \sum_{i\in I}\sum_{r=1}^{s}\frac{\alpha}{s}P\{R_{i}^{*}=r\}=\frac{ |I|}{s}\alpha\leq\alpha.\]

Here, the second equality follows from the fact that \(F=\sum_{i\in I}I\{\hat{p}_{i}\leq R\alpha/s\}\). The third equality follows from the fact that \(R_{i}^{*}=R\) when \(\hat{p}_{i}\leq R\alpha/s\). The fourth equality follows from the assumption of independence of the \(p\)-values and \(R_{i}^{*}\) is a function of the \(\hat{p}_{j}\)'s excluding \(\hat{p}_{i}\). The first inequality follows from the assumption that the true null \(p\)-values are stochastically larger than or equal to \(U(0,\,1)\). Of course, when the true null \(p\)-values are exactly distributed as \(U(0,\,1)\), the inequality becomes an equality. The last equality follows from the fact that \(R_{i}^{*}\geq 1\).

Theorem 9.3.1 is limited by the assumption of independence. However, Benjamini and Yekutieli (2001) have shown that the result can be extended to cases of positive dependence. To define the notion of positive regression dependence on a subset (PRDS), we first recall the definition of an increasing set (previously used in Section 8.7).

**Definition 9.3.2**: A set \(D\subseteq\mathbb{R}^{s}\) is called monotone increasing (or just increasing) if \(x\in D\) and \(y\geq x\) implies \(y\in D\). Here, the inequality between vectors \(x\) and \(y\) is interpreted coordinate-wise.

Similarly, \(D\) is decreasing if \(x\in D\) and \(y\leq x\) implies \(y\in D\). Then, \(D\) is increasing if and only if \(D^{c}\) is decreasing.

**Definition 9.3.3**: The random variables \((X_{1},\ldots,X_{s})\) are PRDS on \(I\) if for any increasing \(D\) and each \(i\in I\),

\[P\{(X_{1},\ldots,X_{s})\in D|X_{i}=x\} \tag{9.40}\]

is nondecreasing in \(x\).

**Example 9.3.1**: **(Multivariate Normal)** Suppose \(X=(X_{1},\ldots,X_{s})^{\top}\) is multivariate normal with mean vector \(\mu\) and covariance matrix \(\Sigma\). Assume all entries \(\Sigma_{i,j}\) are nonnegative. Let \(I\) be the set of indices for which \(\mu_{i}=0\). Then, the distribution of \(X\) is PRDS on \(I\). To see why, suppose \(\mu_{1}=0\) and let \(Y=(X_{2},\ldots,X_{s})^{\top}\). In order to obtain the conditional distribution of \(Y\) given \(X_{1}\), partition \(\mu\) and \(\Sigma\) as

\[\mu=\begin{pmatrix}\mu_{1}\\ \mu_{(-1)}\end{pmatrix}\]

and

\[\Sigma=\begin{pmatrix}\Sigma_{1,1}&\Sigma_{1,2}\\ \Sigma_{2,1}&\Sigma_{2,2}\end{pmatrix}\;,\]

where \(\Sigma_{2,2}\) is the covariance matrix of \(Y\). Then, the conditional distribution of \(Y\) given \(X_{1}=x\) is normal with mean

\[E(Y|X_{1}=x)=\mu_{(-1)}+\Sigma_{2,1}\Sigma_{1,1}^{-1}(x-\mu_{1})\]

and covariance matrix \(\Sigma_{2,2}-\Sigma_{2,1}\Sigma_{1,1}^{-1}\Sigma_{1,2}\). Since this conditional covariance matrix does not depend on \(x\) and the conditional mean is nondecreasing in \(x\) (due to the assumption on \(\Sigma\)), it follows that, for any nondecreasing function \(f\), \(E[f(X)|X_{1}=x]\) is nondecreasing in \(x\). The same argument applies to any \(i\in I\), and so the property of PRDS on \(I\) holds.

The next result assumes the joint distribution of \(p\)-values is PRDS on the set \(I=I(P)\) of true null hypotheses. The PRDS property implies (Problem 9.19) that the \(p\)-values are positively dependent on the set \(I\) of true null hypotheses in the sense that, for any increasing \(D\),

\[P\{(\hat{p}_{1},\ldots,\hat{p}_{s})\in D|\hat{p}_{i}\leq u\} \tag{9.41}\]is nondecreasing in \(u\) for any \(i\in I\). Note that the quantities in (9.40) and (9.41) are nonincreasing in \(x\) and \(u\), respectively, if \(D\) is decreasing.

**Theorem 9.3.2**: **(Benjamini and Yekutieli 2001)** _Consider testing \(H_{1},\ldots,H_{s}\) based on \(p\)-values \(\hat{p}_{1},\ldots,\hat{p}_{s}\), where the \(p\)-values corresponding to true null hypotheses satisfy the requirement (9.5). If the joint distribution of the statistics (or joint distribution of the \(p\)-values) is PRDS on the set of true nulls \(I(P)\), then the BH procedure controls the FDR at level \(\frac{|I(P|}{s}\alpha\)._

Proof. As before, let \(R\) denote the total number of rejections of the BH procedure. Then,

\[\mbox{FDR}=\sum_{i\in I}\sum_{r=1}^{s}\frac{1}{r}P\{\hat{p}_{i}\leq r \alpha/s,\,R=r\}\] \[\leq\sum_{i\in I}\sum_{r=1}^{s}\frac{\alpha}{s}P\{R=r|\hat{p}_{i} \leq r\alpha/s\}\] \[=\sum_{i\in I}\sum_{r=1}^{s}\frac{\alpha}{s}\left[P\{R\geq r|\hat {p}_{i}\leq r\alpha/s\}-P\{R\geq r+1|\hat{p}_{i}\leq r\alpha/s\}\right]\] \[\leq\sum_{i\in I}\sum_{r=1}^{s}\frac{\alpha}{s}\left[P\{R\geq r| \hat{p}_{i}\leq r\alpha/s\}-P\{R\geq r+1|\hat{p}_{i}\leq(r+1)\alpha/s\}\right]\] \[=\frac{\alpha}{s}\sum_{i\in I}\left[P\{R\geq 1|\hat{p}_{i}\leq \alpha/s\}-P\{R\geq s+1|\hat{p}_{i}\leq(s+1)\alpha/s\}\right]\] \[=\frac{|I(P)|\alpha}{s}\leq\alpha.\]

Here, the first inequality follows from the assumption that the true null \(p\)-values are stochastically larger than or equal to \(U(0,1)\) and the second follows from the assumption of PRDS and the fact that the positive dependence condition (9.41) holds. (Note that the event \(\{R\geq r+1\}\) is a decreasing set because if the \(p\)-values increase, the number of rejections can only decrease.) The last equality follows from the fact that \(R\geq 1\) when \(\hat{p}_{i}\leq\alpha/s\) and \(R\leq s\).

We note the following corollary, which extends Simes' identity to positive dependence.

**Corollary 9.3.1**: **(Simes' Inequality)** _Suppose that \(p\)-values \(\hat{p}_{1},\ldots,\hat{p}_{s}\) satisfy (9.5). Further assume the \(p\)-values are positively dependent in the sense (9.41) holds for all \(i=1,\ldots,s\). Order the \(p\)-values_

\[\hat{p}_{(1)}\leq\cdots\leq\hat{p}_{(s)}\]_and let \(E_{s}=E_{s}(\alpha)\) be the event defined in (9.30). Then,_

\[P\left\{E_{s}(\alpha)\right\}\leq\alpha. \tag{9.42}\]

In fact, there are many joint distributions of positively dependent variables for which Simes' inequality is known to hold. In particular, Sarkar and Chang (1997) and Sarkar (1998) have shown that the Simes' inequality holds for the family of distributions which is characterized by the multivariate positive of order two condition, as well as some other important distributions.

When no assumptions are made about the joint dependence structure of the \(p\)-values, Benjamini and Yekutieli (2001) modify the BH procedure as follows. Define

\[C_{s}=\sum_{j=1}^{s}1/j. \tag{9.43}\]

As is well known, \(C_{s}\approx\log(s+0.5)+\zeta_{E}\), with \(\zeta_{E}\approx 0.5772156649\) known as Euler's constant. Now, apply a stepup procedure with constants

\[\alpha_{i}=\frac{i\alpha}{sC_{s}}\.\]

The penalty of course is that one must use much smaller critical constants since the original BH critical values must all be divided by \(C_{s}\). When no assumptions are made about the dependence structure of the underlying \(p\)-values, the BY procedure of Benjamini and Yekutieli (2001) controls the FDR.

**Theorem 9.3.3**: **(FDR Control of the BY procedure)** _Consider the problem of testing \(H_{1}\),..., \(H_{s}\) based on \(p\)-values \(\hat{p}_{1}\),..., \(\hat{p}_{s}\), where the \(p\)-values corresponding to true null hypotheses satisfy the requirement (9.5). Then, the stepup procedure based on critical constants \(\alpha_{i}=\frac{i\alpha}{sC_{s}}\) satisfies_

\[\mathrm{FDR}\leq\frac{|I(P)|}{s}\alpha\leq\alpha\, \tag{9.44}\]

_where \(C_{s}=\sum_{j=1}^{s}1/j\)._

Proof. For purposes of the proof, let \(R\) denote the total number of rejections and \(F\) the number of falsely rejected null hypotheses of the BY procedure. Note that \[\text{FDR} = E\left\{\frac{F}{R\lor 1}\right\}=\sum_{i\in I}\sum_{r=1}^{s}\frac{1}{r}P\{\hat{p}_{i}\leq\alpha_{r},\,R=r\}\] \[\leq\sum_{i\in I}\sum_{r=1}^{s}\frac{1}{r}P\{\hat{p}_{i}\leq \alpha_{r},\,R\geq r\}-\sum_{i\in I}\sum_{r=1}^{s}\frac{1}{r+1}P\{\hat{p}_{i} \leq\alpha_{r},\,R\geq r+1\}\] \[=\sum_{i\in I}\sum_{r=1}^{s}\frac{1}{r}P\{\hat{p}_{i}\leq\alpha_{r},\,R \geq r\}-\sum_{i\in I}\sum_{r=2}^{s}\frac{1}{r}P\{\hat{p}_{i}\leq\alpha_{r-1},\,R\geq r\}\] \[=\sum_{i\in I}P\{\hat{p}_{i}\leq\alpha_{1},\,R\geq 1\}+\sum_{i\in I}\sum_{r=2}^{s}\frac{1}{r}P\{\alpha_{r-1}<\hat{p}_{i}\leq\alpha_{r},\,R\geq r\}\] \[\leq\sum_{i\in I}P\{\hat{p}_{i}\leq\alpha_{1}\}+\sum_{i\in I}\sum_{r= 2}^{s}\frac{P\{\hat{p}_{i}\leq\alpha_{r}\}-P\{\hat{p}_{i}\leq\alpha_{r-1}\}}{r}\] \[=\sum_{i\in I}\sum_{r=1}^{s}\frac{P\{\hat{p}_{i}\leq\alpha_{r}\}}{r}- \sum_{i\in I}\sum_{r=1}^{s-1}\frac{P\{\hat{p}_{i}\leq\alpha_{r}\}}{r+1}\] \[\leq\frac{|I(P)|\alpha_{s}}{s}+\sum_{i\in I}\sum_{r=1}^{s-1}\frac{P \{\hat{p}_{i}\leq\alpha_{r}\}}{r(r+1)}\] \[\leq\frac{|I(P)|\alpha_{s}}{s}+|I(P)|\sum_{r=1}^{s-1}\frac{\alpha_{r}}{r(r+1)}\] \[=\frac{|I(P)|\alpha}{sC_{s}}\left(1+\sum_{r=1}^{s-1}\frac{1}{r+1} \right)=\frac{|I(P)|\alpha}{s}\leq\alpha\.\]

The first inequality follows from the fact that \(1/r>1/(r+1)\) for \(r=1,\,\ldots,s.\) The third equality follows by a change of index and the fact that \(R\leq s.\) The third and fourth inequalities follow from the assumption that the true null \(p\)-values satisfy (9.5).

#### Control of the \(k\)-Fwer

The Bonferroni methods rejects any \(H_{i}\) whose corresponding \(p\)-value satisfies \(\hat{p}_{i}\leq\alpha/s.\) For general \(k,\) control of the \(k\)-FWER allows one to increase the critical value from \(\alpha/s\) to \(k\alpha/s,\) which makes it much easier to detect false null hypotheses. Without any dependence assumptions among the \(p\)-values, such a simple modification results in control of the \(k\)-FWER.

**Theorem 9.3.4**: _For testing \(H_{i}:P\in\omega_{i},\ i=1,\,\ldots,s,\) suppose \(\hat{p}_{i}\) satisfies (9.5). Consider the procedure that rejects any \(H_{i}\) for which \(\hat{p}_{i}\leq k\alpha/s.\) (i) This procedure controls the \(k\)-FWER at level \(\alpha\) in the sense that (9.34) holds. Actually,_\[k\text{-}FWER_{P}\leq\frac{|I(P)|}{s}\alpha\leq\alpha\;.\]

_(ii) For this procedure, the inequality in (9.34) is sharp in the sense that there exists a joint distribution of \(p\)-values \((\hat{p}_{1},\ldots,\hat{p}_{s})\) for which equality is attained in (9.34)._

Proof.: (i) Fix any \(P\) and let \(I(P)\) denote the indices of the true null hypotheses, with \(|I(P)|\) denoting the number of true null hypotheses. Let \(F\) be the number of false rejections. Applying Markov's inequality yields

\[P\{F\geq k\}\leq\frac{E(F)}{k}=\frac{E\left[\sum_{i\in I(P)}I\{\hat{p}_{i}\leq k \alpha/s\}\right]}{k}=\sum_{i\in I(P)}\frac{P\{\hat{p}_{i}\leq k\alpha/s\}}{k}\]

\[\leq\sum_{i\in I(P)}\frac{k\alpha/s}{k}=|I(P)|\frac{\alpha}{s}\leq\alpha\;.\]

To prove (ii), construct a joint distribution of \(p\)-values \((\hat{p}_{1},\ldots,\hat{p}_{s})\) as follows. First, choose \(k\) indices at random without replacement from \(\{1,\ldots,s\}\). Call them \(J\). Conditional on \(i\in J\), let \(\hat{p}_{i}=U_{1}\), where \(U_{1}\) is uniform on \((0,k/s)\); that is, \(U_{1}\sim U(0,k/s)\). But conditional on \(i\notin J\), let \(\hat{p}_{i}=U_{2}\), where \(U_{2}\) is independent of \(U_{1}\) and \(U_{2}\sim U(k/s,\,1)\). Then, for any \(i\), \(\hat{p}_{i}\) has the mixture distribution

\[\hat{p}_{i}\sim\frac{k}{s}U(0,\,\frac{k}{s})+(1-\frac{k}{s})U(\frac{k}{s},\,1) \sim U(0,1)\;.\]

Indeed, if \(u\leq k/s\),

\[P\{\hat{p}_{i}\leq u\}=P\{i\in J\}\cdot P\{U_{1}\leq u\}=\frac{k}{s}\cdot\frac {u}{k/s}=u\]

and if \(u\geq k/s\),

\[P\{\hat{p}_{i}\leq u\}=P\{i\in J\}\cdot 1+P\{i\notin J\}\cdot P\{U_{2}\leq u\}= \frac{k}{s}+(1-\frac{k}{s})\cdot\frac{u-\frac{k}{s}}{1-\frac{k}{s}}=u\;.\]

Therefore, the marginal distributions of all the \(\hat{p}_{i}\) are uniform on \((0,\,1)\). By construction, exactly \(k\) of them are \(\leq k/s\) (with probability one). The probability that these are all \(\leq\alpha k/s\) is

\[P\left\{U_{1}\leq\frac{\alpha k}{s}\right\}=\frac{\alpha k/s}{k/s}=\alpha\;.\]

The above single-step method using the critical value \(k\alpha/s\) can be viewed a generalized Bonferroni method. Similar to the Bonferroni method, a stepdown improvement is possible. Specifically, consider the stepdown procedure described in (9.18), where \(\alpha_{i}=k\alpha/s\) if \(i\leq k\) and \[\alpha_{i}=\frac{k\alpha}{s+k-i}\ \ \ \mbox{for}\ i>k. \tag{9.45}\]

Of course, the \(\alpha_{i}\) depend on \(s\) and \(k\), but we suppress this dependence in the notation. Such a method may be viewed as a generalized Holm method.

**Theorem 9.3.5**: _For testing \(H_{i}:P\in\omega_{i},i=1,\ldots,s\), suppose \(\hat{p}_{i}\) satisfies (9.5). The stepdown procedure described in (9.18) with \(\alpha_{i}\) given by (9.45) controls the \(k\)-FWER at level \(\alpha\)._

Proof. As before, fix any \(P\) and let \(I(P)\) be the indices of the true null hypotheses. Assume \(|I(P)|\geq k\) or there is nothing to prove. Among the the \(|I(P)|\) true null hypotheses, call the ordered \(p\)-values

\[\hat{q}_{(1)}\leq\cdots\leq\hat{q}_{|I(P)|}\.\]

Also let

\[\hat{p}_{(1)}\leq\cdots\leq\hat{p}_{(s)}\]

be the ordered \(p\)-values among all \(s\) hypotheses. Let \(j\) be the smallest (random) index where at least \(k\) true null hypotheses have been rejected, or \(\hat{p}_{(j)}=\hat{q}_{(k)}\). Such a \(j\) must satisfy

\[k\leq j\leq s-|I(P)|+k \tag{9.46}\]

because the largest possible value for \(j\) occurs when all the smallest \(p\)-values correspond to the \(s-|I(P)|\) false null hypotheses and the next \(|I(P)|\)\(p\)-values correspond to the true null hypotheses. Then, the generalized Holm procedure commits at least \(k\) false rejections if and only if

\[\hat{p}_{(1)}\leq\alpha_{1},\,\hat{p}_{(2)}\leq\alpha_{2},\ldots,\,\hat{p}_{( j)}\leq\alpha_{j}\,\]

which implies that

\[\hat{q}_{(k)}=\hat{p}_{(j)}\leq\alpha_{j}=\frac{k\alpha}{s+k-j}\.\]

But by (9.46),

\[\frac{k\alpha}{s+k-j}\leq\frac{k\alpha}{|I(P)|}\.\]

Therefore, the probability of at least \(k\) false rejections satisfies

\[k\mbox{-FWER}_{P}\leq P\ \left\{\hat{q}_{(k)}\leq\frac{k\alpha}{|I(P)|} \right\}\.\]

The right side is the probability that the \(k\)th largest among \(|I(P)|\) null \(p\)-values is \(\leq k\alpha/|I(P)|\), which by Theorem 9.3.4(i) is bounded by \(\alpha\).

Note that one can always reject the hypotheses corresponding to the smallest \(k\,-\,1\) of the \(p\)-values without violating \(k\)-FWER control, since there is no penalty for any procedure which rejects \(k\,-\,1\) hypotheses. Such a method would seem arbitrary. It seems more appropriate to only consider stepdown procedures whose corresponding \(\alpha_{i}\) are monotone nondecreasing. To ensure monotonicity, the above stepdown procedure uses \(\alpha_{i}=k\alpha/s\) for \(i=1,\ldots,k\). Even if one were to adopt the more liberal strategy of always rejecting the hypotheses corresponding to the first \(k\,-\,1\) hypotheses, one could still only reject \(k\) or more hypotheses if \(\hat{p}_{(k)}\leq k\alpha/s\), which is also true for the specific procedure of Theorem 9.3.5.

As with the Holm method, one may consider whether it is possible to improve the procedure further by increasing the critical values \(\alpha_{1}\), \(\alpha_{2}\), \(\ldots\) without violating control of the \(k\)-FWER (9.34). As just mentioned, we can always increase \(\alpha_{i}\) to \(1\) for \(i\,<\,k\). A more interesting question is whether we can increase \(\alpha_{i}\) for \(i\geq k\). In fact, this is not possible; that is, for any \(i\geq k\), there exists a joint distribution of \(p\)-values satisfying

\[P\{\hat{p}_{(1)}\leq\alpha_{1},\,\hat{p}_{(2)}\leq\alpha_{2},\,\ldots,\,\hat{p }_{(i-1)}\leq\alpha_{i-1},\,\hat{p}_{(i)}\leq\alpha_{i}\}=\alpha\;. \tag{9.47}\]

Moreover, changing \(\alpha_{i}\) to \(\beta_{i}>\alpha_{i}\) results in the right-hand side being \(>\alpha\). Thus, with \(i\geq k\), one cannot increase \(\alpha_{i}\) without violating the \(k\)-FWER. Then, having picked \(\alpha_{1},\ldots,\alpha_{k},\ldots,\alpha_{i-1}\), the largest possible choice for \(\alpha_{i}\) is as stated in the procedure. Such a construction is detailed in Lehmann and Romano (2005).

#### Control of the False Discovery Proportion

The number \(k\) of false rejections that one is willing to tolerate will often increase with the number of hypotheses rejected. So, it might be of interest to control not the number of false rejections (or false discoveries) but the proportion of false discoveries. Recall that the _false discovery proportion_ (FDP) is defined by

\[\text{FDP}=\frac{F}{R}\cdot I\{R>0\}\;, \tag{9.48}\]

where \(R\) is the number of total rejections and \(F\) is the number of false rejections. Thus, the FDP is the proportion of rejected hypotheses that are rejected erroneously. When none of the hypotheses are rejected, the FDP is defined to be \(0\).

In Section 9.3.2, we considered the FDR, or \(E(\text{FDP})\), as a measure of error control. But, control of the FDR does not prohibit the FDP from varying, even if its average value is bounded. Instead, we consider an alternative measure of control that guarantees the FDP is bounded, at least with prescribed probability. That is, for a given \(\gamma\) and \(\alpha\) in (0, 1), we require

\[P\{\text{FDP}>\gamma\}\leq\alpha\;. \tag{9.49}\]We now develop a stepwise procedure satisfying (9.49). The motivation is easiest in terms of a stepdown procedure. As before, let \(F\) denote the number of false rejections. At step \(i\), having rejected \(i\ -1\) hypotheses, we want to guarantee \(F/i\leq\gamma\), i.e., \(F\leq\lfloor\gamma i\rfloor\), where \(\lfloor x\rfloor\) is the greatest integer \(\leq x\). So, if \(k=\lfloor\gamma i\rfloor+1\), then \(F\geq k\) should have probability no greater than \(\alpha\); that is, we must control the number of false rejections to be \(\leq k\). Therefore, we use the stepdown constant \(\alpha_{i}\) used for controlling the \(k\)-FWER with this choice of \(k\) (which now depends on \(i\)); that is,

\[\alpha_{i}=\frac{(\lfloor\gamma i\rfloor+1)\alpha}{s+\lfloor\gamma i\rfloor+1-i }. \tag{9.50}\]

In fact, the stepup procedure using the constants (9.50) controls the FDP in the sense of (9.49) under a positive dependence assumption.

**Theorem 9.3.6**: _Consider testing \(s\) null hypotheses, with \(|I|\) of them true. Let \(\hat{q}_{(1)}\leq\cdots\leq\hat{q}_{(|I|)}\) denote their corresponding ordered \(p\)-values corresponding to true null hypotheses. (i) For the stepup procedure with \(\alpha_{i}\) given by (9.50),_

\[P\{FDP>\gamma\}\leq P\{\bigcup_{i=1}^{|I|}\{\hat{q}_{(i)}\leq\frac{i\alpha}{|I |}\}\}. \tag{9.51}\]

_(ii) Therefore, if the joint distribution of the \(p\)-values of the true null hypotheses satisfies Simes' inequality, i.e.,_

\[P\{\{\hat{q}_{(1)}\leq\frac{\alpha}{|I|}\}\cup\{\hat{q}_{(2)}\leq\frac{2\alpha} {|I|}\}\cup\ldots\cup\{\hat{q}_{(|I|)}\leq\alpha\}\}\leq\alpha\,\]

_then \(P\{FDP>\gamma\}\leq\alpha\)._

Proof. Define \(g(R)=\lfloor\gamma R\rfloor+1\). Then,

\[\{\text{FDP}>\gamma\}=\{F\geq g(R)\}=\bigcup_{i=1}^{|I|}\{\hat{q}_{(i)}\leq \alpha_{R},\,g(R)\leq i,\,F=i\}\]

\[=\bigcup_{i=1}^{|I|}\{\hat{q}_{(i)}\leq\frac{g(R)\alpha}{n-R+g(R)},\,g(R)\leq i,\,F=i\}\subseteq\bigcup_{i=1}^{|I|}\{\hat{q}_{(i)}\leq\frac{i\alpha}{n-R+i}, \,F=i\}\]

\[\subseteq\bigcup_{i=1}^{|I|}\{\hat{q}_{(i)}\leq\frac{i\alpha}{|I|},\,F=i\} \subseteq\bigcup_{i=1}^{|I|}\{\hat{q}_{(i)}\leq\frac{i\alpha}{|I|}\}\.\]

Part (i) is now immediate, as is (ii). \(\blacksquare\)Conditions where Simes' inequality hold were obtained under positive dependence in Corollary 9.3.1.

Theorem 9.3.6 points towards a method that controls the FDP without any dependence assumptions. One simply needs to bound the right-hand side of (9.51). To do this, we can apply the following useful result from Hommel (1983) and generalized in Lehmann and Romano (2005).

**Lemma 9.3.1**: _Suppose \(\hat{q}_{1},\ldots,\hat{q}_{t}\) are \(p\)-values in the sense that \(P\{\hat{q}_{i}\leq u\}\leq u\) for all \(i\) and \(u\) in \((0,1)\). Let their ordered values be \(\hat{q}_{(1)}\leq\cdots\leq\hat{q}_{(t)}\). Let_

\[0=\beta_{0}\leq\beta_{1}\leq\beta_{2}\leq\cdots\leq\beta_{m}\leq 1\]

_for some \(m\leq t\). Then,_

\[P\{\bigcup_{i=1}^{m}[\hat{q}_{(i)}\leq\beta_{i}\}]\leq t\sum_{i=1}^{m}(\beta_ {i}-\beta_{i-1})/i. \tag{9.52}\]

Proof. Define the random variable \(J\) as the smallest index \(j\) among \(1\leq j\leq m\) for which \(\hat{q}_{(j)}\leq\beta_{j}\); if \(\hat{q}_{(j)}>\beta_{j}\) for all \(1\leq j\leq m\), then just define \(J\) to be \(t+1\). The events \(\{J=k\}\) are mutually exclusive, and so the left side of (9.52) is equal to

\[P\{\bigcup_{k=1}^{m}\{J=k\}\}=\sum_{k=1}^{m}\theta_{k}\,\]

where \(\theta_{k}=P\{J=k\}\). In order to bound \(\sum_{k}\theta_{k}\), note that for any \(1\leq j\leq m\),

\[\sum_{k=1}^{j}JI\{J=k\}=JI\{J\leq j\}\leq S_{j}\,\]

where \(S_{j}\) is the number of \(p\)-values \(\leq\beta_{j}\). Taking expectations yields

\[\sum_{k=1}^{j}k\theta_{k}\leq t\beta_{j}\,\ \ \ \ j=1,\ldots,m. \tag{9.53}\]

For \(j=1,\ldots,m-1\), multiply both sides of (9.53) by \(1/[j(j+1)]\), and for \(j=m\), multiply both sides by \(1/m\); then, sum over \(j\) to yield

\[\sum_{j=1}^{m-1}\frac{1}{j(j+1)}\sum_{k=1}^{j}k\theta_{k}+\frac{1}{m}\sum_{k=1 }^{m}k\theta_{k}\leq\sum_{j=1}^{m-1}\frac{t\beta_{j}}{j(j+1)}+\frac{t\beta_{m} }{m}. \tag{9.54}\]

[MISSING_PAGE_FAIL:458]

We shall assume further that, for any \(B\),

\[P_{\theta_{i}}\{T_{i}\leq B\}\to 1\;\;\text{as}\;\;\theta_{i}\to-\infty\;\;\text{ and}\;\;\;P_{\theta_{i}}\{T_{i}\geq B\}\to 1\;\;\text{as}\;\;\theta_{i}\to+\infty\;.\]

A crucial assumption will be that the distributions \(F_{\theta}\) are stochastically increasing in the following sense, as previously discussed in Section 8.7. A set \(\omega\) in \(\hbox{\rm I\kern-1.8ptR}^{s}\) is said to be monotone increasing if

\[t=(t_{1},\ldots,t_{s})^{\top}\in\omega\;\;\text{and}\;\;t_{i}\leq t_{i}^{\prime}\;\text{for all}\;i\;\text{implies}\;t^{\prime}\in\omega\;,\]

and the distributions \(F_{\theta}\) will be called stochastically increasing if \(\theta_{i}\leq\theta_{i}^{\prime}\) for all \(i\) implies

\[\int_{\omega}dF_{\theta}\leq\int_{\omega}dF_{\theta^{\prime}} \tag{9.55}\]

for every monotone increasing set \(\omega\).

The condition will be assumed not only for the distributions of \((T_{1},\ldots,T_{s})\) but also for \((\pm T_{1},\ldots,\pm T_{s})\). Thus, for example, for \((-T_{1},\ldots,-T_{s})\) it means that for any decreasing region the inequality (9.55) will be reversed. A class of models for which (9.55) holds is given in Problem 9.21.

For the sake of simplicity, we shall suppose that when \(\theta_{1}=\ldots=\theta_{s}\), the variables \((T_{1},\ldots,T_{s})\) are _exchangeable_, i.e., that the joint distribution is invariant under permutations of the components. In addition, we assume that the joint distribution of \((T_{1},\ldots,T_{s})\) has a density with respect to Lebesgue measure.1 In order for the critical constants to be uniquely defined, we further assume that the joint density is positive on its (assumed rectangular) region of support, but this can be weakened.

Footnote 1: This assumption is used only so that the critical constants of the optimal procedures lead to control at exact level \(\alpha\).

Under these assumptions we shall restrict attention to decision rules satisfying the following monotonicity condition. A decision procedure \(E\) for the simultaneous testing of \(H_{1},\ldots,H_{s}\) based on \(T=(T_{1},\ldots,T_{s})\) states for each possible observation vector \(t\) the subset \(I_{t}\) of \(\{1,\ldots,s\}\) of values \(i\) for which the hypothesis \(H_{i}\) is rejected. A decision rule \(E\) is said to be monotone increasing if \(t_{i}\leq t_{i}^{\prime}\) for \(i\in I_{t}\) and \(t_{i}^{\prime}<t_{i}\) for \(i\notin I_{t}\) implies that \(I_{t}=I_{t^{\prime}}\).

The ordered \(T\)-values will be denoted by \(T_{(1)}\leq T_{(2)}\leq\cdots\leq T_{(s)}\) and the corresponding hypotheses by \(H_{(1)},\ldots,H_{(s)}\). Consider the following monotone decision procedure \(D\), which can be viewed as an application of Procedure 9.1.1.

_The Stepdown Procedure \(D\)_:

_Step 1._ If \(T_{(s)}<C_{1}\), accept \(H_{1},\ldots,H_{s}\). If \(T_{(s)}\geq C_{1}\) but \(T_{(s-1)}<C_{2}\), reject \(H_{(s)}\) and accept \(H_{(1)},\ldots,H_{(s-1)}\).

_Step 2._ If \(T_{(s)}\geq C_{1}\), and \(T_{(s-1)}\geq C_{2}\), but \(T_{(s-2)}<C_{3}\) reject \(H_{(s)}\) and \(H_{(s-1)}\) and accept \(H_{(1)},\ldots,H_{(s-2)}\).

And so on. The \(C\)'s are determined by

\[\underbrace{P_{0},\ldots,0}_{j}\{\max(T_{1},\ldots,T_{j})\geq C_{s-j+1}\}=\alpha\, \tag{9.56}\]

and therefore the \(C\)'s are nonincreasing.

**Lemma 9.4.1**: _Under the above assumptions, the procedure \(D\) with critical constants given by (9.56) controls the FWER in the strong sense._

Proof. Apply Theorem 9.1.3 with \(\hat{c}_{n,K}(1-\alpha)=C_{s-|K|+1}\), where \(|K|\) is the cardinality of \(K\). Then, by the monotonicity of the \(C\)s, condition (9.22) holds. We must verify (9.24) for every \(P_{\theta}\). Suppose \(\theta\) is such that exactly \(p\) hypotheses are true. By exchangeability, we can assume \(H_{1}\),..., \(H_{p}\) are true and \(H_{p+1}\),..., \(H_{s}\) are false. A false rejection occurs if and only if at least one of \(H_{1}\),..., \(H_{p}\) is rejected. Since \(D\) is monotone, the probability of this event is largest when

\[\theta_{1}=\cdots=\theta_{p}=0\ \ \mbox{and}\ \ \theta_{p+1}\to\infty,\cdots, \theta_{s}\to\infty\,\]

and, by (9.56), the sup of this probability is equal to

\[\underbrace{P_{0},\ldots,0}_{p}\{T_{i}\geq C_{s-p+1}\ \ \mbox{for some}\ i=1,\ldots,p\}=\alpha.\]

The procedure \(D\) defined above is an example of a stepdown procedure in that it starts with the most significant (or, in this case, the largest) test statistic and continues rejecting hypotheses as long as their corresponding test statistics are large. In contrast, stepup procedures begin with the least significant test statistic. Consider the following monotone stepup procedure \(U\).

_The Stepup Procedure \(U\)_:

_Step 1._ If \(T_{(1)}>C_{1}^{*}\) reject \(H_{1}\),..., \(H_{s}\). If \(T_{(1)}\leq C_{1}^{*}\) but \(T_{(2)}>C_{2}^{*}\), accept \(H_{(1)}\) and reject \(H_{(2)}\),..., \(H_{(s)}\).

_Step 2._ If \(T_{(1)}\leq C_{1}^{*}\), and \(T_{(2)}\leq C_{2}^{*}\) but \(T_{(3)}>C_{3}^{*}\), accept \(H_{(1)}\) and \(H_{(2)}\) and reject \(H_{(3)}\),..., \(H_{(s)}\).

And so on. The \(C^{*}\)'s are determined by

\[\underbrace{P_{0},\ldots,0}_{j}\{L_{j}\}=1-\alpha\, \tag{9.57}\]

where

\[L_{j}=\{T_{\pi(1)}\leq C_{1}^{*},\ldots,T_{\pi(j)}\leq C_{j}^{*}\ \ \mbox{for some permutation of}\ \{1,\ldots,j\}\}\.\]

The following lemma proves control of the FWER and is left as an exercise (Problem 9.22).

**Lemma 9.4.2**: _Under the above assumptions, the stepup procedure \(U\) with critical constants given by (9.57) controls the FWER in the strong sense._

Subject to controlling the FWER we want to maximize what corresponds to the power of a single test, i.e., the probability of rejecting hypotheses that are in fact false. Let

\[\beta_{i}(\theta)=P_{\theta}\{\text{reject at least $i$ hypotheses}\}\]

and, for any \(\epsilon>0\), let \(A_{i}(\epsilon)\) denote the set in the parameter space for which at least \(i\) of the \(\theta\)'s are \(>\epsilon\). Then we shall be interested in maximizing

\[\inf_{\theta\in A_{i}(\epsilon)}\beta_{i}(\theta)\;\;\text{for $i=1,2,\ldots,s$}. \tag{9.58}\]

This is in the same spirit as the maximin criterion of Chapter 8. However, it is the false hypotheses we should like to reject, and so we also consider maximizing

\[\inf_{\theta\in A_{i}(\epsilon)}P_{\theta}\{\text{reject at least $i$ false hypotheses}\}\;. \tag{9.59}\]

We note the following obvious fact.

**Lemma 9.4.3**: _Under (9.55), for any monotone increasing procedure \(E\), the functions \(\beta_{i}(\theta_{1},\ldots,\theta_{s})\) are nondecreasing in each of the variables \(\theta_{1},\ldots,\theta_{s}\)._

For the sake of simplicity we shall now consider the maximin problem first for the case \(s=2\). Corresponding to any decision rule \(E\), let \(e_{0,0}\) denote the part of the sample space where both hypotheses are accepted, \(e_{0,1}\) where \(H_{1}\) is accepted and \(H_{2}\) is rejected, \(e_{1,0}\) where \(H_{1}\) is rejected and \(H_{2}\) is accepted, and \(e_{1,1}\) where both \(H_{1}\) and \(H_{2}\) are rejected. The following is an optimality result for the stepdown procedure \(D\). It will be convenient in the following theorem to restate the procedure \(D\) in the case \(s=2\).

**Theorem 9.4.1**: _Assume the conditions described at the beginning of this section. (i) A monotone increasing decision procedure with FWER \(\leq\alpha\) will maximize (9.58) for \(i=1\) if and only if it rejects at least one hypothesis when_

\[\max(T_{1},T_{2})\geq C_{1}\;, \tag{9.60}\]

_in which case \(H_{i}\) is rejected if \(T_{i}>C_{1}\); in the contrary case, both hypotheses are accepted. The constant \(C_{1}\) is determined by_

\[P_{0,0}\{\max(T_{1},T_{2})\geq C_{1}\}=\alpha \tag{9.61}\]

_The minimum value of \(\beta_{1}(\theta)\) over \(A_{1}(\epsilon)\) is \(P_{\epsilon}\{T_{i}\geq C_{1}\}\).__(ii) A monotone increasing decision rule with FWER \(\leq\alpha\) and satisfying (9.60) will maximize (9.58) for \(i=2\) if and only if it takes the following decisions: \(d_{0,0}\): accept \(H_{1}\) and \(H_{2}\) when \(\max(T_{1},T_{2})<C_{1}\) \(d_{1,0}\): reject \(H_{1}\) and accept \(H_{2}\) when \(T_{1}\geq C_{1}\) and \(T_{2}<C_{2}\) \(d_{0,1}\): accept \(H_{1}\) and reject \(H_{2}\) when \(T_{1}<C_{2}\) and \(T_{2}\geq C_{1}\) \(d_{1,1}\): reject both \(H_{1}\) and \(H_{2}\) when both \(T_{1}\) and \(T_{2}\) are \(\geq C_{2}\) (and when 9.60 holds). Here \(C_{2}\) is determined by_

\[P_{0}\{T_{i}\geq C_{2}\}=\alpha, \tag{9.62}\]

_and hence \(C_{2}<C_{1}\)._

_The minimum probability over \(A_{2}(\epsilon)\) of rejecting both hypotheses is_

\[P_{\epsilon,\epsilon}\{\text{at least one }T_{i}\text{ is }\geq C_{1}\text{ and both are }\geq C_{2}\}\;.\]

_(iii) The result (i) holds if the criterion (9.58) is replaced by (9.59) with \(i=1\), and \(P_{\epsilon}\{T_{i}\geq C_{1}\}\) is also the maximum value of criterion (9.59)._

The optimal stepdown decision rule is depicted in Figure 9.2.

Proof of Theorem 9.4.1. To prove (i), note that the claimed optimal solution has minimum power when \(\theta=(\epsilon,\,-\infty)\) and \(D\) has \(P_{\epsilon}\{T_{1}\geq C_{1}\}\) for the claimed optimal value of \(\beta_{1}(\theta)\). Now, suppose that \(E\) is any other monotone decision rule with FWER \(\leq\alpha\). Assume there exists \((t_{1},t_{2})\notin d_{0,0}\), i.e., rejecting at least one hypothesis, but

Figure 9.2: Optimal stepdown rule

\((t_{1},\,t_{2})\in e_{0,0}\). Then, there exists at least one component of \((t_{1},\,t_{2})\) that is \(\geq C_{1}\), say \(t_{1}\geq C_{1}\). It follows that

\[P_{\epsilon,-\infty}\{e_{0,0}\}\geq P_{\epsilon,-\infty}\{T_{1}<t_{1},\ T_{2}<t _{2}\}=P_{\epsilon}\{T_{1}<t_{1}\}>P_{\epsilon}\{T_{1}<C_{1}\}\]

and hence

\[P_{\epsilon,-\infty}\{e_{0,0}^{c}\}<P_{\epsilon,-\infty}\{T_{1}\geq C_{1}\}=P_ {\epsilon}\{T_{1}\geq C_{1}\}\;.\]

Thus, \(E\) has a smaller value of criterion (9.58) than does the claimed optimal \(D\). Therefore, \(e_{0,0}\) cannot have points outside of \(d_{0,0}\), i.e., \(e_{0,0}\) must be a proper subset of \(d_{0,0}\). But then, since both procedures are monotone, \(e_{0,0}^{c}\) is bigger than \(d_{0,0}^{c}\) on a set of positive Lebesgue measure and so

\[P_{0,0}\{e_{0,0}^{c}\}>P_{0,0}\{d_{0,0}^{c}\}=\alpha\;.\]

It follows that for the maximin procedure, the region \(d_{0,0}^{c}\) must be given by (9.60).

To prove (ii), the goal now is to show that, among all monotone nondecreasing procedures which control the FWER and satisfy (9.60), \(D\) maximizes

\[\inf_{A_{2}(\epsilon)}\beta_{2}(\theta)=\inf_{A_{2}(\epsilon)}P_{\theta}\{d_{1,1}\}\;.\]

To prove this, consider any other monotone procedure \(E\) which controls the FWER and satisfying \(e_{0,0}=d_{0,0}\), and suppose that \(e_{1,1}\) contains a point \((t_{1},\ t_{2})\) with \(t_{i}<C_{2}\) for some \(i\), say \(t_{1}<C_{2}\). Then, since \(E\) is monotone, it contains the quadrant \(\{T_{1}\geq t_{1},\ T_{2}\geq t_{2}\}\), and hence

\[P_{0,\infty}\{e_{1,1}\}\geq P_{0,\infty}\{T_{1}\geq t_{1},\ T_{2}\geq t_{2}\}= P_{0}\{T_{1}\geq t_{1}\}>P_{0}\{T_{1}\geq C_{2}\}=\alpha\;,\]

which contradicts strong control. It follows that \(e_{1,1}\) is a proper subset of \(d_{1,1}\), and

\[P_{\theta}\{e_{1,1}\}<P_{\theta}\{d_{1,1}\}\quad\mbox{for all $\theta$}\;.\]

Since the inf over \(A_{2}(\epsilon)\) of both sides is attained at \((\epsilon,\,\epsilon)\),

\[\inf_{A_{2}(\epsilon)}P_{\theta}\{e_{1,1}\}<\inf_{A_{2}(\epsilon)}P_{\theta}\{ d_{1,1}\}\;,\]

as was to be proved.

To prove (iii), observe that, for any \(\theta\),

\[P_{\theta}\{\mbox{rejecting at least one false $H_{i}$}\}\leq P_{\theta}\{\mbox{rejecting at least one $H_{i}$}\}\;,\]

and so \[\inf_{\theta\in A_{1}(\epsilon)}P_{\theta}\{\text{rejecting at least one false }H_{i}\}\leq\inf_{\theta\in A_{1}(\epsilon)}P_{\theta}\{\text{rejecting at least one }H_{i}\}\;.\]

But, the right side is \(P_{\epsilon}\{T_{1}>C_{1}\}\), and so it suffices to show that \(D\) satisfies

\[\inf_{\theta\in A_{1}(\epsilon)}P_{\theta}\{D\text{ rejects at least one false }H_{i}\}=P_{\epsilon}\{T_{1}>C_{1}\}\;.\]

But, this last result is easily checked.

Finally, once \(d_{0,0}\) and \(d_{1,1}\) are determined, so are \(d_{0,1}\) and \(d_{1,0}\) by monotonicity, and this completes the proof.

Theorem 9.4.1 provides the maximin test which first maximizes \(\inf\beta_{1}(\theta)\) and then \(\inf\beta_{2}(\theta)\). In the next result, the order in which these aspects are maximized is reversed, which results in the stepup procedure \(U\) being optimal.

**Theorem 9.4.2**: _Assume the conditions described at the beginning of this section. (i) A monotone decision rule with \(FWER\leq\alpha\) will maximize (9.58) for \(i=2\) if and only if it rejects both hypotheses, i.e., takes decision \(u_{1,1}\), when_

\[\min(T_{1},T_{2})\geq C_{1}^{*} \tag{9.63}\]

_and accepts \(H_{i}\) if \(T_{i}<C_{1}^{*}\), where \(C_{1}^{*}=C_{2}\) is determined by (9.62). Its minimum power \(\beta_{2}(\theta)\) over \(A_{2}(\epsilon)\) is_

\[P_{\epsilon}\{\min(T_{1},T_{2})\geq C_{1}^{*}\}\;. \tag{9.64}\]

_(ii) The monotone procedure with \(FWER\leq\alpha\) and satisfying (9.63) maximizes (9.58) for \(i=1\) if and only it takes the following decisions:_

\[u_{0,1}=\{T_{1}<C_{1}^{*},\ T_{2}\geq C_{2}^{*}\}\]

\[u_{1,0}=\{T_{1}\geq C_{2}^{*},\ T_{2}<C_{1}^{*}\}\]

\[u_{0,0}=\{T_{1}<C_{1}^{*},\ T_{2}<C_{2}^{*}\}\bigcap u_{1,1}^{c}\;,\]

_where \(C_{2}^{*}\) is determined by_

\[P_{0,0}\{u_{0,0}^{c}\}=\alpha\;. \tag{9.65}\]

_Its minimum power \(\beta_{1}(\theta)\) over \(A_{1}(\epsilon)\) is_

\[P_{\epsilon}\{T_{i}\geq C_{2}^{*}\}\;. \tag{9.66}\]

_(iii) The result (ii) holds if criterion (9.58) with \(i=1\) is replaced by (9.59) with \(i=1\)._

The optimal stepup decision rule is depicted in Figure 9.3. Note that

\[C_{1}^{*}=C_{2}<C_{1}<C_{2}^{*}\;. \tag{9.67}\]Also, the best minimum power \(\beta_{1}(\theta)\) over \(A_{1}(\epsilon)\) for the procedure of Theorem 9.4.1 exceeds that for Theorem 9.4.2, while the situation is reversed for the best minimum power of \(\beta_{2}(\theta)\) over \(A_{2}(\epsilon)\). This is, of course, as it must be since the first of these two procedures maximized the minimum value of \(\beta_{1}(\theta)\) over \(A_{1}(\epsilon)\) while the second maximized the minimum value of \(\beta_{2}(\theta)\) over \(A_{2}(\epsilon)\).

Proof of Theorem 9.4.2.: (i) Suppose that \(E\) is any other monotone procedure with FWER \(\leq\alpha\). Assume there exists \((t_{1},\ t_{2})\in e_{1,1}\) such that \(t_{i}<C_{1}^{*}\) for some \(i\), say \(t_{1}<C_{1}^{*}\). Then,

\[P_{0,\infty}\{e_{1,1}\}\geq P_{0,\infty}\{T_{1}\geq t_{1},\ T_{2}\geq t_{2}\} =P_{0}\{T_{1}\geq t_{1}\}>P_{0}\{T_{1}\geq C_{1}^{*}\}=\alpha\,\]

which would violate the FWER condition. Therefore, \(e_{1,1}\subseteq u_{1,1}\). But then

\[\inf_{A_{2}(\epsilon)}\beta_{2}(\theta)\]

is smaller for \(E\) than for \(U\), as was to be proved.

(ii) Note that the claimed solution \(\inf_{A_{1}(\epsilon)}\beta(\theta)\) is given by

\[\inf_{\theta\in A_{1}(\epsilon)}P_{\theta}\{u_{0,0}^{c}\}=P_{\epsilon,-\infty} \{u_{0,0}^{c}\}=P_{\epsilon}\{T_{1}\geq C_{1}^{*}\}\.\]

Figure 9.3: Optimal stepup rule

We now seek to determine \(u_{0,0}\), as in Theorem 9.4.1, but with the added constraint that \(u_{0,0}\subseteq u_{1,1}^{c}\).

To prove optimality for the claimed solution, suppose that \(E\) is another monotone procedure controlling FWER at \(\alpha\), and satisfying \(e_{1,1}=u_{1,1}\) with \(u_{1,1}\) given by (9.63). Assume \((t_{1},\ t_{2})\in e_{0,0}\) but \(\notin u_{0,0}\), so that \(T_{i}>C_{2}^{*}\) for some \(i\), say \(i=1\). Then,

\[P_{\epsilon,-\infty}\{e_{0,0}\}\geq P_{\epsilon,-\infty}\{T_{1}\leq t_{1},\ T_{2}\leq t_{2}\}=P_{\epsilon}\{T_{1}\leq t_{1}\}>P_{\epsilon}\{T_{1}>C_{2}^{*}\}\.\]

Hence,

\[P_{\epsilon,-\infty}\{e_{0,0}^{c}\}<P_{\epsilon}\{T_{1}>C_{2}^{*}\}\,\]

so that \(E\) cannot be optimal. It follows that \(e_{0,0}\subseteq u_{0,0}\). But if \(e_{0,0}\) is a proper subset of \(u_{0,0}\), the set \(e_{0,0}^{c}\) in which \(E\) rejects at least one hypothesis contains \(u_{0,0}^{c}\) and so

\[P_{0,0}\{e_{0,0}^{c}\}>P_{0,0}\{u_{0,0}^{c}\}=\alpha\,\]

and \(E\) does not control the FWER at \(\alpha\).

Finally, the proof of (iii) is analogous to the proof of (iii) in Theorem 9.4.1.

Theorems 9.4.1 and 9.4.2 have natural extensions to the case of \(s\) hypotheses where the aim is to maximize the \(s\) quantities (9.58). As in the case \(s=2\), these maximizations lead to different procedures, and one must choose their order of importance. The two most natural choices are the following:

1. Begin by maximizing inf \(\beta_{1}(\theta)\), which will lead to an optimal choice for \(d_{0,0,\ldots,0}\), the decision to accept all hypotheses. With \(d_{0,\ldots,0}\) fixed, the partition of \(d_{0,\ldots,0}^{c}\) into the subsets in which the remaining decisions should be taken is begun by maximizing the minimum of \(\beta_{2}(\theta)\) over the part of the parameter space in which at least 2 hypotheses are false, and so on.
2. Alternatively, we may start at the other end by maximizing inf \(\beta_{s}(\theta)\), and from there proceed downward.

We shall here only state the result for case (a). For its proof and the statement and proof for case (b), see Lehmann et al. (2005).

**Theorem 9.4.3**: _Under the assumptions made at the beginning of this section, among all monotone procedures \(E\) with FWER \(\leq\alpha\), the stepdown procedure \(D\) with critical constants given by (9.56), has the following properties: (i) it maximizes inf \(\beta_{1}(\theta)\) over \(A_{1}(\epsilon)\) (ii) it maximizes inf \(\beta_{2}(\theta)\) over \(A_{2}(\epsilon)\) subject to the additional condition \(e_{s,2}\subseteq d_{s,1}\), where \(e_{s,i}\) and \(d_{s,i}\) denote the events that the procedures \(E\) and \(D\) reject at least \(i\) of the hypotheses \(H_{1}\),..., \(H_{s}\). (iii) Quite generally, it maximizes both (9.58) and (9.59) among all monotone procedures \(E\) with FWER \(\leq\alpha\) and satisfying \(e_{s,i}\subseteq d_{s,i-1}\)._

We shall now provide a canonical form for certain stepdown procedures, and particularly for the maximin procedure \(D\) of Theorem 9.4.3, that provides additional insights.

[MISSING_PAGE_EMPTY:2069]

and hence

\[\alpha_{1}\geq\frac{\alpha}{s}. \tag{9.75}\]

We shall now show that the lower bound (9.75) is sharp by giving an example of a joint distribution of (\(T_{1},\ldots,T_{s}\)) for which it is attained.

**Example 9.4.1** (A Least Favorable Distribution): Let \(U\) be uniformly distributed on (0, 1) and suppose that when \(H_{1},\ldots,H_{s}\) are all true,

\[Y_{1}=U,\ \ Y_{2}=U+\frac{1}{s}(mod\ 1),\ldots,Y_{s}=U+\frac{s-1}{s}(mod\ 1)\.\]

Since (\(Y_{1},\ldots,Y_{s}\)) does not satisfy our assumption of exchangeability, replace it by the exchangeable set of variables (\(X_{1},\ldots,X_{s}\)) = (\(Y_{\pi(1)},\ldots,Y_{\pi(s)}\)), where (\(\pi(1),\ldots,\pi(s)\)) is a random permutation of (\(1,\ldots,s\)) (and independent of \(U\)). Let \(T_{i}=1-X_{i}\) and suppose that \(H_{i}\) is rejected when \(T_{i}\) is large. To show that

\[F[G_{s}^{-1}(1-\alpha)]=1-\frac{\alpha}{s}\, \tag{9.76}\]

note that the \(T\)'s are uniformly distributed on (0, 1) so that (9.76) becomes

\[G_{s}(1-\frac{\alpha}{s})=1-\alpha\.\]

Now

\[1-G_{s}(1-\frac{\alpha}{s})=P\{\mbox{at least one }T_{i}\geq 1-\frac{\alpha}{s}\}=P\{\mbox{at least one }X_{i}\leq\frac{\alpha}{s}\}\.\]

But the events \(\{X_{i}\leq\alpha/s\}\) are mutually exclusive, and therefore

\[P\{\mbox{at least one }X_{i}\leq\frac{\alpha}{s}\}=\sum_{i=1}^{s}P\{X_{i}\leq\frac{\alpha}{s}\}=s\cdot\frac{\alpha}{s}=\alpha\,\]

which implies (9.76).

We shall now briefly sketch the corresponding development for \(\alpha_{2}\), defined by the fact that \(\hat{p}_{(2)}\leq\alpha_{2}\) is equivalent to \(T_{(s-1)}\geq C_{2}\), where \(C_{2}\) is determined by (9.56) so that

\[G_{s-1}(C_{2})=1-\alpha\.\]

Note that \(G_{s-1}\) is _not_ the distribution of \(T_{(s-1)}\), i.e., of the 2nd largest of \(s\)\(T\)'s, but of the largest of \(T_{1},\ldots,T_{s-1}\) (i.e., the largest of \(s-1\)\(T\)'s). In exact analogy with the derivation of (9.72) it now follows that

\[1-\alpha_{2}=F[G_{s-1}^{-1}(1-\alpha)]. \tag{9.77}\]The maximum value of \(\alpha_{2}\), as in the case of \(\alpha_{1}\), is equal to \(\alpha\) and is attained when \(T_{1}=\cdots=T_{s-1}\).

The argument giving the lower bound shows that \(\alpha_{2}\geq\alpha/(s-1)\). To show that this value is attained, we must find an example for which

\[G_{s-1}(1-\frac{\alpha}{s-1})=1-\alpha\.\]

Example 9.4.1 will serve this purpose since in that case

\[1-G_{s-1}(1-\frac{\alpha}{s-1})=P\{\mbox{at least one of }T_{1},\ldots,T_{s-1}\geq 1-\frac{\alpha}{s-1}\}\]

\[=\sum_{i=1}^{s-1}P\{X_{i}\leq\frac{\alpha}{s-1}\}=(s-1)\cdot\frac{\alpha}{s-1}=\alpha\]

for any \(\alpha\) satisfying \(\alpha/(s-1)<1/s\), i.e., \(\alpha<(s-1)/s\).

Continuing in this way we arrive at the following result.

**Theorem 9.4.4**: _(i) The step levels \(\alpha_{i}\) defined by the procedure \(D\) with critical constants given by (9.56) and the equivalence of (9.70) and (9.71) are given by_

\[1-\alpha_{i}=F[G_{s-i+1}(1-\alpha)]\, \tag{9.78}\]

_where \(G_{j}\) is the distribution of \(\max(T_{1},\ldots,T_{j})\)._

_(ii) The range of \(\alpha_{i}\) is_

\[\frac{\alpha}{s-i+1}\leq\alpha_{i}\leq\alpha. \tag{9.79}\]

_Furthermore, the upper bound \(\alpha\) is attained when \(T_{1}=\cdots=T_{s}\), i.e., when there really is no multiplicity. The lower bound \(\alpha/(s-i+1)\) is attained when the distribution of \(T_{1},\ldots,T_{s-i+1}\) is that of Example 9.4.1._

Not all points in the \(s\)-dimensional rectangle (9.79) are possible for \((\alpha_{1},\ldots,\alpha_{s})\). In particular, since for all \(t\)

\[G_{i}(t)\geq G_{j}(t)\ \ \ \mbox{when}\ i<j\,\]

it follows that

\[\alpha_{1}\leq\alpha_{2}\leq\cdots\leq\alpha_{s}. \tag{9.80}\]

The values of \(\alpha_{i}\) given by (9.78) can be determined when the joint distribution of \((T_{1},\ldots,T_{s})\) (and hence the distributions \(G_{s}\)) is known. Consider, however, the situation in which the common marginal distribution \(F\) of the statistics \(T_{i}\) needed to carry out the tests of the individual hypotheses \(H_{i}\) at a given level is known, but the joint distribution of the \(T\)'s is unknown. Then, we are unable to determine the step levels (9.78).

It follows, however, from (9.79) that the procedure (9.71) with

\[\alpha_{i}=\alpha/(s-i+1)\ \ \ \text{for }i=1,\ldots,s \tag{9.81}\]

will control the FWER for all joint distributions of \((T_{1},\ldots,T_{s})\), since these levels are conservative in all cases. This is just the Holm procedure of Theorem 9.1.2.

Also, none of the levels \(\alpha_{i}\) can be larger than \(\alpha/(s-i+1)\) without violating the FWER condition for some distribution. To see this, note that if levels \(\alpha_{i}\) are used in Example 9.4.1, it follows from the discussion of this example that when \(i\) of the hypotheses are true, the probability of at least one false rejection is \((s-i+1)\alpha_{i}\). Thus, if \(\alpha_{i}\) exceeds \(\alpha/(s-i+1)\), the FWER condition will be violated.

Of course, if the class of joint distributions of the \(T\)'s is restricted, the range of \(\alpha_{i}\) may be smaller than (9.79). For example, suppose that the \(T\)'s are independent. Then, putting \(u=G_{s}^{-1}(1-\alpha)\) as before, we see from (9.74) that

\[1-\alpha_{1}=F(u)\ \ \ \text{and}\ \ \ 1-\alpha=F^{s}(u)\]

so that

\[\alpha_{1}=1-(1-\alpha)^{1/s}\,\]

and more generally that

\[\alpha_{i}=1-(1-\alpha)^{1/(s-i+1)}\.\]

In this case, the range reduces to a single point.

More interesting is the case of positive quadrant dependence when

\[G_{s}(u)\geq F^{s}(u)\]

and hence

\[1-\alpha\geq(1-\alpha_{1})^{1/s}\]

and

\[1-(1-\alpha)^{s}\leq\alpha_{1}\leq\alpha. \tag{9.82}\]

The bounds are sharp since the upper bound is attained when \(T_{1}=\cdots=T_{s}\) and the lower bound is attained in the case of independence.

### The Hypothesis of Homogeneity

The previous section dealt with situations in which each of the parameters varies independently, so that any subset of the hypotheses \(H_{1}\),..., \(H_{s}\) can be true with the remaining ones being false. This condition is not satisfied, for example, when the set of hypotheses is \[H_{i,j}:\ \theta_{i}=\theta_{j}\,\ \ \ \ i<j \tag{9.83}\]

for all \({s\choose 2}\) pairs \(i<j\). Then, for instance, the set \(\{H_{1,2},\ \ H_{2,3}\}\) can not be the set of all true hypotheses since the truth of \(H_{1,2}\) and \(H_{2,3}\) implies the truth of \(H_{1,3}\). It follows from this transitivity that the set of true hypotheses constitutes a partition of the \(\mu\)'s, say

\[\mu_{i_{1}}=\cdots=\mu_{i_{r}}\ ;\ \ \ \mu_{i_{r+1}}=\cdots=\mu_{i_{r+k}}\ ; \cdots. \tag{9.84}\]

All pairs within a set of the partition are equal, and two \(\mu\)s in different sets are unequal. We shall therefore use the statement \(\mu_{i_{1}}=\cdots=\mu_{i_{r}}\) as shorthand for the statement that all hypotheses \(H_{k,l}\) with \((k,l)\) any pair of subscripts from the set \(\{i_{1},\ldots,i_{r}\}\) are true.

Unfortunately, the results of the tests of the hypotheses (9.83) do not share this simple structure since it is possible to accept \(H_{1,2}:\mu_{1}=\mu_{2}\) and \(H_{2,3}:\mu_{2}=\mu_{3}\) while rejecting \(H_{1,3}:\mu_{1}=\mu_{3}\). We shall return to this point at the end of the section.

We shall now consider the simultaneous testing of the \({s\choose 2}\) hypotheses (9.83) by means of a Holm-type stepdown procedure, as in the preceding section. We assume that statistics \(T_{i,j}\) are available for testing the individual hypotheses \(H_{i,j}\). For the remainder of this section, we consider the case of normal variables with \(s\) sample means \(\bar{X}_{i}\), each obtained from \(n\) observations, and common variance \(\sigma^{2}\). A reasonable choice of statistic is then \(T_{i,j}=|\bar{X}_{i}-\bar{X}_{j}|/\hat{\sigma}\), where \(\hat{\sigma}\) is given by

\[\hat{\sigma}^{2}=\frac{\sum_{i=1}^{s}\sum_{j=1}^{n}(X_{i,j}-\bar{X}_{i})^{2}}{ s(n-1)}. \tag{9.85}\]

The procedure begins with the largest of the \(T\)'s corresponding to the pair \((i,\,j)\) with the largest difference \(|\bar{X}_{i}-\bar{X}_{j}|\). This would be tested at level \(\alpha/{s\choose 2}\), since \({s\choose 2}\) is the total number of hypotheses being tested. If this hypothesis is accepted, all the hypotheses (9.83) are accepted and the procedure is terminated. In the contrary case, we next test the second largest of the \(T\)'s at level \(\alpha/({s\choose 2}-1)\), and so on. By Theorem 9.1.2, this procedure controls the FWER, regardless of the joint distribution of the \(T_{i,j}\).

However, the fact that the parameters \(\theta_{i,j}=\mu_{i}-\mu_{j}\) do not vary independently but are subject to certain logical restrictions enables us to do better. To illustrate the situation, suppose that \(s=6\). Let

\[\bar{X}_{(1)}\leq\cdots\leq\bar{X}_{(s)}\]

denote the ordered values of the sample means, and let \(\mu_{(i)}\) be the mean corresponding to \(\bar{X}_{(i)}\). At the first stage, we test \(\mu_{(1)}=\mu_{(6)}\). If \((\bar{X}_{(6)}-\bar{X}_{(1)})/\hat{\sigma}<C\), we accept all the hypotheses \(H_{i,j}\) and terminate the procedure. If \((\bar{X}_{(6)}-\bar{X}_{(1)})/\hat{\sigma}\geq C\), we reject the hypothesis \(\mu_{(1)}=\mu_{(6)}\) and test the largest of the differences \(\bar{X}_{(6)}-\bar{X}_{(2)}\) and \(\bar{X}_{(5)}-\bar{X}_{(1)}\).

Let us now express the rule in terms of the \(p\)-values. By (9.68),\[\hat{p}_{i,j}=1-F(T_{i,j})\, \tag{9.86}\]

where \(F\) is the distribution of \(|\bar{X}_{i}-\bar{X}_{j}|/\hat{\sigma}\), and the rejection region \(|\bar{X}_{(6)}-\bar{X}_{(1)}|/\hat{\sigma}\geq C\) becomes \(\min_{i,j}\hat{p}_{i,j}\leq\alpha/{s\choose 2}\). If the next largest difference is \((\bar{X}_{(5)}-\bar{X}_{(1)})/\hat{\sigma}\), say, we would at the next step compare \(1-F[(\bar{X}_{(5)}-\bar{X}_{(1)})/\hat{\sigma}]\) with \(\alpha/({s\choose 2}-1)\), and so on.

However, using the relations between the differences \(|\bar{X}_{j}-\bar{X}_{i}|\), we can in the present situation do considerably better than that.

To see this, consider the case where one hypothesis is false, say \(\mu_{1}\neq\mu_{4}\). Then, \(\mu_{2}\) cannot be equal to both \(\mu_{1}\) and \(\mu_{4}\); thus, one of the hypotheses \(\mu_{1}=\mu_{2}\) or \(\mu_{2}=\mu_{4}\) must be false, and similarly for \(\mu_{3}\), \(\mu_{5}\) and \(\mu_{6}\). Therefore, at step 2 when one hypothesis is false, at least 5 must be false, and the number of possible true hypotheses is not \({6\choose 2}-1=14\) but instead is \({6\choose 2}-5=10\).

An argument similar to that of Theorem 9.1.2 shows that at the second step of the Holm procedure, we can increase \(\alpha/14\) to \(\alpha/10\) without violating the FWER. Indeed, suppose that at least one hypothesis is false, and so at most 10 are true. Let \(I\) be the set (\(i\), \(j\)) of true hypothesis \(H_{i,j}\), and let

\[\hat{p}_{\min}=\min\{\hat{p}_{i,j}:\ (i,j)\in I\}\.\]

Then, if a false rejection occurs, it occurs at step 1 or step 2, but in either case, it must be that \(\hat{p}_{\min}\leq\alpha/10\). But, by Bonferroni,

\[P\{\hat{p}_{\min}\leq\frac{\alpha}{10}\}\leq\sum_{(i,j)\in I}P\{\hat{p}_{i,j} \leq\frac{\alpha}{10}\}\leq|I|\cdot\frac{\alpha}{10}\leq\alpha\.\]

Similar improvements are possible at the succeeding steps.

As pointed out at the beginning of the section, each set of true hypotheses (9.84) corresponds to a partition of the integers \(\{1,\ldots,s\}\) and determines the corresponding number of possible true hypotheses

\[{r\choose 2}+{k\choose 2}+\cdots\.\]

The following table, adapted from Shaffer (1986), where this improvement was first proposed, shows for \(s=3\)-10 the maximum possible number of true hypotheses (Table 9.1).

Here, for example, the entries 0-4, 6, 10 for \(s=5\) correspond to the numbers of possible true pairs \(\mu_{i}=\mu_{j}\) for the given partitions. Thus, the case \(\mu_{1}=\mu_{2}=\mu_{3}=\mu_{4}=\mu_{5}\) corresponds to the partition (\(\mu_{1}\),..., \(\mu_{5}\)) and allows \({5\choose 2}=10\) true pairs \(\mu_{i}=\mu_{j}\). The case \(\mu_{1}\neq\mu_{2}=\mu_{3}=\mu_{4}=\mu_{5}\) corresponds to the partition {\(\mu_{1}\)}, {\(\mu_{2}\), \(\mu_{3}\), \(\mu_{4}\), \(\mu_{5}\)} and allows \({4\choose 2}=6\) true pairs \(\mu_{i}=\mu_{j}\). The case \(\mu_{1}=\mu_{2}\neq\mu_{3}=\mu_{4}=\mu_{5}\) corresponds to the partition {\(\mu_{1}\), \(\mu_{2}\)}, {\(\mu_{3}\), \(\mu_{4}\), \(\mu_{5}\)} and allows \({2\choose 2}+{3\choose 2}=4\) true pairs, and so on.

The reductions are substantial. At the second step, for example, \(\binom{s}{2}-1=\frac{(s-2)(s+1)}{2}\) is decreased to \(\binom{s-1}{2}=\frac{(s-2)(s-1)}{2}\); the difference is \(s-2\) and hence tends to \(\infty\) as \(s\to\infty\).

Shaffer gave a simple algorithm for finding the maximum number of true hypotheses given that \(i\) hypotheses have been declared false. Use of the procedure based on these numbers has been called \(S_{1}\) (Donoghue 2004). A more powerful procedure, called \(S_{2}\), uses the maximum number of true hypotheses given the particular hypotheses that have been declared false. A difficulty with the \(S_{2}\) procedure, particularly when \(s\) gets large, is to determine the maximum numbers of true hypotheses that are possible at any given step. An algorithm to deal with this problem has been developed by Donoghue (2004).

Like the Holm procedure itself, this modification only utilizes the marginal distributions of the statistics \(T_{i,\,j}=|\tilde{X}_{i}-\tilde{X}_{j}|/\hat{\sigma}\), which are proportional to \(t\)-statistics. However, under the assumption of normality, the joint distribution of these statistics is also known, and so the levels (9.78) could be used--with \(s-i+1\) replaced by the number of true hypotheses possible at this stage--to achieve a further improvement. Note, however, that this can be difficult because the set of possible true hypotheses is not unique, so a number of joint distributions would have to be determined. An alternative approach that incorporates logical constraints and dependence among the test statistics is described in Westfall (1997).

Multiple comparison procedures, many of them going back to the 1950s, employ not only tests based on ranges but also the corresponding procedures based on \(F\)-tests. Most of them are special cases of a general class of stagewise stepdown procedures which we shall now consider for testing homogeneity of \(s\) normal populations with common variance based on samples of equal size \(n_{i}=n\).

For this purpose, we require a slight shift of point of view. The hypothesis \(H:\mu_{i_{1}}=\cdots=\mu_{i_{r}}\) was previously considered as shorthand for the hypothesis that all pairs within this set are equal, and the problem as that of testing these \(\binom{r}{2}\) separate hypotheses. Now we shall also admit the more traditional interpretation of \(H\) as a hypothesis in its own right for which a global test such as an \(F\)-test might be

\begin{table}
\begin{tabular}{l|l|l} \hline \(s\) & Total \# of hypotheses \(H_{i,\,j}\) & Possible number of true hypotheses \\ \hline
3 & 3 & 0, 1, 3 \\ \hline
4 & 6 & 0–3, 6 \\ \hline
5 & 10 & 0–4, 6, 10 \\ \hline
6 & 15 & 0–4, 6, 7, 10, 15 \\ \hline
7 & 21 & 0–7, 9, 10, 11, 15, 21 \\ \hline
8 & 28 & 0–13, 15, 16, 21, 28 \\ \hline
9 & 36 & 0–13, 15, 16, 18, 21, 22, 28, 36 \\ \hline
10 & 45 & 0–18, 20, 21, 22, 24, 28, 29, 36, 45 \\ \hline \end{tabular}
\end{table}
Table 9.1: Possible number of true hypotheses appropriate. It should be emphasized that, logically, the two interpretations are of course equivalent; they differ only in the way they are analyzed.

The first step in the class of procedures to be considered is to test the hypothesis

\[H_{s}:\ \mu_{1}=\cdots=\mu_{s} \tag{9.87}\]

either with a range test or an \(F\)-test at a critical value \(C_{s}\) corresponding to some level \(\alpha_{s}\). In case of acceptance, the means are judged to exhibit no significant differences, the set \(\{\mu_{1},\,\ldots,\,\mu_{s}\}\) is declared homogeneous, and the procedure terminates. If \(H_{1}\) is rejected, a search for the source of the differences is initiated by proceeding to the second stage, which consists in testing the \(s\) hypotheses

\[H_{s-1,i}:\ \mu_{1}=\cdots=\mu_{i-1}=\mu_{i+1}=\cdots=\mu_{s}\]

each by means of a range or an \(F\) test at a common critical value corresponding to a common level \(\alpha_{s-1}\). For any hypothesis that is accepted, the associated set of means (and all of its subsets) are judged not to have shown any significant differences and are not tested further. For any rejected hypothesis, the \(s-1\) subsets of size \(s-2\) are tested (except those that are subsets of an \((s-1)\)-set whose homogeneity has been accepted). At stage \(i\), the \(k=s-i+1\) differences would be tested for all subsets that are not included in an \((s-i+2)\)-set whose homogeneity has been accepted. Moreover, assume that all tests at stage \(i\) are performed at the same level, and denote this level by \(\alpha_{k}\) corresponding to a critical value \(C_{k}\). The procedure is continued in this way until no hypotheses are left to be tested.

To see the relation of this stagewise procedure to the fully sequential approach described at the beginning of the section which is based on the ordered differences \(|\vec{X}_{i}-\vec{X}_{j}|/\hat{\sigma}\), let us compare the two procedures when all the tests of the stagewise procedure are based on standardized ranges. In both cases the first step is based on \(|\vec{X}_{(s)}-\vec{X}_{(1)}|/\hat{\sigma}\) and rejects the homogeneity of \(\{\mu_{1},\,\ldots,\,\mu_{s}\}\) if this statistic is \(\geq\) some constant \(C_{s}\). The stagewise procedure next compares the two subranges

\[|\vec{X}_{(s)}-\vec{X}_{(2)}|/\hat{\sigma}\ \ \ {\rm and}\ \ \ |\vec{X}_{(s-1)}- \vec{X}_{(1)}|/\hat{\sigma}\]

with a common critical value \(C_{s-1}\). Note, however, that if the larger of the two is \(<C_{s-1}\), this will a fortiori be true of the smaller one. This second step could thus equally well be described as comparing the second largest of the ranges \(|\vec{X}_{i}-\vec{X}_{j}|/\hat{\sigma}\) with \(C_{s-1}\), and in case of acceptance terminating the procedure. In case of rejection, we would next compare the smaller of the two \((s-1)\)-ranges with \(C_{s-1}\). Continuing in this way, \(C_{i}\) would be used to test all eligible \(i\)-ranges.

The fully sequential procedure described at the beginning of the section also would terminate at the second step if the larger of the two \((s-1)\) ranges is too small. But if it is large enough for rejection, the next step would differ in two ways: (i) the critical level would be lowered further; (ii) the next test statistic would be the 3rd largest of the differences \(|\vec{X}_{i}-\vec{X}_{j}|/\hat{\sigma}\), which may but need not coincide with the smaller of the \((s-1)\)-ranges. Thus, the two procedures differ slightly, although they are very much in the same spirit.

To complete the description of a stagewise procedure, once the test statistics have been chosen, it is necessary to specify the critical values \(C_{2}\),..., \(C_{s}\) for the successive stages or equivalently the levels \(\alpha_{2}\),..., \(\alpha_{s}\) at which the tests are performed. Note that there is no \(\alpha_{1}\) of \(C_{1}\) since at the \(s\)th stage only singlets are left, and hence there are no longer any hypotheses to be tested.

Before discussing the best choice of \(\alpha\)'s let us consider some specific methods that have been proposed in the literature. Additional properties and uses of some of these will be mentioned at the end of the section.

(i) _Tukey's \(T\)-method_. This procedure employs the Studentized range test at each stage with a common critical value \(C_{k}=C\) for all \(k\). The method has an unusual feature which makes it particularly simple to apply. In general, in order to determine whether a particular subset \(S_{0}\) of means should be called nonhomogeneous, it is necessary to proceed stagewise since the homogeneity of \(S_{0}\) itself is not tested unless homogeneity has been rejected for all sets containing \(S_{0}\). However, with Tukey's \(T\)-method it is only necessary to test \(S_{0}\) itself. If the Studentized range of \(S_{0}\) exceeds \(C\), so will that of any set containing \(S_{0}\), and \(S_{0}\) is declared nonhomogeneous. In the contrary case, homogeneity of \(S_{0}\) is accepted. The two facts which jointly eliminate the need for a stagewise procedure in this case are (a) that the range, and hence the Studentized range, of \(S_{0}\) cannot exceed that of any set \(S\) containing \(S_{0}\), and (b) the constancy of the critical value. The next method applies this idea to a procedure based on \(F\)-tests.

(ii) _Gabriel's simultaneous test procedure_. \(F\)-statistics do not have property (a) above. However, this property is possessed by the statistics \(\nu F\), where \(\nu\) is the number of numerator degrees of freedom (Problem 9.28). Hence a procedure based on \(F\)-statistics with critical values \(C_{k}=C/(k-1)\) satisfies both (a) and (b), since \(k-1\) is the number of numerator degrees of freedom when \(k\) means are tested, that is, at the \(s-k+1\)st stage. This procedure, which in this form was proposed by Gabriel (1964), permits the testing of many additional hypotheses and when these are included becomes Scheffe's \(S\)-method, which will be discussed in Sections. 9.6 and 9.7.

(iii) _Fisher's least significant difference method_. This procedure employs an \(F\)-test at the first stage, and Studentized range tests with a common critical value \(C_{2}=\cdots=C_{s}\) at all succeeding stages. The constants \(C_{s}\) and \(C_{2}\) are related by the fact that the first stage \(F\)-test and the pairwise \(t\)-test of the last stage have the same level.

The usual descriptions of (i) and (iii) consider only the first and last stages of these procedures, and omit the conclusions which can be drawn from the intermediate stages.

Several classes of procedures have been defined by prescribing the significance levels \(\alpha_{k}\), which can then be applied to the chosen test statistics at each stage. Examples are:* _The Newman-Keuls levels_: \[\alpha_{k}=\alpha.\]
* _The Duncan levels_: \[\alpha_{k}=1-\gamma^{k-1}.\]
* _The Tukey levels_: \[\alpha_{k}=\left\{\begin{array}{ll}1-\gamma^{k/2},&2<k<s-1\\ 1-\gamma^{s/2},&k=\ s-1,s.\end{array}\right.\]

In both (v) and (vi), \(\gamma=1-\alpha_{2}\).

Most of the above methods and some others are reviewed in the books by Hochberg and Tamhane (1987) and Hsu (1996).

Let us now consider the choice of the levels \(\alpha_{k}\) more systematically. For this purpose, denote the probability of at least one false rejection, that is, of rejecting homogeneity of at least one set of \(\mu\)'s which in fact is homogeneous, by \(\alpha(\mu_{1},\ldots,\mu_{s})\). As before we impose the restriction that the FWER should not exceed \(\alpha\), so that

\[\alpha(\mu_{1},\ldots,\mu_{s})\leq\alpha\quad\mbox{for all ($\mu_{1},\ldots,\mu_{s}$)}. \tag{9.88}\]

In order to study the best choice of \(\alpha_{2},\ldots\alpha_{s}\) subject to (9.88), let us begin by assuming \(\sigma^{2}\) to be known, say \(\sigma^{2}=1\). Then the \(F\)-tests are replaced by \(\chi^{2}\)-tests and the Studentized range tests by range tests; the latter reject when the range of the subgroup being tested is too large.

To evaluate the maximum of the left side of (9.88), suppose that the \(\mu\)'s fall into \(r\) distinct subgroups of sizes \(v_{1},\ldots,v_{r}\) (\(\sum v_{i}=s\)), say

\[\mu_{i_{1}}=\cdots=\mu_{i_{v_{1}}};\quad\mu_{i_{v_{1}+1}}=\cdots=\mu_{i_{v_{1} +v_{2}}};\ldots, \tag{9.89}\]

where \((i_{1},\ldots,i_{s})\) is a permutation of \((1,\ldots,s)\). Then, both \(\chi^{2}\) and range statistics for testing the \(r\) hypotheses

\[H_{1}^{\prime}:\mu_{i_{1}}=\cdots=\mu_{i_{v_{1}}};\quad\quad H_{2}^{\prime}:\mu _{i_{v_{1}+1}}=\cdots=\mu_{i_{v_{1}+v_{2}}};\ldots \tag{9.90}\]

are independent. The following result then gives conditions on the individual levels \(\alpha_{i}\) so that the FWER is controlled.

**Lemma 9.5.1**: _If the test statistics for testing the \(r\) hypotheses (9.90) are independent, then the sup of \(\alpha(\mu_{1},\ldots,\mu_{s})\) over all \((\mu_{1},\ldots,\mu_{s})\) satisfying (9.89) is given by_

\[\sup\alpha(\mu_{1},\ldots,\mu_{s})=1-\prod_{i=1}^{r}(1-\alpha_{v_{i}})\, \tag{9.91}\]

[MISSING_PAGE_FAIL:477]

is equal to \(k+1\) by two \(v\)'s--one equal to \(k\) and one equal to \(1\)--and denote the resulting set of \(v\)'s by \(w_{1}\),..., \(w_{r^{\prime}}\). Then

\[\prod_{i=1}^{r}(1-\alpha^{\prime}_{v_{i}})=\prod_{i=1}^{r^{\prime}}(1-\alpha_{w _{i}})\geq 1-\alpha\.\]

(ii) The left side of (9.92) involves \(\alpha_{s}\) if and only if \(r=1\), \(v_{1}=s\). Thus the only restriction on \(\alpha_{s}\) is \(\alpha_{s}\leq\alpha\), and the only admissible choice is \(\alpha_{s}=\alpha\). The argument for \(\alpha_{s-1}\) is analogous (Problem 9.30).

Part (ii) of this lemma shows that Tukey's \(T\)-method and Gabriel's simultaneous test procedure are inadmissible since in both \(\alpha_{s-1}<\alpha_{s}\). The same argument shows Duncan's set of levels to be inadmissible. (These choices can however be justified from other points of view; see for example Spjotvoll (1974) and the comments at the end of the section.) It also follows from the lemma that for \(s=3\) there is a unique best choice of levels, namely

\[\alpha_{2}=\alpha_{3}=\alpha. \tag{9.94}\]

Having fixed \(\alpha_{s}=\alpha_{s-1}=\alpha\), how should we choose the remaining \(\alpha\)'s? In order to have a reasonable chance of detecting existing inhomogeneities for all patterns, we should like to have none of the \(\alpha\)'s too small. In view of part (i) of Lemma 9.5.2, this aim is perhaps best achieved by maximizing \(\alpha_{2}\), the level at the last stage when individual pairs are being tested.

**Lemma 9.5.3**: _Under the assumptions of Lemma 9.5.1, the maximum value of \(\alpha_{2}\) subject to (9.88) is_

\[\alpha_{2}=1-(1-\alpha)^{[s/2]^{-1}}, \tag{9.95}\]

_where \([A]\) denotes the largest integer \(\leq A\)._

Proof. Instead of fixing \(\alpha\) and maximizing \(\alpha_{2}\), it is more convenient to fix \(\alpha_{2}\), say at \(\alpha^{*}\), and then to minimize \(\alpha\). The lemma will be proved by showing that the resulting minimum value of \(\alpha\) is

\[\alpha=1-(1-\alpha^{*})^{[s/2]}. \tag{9.96}\]

Suppose first that \(s\) is even. Since \(\alpha_{2}\) is fixed at \(\alpha^{*}\), it follows from Lemma 9.5.1 that the right side of (9.91) can be made arbitrarily close to \(\alpha\) given by (9.96). This is seen by letting \(\nu_{1}=\cdots=\nu_{s/2}=2\). When \(s\) is odd, the same argument applies if we put an additional \(\nu\) equal to \(1\).

Lemmas 9.5.2 and 9.5.3 show that any procedure with \(\alpha_{s}=\alpha_{2}\), and hence Fisher's least-significant-difference procedure and the Newman-Keuls choice of levels, is admissible for \(s=3\) but inadmissible for \(s\geq 4\). The second of these statements is seen from the fact that (9.88) implies \(\alpha_{2}\leq 1-(1-\alpha)^{[s/2]^{-1}}<\alpha\) when \(s\geq 4\). The choice \(\alpha_{2}=\alpha_{s}\) thus violates Lemma 9.5.2(ii).

Once \(\alpha_{2}\) has been fixed at the value given by (9.95), it turns out that subject to (9.88) there exists a unique optimal choice of the remaining \(\alpha\)'s when \(s\) is odd, and a narrow range of choices when \(s\) is even.

**Theorem 9.5.1**: _When \(s\) is odd, then \(\alpha_{3},\ldots,\alpha_{s}\) are maximized, subject to (9.88) and (9.95), by_

\[\alpha_{i}^{s}=1-(1-\alpha_{2})^{[i/2]}\;, \tag{9.97}\]

_and these values can be attained simultaneously._

Proof. If we put \(\gamma_{i}=1-\alpha_{i}\) and \(\gamma=1-\alpha_{2}\), then by (9.90) and (9.97) any procedure satisfying the conditions of the theorem must satisfy

\[\prod\gamma_{v_{i}}\geq\gamma^{[s/2]}=\gamma^{(s-1)/2}\]

Let \(i\) be odd, and consider any configuration in which \(v_{1}=i\) and all the remaining \(v\)'s are equal to 2. Then

\[\gamma_{i}\gamma^{(s-i)/2}\geq\gamma^{(s-1)/2},\]

and hence

\[\gamma_{i}\geq\gamma_{i}^{*}=1-\alpha_{i}^{*}\;. \tag{9.98}\]

An analogous argument proves (9.97) for even \(i\).

Consider now the procedure defined by (9.97). This clearly satisfies (9.95), and it only remains to check that it also satisfies (9.88) or equivalently (9.92), and hence that

\[\prod\gamma^{[v_{i}/2]}\geq\gamma^{(s-1)/2}\]

or that

\[\sum_{i=1}^{r}\left[\frac{v_{i}}{2}\right]\leq\frac{s-1}{2}\]

Now \(\sum[v_{i}/2]=(s-b)/2\), where \(b\) is the number of odd \(v\)'s (including ones). Since \(s\) is odd, \(b\geq 1\), and this completes the proof.

Note that the levels (9.97) are close to the Tukey levels (vi), which are admissible but do not satisfy (9.95).

When \(s\) is even, a uniformly best choice is not available. In this case, the Tukey levels (vi) satisfy (9.95), are admissible, and constitute a reasonable choice. [See Lehmann and Shaffer (1979).]

So far we have assumed \(\sigma^{2}=1\) in order to get independence of the \(r\) test statistics used for testing the hypotheses \(H_{i}^{\prime}\), \(i=1,\ldots,r\). If \(\sigma^{2}\) is unknown, the \(\chi^{2}\) and range statistics are replaced by \(F\) and studentized range statistics. These are no longer independent but are positively quadrant dependent in the sense that 

[MISSING_PAGE_EMPTY:2082]

Proof.: That the right side of (9.101) is an upper bound for \(\alpha(\mu_{1},\ldots,\mu_{s})\) follows from the proof of Lemma 9.5.1 and the assumption of positive quadrant dependence. 

Note, however, that we can no longer assert that the upper bound is sharp. For the \(F\) and Studentized range tests, the sharp upper bound will depend on the total sample size \(n\).

Theorem 9.5.2 guarantees that the procedures using the \(\alpha\)-levels derived under the assumption of independence continue to control the FWER even in the case of positive dependence. The proof of Lemma 9.5.2 shows that \(\alpha_{s}=\alpha_{s-1}=\alpha\) continues to be necessary for admissibility even in the positively dependent case. However, the maximization results for \(\alpha_{2},\ldots,\alpha_{s}\) can then no longer be asserted. They nevertheless have the great advantage that they define procedures that do not require detailed knowledge of the joint distribution of the various test statistics.

Even in the simplified version with known variance the multiple testing problem considered in the present section is clearly much more difficult than the testing of a single hypothesis; the procedures presented above still ignore many important aspects of the problem.

1. _Choice of test statistic._ The most obvious feature that has not been dealt with is the choice of test statistics. Unfortunately it does not appear that the invariance considerations which were so helpful in the case of a single hypothesis play a similar role here.
2. _Order relation of significant means._ Whenever two means \(\mu_{i}\) and \(\mu_{j}\) are judged to differ, we should like to state not only that \(\mu_{i}\neq\mu_{j}\), but that if \(\bar{X}_{i}<\bar{X}_{j}\) then also \(\mu_{i}<\mu_{j}\). Such additional statements introduce the possibility of additional errors (stating \(\mu_{i}<\mu_{j}\) when in fact \(\mu_{i}>\mu_{j}\)), and it is not obvious that when these are included, the probability of at least one error is still bounded by \(\alpha\). [For work on directional errors, see Finner (1999), Grandhi et al. (2019) and Shaffer (1990, 2002).]
3. _Nominal versus true levels._ The levels \(\alpha_{2},\ldots,\alpha_{s}\), sometimes called _nominal levels_, are the levels at which the hypotheses \(\mu_{i}=\mu_{j},\mu_{i}=\mu_{j}=\mu_{k},\ldots\) are tested. They are however not the true probabilities of falsely rejecting the homogeneity of these sets, but only the upper bounds of these probabilities with respect to variation of the remaining \(\mu\)'s. The true probabilities tend to be much smaller (particularly when \(s\) is large), since they take into account that homogeneity of a set \(S_{0}\) is rejected only if it is also rejected for all sets \(S\) containing \(S_{0}\).
4. _Interpretability._ As pointed out at the beginning of the section, the totality of acceptance and rejection statements resulting from a multiple comparison procedure typically does not lead to a simple partition of means. This is illustrated by the possibility that the hypothesis of homogeneity is rejected for a set \(S\) but for none of its subsets. As another example, consider the case \(s=3\), where it may happen that the hypotheses \(\mu_{i}=\mu_{j}\) and \(\mu_{j}=\mu_{k}\) are accepted but \(\mu_{i}=\mu_{k}\) is rejected. The number of such "inconsistencies" and the corresponding difficulty of interpreting the results may be formidable. Measures of the complexity of the totality of statements as a third criterion (besides level and power) are discussed by Shaffer (1981). The inconsistencies and resulting difficulties of interpretation suggest the consideration of an alternative formulation of the problem which avoids this difficulty. Instead of testing the \(\binom{s}{2}\) hypotheses \(H_{i,j}:\ \mu_{i}=\mu_{j}\), estimate the (unknown) partition of the \(\mu\)'s defined by (9.89). Possible approaches to such procedures are discussed for example in Hochberg and Tamhane (1987, Chapter 10, Section 6) and by Dayton (2003).
5. Procedures (i) and (ii) can be inverted to provide simultaneous confidence intervals for all differences \(\mu_{j}-\mu_{i}\). The \(T\)-method (discussed in Problems 9.40-9.43) was designed to give simultaneous intervals for all differences \(\mu_{j}-\mu_{i}\); it can be extended to cover also all _contrasts_ in the \(\mu\)'s, that is, all linear functions \(\sum c_{i}\mu_{i}\) with \(\sum c_{i}=0\), but against more complex contrasts the intervals tend to be longer than those of Scheffes \(S\)-method, which was intended for the simultaneous consideration of all contrasts. [For a comparison of the two methods, see for example Scheffe (1959, Section 3.7) and Arnold (1981, Chapter 12).] It is a disadvantage of the remaining (truly stagewise or sequential) procedures of this section that the problem of corresponding confidence sets is considerably more complicated. For a discussion of such confidence methods, see Holm (1999) and the references cited there.
6. To control the rate of false rejections, we have restricted attention to procedures controlling the FWER, the probability of at least one error. Instead, one might wish to control the false discovery rate as defined at the end of Section 9.1; see Benjamini and Hochberg (1995). Alternatively, an optimality theory based on the number of false rejections is given in Spjotvoll (1972). Another possibility is the control the \(k\)-FWER, the probability of making \(k\) or more false rejections, as well as the probability that the false discovery proportion exceeds some threshold; see Kornetal (2004), Romano and Shaikh (2006a, b) and Lehmann and Romano (2005).
7. The optimal choice of the \(\alpha_{k}\) discussed in this section can be further improved, at the cost of considerable additional complication, by permitting the \(\alpha\)'s to depend on the outcomes of the other tests. This possibility is discussed, for example, in Marcus et al. (1976); see also Holm (1979) and Shaffer (1984).

The procedures discussed in this section were concerned with testing the equality of means. In more complex situations, further problems arise. Consider, for example, the two-way layout of Section 7.5 with

\[\mu_{i,j}=\mu+\alpha_{i}+\beta_{j}+\gamma_{i,j}\quad(\sum\alpha_{i}=\sum\beta_{ j}=\sum_{i}\gamma_{i,j}=\sum_{j}\gamma_{i,j}=0)\.\]

If we are interested in multiple testing of the \(\alpha\)'s, \(\beta\)'s, and \(\gamma\)'s, the first question that arises is whether we want to treat these three cases (\(\alpha\)'s, \(\beta\)'s, \(\gamma\)'s) as a single family, as two families (the main effects forming one family, the interactions the other), or as three families in which each of the three sets is handled separately.

The most appropriate designation of what constitutes a family depends very much on context. Consider, for example, the National Assessment of Educational Progress which makes it possible to compare the progress made by any two states. For a federal report, the set of all \(\binom{50}{2}\) possible hypotheses would constitute an appropriate family. However, a particular state would be interested primarily in the comparison of its performance with those of the other 49 states, thus leading to a family of size 49. A comparison which is not significant in the federal report might then turn out to be significant in the state report. Some of the issues concerning the most suitable definition of family are discussed in Tukey (1991) and in the books by Hochberg and Tamhane (1987), and Westfall and Young (1993).

We shall in the next two sections consider simultaneous inferences for various families of linear functions of means in normal linear models. However, since we are assuming fully articulated parametric models, we shall consider the slightly more demanding problem of obtaining simultaneous confidence intervals rather than restricting attention to hypothesis testing.

As the simplest example, suppose that \(X_{1},\ldots,X_{s}\) are normal variables with means \(\mu_{1}\),..., \(\mu_{s}\) and unit variance. We can then apply to the hypotheses \(H_{i}:\;\mu_{i}=\mu_{i,0}\) the approach of Section 9.1 and test these hypotheses by means of a stepdown procedure. The resulting acceptance regions can then be converted in the usual way into confidence sets. It is shown in Holm (1999) that these sets are rather complicated and not rectangular, so that they do not consist of intervals for the individual \(\mu_{i}\)'s. (They can, of course, be enclosed in a larger rectangle, but the intervals obtained by such a process tend to be unnecessarily large.)

### Scheffe's \(S\)-Method: A Special Case

If \(X_{1},\ldots,X_{r}\) are independent normal with common variance \(\sigma^{2}\) and expectations \(E(X_{i})=\alpha+\beta t_{i}\), confidence sets for (\(\alpha\), \(\beta\)) were obtained in Section 7.6. A related problem is that of determining confidence bands for the whole regression line \(\xi=\alpha+\beta t\), that is, functions \(L^{\prime}(t;\,X)\), \(M^{\prime}(t;\,X)\) such that

\[P\{L^{\prime}(t;\,X)\leq\alpha+\beta t\leq M^{\prime}(t;\,X)\mbox{ for all }t\}=\gamma. \tag{9.102}\]

The problem of obtaining simultaneous confidence intervals for a continuum of parametric functions arises also in other contexts. In the present section, a general problem of this kind will be considered for linear models. Confidence bands for an unknown distribution function were treated in Section 6.13.

Suppose first that \(X_{1},\ldots,X_{r}\) are independent normal with variance \(\sigma^{2}=1\) and with means \(E(X_{i})=\xi_{i}\), and that simultaneous confidence intervals are required for all linear functions \(\sum u_{i}\xi_{i}\). No generality is lost by dividing \(\sum u_{i}\xi_{i}\) and its lower and upper bound by \(\sqrt{\sum u_{i}^{2}}\), so that attention can be restricted to confidence sets

\[S(x)=\{\xi:\;L(u;\,x)\leq\sum u_{i}\xi_{i}\leq M(u;\,x)\quad\mbox{for all }\;u\in U\}\,, \tag{9.103}\]

where \(x\), \(u\) denote both the vectors with coordinates \(x_{i}\), \(u_{i}\) and the \(r\times 1\) column matrices with these elements, and where \(U\) is the set of all \(u\) with \(\sum u_{i}^{2}=1\). The sets \(S(x)\) are to satisfy

\[P_{\xi}\{\xi\in S(X)\}=\gamma\;\;\mbox{for all}\;\;\xi=(\xi_{1},\ldots,\xi_{r}). \tag{9.104}\]

Since \(u=(u_{1},\ldots,u_{r})\in U\) if and only if \(-u=(-u_{1},\ldots,-u_{r})\in U\), the simultaneous inequalities (9.103) imply \(L(-u;\,x)\leq-\sum u_{i}\xi_{i}\leq M(-u;\,x)\), and hence

\[-M(-u;\,x)\leq\sum u_{i}\xi_{i}\leq-L(-u;\,x)\]

and

\[\max(L(u;\,x),\,-M(-u;\,x))\leq\sum u_{i}\xi_{i}\leq\min(M(u;\,x),\,-L(-u;\,x)).\]

Nothing is therefore lost by assuming that \(L\) and \(M\) satisfy

\[L(u;\,x)=-M(-u;\,x). \tag{9.105}\]

The problem of determining suitable confidence bounds \(L(u;\,x)\) and \(M(u;\,x)\) is invariant under the group \(G_{1}\) of orthogonal transformations

\[G_{1}:gx=Qx,\,\bar{g}\xi=Q\xi\;\;\;\;\;\;(Q\;\mbox{an orthogonal $r\times r$ matrix}).\]

Writing \(\sum u_{i}\xi_{i}=u^{\prime}\xi\), we have

\[g^{*}S(x) = \{Q\xi:L(u;\,x)\leq u^{\prime}\xi\leq M(u;\,x)\;\mbox{for all}\;u \in U\}\] \[= \{\xi:L(u;\,x)\leq u^{\prime}(Q^{-1}\xi)\leq M(u;\,x)\;\mbox{for all}\;u \in U\}\] \[= \{\xi:L(Q^{-1}u;\,x)\leq u^{\prime}\xi\leq M(Q^{-1}u;\,x)\;\mbox{ for all}\;u\in U\},\]

where the last equality uses the fact that \(U\) is invariant under orthogonal transformations of \(u\).

Since

\[S(gx)=\{\xi:L(u;\,Qx)\leq u^{\prime}\xi\leq M(u;\,Qx)\;\mbox{for all}\;u\in U\},\]

the confidence sets \(S(x)\) are equivariant under \(G_{1}\) if and only if

\[L(u;\,Qx)=L(Q^{-1}u;\,x),\;\;\;\;\;\;M(u;\,Qx)=M(Q^{-1}u;\,x),\]

or equivalently if

\[L(\,Qu;\,Qx)=L(u;\,x),\;\;\;\;\;\;M(\,Qu;\,Qx)=M(u;\,x) \tag{9.106}\] \[\mbox{for all}\;\;\;x,\;\;Q\;\mbox{and}\;u\in U,\]that is, if \(L\) and \(M\) are invariant under common orthogonal transformations of \(u\) and \(x\).

A function \(L\) of \(u\) and \(x\) is invariant under these transformations if and only if it depends on \(u\) and \(x\) only through \(u^{\prime}x\), \(x^{\prime}x\), and \(u^{\prime}u\) (Problem 9.34(i)) and hence (since \(u^{\prime}u=1\)) if there exists \(h\) such that

\[L(u;x)=h(u^{\prime}x,x^{\prime}x). \tag{9.107}\]

A second group of transformations leaving the problem invariant is the group of translations

\[G_{2}:gx=x+a,\,\bar{g}\xi=\xi+a\]

where \(x+a=(x_{1}+a_{1},\ldots,x_{r}+a_{r})\). An argument paralleling that leading to (9.106) shows that \(L(u;x)\) is equivariant under \(G_{2}\) if and only if (Problem 9.34(ii))

\[L(u;x+a)=L(u;x)+\sum a_{i}u_{i}\ \ \mbox{for all}\ \ x,a,\ \mbox{and}\ u. \tag{9.108}\]

The function \(h\) of (9.107) must therefore satisfy

\[h[u^{\prime}(x+a),\,(x+a)^{\prime}(x+a)] = h(u^{\prime}x,\,x^{\prime}x)+a^{\prime}u\] \[\ \ \mbox{for all}\ \ \ a,\,x\ \mbox{and}\ u\in U,\]

and hence, putting \(x=0\),

\[h(u^{\prime}a,a^{\prime}a)=a^{\prime}u+h(0,0).\]

A necessary condition (which clearly is also sufficient) for \(S(x)\) to be equivariant under both \(G_{1}\) and \(G_{2}\) is therefore the existence of constants \(c\) and \(d\) such that

\[S(x)=\left\{\xi:\sum u_{i}x_{i}-c\leq\sum u_{i}\xi_{i}\leq\sum u_{i}x_{i}+d\ \ \mbox{for all}\ u\in U\right\}\]

From (9.105), it follows that \(c=d\), so that the only equivariant families \(S(x)\) are given by

\[S(x)=\left\{\xi:\left|\sum u_{i}(x_{i}-\xi_{i})\right|\leq c\ \ \mbox{for all}\ u\in U\right\} \tag{9.109}\]

The constant \(c\) is determined by (9.104), which now reduces to

\[P_{0}\left\{\left|\sum u_{i}X_{i}\right|\leq c\ \ \mbox{for all}\ u\in U\right\}=\gamma. \tag{9.110}\]

By the Cauchy-Schwarz inequality \((\sum u_{i}X_{i})^{2}\leq\sum X_{i}^{2}\), since \(\sum u_{i}^{2}=1\), and hence

\[\left|\sum u_{i}X_{i}\right|\leq c\ \ \mbox{for all}\ u\in U\ \mbox{if and only if}\ \sum_{i}X_{i}^{2}\leq c^{2}. \tag{9.111}\]The constant \(c\) in (9.109) is therefore given by

\[P(\chi_{r}^{2}\leq c^{2})=\gamma. \tag{9.112}\]

In (9.109), it is of course possible to drop the restriction \(u\in U\) by writing (9.109) in the equivalent form

\[S(x)=\left\{\xi:\left|\sum u_{i}(x_{i}-\xi_{i})\right|\leq c\sqrt{\sum u_{i}^{2 }}\ \ \text{for all}\ u\right\}. \tag{9.113}\]

So far attention has been restricted to the confidence bands (9.103). However, confidence sets do not have to be intervals, and it may be of interest to consider more general simultaneous confidence sets

\[S(x):\sum u_{i}\xi_{i}\in A(u,x)\ \text{for all}\ u\in U. \tag{9.114}\]

For these sets, the equivariance conditions (9.106) and (9.108) become respectively (Problem 9.35)

\[A(Qu,\,Qx)=A(u,x)\ \ \text{for all}\ \ x,\,Q\ \text{and}\ u\in U \tag{9.115}\]

and

\[A(u,x+a)=A(u,x)+u^{\prime}a\ \ \text{for all}\ \ u,x,\ \text{and}\ a. \tag{9.116}\]

The first of these is equivalent to the condition that the set \(A(u,x)\) depends on \(u\in U\) and \(x\) only through \(u^{\prime}x\) and \(x^{\prime}x\). On the other hand putting \(x=0\) in (9.116) gives

\[A(u,a)=A(u,0)+u^{\prime}a.\]

It follows from (9.115) that \(A(u,0)\) is a fixed set \(A_{1}\) independent of \(u\), so that

\[A(u,x)=A_{1}+u^{\prime}x. \tag{9.117}\]

The most general equivariant sets (under \(G_{1}\) and \(G_{2}\)) are therefore of the form

\[\sum u_{i}(x_{i}-\xi_{i})\in A\ \ \text{for all}\ \ u\in U, \tag{9.118}\]

where \(A=-A_{i}\).

We shall now suppose that \(r>1\) and then show that among all \(A\) which define confidence sets (9.118) with confidence coefficient \(\geq\gamma\), the sets (9.109) are smallest2in the very strong sense that if \(A_{0}=[-c_{0},\,c_{0}]\) denotes the set (9.109) with confidence coefficient \(\gamma\), then \(A_{0}\) is a subset of \(A\).

To see this, note that if \(Y_{i}=X_{i}-\xi_{i}\), the sets \(A\) are those satisfying

\[P\left(\sum u_{i}Y_{i}\in A\ \ \mbox{for all}\ u\in U\right)\geq\gamma. \tag{9.119}\]

Now the set of values taken on by \(\sum u_{i}y_{i}\) for a fixed \(y=(y_{1},\,\ldots,\,y_{r})\) as \(u\) ranges over \(U\) is the interval (Problem 9.35)

\[I\left(y\right)=\left[-\sqrt{\sum y_{i}^{2}},+\sqrt{\sum y_{i}^{2}}\right].\]

Let \(c^{*}\) be the largest value of \(c\) for which the interval \([-c,\,c]\) is contained in \(A\). Then the probability (9.119) is equal to

\[P\{I(Y)\subset A\}=P\{I(Y)\subset[-c^{*},c^{*}]\}.\]

Since \(P\{I(Y)\subset A\}\geq\gamma\), it follows that \(c^{*}\geq c_{0}\), and this completes the proof.

It is of interest to compare the simultaneous confidence intervals (9.109) for all \(\sum u_{i}\xi_{i}\), \(u\in U\), with the joint confidence spheres for \((\xi_{1},\,\ldots,\,\xi_{r})\) given by (6.46). These two sets of confidence statements are equivalent in the following sense.

**Theorem 9.6.1**: _The parameter vector \((\xi_{1},\,\ldots,\,\xi_{r})\) satisfies \(\sum(X_{i}-\xi_{i})^{2}\leq c^{2}\) if and only if it satisfies (9.109)._

Proof. The result follows immediately from (9.111) with \(X_{i}\) replaced by \(X_{i}-\xi_{i}\).

Another comparison of interest is that of the simultaneous confidence intervals (9.113) for all \(u\) with the corresponding interval

\[S^{\prime}(x)=\left\{\xi:\left|\sum u_{i}(x_{i}-\xi_{i})\right|\leq c^{\prime} \sqrt{\sum u_{i}^{2}}\right\} \tag{9.120}\]

for a single given \(u\). Since \(\sum u_{i}(X_{i}-\xi_{i})/\sqrt{\sum u_{i}^{2}}\) has a standard normal distribution, the constant \(c^{\prime}\) is determined by \(P(\chi_{1}^{2}\leq c^{\prime}{}^{2})=\gamma\) instead of by (9.112). If \(r>1\), the constant \(c^{2}=c_{r}^{2}\) is clearly larger than \(c^{\prime}{}^{2}=c_{1}^{2}\). The lengthening of the confidence intervals by the factor \(c_{r}/c_{1}\) in going from (9.120) to (9.113) is the price one must pay for asserting confidence \(\gamma\) for all \(\sum u_{i}\xi_{i}\) instead of a single one.

In (9.120), it is assumed that the vector \(u\) defines the linear combination of interest and is given before any observations are available. However, it often happens that an interesting linear combination \(\sum\hat{u}_{i}\xi_{i}\) to be estimated is suggested by the data. The intervals

\[\left|\sum\hat{u}_{i}(x_{i}-\xi_{i})\right|\leq c\sqrt{\sum\hat{u}_{i}^{2}} \tag{9.121}\]

[MISSING_PAGE_EMPTY:2090]

These _maximum-modulus_ intervals for the \(\xi\)'s can be extended to all linear combinations \(\sum u_{i}\xi_{i}\) of the \(\xi\)'s by noting that the right side of (9.124) is equal to the set (Problem 9.36(ii))

\[\left\{\xi:\left|\sum u_{i}\left(X_{i}-\xi_{i}\right)\right|\leq\Delta\sum|u_{i} |\text{ for all }u\right\}, \tag{9.126}\]

which therefore also has probability \(\gamma\), but which is not equivariant under \(G_{1}\). A comparison of the intervals (9.126) with the Scheffe intervals (9.113) shows (Problem 9.36(iii)) that the intervals (9.126) are shorter when \(\sum u_{j}\xi_{j}=\xi_{i}\) (i.e., when \(u_{j}=1\) for \(j=i\), and \(u_{j}=0\) otherwise), but that they are longer for example when \(u_{1}=\cdots=u_{r}\).

### Scheffe's \(S\)-Method for General Linear Models

The results obtained in the preceding section for the simultaneous estimation of all linear functions \(\sum u_{i}\xi_{i}\) when the common variance of the variables \(X_{i}\) is known easily extend to the general linear model of Section 7.1. In the canonical form (7.2), the observations are \(n\) independent normal random variables with common unknown variance \(\sigma^{2}\) and with means \(E(Y_{i})=\eta_{i}\) for \(i=1,\ldots,s\) and \(E(Y_{i})=0\) for \(i=s+1,\ldots,n\). Simultaneous confidence intervals are required for all linear functions \(\sum_{i=1}^{r}u_{i}n_{i}\) with \(u\in U\), where \(U\) is the set of all \(u=(u_{1},\ldots,u_{r})\) with \(\sum_{i=1}^{r}u_{i}^{2}=1\). Invariance under the translation group \(Y_{i}^{\prime}=Y_{i}\,+\,a_{i}\), \(i=r+1,\ldots,s\), leaves \(Y_{1},\ldots,Y_{r}\); \(Y_{s+1},\ldots,Y_{n}\) as maximal invariants, and sufficiency justifies restricting attention to \(Y=(Y_{1},\ldots,Y_{r})\) and \(S^{2}=\sum_{j=s+1}^{n}Y_{j}^{2}\). The confidence intervals corresponding to (9.103) are therefore of the form

\[L(u;\,y,\,S)\leq\sum_{i=1}^{r}u_{i}\eta_{i}\leq M(u;\,y,\,S)\quad\text{ for all }\;\;u\in U, \tag{9.127}\]

and in analogy to (9.105) may be assumed to satisfy

\[L(u;\,y,\,S)=-M(-u;\,y,\,S). \tag{9.128}\]

By the argument leading to (9.107), it is seen in the present case that equivariance of \(L(u;\,y,\,S)\) under \(G_{1}\) requires that

\[L(u;\,y,\,S)=h(u^{\prime}y,\,y^{\prime}y,\,S),\]

and equivariance under \(G_{2}\) requires that \(L\) be of the form \[L(u;\,y,\,S)=\sum_{i=1}^{r}u_{i}y_{i}-c(S).\]

Since \(\sigma^{2}\) is unknown, the problem is now also invariant under the group of scale changes

\[G_{3}:y_{i}^{\prime}=by_{i}\ (i=1,\ldots,r),\,S^{\prime}=bS\ (b>0).\]

Equivariance of the confidence intervals under \(G_{3}\) leads to the condition (Problem 9.37(i))

\[L(u;\,by,\,bS)=bL(u;\,y,\,S)\quad\text{ for all }\ \ b>0,\]

and hence to

\[b\sum u_{i}y_{i}-c(bS)=b\Bigl{[}\sum u_{i}y_{i}-c(S)\Bigr{]},\]

or \(c(bS)=bc(S)\). Putting \(S=1\) shows that \(c(S)\) is proportional to \(S\). Thus

\[L(u;\,y,\,S)=\sum u_{i}y_{i}-cS,\quad\quad M(u;\,y,\,S)=\sum u_{i}y_{i}+dS,\]

and by (9.128), \(c=d\), so that the equivariant simultaneous intervals are given by

\[\sum u_{i}y_{i}-cS\leq\sum u_{i}\eta_{i}\leq\sum u_{i}y_{i}+cS\ \ \text{for all }\ \ u\in U. \tag{9.129}\]

Since (9.129) is equivalent to

\[\frac{\sum(y_{i}-\eta_{i})^{2}}{S^{2}}\leq c^{2},\]

the constant \(c\) is determined from the \(F\)-distribution by

\[P_{0}\left\{\frac{\sum Y_{i}^{2}/r}{S^{2}/(n-s)}\leq\frac{n-s}{r}c^{2}\right\} =P_{0}\left\{F_{r,n-s}\leq\frac{n-s}{r}c^{2}\right\}=\gamma. \tag{9.130}\]

As in (9.113), the restriction \(u\in U\) can be dropped; this only requires replacing \(c\) in (9.129) and (9.130) by \(c\sqrt{\sum u_{i}^{2}}=c\sqrt{\text{Var}\sum u_{i}Y_{i}/\sigma^{2}}\).

As in the case of known variance, instead of restricting attention to the confidence bands (9.129), one may wish to permit more general simultaneous confidence sets

\[\sum u_{i}\eta_{i}\in A(u;\,y,\,S). \tag{9.131}\]

The most general equivariant confidence sets are then of the form (Problem 9.37(ii))

\[\frac{\sum u_{i}(y_{i}-\eta_{i})}{S}\in A\ \ \text{for all }\ u\in U, \tag{9.132}\]and for a given confidence coefficient, the set \(A\) is minimized by \(A_{0}=[-c,\,c]\), so that (9.132) reduces to (9.129).

For applications, it is convenient to express the intervals (9.129) in terms of the original variables \(X_{i}\) and \(\xi_{i}\). Suppose as in Section 7.1 that \(X_{1},\ldots,X_{n}\) are independently distributed as \(N(\xi_{i},\sigma^{2})\), where \(\xi=(\xi_{1},\ldots,\xi_{n})\) is assumed to lie in a given \(s\)-dimensional linear subspace \(\prod_{\Omega}\) (\(s<n\)). Let \(V\) be an \(r\)-dimensional subspace of \(\prod_{\Omega}\) (\(r\leq s\)), let \(\hat{\xi}_{i}\) be the least squares estimates of the \(\xi\)'s under \(\prod_{\Omega}\), and let \(S^{2}=\sum(X_{i}-\hat{\xi}_{i})^{2}\). Then the inequalities

\[\sum v_{i}\hat{\xi}_{i}-cS\sqrt{\frac{\operatorname{Var}\bigl{(} \sum v_{i}\hat{\xi}_{i}\bigr{)}}{\sigma^{2}}} \leq\sum v_{i}\xi_{i}\leq\sum v_{i}\hat{\xi}_{i}+cS\sqrt{\frac{ \operatorname{Var}\bigl{(}\sum v_{i}\hat{\xi}_{i}\bigr{)}}{\sigma^{2}}}\] \[\text{for all}\quad v\in V, \tag{9.133}\]

with \(c\) given by (9.130), provide simultaneous confidence intervals for \(\sum v_{i}\xi_{i}\) for all \(v\in V\) with confidence coefficient \(\gamma\).

This result is an immediate consequence of (9.129) and (9.130) together with the following three facts, which will be proved below:

1. If \(\sum_{i=1}^{s}u_{i}\eta_{i}=\sum_{j=1}^{n}v_{j}\xi_{j}\), then \(\sum_{i=1}^{s}u_{i}Y_{i}=\sum_{j=1}^{n}v_{j}\hat{\xi}_{j}\);
2. \(\sum_{i=s+1}^{n}Y_{i}^{2}=\sum_{j=1}^{n}(X_{j}-\hat{\xi}_{j})^{2}\),

To state (iii), note that the \(\eta\)'s are obtained as linear functions of the \(\xi\)'s through the relationship

\[(\eta_{1},\ldots,\eta_{r},\,\eta_{r+1},\ldots,\eta_{s},0,\,\ldots,0)^{\prime} =C(\xi_{1},\ldots,\xi_{n})^{\prime} \tag{9.134}\]

where \(C\) is defined by (7.1) and the prime indicates a transpose. This is seen by taking the expectation of both sides of (7.1). For each vector \(u=(u_{1},\ldots,u_{r})\), (9.134) expresses \(\sum u_{i}\eta_{i}\) as a linear function \(\sum v_{j}^{(u)}\xi_{j}\) of the \(\xi\)'s.
3. As \(u\) ranges over \(r\)-space, \(v^{(u)}=(v_{1}^{(u)},\ldots,v_{n}^{(u)})\) ranges over \(V\).

Proof of (i). Recall from Section 7.2 that

\[\sum_{j=1}^{n}(X_{j}-\xi_{j})^{2}=\sum_{i=1}^{s}(Y_{i}-\eta_{i})^{2}+\sum_{j=s +1}^{n}Y_{j}^{2}.\]

Since the right side is minimized by \(\eta_{i}=Y_{i}\) and the left side by \(\xi_{j}=\hat{\xi}_{j}\), this shows that

\[(Y_{1}\cdots Y_{s}0\cdots 0)^{\prime}=C(\hat{\xi}_{1}\cdots\hat{\xi}_{j})^{\prime},\]

and the result now follows from comparison with (9.134).

Proof of (ii). This is just equation (7.13).

Proof of (iii). Since \(\eta_{i}=\sum_{j=1}^{n}c_{ij}\xi_{j}\), we have \(\sum u_{i}\eta_{i}=\sum v_{j}^{(u)}\xi_{j}\) with \(v_{j}^{(u)}=\sum_{i=1}^{r}u_{i}c_{ij}\). Thus, the vectors \(v^{(u)}=(v_{1}^{(u)},\ldots,v_{n}^{(u)})\) are linear combinations, with weights \(u_{1},\ldots,u_{r}\), of the first \(r\) row vectors of \(C\). Since the space spanned by these row vectors is \(V\), the result follows.

The set of linear functions \(\sum v_{i}\xi_{i}\), \(v\in V\), for which the interval (9.133) does not cover the origin--that is, for which \(v\) satisfies

\[\big{|}\sum v_{i}\hat{\xi}_{i}\big{|}>cS\sqrt{\frac{\operatorname{Var}\bigl{(} \sum v_{i}\hat{\xi}_{i}\bigr{)}}{\sigma^{2}}} \tag{9.135}\]

--is declared significantly different from \(0\) by the intervals (9.133). Thus (9.135) is a rejection region at level \(\alpha=1-\gamma\) of the hypothesis \(H:\sum v_{i}\xi_{i}=0\) for all \(v\in V\) in the sense that \(H\) is rejected if and only if at least one \(v\in V\) satisfies (9.135). If \(\prod_{\omega}\) denotes the \((s-r)\)-dimensional space of vectors \(v\in\prod_{\Omega}\) which are orthogonal to \(V\), then \(H\) states that \(\xi\in\prod_{\omega}\), and the rejection region (9.135) is in fact equivalent to the \(F\)-test of \(H:\xi\in\prod_{\omega}\) of Section 7.1. In canonical form, this was seen in the sentence following (9.129).

To implement the intervals (9.133) in specific situations in which the corresponding intervals for a single given function \(\sum v_{i}\xi_{i}\) are known, it is only necessary to designate the space \(V\) and to obtain its dimension \(r\), the constant \(c\) then being determined by (9.130).

**Example 9.7.1**: **(All contrasts)** Let \(X_{ij}\) (\(j=1,\ldots,n_{i};\,i=1,\ldots,s\)) be independently distributed as \(N(\xi_{i},\,\sigma^{2})\), and is suppose \(V\) is the space of all vectors \(v=(v_{1},\ldots,\,v_{n})\) satisfying

\[\sum v_{i}=0. \tag{9.136}\]

Any function \(\sum v_{i}\xi_{i}\) with \(v\in V\) is called a _contrast_ among the \(\xi_{i}\). The set of contrasts includes in particular the differences \(\bar{\xi}_{+}-\bar{\xi}_{-}\) discussed in Example 9.6.1. The space \(\prod_{\Omega}\) is the set of all vectors (\(\xi_{1},\ldots,\xi_{1};\,\xi_{2},\ldots,\,\xi_{2};\,\xi_{s},\ldots,\,\xi_{s}\)) and has dimension \(s\), while \(V\) is the subspace of vectors \(\prod_{\Omega}\) that are orthogonal to (1,..., 1) and hence has dimension \(r=s-1\). It was seen in Section 7.3 that \(\hat{\xi}_{i}=X_{i}.\), and if the vectors of \(V\) are denoted by

\[\left(\frac{w_{1}}{n_{1}},\ldots,\,\frac{w_{1}}{n_{1}};\,\frac{w_{2}}{n_{2}}, \ldots,\,\frac{w_{2}}{n_{2}};\,\frac{w_{s}}{n_{s}},\ldots,\,\frac{w_{s}}{n_{s }}\right),\]

the simultaneous confidence intervals (9.133) become (Problem 9.39)\[\sum w_{i}X_{i.}-cS\sqrt{\frac{\sum w_{i}^{2}}{n_{i}}} \leq \sum w_{i}\xi_{i}\leq\sum w_{i}X_{i.}+cS\sqrt{\frac{\sum w_{i}^{2}} {n_{i}}}\] \[\text{for all }(w_{1},\ldots,w_{s})\text{ satisfying }\sum w_{i}=0,\]

with \(S^{2}=\sum\sum(X_{ij}-X_{i.})^{2}\).

In the present case the space \(\prod_{\omega}\) is the set of vectors with all coordinates equal, so that the associated hypothesis is \(H:\xi_{1}=\cdots=\xi_{s}\). The rejection region (9.135) is thus equivalent to that given by (7.19).

Instead of testing the overall homogeneity hypothesis \(H\), we may be interested in testing one or more subhypotheses suggested by the data. In the situation corresponding to that of Example 9.6.1 (but with replications), for instance, interest may focus on the hypotheses \(H_{1}:\xi_{i_{1}}=\cdots=\xi_{i_{k}}\) and \(H_{2}:\xi_{j_{1}}=\cdots=\xi_{j_{i-k}}\). A level \(\alpha\) simultaneous test of \(H_{1}\) and \(H_{2}\) is given by the rejection region

\[\frac{\sum^{(1)}n_{i}(X_{i.}-X_{\cdots}^{(1)})^{2}/(k-1)}{S^{2}/(n-s)}>C,\quad \quad\frac{\sum^{(2)}n_{i}(X_{i.}-X_{\cdots}^{(2)})^{2}/(s-k-1)}{S^{2}/(n-s)}>C,\]

where \(\sum^{(1)}\), \(\sum^{(2)}\), \(X_{\cdots}^{(1)}\), \(X_{\cdots}^{(2)}\) indicate that the summation or averaging extends over the sets \((i_{1},\ldots,i_{k})\) and \((j_{1},\ldots,j_{s-k})\), respectively, \(S^{2}=\sum\sum(X_{ij}-X_{i.})^{2}\), \(\alpha=1-\gamma\), and the constant \(C\) is given by (9.130) with \(r=s\) and is therefore the same as in (7.19), rather than being determined by the \(F_{k-1,n-s}\) and \(F_{s-k-1,n-s}\) distributions. The reason for this larger critical value is, of course, the fact the \(H_{1}\) and \(H_{2}\) were suggested by the data. The present procedure is an example of Gabriel's simultaneous test procedure mentioned in Section 9.5.

**Example 9.7.2**: **(Two-way layout)** As a second example, consider first the additive model in the two-way classification of Section 7.4 or 7.5, and then the more general interaction model of Section 7.5.

Suppose \(X_{ij}\) are independent \(N(\xi_{ij},\sigma^{2})\) (\(i=1,\ldots,a\); \(j=1,\ldots,b\)), with \(\xi_{ij}\) given by (7.20), and let \(V\) be the space of all linear functions \(\sum w_{i}\alpha_{i}=\sum w_{i}(\xi_{i.}-\xi_{.})\). As was seen in Section 7.4, \(s=a+b-1\). To determine \(r\), note that \(V\) can also be represented as \(\sum_{i=1}w_{i}\xi_{i.}\) with \(\sum w_{i}=0\) (Problem 9.38(i)), which shows that \(r=a-1\). The least squares estimators \(\hat{\xi}_{i}\) were found in Section 7.4 to be \(\hat{\xi}_{ij}=X_{i.}+X_{.j}-X_{\cdots}\), so that \(\hat{\xi}_{i.}=X_{i.}\) and \(S^{2}=\sum\sum(X_{ij}-X_{i.}-X_{.j}+X_{\cdots})^{2}\). The simultaneous confidence intervals (9.133) therefore can be written as

\[\sum w_{i}X_{i.}-cS\sqrt{\frac{\sum w_{i}^{2}}{b}} \leq \sum w_{i}\xi_{i.}\leq\sum w_{i}X_{i.}+cS\sqrt{\frac{\sum w_{i}^ {2}}{b}}\] \[\text{for all }w\text{ with }\sum_{i=1}^{a}w_{i}=0.\]If there are \(m\) observations in each cell, and the model is additive as before, the only changes required are to replace \(X_{i.}\) by \(X_{i..}\), \(S^{2}\) by \(\sum\sum\sum(X_{ijk}-X_{i..}-X_{.j.}+X_{...})^{2}\), and the expression under the square root by \(\sum w_{i}^{2}/bm\).

Let us now drop the assumption of additivity and consider the general linear model \(\xi_{ijk}=\mu+\alpha_{i}+\beta_{j}+\gamma_{ij}\), with \(\mu\) and the \(\alpha\)'s, \(\beta\)'s, and \(\gamma\)'s defined as in Section 7.5. The dimension \(s\) of \(\prod_{\Omega}\) is then \(ab\), and the least-squares estimators of the parameters were seen in Section 7.5 to be

\[\hat{\mu}=X_{...},\quad\quad\hat{\alpha}_{i}=X_{i..}-X_{...},\quad \quad\hat{\beta}_{j}=X_{.j.}-X_{...},\] \[\hat{\gamma}_{ij}=X_{ij.}-X_{i..}-X_{.j.}+X_{...}\]

The simultaneous intervals for all \(\sum w_{i}\alpha_{i}\), or for all \(\sum w_{i}\xi_{i..}\) with \(\sum w_{i}=0\), are therefore unchanged except for the replacement of \(S^{2}=\sum(X_{ijk}-X_{i..}-X_{.j.}+X_{...})^{2}\) by \(S^{2}=\sum(X_{ijk}-X_{ij.})^{2}\) and of \(n-s=n-a-b+1\) by \(n-s=n-ab=(m-1)ab\) in (9.130).

Analogously, one can obtain simultaneous confidence intervals for the totality of linear functions \(\sum w_{ij}\gamma_{ij}\), or equivalently the set of functions \(\sum w_{ij}\xi_{ij.}\) for the totality of \(w\)'s satisfying \(\sum_{i}w_{ij}=\sum_{j}w_{ij}=0\) (Problem 9.38(ii), (iii)).

**Example 9.7.3**: **(Regression line)** As a last example consider the problem of obtaining confidence bands for a regression line, mentioned at the beginning of the section. The problem was treated for a single value \(t_{0}\) in Section 5.6 (with a different notation) and in Section 7.6. The simultaneous confidence intervals in the present case become

\[\hat{\alpha}+\hat{\beta}t-cS\left[\frac{1}{n}+\frac{(t-\bar{t})^{ 2}}{\sum(t_{i}-\bar{t})^{2}}\right]^{1/2} \leq\alpha+\beta t \tag{9.138}\] \[\leq\hat{\alpha}+\hat{\beta}t+cS\left[\frac{1}{n}+\frac{(t-\bar{ t})^{2}}{\sum(t_{i}-\bar{t})^{2}}\right]^{1/2},\]

where \(\hat{\alpha}\) and \(\hat{\beta}\) are given by (7.23),

\[S^{2}=\sum(X_{i}-\hat{\alpha}-\hat{\beta}t_{i})^{2}=\sum(X_{i}-\bar{X})^{2}- \hat{\beta}^{2}\sum(t_{i}-\bar{t})^{2}\]

and \(c\) is determined by (9.130) with \(r=s=2\). This is the Working-Hotelling confidence band for a regression line.

At the beginning of the section, the Scheffe intervals were derived as the only confidence bands that are equivariant under the indicated groups. If the requirement of equivariance (particular under orthogonal transformations) is dropped, other bounds exist which are narrower for certain sets of vectors \(u\) at the cost of being wider for others (Problems 9.37(iii) and 9.43). A general method that gives special emphasis to a given subset is described by Richmond (1982). Some optimality results not requiring equivariance but instead permitting bands which are narrower for some values of \(t\) at the expense of being wider for others are provided, among others, by Bohrer (1973), Cima and Hochberg (1976), Richmond (1982), Naiman (1984a, b), and Piegorsch (1985a, b). If bounds are required only for a subset, it may be possible that intervals exist at the prescribed confidence level, which are uniformly narrower than the Scheffe intervals. This is the case for example for the intervals (9.138) when \(t\) is restricted to a given finite interval. For a discussion of this and related problems, and references to the literature, see for example Wynn and Bloomfield (1971) and Wynn (1984).

### Problems

#### Section 9.1

**Problem 9.1**: Show that the Bonferroni procedure, while generally conservative, can have \(\mbox{FWER}=\alpha\) by exhibiting a joint distribution for \((\hat{p}_{1},\ldots,\hat{p}_{s})\) and satisfying (9.5) such that \(P\{\min_{i}\hat{p}_{i}\leq\alpha/s\}=\alpha\).

**Problem 9.2**: (i) Generalize Theorem 9.1.1 to the weighted Bonferroni method. _Hint_: Part (i) directly generalizes. To show (ii), let \(J=i\) with probability \(\alpha w_{i}\) and \(J=0\) with probability \(1-\alpha\). Let \(U\sim U(0,1)\) and let \(\hat{p}_{i}=\alpha w_{i}U\) if \(J=i\); otherwise, let \(\hat{p}_{i}=(1-\alpha w_{i})U+w_{i}\alpha\). By conditioning on \(K=i\) or \(K\neq i\), show that \(\hat{p}_{i}\sim U(0,1)\). Then, the FWER is

\[P(\bigcup_{i=1}^{s}\{\hat{P}_{i}\leq w_{i}\alpha\})=P(\bigcup_{i=1}^{s}\{J=i\} )=\sum_{i}P(K=i)=\alpha\;.\]

The first equality follows because a Type 1 error occurs iff \(J\neq 0\); the second follows because the events \(\{J=i\}\) are disjoint.

(ii) What are the adjusted \(p\)-values for the weighted Bonferroni method?

**Problem 9.3**: (i) Under the assumptions of Theorem 9.1.1, suppose also that the \(p\)-values are mutually independent. Show that the Sidak procedure which rejects any \(H_{i}\) for which \(\hat{p}_{i}<c(\alpha,s)=1-(1-\alpha)^{1/s}\) controls the FWER at level \(\alpha\).

(ii) Compare \(\alpha/s\) with \(c(\alpha,s)\) and show

\[\lim_{s\to\infty}\frac{c(\alpha,s)}{(\alpha/s)}=\frac{-\log(1-\alpha)}{\alpha}\;.\]

For \(\alpha=.05\), this limiting value to 3 decimals is 1.026, so the increase in cutoff value is not substantial.

(iii) Derive the adjusted \(p\)-values for Sidak's method.

**Problem 9.4**: Show that (9.14) implies (9.12). Investigate under what conditions the probability of a Type 3 error can be bounded by \(\alpha/2\).

**Problem 9.5**: Show that Duncan's method controls the FWER and the mixed directional famliywise error rate at level \(\alpha\). Find an expression for the adjusted \(p\)-values for Duncan's method.

**Problem 9.6**: Show that, under the assumptions of Theorem 9.1.2, it is not possible to increase any of the critical values \(\alpha_{i}=\alpha/(s-i+1)\) in the Holm procedure (9.18) without violating the FWER.

**Problem 9.7**: Under the assumptions of Theorem 9.1.2 and independence of the \(p\)-values, the critical values \(\alpha/(s-i+1)\) can be increased to \(1-(1-\alpha)^{1/(s-i+1)}\). For any \(i\), calculate the limiting value of the ratio of these critical values, as \(s\to\infty\).

**Problem 9.8**: In Example 9.1.7, verify that the stepdown procedure based on the maximum of \(X_{j}/\sqrt{\sigma_{j,j}}\) improves upon the Holm procedure. By Theorem 9.1.3, the procedure has FWER \(\leq\alpha\). Compare the two procedures in the case \(\sigma_{i,i}=1,\sigma_{i,j}=\rho\) if \(i\neq j\); consider \(\rho=0\) and \(\rho\to 1\).

**Problem 9.9**: Show that a stepdown version of Tukey's method and Duncan's method controls the FWER.

**Problem 9.10**: Consider testing \(H_{1}\),..., \(H_{s}\), with \(H_{i}\) specifying \(\theta_{i}=0\) against two-sided alternatives. In order to control the mixed directional familywise error rate in (9.15), a simple device it to consider the \(2s\) one-sided hypotheses defined as follows. For \(i=1\),..., \(s\), let \(H^{i}\) be the null hypothesis \(\theta_{i}\leq 0\) (tested against \(\theta_{i}>0\), and for \(j=i+s\), let \(H^{j}\) be the null hypothesis \(\theta_{i}\geq 0\) (tested against \(\theta_{i}<0\)). Explain how, if you apply the Holm method to the \(2s\) hypotheses, one gets an improvement over Bonferroni. Compare with the method which applies Holm to the original hypotheses.

### _Section 9.2_

**Problem 9.11**: Show that the Holm method is a special case of the closure method by using the Bonferroni method to test intersection hypotheses.

**Problem 9.12**: As in Procedure 9.1.1, suppose that a test of the individual hypothesis \(H_{j}\) is based on a test statistic \(T_{n,j}\), with large values indicating evidence against the \(H_{j}\). Assume \(\bigcap_{j=1}^{s}\omega_{j}\) is not empty. For any subset \(K\) of \(\{1,\ldots,s\}\), let \(c_{n,K}(\alpha,\,P)\) denote an \(\alpha\)-quantile of the distribution of \(\max_{j\in K}T_{n,j}\) under \(P\). Concretely,

\[c_{n,K}(\alpha,\,P)=\inf\{x:\ P\{\max_{j\in K}T_{n,j}\leq x\}\geq\alpha\}. \tag{9.139}\]

For testing the intersection hypothesis \(H_{K}\), it is only required to approximate a critical value for \(P\in\bigcap_{j\in K}\omega_{j}\). Because there may be many such \(P\), we define\[c_{n,K}(1-\alpha)=\sup\{c_{n,K}(1-\alpha,\,P):\,\,P\in\bigcap_{j\in K}\omega_{j}\}\,. \tag{9.140}\]

(i) In Procedure 9.1.1, show that the choice \(\hat{c}_{n,K}(1-\alpha)=c_{n,K}(1-\alpha)\) controls the FWER, as long as (9.22) holds.

(ii) Further assume that for every subset \(K\subseteq\{1,\ldots,k\}\), there exists a distribution \(P_{K}\) which satisfies

\[c_{n,K}(1-\alpha,\,P)\leq c_{n,K}(1-\alpha,\,P_{K}) \tag{9.141}\]

for all \(P\) such that \(I(P)\supseteq K\). Such a \(P_{K}\) may be referred to being least favorable among distributions \(P\) such that \(P\in\bigcap_{j\in K}\omega_{j}\). (For example, if \(H_{j}\) corresponds to a parameter \(\theta_{j}\leq 0\), then intuition suggests a least favorable configuration should correspond to \(\theta_{j}=0\).) In addition, assume the subset pivotality condition of Westfall and Young (1993); that is, assume there exists a \(P_{0}\) with \(I(P_{0})=\{1,\ldots,s\}\) such that the joint distribution of \(\{T_{n,j}:\,\,j\in I(P_{K})\}\) under \(P_{K}\) is the same as the distribution of \(\{T_{n,j}:\,\,j\in I(P_{K})\}\) under \(P_{0}\). This condition says the (joint) distribution of the test statistics used for testing the hypotheses \(H_{j},\,\,j\in I(P_{K})\) is unaffected by the truth or falsehood of the remaining hypotheses (and therefore we assume all hypotheses are true by calculating the distribution of the maximum under \(P_{0}\)). Show we can use \(\hat{c}_{n,K}(1-\alpha,\,P_{0})\) for \(\hat{c}_{n,K}(1-\alpha)\).

(iii) Further assume the distribution of \((T_{n,1},\ldots,\,T_{n,s})\) under \(P_{0}\) is invariant under permutations (or exchangeable). Then, the critical values \(\hat{c}_{n,K}(1-\alpha)\) can be chosen to depend only on \(|K|\).

**Problem 9.13**: Verfiy that Hommel's method as stated in Example 9.2.5 can be obtained by the closure method when using Simes' tests for the intersection hypotheses.

**Problem 9.14**: For testing \(H_{1},\ldots,\,H_{s}\) based on \(p\)-values \(\hat{p}_{1},\ldots,\,\hat{p}_{s}\), suppose the closure method is applied and large values of \(T_{k}=T_{k}(\hat{p}_{i_{1}},\ldots,\,\hat{p}_{i_{k}})\) is used to test the intersection hypothesis \(H_{K}\), where \(K=\{i_{1},\ldots,i_{k}\}\). Assume \(T_{k}\) is symmetric in its arguments, and the test rejects \(H_{K}\) when \(T_{k}\) exceeds some critical value \(t_{k}(1-\alpha)\), which only depends on the number \(k\) of \(p\)-values tested and \(\alpha\). Further assume \(T_{k}\) is nonincreasing in each component, or that \(T_{k}\) is nondecreasing as each component \(\hat{p}_{i_{j}}\) decreases (so smaller \(p\)-values makes it easier to reject for large values of \(T_{k}\)).

(i) Show that, when applying the closure method when using such tests, it suffices to calculate order \(s^{2}\) number of tests (and so not order \(2^{s}\)). _Hint_: To show, for example, that \(H_{1}\) is rejected by the closure method, any intersection test based on \(k\) hypotheses much be rejected, i.e., \(T_{k}(\hat{p}_{1},\,\hat{p}_{i_{1}},\ldots,\,\hat{p}_{i_{k-1}})\) must be \(>t_{k}(1-\alpha)\) for any choice of \(i_{1}\),..., \(i_{k-1}\). But, it then suffices to consider \(H_{1}\) together with the \(k-1\) hypotheses corresponding to the largest \(k-1\)\(p\)-values (not including \(\hat{p}_{1}\)). Thus, for testing \(H_{1}\), only at most \(k\) tests need be performed.

(ii) Verify the conditions for the Higher Criticism statistic \(HC_{s}\) as well as Fisher's statistic \(F_{s}\).

**Problem 9.15**: The closure method starts with a family of tests of \(H_{K}\) to produce a multiple decision rule. Conversely, given any multiple testing decision rule (not necessarily obtained by the closure method), one can use it to obtain tests of \(H_{K}\) for any intersection hypothesis. More specifically, for testing \(H_{1},\ldots,\)\(H_{s}\), suppose a given multiple testing decision rule controls the FWER at level \(\alpha\). For testing the intersection hypothesis \(H_{K}\) defined by (9.27), if \(H_{K}\) is equal to \(H_{i}\) for some \(i\), then test \(H_{K}\) by the test of that \(H_{i}\). Otherwise, if \(H_{K}\) is not a member of the original family \(H_{1},\ldots,\)\(H_{s}\), consider the following test \(\phi_{K}\) of \(H_{K}\) defined by: reject \(H_{K}\) if the given multiple testing decision rule rejects any \(H_{i}\) with \(i\in K\). Then, show that \(\phi_{K}\) controls the usual probability of a Type 1 error at level \(\alpha\) for testing \(H_{K}\), i.e., it satisfies (9.28).

### _Section 9.3_

**Problem 9.16**: If \(F\) is the number of false discoveries of some multiple testing procedure, then show the per-family error rate \(E(F)\) satisfies the crude inequalities

\[P\{F\geq 1\}\leq E(F)\leq sP\{F\geq 1\}\,\]

where \(s\) is the number of hypotheses under test. Hence, if a method controls \(E(F)\) at \(\alpha\), then it also controls the FWER at \(\alpha\). Show that the Bonferroni method controls both at \(\alpha\). What about the Holm method?

**Problem 9.17**: The problem points to connections between methods that control the FDP in the sense of (9.49) and methods that control its expected value, the FDR.

(i) Show, for any random variable \(X\) on [0, 1], we have

\[\frac{E(X)-\gamma}{1-\gamma}\leq P\{X>\gamma\}\leq\frac{E(X)}{\gamma}\.\]

(ii) Apply the above to \(X=\) FDP to show that if a method controls the FDR at level \(q\), then it controls the FDP in the sense \(P\{\mbox{FDP}>\gamma\}\leq q/\gamma\). Also, if the FDP is controlled in the sense of (9.49), then the FDR is controlled at level \(\alpha(1-\gamma)+\gamma\), which is \(\geq\alpha\) but typically only slightly. (These crude arguments suggest that control of the FDP is perhaps more stringent than control of the FDR.)

**Problem 9.18**: In Example 9.3.1, suppose the multiple testing problem specifies \(H_{i}:\ \mu_{i}=0\) against \(H_{j}^{\prime}:\ \mu_{i}>0\), with \(\Sigma\) known. As in the example, assume all components of \(\Sigma\) are nonnegative. Define \(p\)-values by \(\hat{p}_{i}=1-\Phi(X_{i}/\sigma_{i})\), where \(\sigma_{i}^{2}=\Sigma_{i,i}=Var(X_{i})\) is assumed positive. Show that the joint distribution of \(p\)-values is PRDS on \(I\), the index set where \(\mu_{i}=0\).

**Problem 9.19**: Assume the joint distribution of \(p\)-values is PRDS on the set \(I\) of true null hypotheses.

(i) Show that, for any increasing set \(D\),

\[P\{(\hat{p}_{1},\ldots,\hat{p}_{s})\in D|\hat{p}_{i}\leq u\} \tag{9.142}\]

is nondecreasing in \(u\) for any \(i\in I\).

(ii) More generally, show that

\[E\left[\Psi(\hat{p}_{1},\ldots,\hat{p}_{s})\mid\hat{p}_{i}\leq u\right]\]

is nondecreasing in \(j\) for any \(i\in I\) and any increasing (coordinate-wise) function \(\Psi\).

**Problem 9.20**: Suppose you apply the BH method based on \(p\)-values \(\hat{p}_{1},\ldots\hat{p}_{s}\). If each \(p\)-value is actually recorded twice (so that you now have \(2s\)\(p\)-values), how would the two applications of the BH method compare? Repeat by applying the BY method in each case. Comment on the appropriateness of each method.

### _Section 9.4_

**Problem 9.21**: Suppose \((X_{1},\ldots,X_{s})^{\top}\) has a multivariate c.d.f. \(F(\cdot)\). For \(\theta\in\mathbb{R}^{s}\), let \(F_{\theta}(x)=F(x-\theta)\) define a multivariate location family. Show that (9.55) is satisfied for this family. (In particular, it holds if \(F\) is any multivariate normal distribution.) Moreover, it holds when any subset of the \(X_{i}\) is replaced with \(-X_{i}\) as well.

**Problem 9.22**: Prove Lemma 9.4.2.

**Problem 9.23**: We have suppressed the dependence of the critical constants \(C_{1},\ldots,C_{s}\) in the definition of the stepdown procedure \(D\), and now more accurately call them \(C_{s,1},\ldots,C_{s,s}\). Argue that, for fixed \(s\), \(C_{s,j}\) is nonincreasing in \(j\) and only depends on \(s-j\).

**Problem 9.24**: Under the assumptions of Theorem 9.4.1, suppose there exists another monotone rule \(E\) that strongly controls the FWER, and such that

\[P_{\theta}\{d_{0,0}^{c}\}\leq P_{\theta}\{e_{0,0}^{c}\}\quad\mbox{for all } \theta\in\omega_{0,0}^{c}\, \tag{9.143}\]

with strict inequality for some \(\theta\in\omega_{0,0}^{c}\). Argue that the \(\leq\) in (9.143) is an equality, and hence \(e_{0,0}\triangle d_{0,0}\) has Lebesgue measure \(0\), where \(A\triangle B\) denotes the symmetric difference between sets \(A\) and \(B\). A similar result for the region \(d_{1,1}\) can be made as well.

**Problem 9.25**: In general, the optimality results of Section 9.4 require the procedures to be monotone. To see why this is required, consider Theorem 9.4.2(i). Show the procedure \(E\) to be inadmissible. _Hint: One can always add large negative values of \(T_{1}\) and \(T_{2}\) to the region \(u_{1,1}\) without violating the FWER._

**Problem 9.26**: Prove part (i) of Theorem 9.4.3.

**Problem 9.27**: In general, show \(C_{s}=C_{1}^{*}\). In the case \(s=2\), show (9.67).

### _Section 9.5_

**Problem 9.28**: Show that

\[\sum_{i=1}^{r+1}\left(Y_{i}-\frac{Y_{1}+\cdots+Y_{r+1}}{r+1}\right)^{2}-\sum_{ i=1}^{r}\left(Y_{i}-\frac{Y_{1}+\cdots+Y_{r}}{r}\right)^{2}\geq 0.\]

**Problem 9.29**:
1. For the validity of Lemma 9.5.1 it is only required that the probability of rejecting homogeneity of any set containing \(\{\mu_{i_{1}},\ldots,\mu_{i_{n}}\}\) as a proper subset tends to \(1\) as the distance between the different groups (9.89) all \(\to\infty\), with the analogous condition holding for \(H_{2}^{\prime}\),..., \(H_{r}^{\prime}\).
2. The condition of part (i) is satisfied for example if homogeneity of a set \(S\) is rejected for large values of \(\sum|X_{i}.-X_{..}|\), where the sum extends over the subscripts \(i\) for which \(\mu_{i}\in S\).

**Problem 9.30**: In Lemma 9.5.2, show that \(\alpha_{s-1}=\alpha\) is necessary for admissibility.

**Problem 9.31**: Prove Lemma 9.5.3 when \(s\) is odd.

**Problem 9.32**: Show that the Tukey levels (vi) satisfy (9.95) when \(s\) is even but not when \(s\) is odd.

**Problem 9.33**: The Tukey \(T\)-method leads to the simultaneous confidence intervals

\[\left|\left(X_{j\cdot}-X_{i\cdot}\right)-\left(\mu_{j}-\mu_{i}\right)\right| \leq\frac{C\hat{\sigma}}{\sqrt{sn(n-1)}}\quad\text{ for all }i,\,j. \tag{9.144}\]

[The probability of (9.144) is independent of the \(\mu\)'s and hence equal to \(1-\alpha_{s}\).]

### _Section 9.6_

**Problem 9.34**:
1. A function \(L\) satisfies the first equation of (9.106) for all \(u\), \(x\), and orthogonal transformations \(Q\) if and only if it depends on \(u\) and \(x\) only through \(u^{\prime}x\), \(x^{\prime}x\), and \(u^{\prime}u\).
2. A function \(L\) is equivariant under \(G_{2}\) if and only if it satisfies (9.108).

**Problem 9.35**:
1. For the confidence sets (9.114), equivariance under \(G_{1}\) and \(G_{2}\) reduces to (9.115) and (9.116) respectively.

2. For fixed \((y_{1},\ldots,y_{r})\), the statements \(\sum u_{i}y_{i}\in A\) hold for all \((u_{1},\ldots,u_{r})\) with \(\sum u_{i}^{2}=1\) if and only if \(A\) contains the interval \(I(y)=[-\sqrt{\sum Y_{i}^{2}},+\sqrt{\sum Y_{i}^{2}}]\).
3. Show that the statement following (9.118) ceases to hold when \(r=1\).

**Problem 9.36**: Let \(X_{i}\) (\(i=1,\ldots,r\)) be independent \(N(\xi_{i},1)\).

1. The only simultaneous confidence intervals equivariant under \(G_{0}\) are those given by (9.124).
2. The inequalities (9.124) and (9.126) are equivalent.
3. Compared with the Scheffe intervals (9.113), the intervals (9.126) for \(\sum u_{j}\xi_{j}\) are shorter when \(\sum u_{j}\xi_{j}=\xi_{i}\) and longer when \(u_{1}=\cdots=u_{r}\).

[(ii): For a fixed \(u=(u_{1},\ldots,u_{r})\), \(\sum u_{i}y_{i}\) is maximized subject to \(|y_{i}|\leq\Delta\) for all \(i\), by \(y_{i}=\Delta\) when \(u_{i}>0\) and \(y_{i}=-\Delta\) when \(u_{i}<0\).]

#### Section 9.7

**Problem 9.37**:
1. The confidence intervals \(L(u;\,y,\,S)=\sum u_{i}y_{i}-c(S)\) are equivariant under \(G_{3}\) if and only if \(L(u;\,by,\,bS)=bL(u;\,y,\,S)\) for all \(b>0\).
2. The most general confidence sets (9.131) which are equivariant under \(G_{1}\), \(G_{2}\), and \(G_{3}\) are of the form (9.132).

**Problem 9.38**:
1. In Example 9.7.2, the set of linear functions \(\sum w_{i}\alpha_{i}=\sum w_{i}(\xi_{i\cdot}-\xi_{\cdot\cdot})\) for all \(w\) can also be represented as the set of functions \(\sum w_{i}\xi_{i}\). for all \(w\) satisfying \(\sum w_{i}=0\).
2. The set of linear functions \(\sum\sum w_{ij}\gamma_{ij}=\sum\sum w_{ij}(\xi_{ij\cdot}-\xi_{i\cdot\cdot}-\xi _{\cdot\cdot j\cdot}+\xi_{\cdot\cdot\cdot})\) for all \(w\) is equivalent to the set \(\sum\sum w_{ij}\xi_{ij\cdot}\) for all \(w\) satisfying \(\sum_{i}w_{ij}=\sum_{j}w_{ij}=0\).
3. Determine the simultaneous confidence intervals (9.133) for the set of linear functions of part (ii).

**Problem 9.39**:
1. In Example 9.7.1, the simultaneous confidence intervals (9.133) reduce to (9.137).
2. What change is needed in the confidence intervals of Example 9.7.1 if the \(v\)'s are not required to satisfy (9.136), i.e., if simultaneous confidence intervals are desired for all linear functions \(\sum v_{i}\xi_{i}\) instead of all contrasts? Make a table showing the effect of this change for \(s=2\), \(3\), \(4\), \(5\); \(n_{i}=n=3,5,10\).

**Problem 9.40**: _Tukey's \(T\)-Method._ Let \(X_{i}\) (\(i=1,\ldots,r\)) be independent \(N(\xi_{i},1)\), and consider simultaneous confidence intervals

\[L[(i,j);\,x]\leq\xi_{j}-\xi_{i}\leq M[(i,j);\,x]\ \ \ \mbox{for all}\ i\neq j. \tag{9.145}\]

The problem of determining such confidence intervals remains invariant under the group \(G_{0}^{\prime}\) of all permutations of the \(X\)'s and under the group \(G_{2}\) of translations \(gx=x+a\).

1. In analogy with (9.105), attention can be restricted to confidence bounds satisfying \[L[(i,j);\,x]=-M[(j,i);\,x].\] (9.146)
2. The only simultaneous confidence intervals satisfying (9.146) and equivariant under \(G_{0}^{\prime}\) and \(G_{2}\) are those of the form \[S(x)=\{\xi:x_{j}-x_{i}-\Delta<\xi_{j}-\xi_{i}<x_{j}-x_{i}+\Delta\text{ for all }i\neq j\}.\] (9.147)
3. The constant \(\Delta\) for which (9.147) has probability \(\gamma\) is determined by \[P_{0}\{\max|X_{j}-X_{i}|<\Delta\}=P_{0}\{X_{(n)}-X_{(1)}<\Delta\}=\gamma,\] (9.148) where the probability \(P_{0}\) is calculated under the assumption that \(\xi_{1}=\cdots=\xi_{r}\).

**Problem 9.41**: In the preceding problem consider arbitrary contrasts \(\sum c_{i}\xi_{i}\) with \(\sum c_{i}=0\). The event

\[\big{|}\big{(}X_{j}-X_{i}\big{)}-\big{(}\xi_{j}-\xi_{i}\big{)}\big{|}\leq \Delta\quad\quad\text{for all }i\neq j \tag{9.149}\]

is equivalent to the event

\[\big{|}\sum c_{i}\,X_{i}-\sum c_{i}\,\xi_{i}\,\big{|}\leq\frac{\Delta}{2}\sum| c_{i}|\quad\quad\text{for all }c\text{ with }\sum c_{i}\,=0, \tag{9.150}\]

which therefore also has probability \(\gamma\). This shows how to extend the Tukey intervals for all pairs to all contrasts.

[That (9.150) implies (9.149) is obvious. To see that (9.149) implies (9.150), let \(y_{i}=x_{i}-\xi_{i}\) and maximize \(|\sum c_{i}y_{i}|\) subject to \(|y_{j}-y_{i}|\leq\Delta\) for all \(i\) and \(j\). Let \(P\) and \(N\) denote the sets \(\{i:c_{i}>0\}\) and \(\{i:c_{i}<0\}\), so that

\[\sum c_{i}y_{i}=\sum_{i\in P}c_{i}y_{i}-\sum_{i\in N}|c_{i}|\,y_{i}.\]

Then for fixed \(c\), the sum \(\sum c_{i}y_{i}\) is maximized by maximizing the \(y_{i}\)'s for \(i\in P\) and minimizing those for \(i\in N\). Since \(|y_{j}-y_{i}|\leq\Delta\), it is seen that \(\sum c_{i}y_{i}\) is maximized by \(y_{i}=\Delta/2\) for \(i\in P\), \(y_{i}=-\Delta/2\) for \(i\in N\). The minimization of \(\sum c_{i}y_{i}\) is handled analogously.]

**Problem 9.42**:
1. Let \(X_{ij}\) (\(j=1,\ldots n\); \(i=1,\ldots,s\)) be independent \(N(\xi_{i},\sigma^{2}),\sigma^{2}\) unknown. Then the problem of obtaining simultaneous confidence intervals for all differences \(\xi_{j}-\xi_{i}\) is invariant under \(G_{0}^{\prime}\), \(G_{2}\), and the scale changes \(G_{3}\).
2. The only equivariant confidence bounds based on the sufficient statistics \(X_{i.}\) and \(S^{2}=\sum\sum(X_{ij}-X_{i.})^{2}\) and satisfying the condition corresponding to (9.146) are those given by \[S(x)=\left\{x:x_{j.}-x_{i.}-\frac{\Delta^{\prime}}{\sqrt{n-s}}S\leq\xi_{j}-\xi_{i}\right. \tag{9.151}\] \[\qquad\qquad\left.\leq x_{j.}-x_{i.}+\frac{\Delta^{\prime}}{\sqrt{n-s }}S\right.\qquad\qquad\mbox{for all }i\not=j\right\}\]

with \(\Delta^{\prime}\) determined by the null distribution of the _Studentized range_

\[P_{0}\left\{\frac{\max|X_{j.}-X_{i.}|}{S/\sqrt{n-s}}<\Delta^{\prime}\right\}=\gamma. \tag{9.152}\]
3. Extend the results of Problem 9.41 to the present situation.

**Problem 9.43**: Construct an example [i.e., choose values \(n_{1}=\cdots=n_{s}=n\) and \(\alpha\) particular contrast (\(c_{1},\ldots,c_{s}\))] for which the Tukey confidence intervals (9.150) are shorter than the Scheffe intervals (9.137), and an example in which the situation is reversed.

**Problem 9.44**: _Dunnett's method_. Let \(X_{0j}\) (\(j=1,\ldots,m\)) and \(X_{ik}\) (\(i=1,\ldots,s\); \(k=1,\ldots,n\)) represent measurements on a standard and \(s\) competing new treatments, and suppose the \(X\)'s are independently distributed as \(N(\xi_{0},\sigma^{2})\) and \(N(\xi_{i},\sigma^{2})\) respectively. Generalize Problems 9.40 and 9.42 to the problem of obtaining simultaneous confidence intervals for the \(s\) differences \(\xi_{i}-\xi_{0}\) (\(i=1,\ldots,s\)).

**Problem 9.45**: In generalization of Problem 9.41, show how to extend the Dunnett intervals of Problem 9.44 to the set of all contrasts.

[Use the fact that the event \(|y_{i}-y_{0}|\leq\Delta\) for \(i=1,\ldots,s\) is equivalent to the event \(|\sum_{i=0}^{s}c_{i}y_{i}|\leq\Delta\sum_{i=1}^{s}|c_{i}|\) for all (\(c_{0},\ldots,c_{s}\)) satisfying \(\sum_{i=0}^{s}c_{i}=0\).]

_Note_. As is pointed out in Problems 9.37(iii) and 9.43, the intervals resulting from the extension of the Tukey (and Dunnett) methods to all contrasts are shorter than the Scheffe intervals for the differences for which these methods were designed and for contrasts close to them, and longer for some other contrasts. For details and generalizations, see for example Miller (1981), Richmond (1982), and Shaffer (1977a).

**Problem 9.46**: In the regression model of Problem 7.8, generalize the confidence bands of Example 9.7.3 to the regression surfaces

1. \(h_{1}(e_{1},\ldots,e_{s})=\sum_{j=1}^{s}e_{j}\beta_{j}\);
2. \(h_{2}(e_{2},\ldots,e_{s})=\beta_{1}+\sum_{j=2}^{s}e_{j}\beta_{j}\).

### Notes

Many of the basic ideas for making multiple inferences were pioneered by Tukey (1953); see Tukey (1991), Braun (1994), and Shaffer (1995). See Duncan (1955) for an exposition of the ideas of one of the early workers in the area of multiple comparisons.

Comprehensive accounts on the theory and methodology of multiple testing can be found in Hochberg and Tamhane (1987), Westfall and Young (1993), and Hsu (1996), Dudoit et al. (2003) and Dickhaus (2014). An empirical Bayes approach is presented in Efron (2010).

Some further work on stepwise procedures includes Troendle (1995), Finner and Roters (1998, 2001), and Romano and Wolf (2005a). The concept of coherence was already introduced by Lehmann (1957) under the name _compatibility_. Theorem 9.2.1 was obtained in Sonnemann (1982, 2008) under the assumption that the family is closed, and slightly more generally in Romano et al. (2011). Theorem 9.2.2 was obtained in Sonnemann and Finner (1988). A nice review of closure also appears in Finner and Strassburger (2002).

The FDR remains a subject of very active research, beginning with Benjamini and Hochberg (1995). Further papers include Finner and Roters (2001), Benjamini and Yekutieli (2001), Sarkar (2002, 2007). The condition (9.41) appeared in Sarkar and Guo (2010), and appears slightly weaker than PRDS in Benjamini and Yekutieli (2001). The simplified proofs of FDR control in Section 9.3.2 were personally communicated by Wenge Guo. Fairly recent work on adaptive control includes Blanchard and Roquain (2009), Sarkar et al. (2012) and Sarkar et al. (2013). The knockoff filter of Barber and Candes (2015) is a relatively recent method for controlling the FDR when performing variable selection. Control of the per familywise error rate is discussed in Meng et al. (2014).

Confidence sets based on multiple tests are studied in Haytner and Hsu (1994), Miwa and Hayter (1999) and Holm (1999).

The first simultaneous confidence intervals (for a regression line) were obtained by Working and Hotelling (1929). Scheffe's approach was generalized in Roy and Bose (1953). The optimal property of the Scheffe intervals presented in Section 9.6 is a special case of results of Wijsman (1979, 1980). A review of the literature on the relationship of tests and confidence sets for a parameter vector with the associated simultaneous confidence intervals for functions of its components can be found in Kanoh and Kusunoki (1984). Some alternative methods to construct confidence bands in regression contexts are given in Sun, J. Faraway and Sun (1995) and Spurrier (1999).

## Chapter 10 Conditional Inference

### 10.1 Mixtures of Experiments

The present chapter has a somewhat different character from the preceding ones. It is concerned with problems regarding the proper choice and interpretation of tests and confidence procedures, problems which--despite a large literature--have not found a definitive solution. The discussion will thus be more tentative than in earlier chapters, and will focus on conceptual aspects more than on technical ones.

Consider the situation in which either the experiment \(\mathcal{E}\) of observing a random quantity \(X\) with density \(p_{\theta}\) (with respect to \(\mu\)) or the experiment \(\mathcal{F}\) of observing an \(X\) with density \(q_{\theta}\) (with respect to \(\nu\)) is performed with probability \(p\) and \(q=1-p\), respectively. On the basis of \(X\), and knowledge of which of the two experiments was performed, it is desired to test \(H_{0}:\theta=\theta_{0}\) against \(H_{1}:\theta=\theta_{1}\). For the sake of convenience it will be assumed that the two experiments have the same sample space and the same \(\sigma\)-field of measurable sets. The sample space of the overall experiment consists of the union of the sets

\[\mathcal{X}_{0}=\{(I,x):I=0,\ x\in\mathcal{X}\}\ \ \ \text{and}\ \ \ \mathcal{X}_{1}=\{(I,x):I=1,\ x\in\mathcal{X}\},\]

where \(I\) is \(0\) or \(1\) as \(\mathcal{E}\) or \(\mathcal{F}\) is performed.

A level-\(\alpha\) test of \(H_{0}\) is defined by its critical function

\[\phi_{i}(x)=\phi(i,x)\]

and must satisfy

\[\begin{array}{l}pE_{0}\big{[}\phi_{0}(X)\mid\mathcal{E}\big{]}+q\,E_{0} \big{[}\phi_{1}(X)\mid\mathcal{F}\big{]}=\\ p\int\phi_{0}p_{\theta_{0}}\,d\mu+q\int\phi_{1}q_{\theta_{0}}\,d\nu\leq\alpha. \end{array} \tag{10.1}\]

Suppose that \(p\) is unknown, so that \(H_{0}\) is composite. Then a level-\(\alpha\) test of \(H_{0}\) satisfies (10.1) for all \(0<p<1\), and must therefore satisfy

[MISSING_PAGE_EMPTY:2108]

\[\tfrac{1}{2}(\alpha_{0}+\alpha_{1})=\alpha, \tag{10.8}\]

where

\[\alpha_{0}=E_{\theta_{0}}\bigl{[}\phi_{0}(X)\mid\mathcal{E}\bigr{]},\qquad\alpha_ {1}=E_{\theta_{0}}\bigl{[}\phi_{1}(X)\mid\mathcal{F}\bigr{]}. \tag{10.9}\]

As an illustration of the change, suppose that experiment \(\mathcal{F}\) is reasonably informative, say that the power \(\beta_{1}\) given by (10.6) is 0.8, but that \(\mathcal{E}\) has little ability to distinguish between \(p_{\theta_{0}}\) and \(p_{\theta_{1}}\). Then it will typically not pay to put much of the rejection probability into \(\alpha_{0}\); if \(\beta_{0}\) [given by (10.6)] is sufficiently small, the best choice of \(\alpha_{0}\) and \(\alpha_{1}\) satisfying (10.8) is approximately \(\alpha_{0}\approx 0\), \(\alpha_{1}\approx 2\alpha\). The situation will be reversed if \(\mathcal{F}\) is so informative that \(\mathcal{F}\) can attain power close to 1 with an \(\alpha_{1}\) much smaller than \(\alpha/2\).

When \(p\) is known, there are therefore two issues. Should the procedure be chosen which is best on the average over both experiments, or should the best conditional procedure be preferred; and, for a given test or confidence procedure, should probabilities such as level, power, and confidence coefficient be calculated conditionally, given the experiment that has been selected, or unconditionally? The underlying question is of course the same: Is a conditional or unconditional point of view more appropriate?

The answer cannot be found within the model but depends on the context. If the overall experiment will be performed many times, for example in an industrial or agricultural setting, the average performance may be the principal feature of interest, and an unconditional approach suitable. However, if repetitions refer to different clients, or are potential rather than actual, interest will focus on the particular event at hand, and conditioning seems more appropriate. Unfortunately, as will be seen in later sections, it is then often not clear how the conditioning events should be chosen.

The difference between the conditional and the unconditional approaches tends to be most striking, and a choice between them therefore most pressing, when the two experiments \(\mathcal{E}\) and \(\mathcal{F}\) differ sharply in the amount of information they contain, if for example the difference \(|\beta_{1}-\beta_{0}|\) in (10.6) is large. To illustrate an extreme situation in which this is not the case, suppose that \(\mathcal{E}\) and \(\mathcal{F}\) consist in observing \(X\) with distribution \(N(\theta,1)\) and \(N(-\theta,1)\) respectively, that one of them is selected with known probabilities \(p\) and \(q\), respectively, and that it is desired to test \(H:\theta=0\) against \(K:\theta>0\). Here \(\mathcal{E}\) and \(\mathcal{F}\) contain exactly the same amount of information about \(\theta\). The unconditional most powerful level-\(\alpha\) test of \(H\) against \(\theta_{1}>0\) is seen to reject (Problem 10.5) when \(X>c\) if \(\mathcal{E}\) is performed, and when \(X<-c\) if \(\mathcal{F}\) is performed, where \(P_{0}(X>c)=\alpha\). The test is UMP against \(\theta>0\), and happens to coincide with the UMP conditional test.

The issues raised here extend in an obvious way to mixtures of more than two experiments. As an illustration of a mixture over a continuum, consider a regression situation. Suppose that \(X_{1},\ldots,X_{n}\) are independent, and that the conditional density of \(X_{i}\) given \(t_{i}\) is

\[\frac{1}{\sigma}f\left(\frac{x_{i}-\alpha-\beta t_{i}}{\sigma}\right).\]The \(t_{i}\) themselves are obtained with error. They may for example be independently normally distributed with mean \(c_{i}\) and known variance \(\tau^{2}\), where the \(c_{i}\) are the intended values of the \(t_{i}\). Then it will again often be the case that the most appropriate inference concerning \(\alpha\), \(\beta\), and \(\sigma\) is conditional on the observed values of the \(t\)'s (which represent the experiment actually being performed). Whether this is the case will, as before, depend on the context.

The argument for conditioning also applies when the probabilities of performing the various experiments are unknown, say depend on a parameter \(\vartheta\), provided \(\vartheta\) is unrelated to \(\theta\), so that which experiment is chosen provides no information concerning \(\theta\). A more precise statement of this generalization is given at the end of the next section.

### Ancillary Statistics

Mixture models can be described in the following general terms. Let \(\{{\cal E}_{z},z\in{\cal Z}\}\) denote a collection of experiments of which one is selected according to a known probability distribution over \({\cal Z}\). For any given \(z\), the experiment \({\cal E}_{z}\) consists in observing a random quantity \(X\), which has a distribution \(P_{\theta}(\cdot\mid z)\). Although this structure seems rather special, it is common to many statistical models.

Consider a general statistical model in which the observations \(X\) are distributed according to \(P_{\theta}\), \(\theta\in\Omega\), and suppose there exists an _ancillary statistic_, that is, a statistic \(Z\) whose distribution \(F\) does not depend on \(\theta\). Then one can think of \(X\) as being obtained by a two-stage experiment: observe first a random quantity \(Z\) with distribution \(F\); given \(Z=z\), observe a quantity \(X\) with distribution \(P_{\theta}(\cdot\mid z)\). The resulting \(X\) is distributed according to the original distribution \(P_{\theta}\). Under these circumstances, the argument of the preceding section suggests that it will frequently be appropriate to take the conditional point of view.1 (Unless \(Z\) is discrete, these definitions involve technical difficulties concerning sets of measure zero and the existence of conditional distributions, which we shall disregard.)

Footnote 1: A distinction between experimental mixtures and the present situation, relying on aspects outside the model, is discussed by Basu (1964) and Kalbfleisch (1975).

An important class of models in which ancillary statistics exist is obtained by invariance considerations. Suppose the model \({\cal P}=\{P_{\theta},\theta\in\Omega\}\) remains invariant under the transformations

\[X\to gX,\quad\theta\to\bar{g}\theta;\qquad g\in G,\quad\bar{g}\in\bar{G},\]

and that \(\bar{G}\) is transitive over \(\Omega\).2

Footnote 2: The family \({\cal P}\) is then a group family; see Lehmann and Casella (1998), Section 1.3.

**Theorem 10.2.1**: _If \({\cal P}\) remains invariant under \(G\) and if \(\bar{G}\) is transitive over \(\Omega\), then a maximal invariant \(T\) (and hence any invariant) is ancillary._

[MISSING_PAGE_EMPTY:2111]

(\(i=1,\ldots,n-1\)). Thus conditionally, given the values \(y_{1},\ldots,y_{n-1}\), \(Y_{n}\) and \(Y^{\prime}_{n}\) differ only by a constant, and their conditional distributions (and the critical values \(c(y_{1},\ldots,y_{n-1})\)) differ by the same constant. One can therefore choose \(Y_{n}\), subject to (10.14), to make the conditional calculations as convenient as possible.

**Lemma 10.2.1**: _If \(Y_{n}\) and \(Y^{\prime}_{n}\) both satisfy (10.14), then their difference \(\Delta=Y^{\prime}_{n}-Y_{n}\) depends on \((x_{1},\ldots,x_{n})\) only through the differences \((x_{1}-x_{n},\ldots,x_{n-1}-x_{n})\)._

Proof. Since \(Y_{n}\) and \(Y^{\prime}_{n}\) satisfy (10.14),

\[\Delta(x_{1}+a,\ldots,x_{n}+a)=\Delta(x_{1},\ldots,x_{n})\qquad\mbox{for all $a$}.\]

Putting \(a=-x_{n}\), one finds

\[\Delta(x_{1},\ldots,x_{n})=\Delta(x_{1}-x_{n},\ldots,x_{n-1}-x_{n},0),\]

which is a function of the differences.

The existence of ancillary statistics is not confined to models that remain invariant under a transitive group \(\bar{G}\). The mixture and regression examples of Section 10.1 provide illustrations of ancillaries without the benefit of invariance. Further examples are given in Problems 10.8-10.13.

If conditioning on an ancillary statistic is considered appropriate because it makes the inference more relevant to the situation at hand, it is desirable to carry the process as far as possible and hence to condition on a _maximal_ ancillary. An ancillary \(Z\) is said to be maximal if there does not exist an ancillary \(U\) such that \(Z=f(U)\) without \(Z\) and \(U\) being equivalent. [For a more detailed treatment, which takes account of the possibility of modifying statistics on sets of measure zero without changing their probabilistic properties, see Basu (1959).]

Conditioning, like sufficiency and invariance, leads to a reduction of the data. In the conditional model, the ancillary is no longer part of the random data but has become a constant. As a result, conditioning often leads to a great simplification of the inference. Choosing a maximal ancillary for conditioning thus has the additional advantage of providing the greatest reduction of the data.

Unfortunately, maximal ancillaries are not always unique, and one must then decide which maximal ancillary to choose for conditioning. [This problem is discussed by Cox (1971) and Becker and Gordon (1983).] If attention is restricted to ancillary statistics that are invariant under a given group \(G\), the maximal ancillary of course coincides with the maximal invariant.

Another issue concerns the order in which to apply reduction by sufficiency and ancillarity.

**Example 10.2.1**: Let \((X_{i},Y_{i})\), \(i=1,\ldots,n\), be independently distributed according to a bivariate normal distribution with \(E(X_{i})=E(Y_{i})=0\), \(\mbox{Var}(X_{i})=\mbox{Var}(Y_{i})=1\)and unknown correlation coefficient \(\rho\). Then \(X_{1},\ldots,X_{n}\) are independently distributed as \(N(0,1)\) and are therefore ancillary. The conditional density of the \(Y\)'s given \(X_{1}=x_{i}\),..., \(X_{n}=x_{n}\) is

\[C\exp\left(-\frac{1}{2(1-\rho^{2})}\sum(y_{i}-\rho x_{i})^{2}\right),\]

with the sufficient statistics (\(\sum Y_{i}^{2}\), \(\sum x_{i}Y_{i}\)).

Alternatively, one could begin by noticing that (\(Y_{1}\),..., \(Y_{n}\)) is ancillary. The conditional distribution of the \(X\)'s given \(Y_{1}=y_{1}\),..., \(Y_{n}=y_{n}\) then admits the sufficient statistics (\(\sum X_{i}^{2}\), \(\sum X_{i}y_{i}\)). A unique maximal ancillary \(V\) does not exist in this case, since both the \(X\)'s and \(Y\)'s would have to be functions of \(V\). Thus \(V\) would have to be equivalent to the full sample (\(X_{1}\), \(Y_{1}\)),..., (\(X_{n}\), \(Y_{n}\)), which is not ancillary.

Suppose instead that the data are first reduced to the sufficient statistics \(T=(\sum X_{i}^{2}+\sum Y_{i}^{2}\), \(\sum X_{i}Y_{i})\). Based on \(T\), no nonconstant ancillaries appear to exist.4 This example and others like it suggest that it is desirable to reduce the data as far as possible through sufficiency, before attempting further reduction by means of ancillary statistics. \(\blacksquare\)

Footnote 4: So far, nonexistence has not been proved. It seems likely that a proof can be obtained by the methods of Unni (1978).

Note that contrary to this suggestion, in the location example at the beginning of the section, the problem was not first reduced to the sufficient statistics \(X_{(1)}<\cdots<X_{(n)}\). The omission can be justified in hindsight by the fact that the optimal conditional tests are the same whether or not the observations are first reduced to the order statistics.

In the structure described at the beginning of the section, the variable \(Z\) that labels the experiment was assumed to have a known distribution. The argument for conditioning on the observed value of \(Z\) does not depend on this assumption. It applies also when the distribution of \(Z\) depends on an unknown parameter \(\vartheta\), which is independent of \(\theta\) and hence by itself contains no information about \(\theta\), that is, when the distribution of \(Z\) depends only on \(\vartheta\), the conditional distribution of \(X\) given \(Z=z\) depends only on \(\theta\), and the parameter space \(\Omega\) for (\(\theta\), \(\vartheta\)) is a Cartesian product \(\Omega=\Omega_{1}\times\Omega_{2}\), with

\[(\theta,\vartheta)\in\Omega\quad\Leftrightarrow\quad\theta\in\Omega_{1}\quad \mbox{and}\quad\vartheta\in\Omega_{2}. \tag{10.15}\]

(the parameters \(\theta\) and \(\vartheta\) are then said to be _variation independent_, or unrelated.)

Statistics \(Z\) satisfying this more general definition are called _partial ancillary_ or _S-ancillary_. (The term ancillary without modification will be reserved here for a statistic that has a known distribution.) Note that if \(X=(T\), \(Z)\) and \(Z\) is a partial ancillary, then \(T\) is a partial sufficient statistic in the sense of Problem 3.65. For a more detailed discussion of this and related concepts of partial ancillarity, see for example Basu (1978) and Barndorff-Nielsen (1978).

**Example 10.2.2**: Let \(X\) and \(Y\) be independent with Poisson distributions \(P(\lambda)\) and \(P(\mu)\), and let the parameter of interest be \(\theta=\mu/\lambda\). It was seen in Section 10.4 that the conditional distribution of \(Y\) given \(Z=X+Y=z\) is binomial \(b(p,z)\) with \(p=\mu/(\lambda+\mu)=\theta/(\theta+1)\) and therefore depends only on \(\theta\), while the distribution of \(Z\) is Poisson with mean \(\vartheta=\lambda+\mu\). Since the parameter space \(0<\lambda\), \(\mu<\infty\) is equivalent to the Cartesian product of \(0<\theta<\infty\), \(0<\vartheta<\infty\), it follows that \(Z\) is \(S\)-ancillary for \(\theta\).

The UMP unbiased level-\(\alpha\) test of \(H:\mu\leq\lambda\) against \(\mu>\lambda\) is UMP also among all tests whose conditional level given \(z\) is \(\alpha\) for all \(z\). (The class of conditional tests coincides exactly with the class of all tests that are similar on the boundary \(\mu=\lambda\).)

When \(Z\) is \(S\)-ancillary for \(\theta\) in the presence of a nuisance parameter \(\vartheta\), the unconditional power \(\beta(\theta,\vartheta)\) of a test \(\varphi\) of \(H:\theta=\theta_{0}\) may depend on \(\vartheta\) as well as on \(\theta\). The conditional power \(\beta(\vartheta\mid z)=E_{\theta}[\varphi(X)\mid z]\) can then be viewed as an unbiased estimator of the (unknown) \(\beta(\theta,\vartheta)\), as was discussed at the end of Section 4.4. On the other hand, if no nuisance parameters \(\vartheta\) are present and \(Z\) is ancillary for \(\theta\), the unconditional power \(\beta(\theta)=E_{\theta}\varphi(X)\) and the conditional power \(\beta(\theta\mid z)\) provide two alternative evaluations of the power of \(\varphi\) against \(\theta\), which refer to different sampling frameworks, and of which the latter of course becomes available only after the data have been obtained.

Surprisingly, the \(S\)-ancillarity of \(X+Y\) in Example 10.2.2 does not extend to the corresponding binomial problem.

**Example 10.2.3**: Let \(X\) and \(Y\) have independent binomial distributions \(b(p_{1},m)\) and \(b(p_{2},n)\) respectively. Then it was seen in Section 4.5 that the conditional distribution of \(Y\) given \(Z=X+Y=z\) depends only on the cross-product ratio \(\Delta=p_{2}q_{1}/p_{1}q_{2}\) (\(q_{i}=1-p_{i}\)). However, \(Z\) is not \(S\)-ancillary for \(\Delta\). To see this, note that \(S\)-ancillarity of \(Z\) implies the existence of a parameter \(\vartheta\) unrelated to \(\Delta\) and such that the distribution of \(Z\) depends only on \(\vartheta\). As \(\Delta\) changes, the family of distributions \(\{P_{\vartheta},\vartheta\in\Omega_{2}\}\) of \(Z\) would remain unchanged. This is not the case, since \(Z\) is binomial when \(\Delta=1\) and not otherwise (Problem 10.15). Thus \(Z\) is not \(S\)-ancillary.

In this example, all unbiased tests of \(H:\Delta=\Delta_{0}\) have a conditional level given \(z\) that is independent of \(z\), but conditioning on \(z\) cannot be justified by \(S\)-ancillarity.

Closely related to this example is the situation of the multinomial \(2\times 2\) table discussed from the point of view of unbiasedness in Section 4.6.

**Example 10.2.4**: In the notation of Section 4.6, let the four cell entries of a \(2\times 2\) table be \(X\), \(X^{\prime}\), \(Y\), \(Y^{\prime}\) with row totals \(X+X^{\prime}=M\), \(Y+Y^{\prime}=N\), and column totals \(X+Y=T\), \(X^{\prime}+Y^{\prime}=T^{\prime}\), and with total sample size \(M+N=T+T^{\prime}=s\). Here it is easy to check that (\(M\), \(N\)) is \(S\)-ancillary for \(\theta=(\theta_{1},\theta_{2})=(p_{AB}/p_{B}\), \(p_{A\tilde{B}}/p_{\tilde{B}})\) with \(\vartheta=p_{B}\). Since the cross-product ratio \(\Delta\) can be expressed as a function of \((\theta_{1},\theta_{2})\), it may be appropriate to condition a test of \(H:\Delta=\Delta_{0}\) on (\(M\), \(N\)). Exactly analogously one finds that (\(T\), \(T^{\prime}\)) is \(S\)-ancillary for \(\theta^{\prime}=(\theta^{\prime}_{1},\theta^{\prime}_{2})=(p_{AB}/p_{A}\), \(p_{\tilde{A}\tilde{B}}/p_{\tilde{A}})\)and since \(\Delta\) is also a function of \((\theta^{\prime}_{1},\theta^{\prime}_{2})\), it may be equally appropriate to condition a test of \(H\) on \((T,T^{\prime})\). One might hope that the set of all four marginals \((M,N,T,T^{\prime})=Z\) would be \(S\)-ancillary for \(\Delta\). However, it is seen from the preceding example that this is not the case.

Here, all unbiased tests have a constant conditional level given \(z\). However, \(S\)-ancillarity permits conditioning on only one set of margins (without giving any guidance as to which of the two to choose), not on both.

Despite such difficulties, the principle of carrying out tests and confidence estimation conditionally on ancillaries or \(S\)-ancillaries frequently provides an attractive alternative to the corresponding unconditional procedures, primarily because it is more appropriate for the situation at hand. However, insistence on such conditioning leads to another difficulty, which is illustrated by the following example.

Consider \(N\) populations \(\prod_{i}\), and suppose that an observation \(X_{i}\) from \(\prod_{i}\) has a normal distribution \(N(\xi_{i},1)\). The hypothesis to be tested is \(H:\xi_{1}=\cdots=\xi_{N}\). Unfortunately, \(N\) is so large that it is not practicable to take an observation from each of the populations; the total sample size is restricted to be \(n<N\). A sample \(\prod_{J_{1}},\ldots,\prod_{J_{n}}\) of \(n\) of the \(N\) populations is therefore selected at random, with probability \(1/{N\choose n}\) for each set of \(n\), and an observation \(X_{j_{i}}\) is obtained from each of the populations \(\prod_{j_{i}}\), in the sample.

Here the variables \(J_{1},\ldots,J_{n}\) are ancillary, and the requirement of conditioning on ancillaries would restrict any inference to the \(n\) populations from which observations are taken. Systematic adherence to this requirement would therefore make it impossible to test the original hypothesis \(H\).5 Of course, rejection of the partial hypothesis \(H_{j_{1},\ldots,j_{n}}:\xi_{j_{1}}=\cdots=\xi_{j_{n}}\) would imply rejection of the original \(H\). However, acceptance of \(H_{j_{1},\ldots,j_{n}}\) would permit no inference concerning \(H\).

Footnote 5: For other implications of this requirement, called the weak conditionality principle, see Birnbaum (1962) and Berger and Wolpert (1988).

The requirement to condition in this case runs counter to the belief that a sample may permit inferences concerning the whole set of populations, which underlies much of statistical practice.

With an unconditional approach such an inference is provided by the test with rejection region

\[\sum\left[X_{j_{i}}-\left(\frac{1}{n}\sum_{k=1}^{n}X_{j_{k}}\right)\right]^{2 }\geq c,\]

where \(c\) is the upper \(\alpha\)-percentage point of \(\chi^{2}\) with \(n-1\) degrees of freedom. Not only does this test actually have unconditional level \(\alpha\), but its conditional level given \(J_{1}=j_{1},\ldots,J_{n}=j_{n}\) also equals \(\alpha\) for all \((j_{1},\ldots,j_{n})\). There is in fact no difference in the present case between the conditional and the unconditional tests: they will accept or reject for the same sample points. However, as has been pointed out, there is a crucial difference between the conditional and unconditional interpretations of the results.

If \(\beta_{j_{1},\ldots,j_{n}}(\xi_{j_{1}},\ldots,\xi_{j_{n}})\) denotes the conditional power of this test given \(J_{1}=j_{1},\ldots,J_{n}=j_{n}\), its unconditional power is

\[\frac{\sum\beta_{j_{1},\ldots,j_{n}}(\xi_{j_{1}},\ldots,\xi_{j_{n}})}{{N\choose n }}\]

summed over all \({N\choose n}\)\(n\)-tuples \(j_{1}<\ldots<j_{n}\). As in the case with any test, the conditional power given an ancillary (in the present case \(J_{1},\ldots,J_{n}\)) can be viewed as an unbiased estimate of the unconditional power.

### Optimal Conditional Tests

Although conditional tests are often sensible and are beginning to be employed in practice [see for example Lawless (1972, 1973, 1978) and Kappenman (1975)], not much theory has been developed for the resulting conditional models. Since the conditional model tends to be simpler than the original unconditional one, the conditional point of view will frequently bring about a simplification of the theory. This possibility will be illustrated in the present section on some simple examples.

**Example 10.3.1**: Specializing the example discussed at the beginning of Section 10.1, suppose that a random variable is distributed according to \(N(\theta,\sigma_{1}^{2})\) or \(N(\theta,\sigma_{0}^{2})\) as \(I=1\) or \(0\), and that \(P(I=1)=P(I=0)=\frac{1}{2}\). Then the most powerful test of \(H:\theta=\theta_{0}\) against \(\theta=\theta_{1}(>\theta_{0})\) based on (\(I\), \(X\)) rejects when

\[\frac{x-\frac{1}{2}(\theta_{0}+\theta_{1})}{2\sigma_{i}^{2}}\geq k.\]

A UMP test against the alternatives \(\theta>\theta_{0}\) therefore does not exist. On the other hand, if \(H\) is tested conditionally given \(I=i\), a UMP conditional test exists and rejects when \(X>c_{i}\) where \(P(X>c_{i}\mid I=i)=\alpha\) for \(i=0\), \(1\).

The nonexistence of UMP unconditional tests found in this example is typical for mixtures with known probabilities of two or more families with monotone likelihood ratio, despite the existence of UMP conditional tests in these cases.

**Example 10.3.2**: Let \(X_{1},\ldots,X_{n}\) be a sample from a normal distribution \(N(\xi,a^{2}\xi^{2})\), \(\xi>0\), with known coefficient of variation \(a>0\), and consider the problem of testing \(H:\xi=\xi_{0}\) against \(K:\xi>\xi_{0}\). Here \(T=(T_{1},T_{2})\) with \(T_{1}=\bar{X}\), \(T_{2}=\sqrt{(1/n)\sum X_{i}^{2}}\) is sufficient, and \(Z=T_{1}/T_{2}\) is ancillary. If we let \(V=\sqrt{n}T_{2}/a\), the conditional density of \(V\) given \(Z=z\) is equal to (Problem 10.18)

\[p_{\xi}(v\mid z)=\frac{k}{\xi^{n}}v^{n-1}\exp\left\{-\frac{1}{2}\left[\frac{v} {\xi}-\frac{z\sqrt{n}}{a}\right]^{2}\right\}. \tag{10.16}\]The density has monotone likelihood ratio, so that the rejection region \(V>C(z)\) constitutes a UMP conditional test.

Unconditionally, \(Y=\bar{X}\) and \(S^{2}=\sum(X_{i}-\bar{X})^{2}\) are independent with joint density

\[cs^{(n-3)/2}\exp\left(-\frac{n}{2a^{2}\xi^{2}}(y-\xi)^{2}-\frac{1}{2a^{2}\xi^{2 }}s^{2}\right), \tag{10.17}\]

and a UMP test does not exist. [For further discussion of this example, see Hinkley (1977).]

An important class of examples is obtained from situations in which the model remains invariant under a group of transformations that is transitive over the parameter space, that is, when the given class of distributions constitutes a group family. The maximal invariant \(V\) then provides a natural ancillary on which to condition, and an optimal conditional test may exist even when such a test does not exist unconditionally. Perhaps the simplest class of examples of this kind are provided by location families under the conditions of the following lemma.

**Lemma 10.3.1**: _Let \(X_{1},\ldots,X_{n}\) be independently distributed according to \(f(x_{i}-\theta)\), with \(f\) strongly unimodal. Then the family of conditional densities of \(Y_{n}=X_{n}\) given \(Y_{i}=X_{i}-X_{n}\)\((i=1,\ldots,n-1)\) has monotone likelihood ratio._

Proof. The conditional density (10.11) is proportional to

\[f(y_{n}+y_{1}-\theta)\cdots f(y_{n}+y_{n-1}-\theta)f(y_{n}-\theta). \tag{10.18}\]

By taking logarithms and using the fact that each factor is strongly unimodal, it is seen that the product is also strongly unimodal, and the result follows from Example 8.2.1.

Lemma 10.3.1 shows that for strongly unimodal \(f\) there exists a UMP conditional test of \(H:\theta\leq\theta_{0}\) against \(K:\theta>\theta_{0}\) which rejects when

\[X_{n}>c(X_{1}-X_{n},\ldots,X_{n-1}-X_{n}). \tag{10.19}\]

Conditioning has reduced the model to a location family with sample size one. The double exponential and logistic distributions are both strongly unimodal (Section 9.2), and thus provide examples of UMP conditional tests. In neither case does there exist a UMP unconditional test unless \(n=1\).

As a last class of examples, we shall consider a situation with a nuisance parameter. Let \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) be independent samples from location families with densities \(f(x_{1}-\xi,\ldots,x_{m}-\xi)\) and \(g(y_{1}-\eta,\ldots,y_{n}-\eta)\) respectively, and consider the problem of testing \(H:\eta\leq\xi\) against \(K:\eta>\xi\). Here the differences \(U_{i}=X_{i}-X_{m}\) and \(V_{j}=Y_{j}-Y_{n}\) are ancillary. The conditional density of \(X=X_{m}\) and \(Y=Y_{n}\) given the \(u\)'s and \(v\)'s is seen from (10.18) to be of the form \[f^{*}_{u}(x-\xi)g^{*}_{v}(y-\eta), \tag{10.20}\]

where the subscripts \(u\) and \(v\) indicate that \(f^{*}\) and \(g^{*}\) depend on the \(u\)'s and \(v\)'s respectively. The problem of testing \(H\) in the conditional model remains invariant under the transformations: \(x^{\prime}=x+c\), \(y^{\prime}=y+c\), for which \(Y-X\) is maximal invariant. A UMP invariant conditional test will then exist provided the distribution of \(Z=Y-X\), which depends only on \(\Delta=\eta-\xi\), has monotone likelihood ratio. The following lemma shows that a sufficient condition for this to be the case is that \(f^{*}_{u}\) and \(g^{*}_{v}\) have monotone likelihood ratio in \(x\) and \(y\) respectively.

**Lemma 10.3.2**: _Let \(X\), \(Y\) be independently distributed with densities \(f^{*}(x-\xi)\), \(g^{*}(y-\eta)\) respectively. If \(f^{*}\) and \(g^{*}\) have monotone likelihood with respect to \(\xi\) and \(\eta\), then the family of densities of \(Z=Y-X\) has monotone likelihood ratio with respect to \(\Delta=\eta-\xi\)._

Proof. The density of \(Z\) is

\[h_{\Delta}(z)=\int g^{*}(y-\Delta)f^{*}(y-z)\,dy. \tag{10.21}\]

To see that \(h_{\Delta}(z)\) has monotone likelihood ratio, one must show that for any \(\Delta<\Delta^{\prime}\), \(h_{\Delta^{\prime}}(z)/h_{\Delta}(z)\) is an increasing function of \(z\). For this purpose, write

\[\frac{h_{\Delta^{\prime}}(z)}{h_{\Delta}(z)}=\int\frac{g^{*}(y-\Delta^{\prime} )}{g^{*}(y-\Delta)}\cdot\frac{g^{*}(y-\Delta)f^{*}(y-z)}{\int g^{*}(u-\Delta)f (u-z)\,du}\,dy.\]

The second factor is a probability density for \(Y\),

\[p_{z}(y)=C_{z}g^{*}(y-\Delta)f^{*}(y-z), \tag{10.22}\]

which has monotone likelihood ratio in the parameter \(z\) by the assumption made about \(f^{*}\). The ratio

\[\frac{h_{\Delta^{\prime}}(z)}{h_{\Delta}(z)}=\int\frac{g^{*}(y-\Delta^{\prime} )}{g^{*}(y-\Delta)}\,p_{z}(y)\,dy \tag{10.23}\]

is the expectation of \(g^{*}(Y-\Delta^{\prime})/g^{*}(Y-\Delta)\) under the distribution \(p_{z}(y)\). By the assumption about \(g^{*}\), \(g^{*}(y-\Delta^{\prime})/g^{*}(y-\Delta)\) is an increasing function of \(y\), and it follows from Lemma 3.4.2 that its expectation is an increasing function of \(z\).

It follows from (10.18) that \(f^{*}_{u}(x-\xi)\) and \(g^{*}_{v}(y-\eta)\) have monotone likelihood ratio provided this condition holds for \(f(x-\xi)\) and \(g(y-\eta)\), i.e., provided \(f\) and \(g\) are strongly unimodal. Under this assumption, the conditional distribution \(h_{\Delta}(z)\) then has monotone likelihood ratio by Lemma 10.3.2, and a UMP conditional test exists and rejects for large values of \(Z\). (This result also follows from Problem 8.12.)

The difference between conditional tests of the kind considered in this section and the corresponding (e.g., locally most powerful) unconditional tests typically disappears as the sample size(s) tend(s) to infinity. Some results in this direction are given by Liang (1984); see also Barndorff-Nielsen (1983).

The following multivariate example provides one more illustration of a UMP conditional test when unconditionally no UMP test exists. The results will only be sketched. The details of this and related problems can be found in the original literature reviewed by Marden and Perlman (1980) and Marden (1983).

**Example 10.3.3**: Suppose you observe \(m+1\) independent normal vectors of dimension \(p=p_{1}+p_{2}\),

\[Y=(Y_{1}\ \ \ Y_{2})\ \ \ \mbox{and}\ \ \ \ Z_{1},\ldots,Z_{m},\]

with common covariance matrix \(\Sigma\) and expectations

\[E(Y_{1})=\eta_{1},\ \ \ \ \ E(Y_{2})=E(Z_{1})=\cdots=E(Z_{m})=0.\]

(The normal multivariate two-sample problem with covariates can be reduced to this canonical form.) The hypothesis being tested is \(H:\eta_{1}=0\). Without the restriction \(E(Y_{2})=0\), the model would remain invariant under the group \(G\) of transformations: \(Y^{*}=YB\), \(Z^{*}=ZB\), where \(B\) is any nonsingular \(p\times p\) matrix. However, the stated problem remains invariant only under the subgroup \(G^{\prime}\) in which \(B\) is of the form (Problem 10.22(i))

\[B=\left(\begin{array}{cc}B_{11}&0\\ B_{21}&B_{22}\\ p_{1}&p_{2}\end{array}\right)_{p_{2}}^{p_{1}}.\]

If

\[Z^{\prime}Z=S=\left(\begin{array}{cc}S_{11}&S_{12}\\ S_{21}&S_{22}\end{array}\right)\ \ \ \mbox{and}\ \ \ \Sigma=\left(\begin{array}{cc} \Sigma_{11}&\Sigma_{12}\\ \Sigma_{21}&\Sigma_{22}\end{array}\right),\]

the maximal invariants under \(G^{\prime}\) are the two statistics \(D=Y_{2}S_{22}^{-1}Y_{2}^{\prime}\) and

\[N=\frac{(Y_{1}-S_{12}S_{22}^{-1}Y_{2})(S_{11}-S_{12}S_{22}^{-1}S_{21})^{-1}(Y_ {1}-S_{12}S_{22}^{-1}Y_{2})^{\prime}}{1+D},\]

and the joint distribution of \((N\), \(D)\) depends only on the maximal invariant under \(G^{\prime}\),

\[\Delta=\eta_{1}(\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})^{-1}\eta_{ 1}^{\prime}.\]

The statistic \(D\) is ancillary (Problem 10.22(ii)), and the conditional distribution of \(N\) given \(D=d\) is that of the ratio of two independent \(\chi^{2}\)-variables: the numerator noncentral \(\chi^{2}\) with \(p\) degrees of freedom and noncentrality parameter \(\Delta/(1+d)\), and the denominator central \(\chi^{2}\) with \(m+1-p\) degrees of freedom. It follows from Section 7.1 that the conditional density has monotone likelihood ratio. A conditionally UMP invariant test therefore exists, and rejects \(H\) when \((m+1-p)N/p>C\)where \(C\) is the critical value of the \(F\)-distribution with \(p\) and \(m+1-p\) degrees of freedom. On the other hand, a UMP invariant (unconditional) test does not exist; comparisons of the optimal conditional test with various competitors are provided by Marden and Perlman (1980).

### Relevant Subsets

The conditioning variables considered so far have been ancillary statistics, i.e., random variables whose distribution is fixed, independent of the parameters governing the distribution of \(X\), or at least of the parameter of interest. We shall now examine briefly some implications of conditioning without this constraint. Throughout most of the section we shall be concerned with the simple case in which the conditioning variable is the indicator of some subset \(C\) of the sample space, so that there are only two conditioning events \(I=1\) (i.e., \(X\in C\)) and \(I=0\) (i.e., \(X\in C^{c}\), the complement of \(C\)). The mixture problem at the beginning of Section 10.1, with \(\mathcal{X}_{1}=C\) and \(\mathcal{X}_{0}=C^{c}\), is of this type.

Suppose \(X\) is distributed with density \(p_{\theta}\), and \(R\) is a level-\(\alpha\) rejection region for testing the simple hypothesis \(H:\theta=\theta_{0}\) against some class of alternatives. For any subset \(C\) of the sample space, consider the conditional rejection probabilities

\[\alpha_{C}=P_{\theta_{0}}(X\in R\mid C)\quad\text{and}\quad\alpha_{C^{c}}=P_{ \theta_{0}}(X\in R\mid C^{c}), \tag{10.24}\]

and suppose that \(\alpha_{C}>\alpha\) and \(\alpha_{C^{c}}<\alpha\). Then we are in the difficulty described in Section 10.1. Before \(X\) was observed, the probability of falsely rejecting \(H\) was stated to be \(\alpha\). Now that \(X\) is known to have fallen into \(C\) (or \(C^{c}\)), should the original statement be adjusted and the higher value \(\alpha_{C}\) (or lower value \(\alpha_{C^{c}}\)) be quoted? An extreme case of this possibility occurs when \(C\) is a subset of \(R\) or \(R^{c}\), since then \(P(X\in R\mid X\in C)=1\) or \(0\).

It is clearly always possible to choose \(C\) so that the conditional level \(\alpha_{C}\) exceeds the stated \(\alpha\). It is not so clear whether the corresponding possibility always exists for the levels of a family of confidence sets for \(\theta\), since the inequality must now hold for all \(\theta\).

**Definition 10.4.1** A subset \(C\) of the sample space is said to be a _negatively biased relevant subset_ for a family of confidence sets \(S(X)\) with unconditional confidence level \(\gamma=1-\alpha\) if for some \(\epsilon>0\)

\[\gamma_{C}(\theta)=P_{\theta}[\theta\in S(X)\mid X\in C]\leq\gamma-\epsilon \qquad\text{for all $\theta$}, \tag{10.25}\]

and a _positively biased relevant subset_ if

\[P_{0}[\theta\in S(X)\mid X\in C]\geq\gamma+\epsilon\qquad\text{for all $\theta$}. \tag{10.26}\]The set \(C\) is _semirelevant, negatively or positively biased_, if respectively

\[P_{\theta}[\theta\in S(X)\mid X\in C]\leq\gamma\quad\quad\text{for all }\theta \tag{10.27}\]

or

\[P_{\theta}[\theta\in S(X)\mid X\in C]\geq\gamma\quad\quad\text{for all }\theta, \tag{10.28}\]

with strict inequality holding for at least some \(\theta\).

Obvious examples of relevant subsets are provided by the subsets \(\mathcal{X}_{0}\) and \(\mathcal{X}_{1}\) of the two-experiment example of Section 10.1.

Relevant subsets do not always exist. The following four examples illustrate the various possibilities.

**Example 10.4.1**: Let \(X\) be distributed as \(N(\theta,1)\), and consider the standard confidence intervals for \(\theta\):

\[S(X)=\{\theta:X-c<\theta<X+c\},\]

where \(\Phi(c)-\Phi(-c)=\gamma\). In this case, there exists not even a semirelevant subset.

To see this, suppose first that a positively biased semirelevant subset \(C\) exists, so that

\[A(\theta)=P_{\theta}[X-c<\theta<X+c\text{ and }X\in C]-\gamma P_{\theta}[X\in C]\geq 0\]

for all \(\theta\), with strict inequality for some \(\theta_{0}\). Consider a prior normal density \(\lambda(\theta)\) for \(\theta\) with mean 0 and variance \(\tau^{2}\), and let

\[\beta(x)=P[x-c<\Theta<x+c\mid x],\]

where \(\Theta\) has density \(\lambda(\theta)\). The posterior distribution of \(\Theta\) given \(x\) is then normal with mean \(\tau^{2}x/(1+\tau^{2})\) and variance \(\tau^{2}/(1+\tau^{2})\) (Problem 10.24(i)), and it follows that

\[\beta(x) = \Phi\left[\frac{x}{\tau\sqrt{1+\tau^{2}}}+\frac{c\sqrt{1+\tau^{2} }}{\tau}\right]-\Phi\left[\frac{x}{\tau\sqrt{1+\tau^{2}}}-\frac{c\sqrt{1+\tau^{ 2}}}{\tau}\right]\] \[\leq \Phi\left[\frac{c\sqrt{1+\tau^{2}}}{\tau}\right]-\Phi\left[\frac {-c\sqrt{1+\tau^{2}}}{\tau}\right]\leq\gamma+\frac{c}{\sqrt{2\pi}\tau^{2}}.\]

Next let \(h(\theta)=\sqrt{2\pi}\tau\lambda(\theta)=e^{-\theta^{2}/2\tau^{2}}\) and

\[D=\int h(\theta)A(\theta)\,d\theta\leq\sqrt{2\pi}\tau\int\lambda (\theta)\{P_{\theta}[X-c<\theta<X+c\text{ and }X\in C]\] \[-E_{\theta}[\beta(X)I_{C}(X)]\}\,d\theta+\frac{c}{\tau}.\]The integral on the right side is the difference of two integrals each of which equals \(P[X-c<\Theta<X+c\) and \(X\in C]\), and is therefore \(0\), so that \(D\leq c/\tau\).

Consider now a sequence of normal priors \(\lambda_{m}(\theta)\) with variances \(\tau_{m}^{2}\to\infty\), and the corresponding sequences \(h_{m}(\theta)\) and \(D_{m}\). Then \(0\leq D_{m}\leq c/\tau_{m}\) and hence \(D_{m}\to 0\). On the other hand, \(D_{m}\) is of the form \(D_{m}=\int_{-\infty}^{\infty}A(\theta)h_{m}(\theta)\,d\theta\), where \(A(\theta)\) is continuous, nonnegative, and \(>0\) for some \(\theta_{0}\). There exists \(\delta>0\) such that \(A(\theta)\leq\frac{1}{2}A(\theta_{0})\) for \(|\theta-\theta_{0}|<\delta\) and hence

\[D_{m}\geq\int_{\theta_{0}-\delta}^{\theta_{0}+\delta}\frac{1}{2}A(\theta_{0})h _{m}(\theta)\,d\theta\to\delta A(\theta_{0})>0\quad\mbox{ as }\;\;m\to\infty.\]

This provides the desired contradiction.

That also no negatively semirelevant subsets exist is a consequence of the following result.

**Theorem 10.4.1**: _Let \(S(x)\) be a family of confidence sets for \(\theta\) such that \(P_{\theta}[\theta\in S(X)]=\gamma\) for all \(\theta\), and suppose that \(0<P_{\theta}(C)<1\) for all \(\theta\)._

(i) If \(C\) is semirelevant, then its complement \(C^{c}\) is semirelevant with opposite bias.

(ii) If there exists a constant \(a\) such that

\[1>P_{\theta}(C)>a>0\quad\mbox{ for all }\theta\]

and \(C\) is relevant, then \(C^{c}\) is relevant with opposite bias.

Proof. The result is an immediate consequence of the identity

\[P_{\theta}(C)[\gamma_{C}(\theta)-\gamma]=[1-P_{\theta}(C)][\gamma-\gamma_{C^{c }}(\theta)].\]

The next example illustrates the situation in which a semirelevant subset exists but no relevant one.

**Example 10.4.2**: Let \(X\) be \(N(\theta,1)\), and consider the uniformly most accurate lower confidence bounds \(\underline{\theta}=X-c\) for \(\theta\), where \(\Phi(c)=\gamma\). Here \(S(X)\) is the interval \([X-c,\,\infty)\) and it seems plausible that the conditional probability of \(\theta\in S(X)\) will be lowered for a set \(C\) of the form \(X\geq k\). In fact

\[P_{\theta}(X-c\leq\theta\mid X\geq k)=\cases{\frac{\Phi(c)-\Phi(k-\theta)}{1- \Phi(k-\theta)}&when \;\;$\theta>k-c$,\cr 0&when \;\;$\theta<k-c$.\cr} \tag{10.29}\]

The probability (10.29) is always \(<\gamma\), and tends to \(\gamma\) as \(\theta\to\infty\). The set \(X\geq k\) is therefore semirelevant negatively biased for the confidence sets \(S(X)\).

We shall now show that no relevant subset \(C\) with \(P_{\theta}(C)>0\) exists in this case. It is enough to prove the result for negatively biased sets; the proof for positive bias is exactly analogous. Let \(A\) be the set of \(x\)-values \(-\infty<x<c+\theta\), and suppose that \(C\) is negatively biased and relevant, so that

\[P_{\theta}[X\in A\ |\ C]\leq\gamma-\epsilon\qquad\text{for all $\theta$}.\]

If

\[a(\theta)=P_{\theta}(X\in C),\qquad b(\theta)=P_{\theta}(X\in A\cap C),\]

then

\[b(\theta)\leq(y-\epsilon)\,a(\theta)\qquad\text{for all $\theta$}. \tag{10.30}\]

The result is proved by comparing the integrated coverage probabilities

\[A(R)=\int_{-R}^{R}a(\theta)\,d\theta,\qquad B(R)=\int_{-R}^{R}b(\theta)\,d\theta\]

with the Lebesgue measure of the intersection \(C\cap(-R,\,R)\),

\[\mu(R)=\int_{-R}^{R}I_{C}(x)\,dx,\]

where \(I_{C}(x)\) is the indicator of \(C\), and showing that

\[\frac{A(R)}{\mu(R)}\to 1,\quad\frac{B(R)}{\mu(R)}\to\gamma\quad\text{as} \quad R\to\infty. \tag{10.31}\]

This contradicts the fact that by (10.30),

\[B(R)\leq(\gamma-\epsilon)A(R)\qquad\text{for all $R$},\]

and so proves the desired result.

To prove (10.31), suppose first that \(\mu(\infty)<\infty\). Then if \(\phi\) is the standard normal density

\[A(\infty)=\int_{-\infty}^{\infty}\,d\theta\int_{C}\phi(x-\theta)\,dx=\int_{C} \,dx=\mu(\infty),\]

and analogously \(B(\infty)=\gamma\mu(\infty)\), which establishes (10.31).

When \(\mu(\infty)=\infty\), (10.31) will be proved by showing that

\[A(R)=\mu(R)+K_{1}(R),\qquad B(R)=\gamma\mu(R)+K_{2}(R), \tag{10.32}\]

where \(K_{1}(R)\) and \(K_{2}(R)\) are bounded. To see (10.32), note that \[\mu(R)=\int_{-R}^{R}I_{C}(x)\,dx =\int_{-R}^{R}I_{C}(x)\left[\int_{-\infty}^{\infty}\phi(x-\theta)\,d \theta\right]dx\] \[=\int_{-\infty}^{\infty}\left[\int_{-R}^{R}I_{C}(x)\phi(x-\theta)\, dx\right]d\theta,\]

while

\[A(R)=\int_{-R}^{R}\left[\int_{-\infty}^{\infty}I_{C}(x)\phi(x-\theta)\,dx\right] d\theta. \tag{10.33}\]

A comparison of each of these double integrals with that over the region \(-R<x<R,-R<\theta<R\), shows that the difference \(A(R)-\mu(R)\) is made up of four integrals, each of which can be seen to be bounded by using the fact that \(\int|t|\phi(t)\,dt<\infty\) (Problem 10.24(ii)). This completes the proof.

**Example 10.4.3** Let \(X_{1},\,\ldots,\,X_{n}\) be independently normally distributed as \(N(\xi,\,\sigma^{2})\), and consider the uniformly most accurate equivariant (and unbiased) confidence intervals for \(\xi\) given by (5.36).

It was shown by Buehler and Feddersen (1963) and Brown (1967) that in this case there exist positively biased relevant subsets of the form

\[C:\frac{|\tilde{X}|}{S}\leq k. \tag{10.34}\]

In particular, for confidence level \(\gamma=.5\) and \(n=2\), Brown shows that with \(C:|\tilde{X}|/|X_{2}-X_{1}|\leq\frac{1}{2}(1+\sqrt{2})\), the conditional level is \(>\frac{2}{3}\) for all values of \(\xi\) and \(\sigma\). Goutis and Casella (1992) provide detailed values for general \(n\).

It follows from Theorem 10.4.1 that \(C^{c}\) is negatively biased semirelevant, and Buehler (1959) shows that any set \(C^{*}:S\leq k\) has the same property. These results are intuitively plausible, since the length of the confidence intervals is proportional to \(S\), and one would expect short intervals to cover the true value less often than long ones.

Theorem 10.4.1 does not show that \(C^{c}\) is negatively biased relevant, since the probability of the set (10.34) tends to zero as \(\xi/\sigma\to\infty\). It was in fact proved by Robinson (1976) that no negatively biased relevant subset exists in this case.

The calculations for \(C^{c}\) throw some light on the common practice of stating confidence intervals for \(\xi\) only when a preliminary test of \(H:\xi=0\) rejects the hypothesis. For a discussion of this practice see Olshen (1973), and Meeks and D'Agostino (1983).

The only type of example still missing is that of a negatively biased relevant subset. It was pointed out by Fisher (1956a, 1956b, 1959, 1973) that the Welch-Aspin solution of the Behrens-Fisher problem (discussed in Sections 6.6 and 13.2) provides an illustration of this possibility. The following are much simpler examples of both negatively and positively biased relevant subsets.

**Example 10.4.4**: An extreme form of both positively and negatively biased subsets was encountered in Section 7.7, where lower and upper confidence bounds \(\underline{\Delta}<\Delta\) and \(\Delta<\tilde{\Delta}\) were obtained in (7.42) and (7.43) for the ratio \(\Delta=\sigma_{A}^{2}/\sigma^{2}\) in a model II one-way classification. Since

\[P(\underline{\Delta}\leq\Delta\mid\underline{\Delta}<0)=1\ \ \ \mbox{and}\ \ \ P(\Delta\leq\tilde{\Delta}\mid\tilde{\Delta}<0)=0,\]

the sets \(C_{1}:\underline{\Delta}<0\) and \(C_{2}:\tilde{\Delta}<0\) are relevant subsets with positive and negative bias respectively.

The existence of conditioning sets \(C\) for which the conditional coverage probability of level-\(\gamma\) confidence sets is 0 or 1, such as in Example 10.4.4 or Problems 10.27, 10.28 are an embarrassment to confidence theory, but fortunately they are rare. The significance of more general relevant subsets is less clear,6 particularly when a number of such subsets are available. Especially awkward in this connection is the possibility [discussed by Buehler (1959)] of the existence of two relevant subsets \(C\) and \(C^{\prime}\) with nonempty intersection and opposite bias.

Footnote 6: For a discussion of this issue, see Buehler (1959), Robinson (1976, 1979a), and Bondar (1977).

If a conditional confidence level is to be cited for some relevant subset \(C\), it seems appropriate to take account also of the possibility that \(X\) may fall into \(C^{c}\) and to state in advance the three confidence coefficients \(\gamma\), \(\gamma_{C}\), and \(\gamma_{C^{c}}\). The (unknown) probabilities \(P_{\theta}(C)\) and \(P_{\theta}(C^{c})\) should also be considered. These points have been stressed by Kiefer, who has also suggested the extension to a partition of the sample space into more than two sets. For an account of these ideas, see Kiefer (1977a, 1977b), Brownie and Kiefer (1977), and Brown (1978).

Kiefer's theory does not consider the choice of conditioning set or statistic. The same question arose in Section 10.2 with respect to conditioning on ancillaries. The problem is similar to that of the choice of model. The answer depends on the context and purpose of the analysis, and must be determined from case to case.

### Problems

#### Section 10.1

**Problem 10.1**: Let the experiments of \(\mathcal{E}\) and \(\mathcal{F}\) consist in observing \(X:N(\xi,\sigma_{0}^{2})\) and \(X:N(\xi,\sigma_{1}^{2})\) respectively (\(\sigma_{0}<\sigma_{1}\)), and let one of the two experiments be performed, with \(P(\mathcal{E})=P(\mathcal{F})=\frac{1}{2}\). For testing \(H:\xi=0\,\mbox{against}\,\xi=\xi_{1}\), determine values \(\sigma_{0}\), \(\sigma_{1}\), \(\xi_{1}\), and \(\alpha\) such that

\[\mbox{(i)}\ \ \ \alpha_{0}<\alpha_{1};\ \ \ \ \ \mbox{(ii)}\ \ \ \alpha_{0}>\alpha_{1},\]

where the \(\alpha_{i}\) are defined by (10.9).

**Problem 10.2**: Under the assumptions of Problem 10.1, determine the most accurate invariant (under the transformation \(X^{\prime}=-X\)) confidence sets \(S(X)\) with

\[P(\xi\in S(X)\mid\mathcal{E})+P(\xi\in S(X)\mid\mathcal{F})=2\gamma.\]

Find examples in which the conditional confidence coefficients \(\gamma_{0}\) given \(\mathcal{E}\) and \(\gamma_{1}\) given \(\mathcal{F}\) satisfy

\[\text{(i)}\quad\gamma_{0}<\gamma_{1};\qquad\text{(ii)}\quad\gamma_{0}>\gamma_ {1}.\]

**Problem 10.3**: The test given by (10.3), (10.8), and (10.9) is most powerful under the stated assumptions.

**Problem 10.4**: Let \(X_{1}\),..., \(X_{n}\) be independently distributed, each with probability \(p\) or \(q\) as \(N(\xi,\sigma_{0}^{2})\) or \(N(\xi,\sigma_{1}^{2})\).

* If \(p\) is unknown, determine the UMP unbiased test of \(H:\xi=0\) against \(K:\xi>0\).
* Determine the most powerful test of \(H\) against the alternative \(\xi_{1}\) when it is known that \(p=\frac{1}{2}\), and show that a UMP unbiased test does not exist in this case.
* Let \(\alpha_{k}\) (\(k=0\),..., \(n\)) be the conditional level of the unconditional most powerful test of part (ii) given that \(k\) of the \(X\)'s came from \(N(\xi,\sigma_{0}^{2})\) and \(n-k\) from \(N(\xi,\sigma_{1}^{2})\). Investigate the possible values \(\alpha_{0}\), \(\alpha_{1}\),..., \(\alpha_{n}\).

**Problem 10.5**: With known probabilities \(p\) and \(q\) perform either \(\mathcal{E}\) or \(\mathcal{F}\), with \(X\) distributed as \(N(\theta,\,1)\) under \(\mathcal{E}\) or \(N(-\theta,\,1)\) under \(\mathcal{F}\). For testing \(H:\theta=0\) against \(\theta>0\) there exist a UMP unconditional and a UMP conditional level-\(\alpha\) test. These coincide and do not depend on the value of \(p\).

**Problem 10.6**: In the preceding problem, suppose that the densities of \(X\) under \(\mathcal{E}\) and \(\mathcal{F}\) are \(\theta e^{-\theta x}\) and \((1/\theta)e^{-x/\theta}\) respectively. Compare the UMP conditional and unconditional tests of \(H:\theta=1\) against \(K:\theta>1\).

### _Section 10.2_

**Problem 10.7**: Let \(X\), \(Y\) be independently normally distributed as \(N(\theta,\,1)\), and let \(V=Y-X\) and

\[W=\left\{\begin{array}{ll}Y-X&\text{if $X+Y>0$,}\\ X-Y&\text{if $X+Y\leq 0$.}\end{array}\right.\]

* Both \(V\) and \(W\) are ancillary, but neither is a function of the other.
* \((V,\,W)\) is not ancillary. [Basu (1959).]

**Problem 10.8**: An experiment with \(n\) observations \(X_{1}\),..., \(X_{n}\) is planned, with each \(X_{i}\) distributed as \(N(\theta,\,1)\). However, some of the observations do not materialize (forexample, some of the subjects die, move away, or turn out to be unsuitable). Let \(I_{j}=1\) or \(0\) as \(X_{j}\) is observed or not, and suppose the \(I_{j}\) are independent of the \(X\)'s and of each other and that \(P(I_{j}=1)=p\) for all \(j\).

1. If \(p\) is known, the effective sample size \(M=\sum I_{j}\) is ancillary.
2. If \(p\) is unknown, there exists a UMP unbiased level-\(\alpha\) test of \(H:\theta\leq 0\) versus \(K:\theta>0\). Its conditional level (given \(M=m\)) is \(\alpha_{m}=\alpha\) for all \(m=0\),..., \(n\).

**Problem 10.9**: Consider \(n\) tosses with a biased die, for which the probabilities of \(1,\ldots,6\) points are given by

\[\begin{array}{ccccccc}1&2&3&4&5&6\\ \hline\frac{1-\theta}{12}&\frac{2-\theta}{12}&\frac{3-\theta}{12}&\frac{1+ \theta}{12}&\frac{2+\theta}{12}&\frac{3+\theta}{12}\end{array}\]

and let \(X_{i}\) be the number of tosses showing \(i\) points.

1. Show that the triple \(Z_{1}=X_{1}+X_{5}\), \(Z_{2}=X_{2}+X_{4}\), \(Z_{3}=X_{3}+X_{6}\) is a maximal ancillary; determine its distribution and the distribution of \(X_{1},\ldots,X_{6}\) given \(Z_{1}=z_{1}\), \(Z_{2}=z_{2}\), \(Z_{3}=z_{3}\).
2. Exhibit five other maximal ancillaries. [Basu (1964).]

**Problem 10.10**: In the preceding problem, suppose the probabilities are given by

\[\begin{array}{ccccccc}1&2&3&4&5&6\\ \hline\frac{1-\theta}{6}&\frac{1-2\theta}{6}&\frac{1-3\theta}{6}&\frac{1+\theta }{6}&\frac{1+2\theta}{6}&\frac{1+3\theta}{6}\end{array}\]

Exhibit two different maximal ancillaries.

**Problem 10.11**: Let \(X\) be uniformly distributed on \((\theta,\theta+1)\), \(0<\theta<\infty\), let \([X]\) denote the largest integer \(\leq X\), and let \(V=X-[X]\).

1. The statistic \(V(X)\) is uniformly distributed on \((0,1)\) and is therefore ancillary.
2. The marginal distribution of \([X]\) is given by \[[X]=\left\{\begin{array}{ll}[\theta]&\mbox{with probability $1-V(\theta)$},\\ [\theta]+1&\mbox{with probability $V(\theta)$}.\end{array}\right.\]
3. Conditionally, given that \(V=v\), \([X]\) assigns probability \(1\) to the value \([\theta]\) if \(V(\theta)\leq v\) and to the value \([\theta]+1\) if \(V(\theta)>v\). [Basu (1964).]

**Problem 10.12**: Let \(X\), \(Y\) have joint density

\[p(x,y)=2f(x)f(y)F(\theta xy),\]

where \(f\) is a known probability density symmetric about \(0\), and \(F\) its cumulative distribution function. Then1. \(p(x,\,y)\) is a probability density.
2. \(X\) and \(Y\) each have marginal density \(f\) and are therefore ancillary, but \((X,\,Y)\) is not.
3. \(X\cdot Y\) is a sufficient statistic for \(\theta\). [Dawid (1977).]

**Problem 10.13**: A sample of size \(n\) is drawn with replacement from a population consisting of \(N\) distinct unknown values \(\{a_{1},\ldots,a_{N}\}\). The number of distinct values in the sample is ancillary.

**Problem 10.14**: Assuming the distribution (4.22) of Section 4.9, show that \(Z\) is \(S\)-ancillary for \(p=p_{+}/(p_{+}+p_{-})\).

**Problem 10.15**: In the situation of Example 10.2.3, \(X+Y\) is binomial if and only if \(\Delta=1\).

**Problem 10.16**: In the situation of Example 10.2.2, the statistic \(Z\) remains \(S\)-ancillary when the parameter space is \(\Omega=\{(\lambda,\,\mu):\mu\leq\lambda\}\).

**Problem 10.17**: Suppose \(X=(U,\,Z)\), the density of \(X\) factors into

\[p_{\theta,\vartheta}(x)=c(\theta,\vartheta)g_{\vartheta}(u;z)h_{\vartheta}(z) k(u,\,z),\]

and the parameters \(\theta\), \(\vartheta\) are unrelated. To see that these assumptions are not enough to insure that \(Z\) is \(S\)-ancillary for \(\theta\), consider the joint density

\[C(\theta,\,\vartheta)e^{-\frac{1}{2}(u-\theta)^{2}-\frac{1}{2}(z-\vartheta)^{2 }}I(u,\,z),\]

where \(I(u,\,z)\) is the indicator of the set \(\{(u,\,z):u\leq z\}\). [Basu (1978).]

### _Section 10.3_

**Problem 10.18**: Verify the density (10.16) of Example 10.3.2.

**Problem 10.19**: Let the real-valued function \(f\) be defined on an open interval.

1. If \(f\) is logconvex, it is convex.
2. If \(f\) is strongly unimodal, it is unimodal.

**Problem 10.20**: Let \(X_{1},\,\ldots,\,X_{m}\) and \(Y_{1},\,\ldots,\,Y_{n}\) be positive, independent random variables distributed with densities \(f(x/\sigma)\) and \(g(y/\tau)\), respectively. If \(f\) and \(g\) have monotone likelihood ratios in \((x,\,\sigma)\) and \((y,\,\tau)\), respectively, there exists a UMP conditional test of \(H:\tau/\sigma\leq\Delta_{0}\) against \(\tau/\sigma>\Delta_{0}\) given the ancillary statistics \(U_{i}=X_{i}/X_{m}\) and \(V_{j}=Y_{j}/Y_{n}\) (\(i=1,\,\ldots,\,m-1\); \(j=1,\,\ldots,\,n-1\)).

**Problem 10.21**: Let \(V_{1},\,\ldots,\,V_{n}\) be independently distributed as \(N(0,\,1)\), and given \(V_{1}=v_{1},\,\ldots,\)

\(V_{n}=v_{n}\), let \(X_{i}\) (\(i=1,\,\ldots,\,n\)) be independently distributed as \(N(\theta v_{i},\,1)\).

10.5 Problems

1. There does not exist a UMP test of \(H:\theta=0\) against \(K:\theta>0\).
2. There does exist a UMP conditional test of \(H\) against \(K\) given the ancillary (\(V_{1},\ldots,V_{n}\)). [Buehler (1982).]

**Problem 10.22**: In Example 10.3.3,

1. the problem remains invariant under \(G^{\prime}\) but not under \(G\);
2. the statistic \(D\) is ancillary.

_Section 10.4_

**Problem 10.23**: In Example 10.4.1, check directly that the set \(C=\{x:x\leq-k\text{ or }x\geq k\}\) is not a negatively biased semirelevant subset for the confidence intervals (\(X-c\), \(X+c\)).

**Problem 10.24**: (i) Verify the posterior distribution of \(\Theta\) given \(x\) claimed in Example 10.4.1.
2. Complete the proof of (10.32).

**Problem 10.25**: Let \(X\) be a random variable with cumulative distribution function \(F\). If \(E\left|X\right|<\infty\), then \(\int_{-\infty}^{0}F(x)\,dx\) and \(\int_{0}^{\infty}[1-F(x)]\,dx\) are both finite. [Apply integration by parts to the two integrals.]

**Problem 10.26**: Let \(X\) have probability density \(f(x-\theta)\), and suppose that \(E\left|X\right|<\infty\). For the confidence intervals \(X-c<\theta\) there exist semirelevant but no relevant subsets. [Buehler (1959).]

**Problem 10.27**: Let \(X_{1},\ldots,X_{n}\) be independently distributed according to the uniform distribution \(U(\theta,\theta+1)\).

1. Uniformly most accurate lower confidence bounds \(\underline{\theta}\) for \(\theta\) at confidence level \(1-\alpha\) exist and are given by \[\underline{\theta}=\max(X_{(1)}-k,X_{(n)}-1),\] where \(X_{(1)}=\min(X_{1},\ldots,X_{n})\), \(X_{(n)}=\max(X_{1},\ldots,X_{n})\), and \((1-k)^{n}=\alpha\).
2. The set \(C:x_{(n)}-x_{(1)}\geq 1-k\) is a relevant subset with \(P_{\theta}(\underline{\theta}\leq\theta\mid C)=1\) for all \(\theta\).
3. Determine the uniformly most accurate conditional lower confidence bounds \(\underline{\theta}(v)\) given the ancillary statistic \(V=X_{(n)}-X_{(1)}=v\), and compare them with \(\underline{\theta}\). [The conditional distribution of \(Y=X_{(1)}\) given \(V=v\) is \(U(\theta,\theta+1-v)\).]

[Pratt (1961a), Barnard (1976).]

**Problem 10.28**:
1. Under the assumptions of the preceding problem, the uniformly most accurate unbiased (or invariant) confidence intervals for \(\theta\) at confidence level \(1-\alpha\) are \[\underline{\theta}=\max(X_{(1)}+d,\,X_{(n)})-1<\theta<\min(X_{(1)},\,X_{(n)}-d)= \bar{\theta},\] where \(d\) is the solution of the equation \[\begin{array}{rll}2d^{n}=\alpha&\mbox{if}&\alpha<1/2^{n-1},\\ 2d^{n}-(2d-1)^{n}=\alpha&\mbox{if}&\alpha>1/2^{n-1}.\end{array}\]
2. The sets \(C_{1}:X_{(n)}-X_{(1)}>d\) and \(C_{2}:X_{(n)}-X_{(1)}<2d-1\) are relevant subsets with coverage probability \[P_{\theta}[\underline{\theta}<\theta<\bar{\theta}\mid C_{1}]=1\;\;\mbox{ and}\;\;\;P_{\theta}[\underline{\theta}<\theta<\bar{\theta}\mid C_{2}]=0.\]
3. Determine the uniformly most accurate unbiased (or invariant) conditional confidence intervals \(\underline{\theta}(v)<\theta<\bar{\theta}(v)\) given \(V=v\) at confidence level \(1-\alpha\), and compare \(\underline{\theta}(v)\), \(\bar{\theta}(v)\), and \(\bar{\theta}(v)-\underline{\theta}(v)\) with the corresponding unconditional quantities. [Welch (1939), Pratt (1961a), Kiefer (1977a).]

**Problem 10.29**: Suppose \(X_{1}\) and \(X_{2}\) are i.i.d. with

\[P\{X_{i}=\theta-1\}=P\{X_{i}=\theta+1\}=\frac{1}{2}\;.\]

Let \(C\) be the confidence set consisting of the single point \((X_{1}+X_{2})/2\) if \(X_{1}\neq X_{2}\) and \(X_{1}-1\) if \(X_{1}=X_{2}\). Show that, for all \(\theta\),

\[P_{\theta}\{\theta\in C\}=0.75\;,\]

but

\[P_{\theta}\{\theta\in C|X_{1}=X_{2}\}=0.5\]

and

\[P_{\theta}\{\theta\in C|X_{1}\neq X_{2}\}=1\;.\]

[Berger and Wolpert (1988)]

**Problem 10.30**: Instead of conditioning the confidence sets \(\theta\in S(X)\) on a set \(C\), consider a randomized procedure which assigns to each point \(x\) a probability \(\psi(x)\) and makes the confidence statement \(\theta\in S(x)\) with probability \(\psi(x)\) when \(x\) is observed.71. The randomized procedure can be represented by a nonrandomized conditioning set for the observations (\(X\), \(U\)), where \(U\) is uniformly distributed on (0, 1) and independent of \(X\), by letting \(C=\{(x,u):u<\psi(x)\}\).
2. Extend the definition of relevant and semirelevant subsets to randomized conditioning (without the use of \(U\)).
3. Let \(\theta\in S(X)\) be equivalent to the statement \(X\in A(\theta)\). Show that \(\psi\) is positively biased semirelevant if and only if the random variables \(\psi(X)\) and \(I_{A(\theta)}(X)\) are positively correlated, where \(I_{A}\) denotes the indicator of the set \(A\).

**Problem 10.31**: The nonexistence of (i) semirelevant subsets in Example 10.4.1 and (ii) relevant subsets in Example 10.4.2 extends to randomized conditioning procedures.

### Notes

Conditioning on ancillary statistics was introduced by Fisher (1934a, 1935b, 1936).8 The idea was emphasized in Fisher (1956b,1959,1973) and by Cox (1958), who motivated it in terms of mixtures of experiments providing different amounts of information. The consequences of adopting a general principle of conditioning in mixture situations were explored by Birnbaum (1962) and Durbin (1970). Following Fisher's suggestion (1934), Pitman (1938b) developed a theory of conditional tests and confidence intervals for location and scale parameters. For recent paradox concerning conditioning on an ancillary statistic, see Brown (1990) and Wang (1999).

Footnote 8: Fisher’s contributions to this topic are discussed in Savage (1976, pp. 467–469).

The possibility of relevant subsets was pointed out by Fisher (1956a, 1956b,1959,1973) (who called them _recognizable_). Its implications (in terms of betting procedures) were developed by Buehler (1959), who in particular introduced the distinction between relevant and semirelevant, positively and negatively biased subsets, and proved the nonexistence of relevant subsets in location models. The role of relevant subsets in statistical inference, and their relationship to Bayes and admissibility properties, was discussed by Pierce (1973), Robinson (1976, 1979a, 1979b), Bondar (1977), and Casella (1988), among others.

Fisher (1956a, 1956b,1959,1973) introduced the idea of relevant subsets in the context of the Behrens-Fisher problem. As a criticism of the Welch-Aspin solution, he established the existence of negatively biased relevant subsets for that procedure. It was later shown by Robinson (1976) that no such subsets exist for Fisher's preferred solution, the so-called Behrens-Fisher intervals. This fact may be related to the conjecture [supported by substantial numerical evidence in Robinson (1976) but so far unproved] that the unconditional coverage probability of the Behrens-Fisher intervals always exceeds the nominal level. For a review of these issues, see Wallace (1980) and Robinson (1982).

Maatta and Casella (1987) examine the conditional properties of some confidence intervals for the variance in the one-sample normal problem. The conditional properties of some confidence sets for the multivariate normal mean, including confidence sets centered at James-Stein or shrinkage estimators, see Casella (1987) and George and Casella (1994). The conditional properties of the standard confidence sets in a normal linear model are studied in Hwang and Brown (1991).

In testing a simple hypothesis against a simple alternative, Berger et al. (1994) present a conditional frequentist methodology that agrees with a Bayesian approach.

## Chapter 4 Asymptotic Theory

## Chapter 11 Basic Large-Sample Theory

### 11.1 Introduction

Chapters 3, 4, 5, 6, and 7 were concerned with the derivation of UMP, UMP unbiased, and UMP invariant tests. Unfortunately, the existence of such tests turned out to be restricted essentially to one-parameter families with monotone likelihood ratio, exponential families, and group families, respectively. Tests maximizing the minimum or average power over suitable classes of alternatives exist fairly generally, but are difficult to determine explicitly, and their derivation in Chapter 8 was confined primarily to situations in which invariance considerations apply.

Despite their limitations, these approaches have proved their value by application to large classes of important situations. On the other hand, they are unlikely to be applicable to complex new problems. What is needed for such cases is a simpler, less detailed, more generally applicable formulation. The development and implementation of such an approach will be the subject of the remaining chapters. It replaces optimality by asymptotic optimality obtained by embedding the actual situation in a sequence of situations of increasing sample size, and applying optimality to the limit situation. These limits tend to be of a simple type for which optimality has been established in earlier chapters.

A feature of asymptotic optimality is that it refers not to a single test but to a sequence of tests, although this distinction will often be suppressed. An important consequence is that asymptotically optimal procedures--unlike most optimal procedures in the small-sample approach--are not unique since many different sequences have the same limit. In fact, quite different methods of construction may lead to procedures which are asymptotically optimal.

The following are some specific examples to keep in mind where finite-sample considerations fail to provide optimal procedures, but for which a large-sample approach will be more successful.

**Example 11.1.1**: **(One-parameter families)** Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. according to some family of distributions \(P_{\theta}\) indexed by a real-valued parameter \(\theta\). Then, it was mentioned after Corollary 3.4.1 that UMP tests for testing \(\theta=\theta_{0}\) against \(\theta>\theta_{0}\)exist for all sample sizes (under weak regularity conditions) only when the distributions \(P_{\theta}\) constitute an exponential family. For example, location models typically do not have a monotone likelihood ratio, and so UMP tests rarely exist in this situation, though the normal location model is a happy exception. On the other hand, we shall see that under weak assumptions, there generally exist tests for one-parameter families which are asymptotically UMP in a suitable sense; see Section 15.3. For example, we shall derive an asymptotically optimal one-sided test in the Cauchy location model, among others.

##### (Behrens-Fisher Problem)

Consider testing the equality of means for two independent samples, from normal distributions with possibly different (unknown) variances. As previously mentioned, finite-sample optimality considerations such as unbiasedness or invariance do not lead to an optimal test, even though the setting is a multiparameter exponential family. An optimal test sequence will be derived in Example 15.5.4.

##### (The Chi-squared Test)

Consider \(n\) multinomial trials with \(k+1\) possible outcomes, labeled \(1\) to \(k+1\). Suppose \(p_{j}\) denotes the probability of a result in the \(j\)th category. Let \(Y_{j}\) denote the number of trials resulting in category \(j\), so that (\(Y_{1}\),..., \(Y_{k+1}\)) has the multinomial distribution with joint density obtained in Example 2.7.2. Suppose the null hypothesis is that \(p=\pi=(\pi_{1},\ldots,\pi_{k+1})\). The alternative hypothesis is unrestricted and includes all \(p\neq\pi\) (with \(\sum_{j=1}^{k+1}p_{j}=1\)). The class of alternatives is too large for a UMP test to exist, nor do unbiasedness or invariance considerations rescue the problem. The usual Chi-squared test, which is based on the test statistic \(Q_{n}\) given by

\[Q_{n}=\sum_{j=1}^{k+1}\frac{(Y_{j}-n\pi_{j})^{2}}{n\pi_{j}}\, \tag{11.1}\]

will be seen to possess an asymptotic maximin property; see Section 16.3.

##### (Nonparametric Mean)

Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. from a distribution \(F\) with finite mean \(\mu\) and finite variance. The problem is to test \(\mu=0\). Except when \(F\) is assumed to belong to a number of simple parametric families, optimal tests for the mean rarely exist. Moreover, if we assume only a second moment, it is impossible to construct reasonable tests that are of a given size (Theorem 13.4.4). But, by making a weak restriction on the family, we will see that it is possible to construct tests that are approximately level \(\alpha\) and that in addition possess an asymptotic maximin property; see Sections 13.4 and 15.6.

In the remaining chapters, we shall consider hypothesis testing and estimation by confidence sets from a large-sample or asymptotic point of view. In this approach, exact results are replaced by approximate ones that have the advantage of both greater simplicity and generality. But, the large-sample approach is not just restricted to situations where no finite-sample optimality approach works. As the following example shows, limit theorems often provide an easy way to approximate the critical value and power of a test (whether it has any optimality properties or not).

**Example 11.1.5**: **(Simple versus Simple)** Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. with common distribution \(P\). The problem is to test the simple null hypothesis \(P=P_{0}\) versus the simple alternative \(P=P_{1}\). Let \(p_{i}\) denote the density of \(P_{i}\) with respect to a measure \(\mu\). By the Neyman-Pearson Lemma, the optimal test rejects for large values of \(\sum_{i=1}^{n}\log[p_{1}(X_{i})/p_{0}(X_{i})]\). The exact null distribution of this test statistic may be difficult to obtain since, in general, an \(n\)-fold integration is required. On the other hand, since the statistic takes the simple form of a sum of i.i.d. variables, large-sample approximations to the critical value and power are easily obtained from the Central Limit Theorem (Theorem 11.2.4).

Another application of the large-sample approach (discussed in Section 13.2) is the study of the robustness of tests when the assumptions under which they are derived do not hold. Here, asymptotic considerations have been found to be indispensable. The problem is just too complicated for the more detailed small-sample methods to provide an adequate picture. In general, two distinct types of robustness considerations arise, which may be termed robustness of validity and robustness of efficiency; this distinction has been pointed out by Tukey and McLaughlin (1963), Box and Tiao (1964), and Mosteller and Tukey (1977). For robustness of validity, the issue is whether a level \(\alpha\) test retains its level and power if the parameter space is enlarged to include a wider class of distributions. For example, in testing whether the mean of a normal population is zero, we may wish to consider the validity of a test without assuming normality. However, even when a test possesses a robustness of validity, are its optimality properties preserved when the parameter space is enlarged? This question is one of robustness of efficiency (or inference robustness). In the context of the one-sample normal location model, for example, one would study the behavior of procedures (such as a one-sample \(t\)-test) when the underlying distribution has thicker tails than the normal, or perhaps when the observations are not assumed independent. Large-sample theory offers valuable insights into these issues, as will be seen in Section 13.2.

When neither finite-sample nor large-sample optimal procedures exist for a given problem, it becomes important to determine procedures which have at least reasonable performance characteristics. Large-sample considerations often lead to suitable definitions and methods of construction. An example of this nature that will be treated later is the problem of testing whether an i.i.d. sample is uniformly distributed or, more generally, of goodness of fit.

As the starting point of a large-sample theory of inference, we now define asymptotic analogs of the concepts of size, level of significance, confidence coefficient, and confidence level. Suppose that data \(X^{(n)}\) comes from a model indexed by a parameter \(\theta\in\Omega\). Typically, \(X^{(n)}\) refers to an i.i.d. sample of \(n\) observations, and an asymptotic approach assumes that \(n\to\infty\). Of course, two-sample problems can be considered in this setup, as well as more complex data structures. Nothing is assumed about the family \(\Omega\), so that the problem may be parametric or nonparametric. First, consider testing a null hypothesis \(H\) that \(\theta\in\Omega_{H}\) versus the alternative hypothesis \(K\) that \(\theta\in\Omega_{K}\), where \(\Omega_{H}\) and \(\Omega_{K}\) are two mutually exclusive subsets of \(\Omega\). We will be studying sequences of tests \(\phi_{n}(X^{(n)})\).

**Definition 11.1.1**: For a given level \(\alpha\), a sequence of tests \(\{\phi_{n}\}\) is _pointwise asymptotically level_\(\alpha\) if, for any \(\theta\in\Omega_{H}\),

\[\limsup_{n\to\infty}E_{\theta}[\phi_{n}(X^{(n)})]\leq\alpha. \tag{11.2}\]

Condition (11.2) guarantees that for any \(\theta\in\Omega_{H}\) and any \(\epsilon>0\), the level of the test will be less than or equal to \(\alpha+\epsilon\) when \(n\) is sufficiently large. However, the condition does not guarantee the existence of an \(n_{0}\) (independent of \(\theta\)) such that

\[E_{\theta}[\phi_{n}(X^{(n)})]\leq\alpha+\epsilon\]

for all \(\theta\in\Omega_{H}\) and all \(n\geq n_{0}\). We can therefore not guarantee the behavior of the size

\[\sup_{\theta\in\Omega_{H}}E_{\theta}[\phi_{n}(X^{(n)})]\]

of the test, no matter how large \(n\) is.

**Example 11.1.6**: **(Uniform versus Pointwise Convergence)** To illustrate the above point, consider the function

\[f_{n}(\theta)=\alpha+(1-\alpha)\exp(-n/\theta)\,\]

defined for positive integers \(n\) and \(\theta>0\). Then, for any \(\theta>0\), \(f_{n}(\theta)\to\alpha\) as \(n\to\infty\); that is, \(f_{n}(\theta)\) converges to \(\alpha\) pointwise in \(\theta\). However, this convergence is not uniform in \(\theta\) because

\[\sup_{\theta>0}f_{n}(\theta)=\alpha+(1-\alpha)\sup_{\theta>0}\exp(-n/\theta)=1\.\]

To cast this example in the context of hypothesis testing, assume \(X_{1},\ldots,X_{n}\) are i.i.d. with the exponential distribution function

\[F_{\theta}(t)=P_{\theta}\{X_{i}\leq t\}=1-\exp(-t/\theta)\.\]

Define

\[\phi_{n}(X_{1},\ldots,X_{n})=\alpha+(1-\alpha)I\{\min(X_{1},\ldots,X_{n})>1\}\.\]

Here and throughout, the notation \(I\{E\}\) denotes an _indicator_ random variable that is 1 if the event \(E\) occurs and is 0 otherwise. Then, \(E_{\theta}[\phi_{n}(X_{1},\ldots,X_{n})]=f_{n}(\theta)\). Hence, if \(\Omega_{H}\) is the positive real line, the test sequence \(\phi_{n}\) satisfies (11.2), but its size is 1 for every \(n\).

In order to guarantee the behavior of the limiting size of a test sequence, we require the following stronger condition.

**Definition 11.1.2**: The sequence \(\{\phi_{n}\}\) is _uniformly asymptotically level \(\alpha\)_ if

\[\limsup_{n\to\infty}\sup_{\theta\in\Omega_{H}}E_{\theta}[\phi_{n}(X^{(n)})]\leq \alpha. \tag{11.3}\]

If instead of (11.3), the sequence \(\{\phi_{n}\}\) satisfies

\[\lim_{n\to\infty}\sup_{\theta\in\Omega_{H}}E_{\theta}[\phi_{n}(X^{(n)})]=\alpha\, \tag{11.4}\]

then this value of \(\alpha\) is called the limiting size of \(\{\phi_{n}\}\).

Of course, we will also study the behavior of tests under the alternative hypothesis. The following is a weak condition that we expect reasonable tests to satisfy.

**Definition 11.1.3**: The sequence \(\{\phi_{n}\}\) is _pointwise consistent in power_ if, for any \(\theta\) in \(\Omega_{K}\),

\[E_{\theta}[\phi_{n}(X^{(n)})]\to 1 \tag{11.5}\]

as \(n\to\infty\).

**Example 11.1.7**: **(One-parameter families, Example 11.1.1, continued)** Let \(T_{n}=T_{n}(X_{1},\,\ldots,\,X_{n})\) be a sequence of statistics, with distributions depending on a real-valued parameter \(\theta\). For testing \(H:\ \theta=\theta_{0}\) against \(K:\ \theta>\theta_{0}\), consider the tests \(\phi_{n}\) that reject \(H\) when \(T_{n}\geq C_{n}\). In many applications, it will turn out that, when \(\theta=\theta_{0}\), \(n^{1/2}(T_{n}-\theta_{0})\) has a limiting normal distribution with mean 0 and variance \(\tau^{2}(\theta_{0})\) in the sense that, for any real number \(t\),

\[P_{\theta_{0}}\{n^{1/2}(T_{n}-\theta_{0})\leq t\}\to\Phi(t/\tau(\theta_{0}))\, \tag{11.6}\]

where \(\Phi(\cdot)\) is the standard normal c.d.f. Let \(z_{\alpha}\) satisfy \(\Phi(z_{\alpha})=\alpha\). Then, the test with

\[C_{n}=\theta_{0}+\frac{\tau(\theta_{0})}{n^{1/2}}z_{1-\alpha}\]

has limiting size \(\alpha\), since

\[P_{\theta_{0}}\{T_{n}\geq\theta_{0}+\frac{\tau(\theta_{0})}{n^{1/2}}z_{1- \alpha}\}\to\alpha\ \.\]

Consider next the power of \(\phi_{n}\) under the assumption that not only (11.6) holds, but that it remains valid when \(\theta_{0}\) is replaced by any \(\theta>\theta_{0}\). Then, the power of \(\phi_{n}\) against \(\theta\) is

\[\beta_{n}(\theta)=P_{\theta}\{n^{1/2}(T_{n}-\theta)\geq z_{1-\alpha}\tau( \theta_{0})-n^{1/2}(\theta-\theta_{0})\}\]and hence \(\beta_{n}(\theta)\to 1\) for any \(\theta>\theta_{0}\), so that the test sequence is pointwise consistent in power.

Similar definitions apply to the construction of confidence sets. Let \(g=g(\theta)\) be the parameter function of interest, for some mapping \(g\) from \(\Omega\) to some space \(\Omega_{g}\). Let \(S_{n}=S_{n}(X^{(n)})\in\Omega_{g}\) denote a sequence of confidence sets for \(g(\theta)\).

**Definition 11.1.4**.: A sequence of confidence sets \(S_{n}\) is _pointwise asymptotically level_\(1-\alpha\) if, for any \(\theta\in\Omega\),

\[\liminf_{n\to\infty}P_{\theta}\{g(\theta)\in S_{n}(X^{(n)})\}\geq 1-\alpha. \tag{11.7}\]

The sequence \(\{S_{n}\}\) is uniformly asymptotically level \(1-\alpha\) if

\[\liminf_{n\to\infty}\inf_{\theta\in\Omega}P_{\theta}\{g(\theta)\in S_{n}(X^{(n )})\}\geq 1-\alpha. \tag{11.8}\]

If the \(\liminf\) in the left-hand side of (11.8) can be replaced by a \(\lim\), then the left-hand side is called the limiting confidence coefficient for \(\{S_{n}\}\).

Most of the asymptotic theory we shall consider is local in a sense that we now briefly describe. In the hypothesis testing context, any reasonable test sequence \(\phi_{n}\) is pointwise consistent in power. However, any actual situation has finite sample size \(n\) and its power against any fixed alternative is typically less than one. In order to obtain a meaningful assessment of power, one therefore considers sequences of alternatives \(\theta_{n}\) tending to \(\Omega_{H}\) at a suitable rate, so that the limiting power of \(\phi_{n}\) against \(\theta_{n}\) is less than one. (See Example 11.3.2 for a simple example of such a local approach.)

An alternative to the local approach is to consider the rate at which the power tends to one against a fixed alternative. Although there exists a large literature on this approach based on large-deviation theory, the resulting approximations tend to be less accurate and we shall not treat this topic here.

It is also important to mention that asymptotic results may provide poor approximations to the actual finite-sample setting. Furthermore, convergence to a limit as \(n\to\infty\) certainly does not guarantee that the approximation will improve with increasing \(n\); an example is provided by Hodges (1957). Any asymptotic result should therefore be accompanied by an investigation of its reliability for finite sample sizes. Such checks can be carried out by simulations studies or higher-order asymptotic analysis.

The concepts and definitions presented in this introduction will be explored more fully in the remaining chapters. First, we need techniques to be able to approximate significance levels, power functions, and confidence coefficients. To this end, the rest of this chapter is devoted to useful results from the theory of weak convergence and other convergence concepts.

### Weak Convergence and Central Limit Theorems

In this section, the basic notation, definitions, and results from the theory of weak convergence are introduced. The main theorems will be presented without proof, but we will provide illustrations of their use. For a more complete background, the reader is referred to Pollard (1984), Dudley (1989), or Billingsley (1995).

Let \(X\) denote a \(k\times 1\) random vector (which is just a vector-valued random variable), so that the \(i\)th component \(X_{i}\) of \(X\) is a real-valued random variable. Then, \(X^{\top}=(X_{1},\ldots,X_{k})\). The (multivariate) cumulative distribution function (c.d.f.) of \(X\) is defined to be:

\[F_{X}(x_{1},\ldots,x_{k})=P\{X_{1}\leq x_{1},\ldots,X_{k}\leq x_{k}\}\.\]

Here, the probability \(P\) refers to the probability on whatever space \(X\) is defined. A point \(x^{\top}=(x_{1},\ldots,x_{k})\) at which the c.d.f. \(F_{X}(\cdot)\) is continuous is called a _continuity point_ of \(F_{X}\). Alternatively, \(x\) is a continuity point of \(F_{X}\) if the boundary of the set of \((y_{1},\ldots,y_{k})\) such that \(y_{i}\leq x_{i}\) for all \(i\) has probability \(0\) under the distribution of \(X\).1 As an example, the multivariate normal distribution was first studied in Section 3.9.2.

Footnote 1: In general, the _boundary_ of a set \(E\) in \({\rm I\kern-1.8ptR}^{k}\), denoted by \(\partial E\) is defined as follows. The closure of \(E\), denoted by \(\tilde{E}\), is the set of \(x\in{\rm I\kern-1.8ptR}^{k}\) for which there exists a sequence \(x_{n}\in E\) with \(x_{n}\to x\). The set \(E\) is _closed_ if \(E=\tilde{E}\). The _interior_ of \(E\), denoted by \(E^{\circ}\), is the set of \(x\) such that, for some \(\epsilon>0\), the _Euclidean ball_ with center \(x\) and radius \(\epsilon\), defined by \(\{y\in{\rm I\kern-1.8ptR}^{k}:\ |y-x|<\epsilon\}\), is contained in \(E\). Here \(|\cdot|\) denotes the usual Euclidean norm. The set \(E\) is _open_ if \(E=E^{\circ}\). If \(E^{c}\) denotes the complement of a set \(E\), then evidently \(E^{\circ}\) is the complement of the closure of \(E^{c}\), and so \(E\) is open if and only if \(E^{c}\) is closed. The boundary \(\partial E\) of a set \(E\) is then defined to be \(\tilde{E}-E^{\circ}=\tilde{E}\cap(E^{\circ})^{c}\).

**Definition 11.2.1** A sequence of random vectors \(\{X_{n}\}\) with c.d.f.s \(\{F_{X_{n}}(\cdot)\}\) is said to _converge in distribution_ (or _in law_) to a random vector \(X\) with c.d.f. \(F_{X}(\cdot)\) if

\[F_{X_{n}}(x_{1},\ldots,x_{k})\to F_{X}(x_{1},\ldots,x_{k})\]

at all continuity points \(x^{\top}=(x_{1},\ldots,x_{k})\) of \(F_{X}(\cdot)\). This convergence will also be denoted \(X_{n}\stackrel{{ d}}{{\to}}X\). Because it really only has to do with the laws of the random variables (and not with the random variables themselves), we may also equivalently say \(F_{X_{n}}\) converges weakly to \(F_{X}\), written \(F_{X_{n}}\stackrel{{ d}}{{\to}}F_{X}\).2

Footnote 2: The term _weak convergence_ (also sometimes called weak star convergence) distinguishes this type of convergence from stronger convergence concepts to be discussed later. However, the term is used because it is a special case of convergence in the weak star topology for elements in a Banach space (such as the space of signed measures on \({\rm I\kern-1.8ptR}^{k}\)), though we will make no direct use of any such topological notions.

The limiting random vector \(X\) plays an auxiliary role, since any random variable with the same distribution would serve the same purpose. Therefore, the notation will sometimes be abused so that we also say \(X_{n}\) converges in distribution to the c.d.f. \(F\), written \(X_{n}\stackrel{{ d}}{{\to}}F\).

There are many equivalent characterizations of weak convergence, some of which are recorded in the next theorem.

Theorem 11.2.1 (Portmanteau Theorem): _Suppose \(X_{n}\) and \(X\) are random vectors in \(\mathbb{R}^{k}\). The following are equivalent:_

1. \(X_{n}\stackrel{{ d}}{{\to}}X\)_._
2. \(Ef(X_{n})\to Ef(X)\) _for all bounded, continuous real-valued functions_ \(f\)_._
3. _For any open set_ \(O\) _in_ \(\mathbb{R}^{k}\)_,_ \(\liminf\,P(X_{n}\in O)\geq P(X\in O)\)_._
4. _For any closed set_ \(G\) _in_ \(\mathbb{R}^{k}\)_,_ \(\limsup\,P(X_{n}\in G)\leq P(X\in G)\)_._
5. _For any set_ \(E\) _in_ \(\mathbb{R}^{k}\) _for which_ \(\partial E\)_, the boundary of_ \(E\)_, satisfies_ \(P(X\in\partial E)=0\)_,_ \(P(X_{n}\in E)\to P(X\in E)\)_._
6. \(\liminf\,\ Ef(X_{n})\geq Ef(X)\) _for any nonnegative continuous_ \(f\)_._

Another equivalent characterization of weak convergence is based on the notion of the characteristic function of a random vector.

Definition Definition 11.2.2: The _characteristic function_ of a random vector \(X\) (taking values in \(\mathbb{R}^{k}\)) is the function \(\zeta_{X}(\cdot)\) from \(\mathbb{R}^{k}\) to the complex plane given by

\[\zeta_{X}(t)=E(e^{i(t,X)}).\]

In the definition, \(\langle t,\,X\rangle\) refers to the usual inner product, so that

\[\langle t,\,X\rangle=t^{T}X=\sum_{j=1}^{k}t_{j}X_{j}.\]

Two important properties of characteristic functions are the following. First, the distribution of \(X\) is uniquely determined by its characteristic function. Second, the characteristic function of a sum of independent real-valued random variables is the product of the individual characteristic functions (Problem 11.7).

Example 11.2.1 (Multivariate Normal Distribution): Suppose a random vector \(X^{\top}=(X_{1},\,\ldots,\,X_{k})\) is \(N(\mu,\,\Sigma)\), the multivariate normal distribution with mean vector \(\mu^{\top}=(\mu_{1},\,\ldots,\,\mu_{k})\) and covariance matrix \(\Sigma\). In the case \(k=1\), if \(X\) is normally distributed with mean \(\mu\) and variance \(\sigma^{2}\), its characteristic function is:

\[E(e^{itX})=\int_{-\infty}^{\infty}e^{itx}\frac{1}{\sqrt{2\pi}\sigma}e^{[-(x- \mu)^{2}/2\sigma^{2}]}dx=\exp(it\mu-\frac{1}{2}\sigma^{2}t^{2})\, \tag{11.9}\]

which can be verified by a simple integration (Problem 11.8). To obtain the characteristic function for \(k>1\), note that

\[\zeta_{X}(t)=E(e^{i(t,X)})\]

is the characteristic function

\[\zeta_{(t,X)}(\lambda)=E(e^{\lambda i(t,X)})\]

[MISSING_PAGE_EMPTY:2142]

For most applications, _Lindeberg's condition_ (11.11) can be verified by _Lyapunov's Condition_, which says that, for some \(\delta>0\), \(|X_{n,i}|^{2+\delta}\) are integrable and

\[\lim_{n\to\infty}\sum_{i=1}^{r_{n}}\frac{1}{s_{n}^{2+\delta}}E[|X_{n,i}|^{2+ \delta}]=0. \tag{11.12}\]

Indeed, (11.12) implies (11.11) (Problem 11.11), and the result may be stated as follows.

Corollary 11.2.1 (Lyapounov Central Limit Theorem): _Suppose, for each \(n\), \(X_{n,1},\ldots,\)\(X_{n,r_{n}}\) are independent. Assume \(E(X_{n,i})=0\) and \(\sigma_{n,i}^{2}=E(X_{n,i}^{2})<\infty\). Let \(s_{n}^{2}=\sum_{i=1}^{r_{n}}\sigma_{n,i}^{2}\). Suppose, for some \(\delta>0\), (11.12) holds. Then,_

\[\sum_{i=1}^{r_{n}}X_{n,i}/s_{n}\stackrel{{ d}}{{\to}}N(0,1).\]

**Example 11.2.2** (Uniformly Bounded \(2+\delta\) Moments): Suppose, for each \(n\), \(X_{n,1},\ldots,\)\(X_{n,n}\) are independent. Assume \(E(X_{n,i})=0\) and, for some \(\delta>0\),

\[\sup_{n,i}E(|X_{n,i}|^{2+\delta})<\infty\.\]

Let \(\sigma_{n,i}^{2}=E(X_{n,i}^{2})\) and set

\[\bar{\sigma}_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\sigma_{n,i}^{2}\.\]

Assume \(\bar{\sigma}_{n}^{2}\to\sigma_{\infty}^{2}<\infty\). Then, Lyapounov's Condition (11.12) holds (Problem 11.12) and, letting \(\bar{X}_{n}=\sum_{i=1}^{n}X_{n,i}/n\), we have

\[\sqrt{n}\bar{X}_{n}\stackrel{{ d}}{{\to}}N(0,\sigma_{\infty}^{2})\.\]

The result holds even if \(\sigma_{\infty}^{2}=0\) with the interpretation that \(N(0,0)\) is the distribution that is point mass at zero.

There also exists a partial converse to Lindeberg's Central Limit Theorem, due to Feller and Levy. (See Billingsley (1995), p. 574.)

Theorem 11.2.6: _Suppose, for each \(n\), \(X_{n,1},\ldots,\)\(X_{n,r_{n}}\) are independent, mean 0, \(\sigma_{n,i}^{2}=E(X_{n,i}^{2})<\infty\) and \(s_{n}^{2}=\sum_{i=1}^{r_{n}}\sigma_{n,i}^{2}\). Also, assume the array is uniformly asymptotically negligible; that is,_

\[\max_{1\leq i\leq r_{n}}P\{|X_{n,i}/s_{n}|\geq\epsilon\}\to 0 \tag{11.13}\]

_for any \(\epsilon>0\). If \(\sum_{i=1}^{r_{n}}X_{n,i}/s_{n}\stackrel{{ d}}{{\to}}N(0,1)\), then the Lindeberg Condition (11.11) is satisfied._

**Corollary 11.2.2**: _Suppose, for each \(n\), \(X_{n,1},\ldots,X_{n,n}\) are i.i.d. with mean 0 and variance \(\sigma_{n}^{2}\). Let \(s_{n}^{2}=n\sigma_{n}^{2}\). Assume \(\sum_{i=1}^{n}X_{n,i}/s_{n}\stackrel{{ d}}{{\to}}N(0,1)\). Then, the Lindeberg Condition (11.11) is satisfied._

Corollary 11.2.2 follows from Theorem 11.2.6 because the assumption that the \(n\)th row of the triangular array is i.i.d. implies the array is uniformly asymptotically negligible, so the condition (11.13) holds. Indeed,

\[P\{|X_{n,i}|/s_{n}\geq\epsilon\}\leq\frac{E(|X_{n,i}|^{2})}{s_{n}^{2}\epsilon^ {2}}=\frac{1}{n\epsilon^{2}}\to 0\.\]

The following Berry-Esseen Theorem gives information on the error in the normal approximation provided by the Central Limit Theorem.

**Theorem 11.2.7**: _Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. real-valued random variables with c.d.f. \(F\). Let \(\mu(F)\) denote the mean of \(F\) and let \(\sigma^{2}(F)\) denote the variance of \(F\), assumed finite and nonzero. Let \(S_{n}=\sum_{i=1}^{n}X_{i}\). Then, there exists a universal constant \(C\) (not depending on \(F\), \(n\), or \(x\)) such that_

\[\left|P\left\{\frac{S_{n}-n\mu(F)}{n^{1/2}\sigma(F)}\leq x\right\}-\Phi(x) \right|\leq\frac{C}{n^{1/2}}\frac{E_{F}[|X_{1}-\mu(F)|^{3}]}{\sigma(F)^{3}}\, \tag{11.14}\]

_where \(\Phi(\cdot)\) denotes the standard normal c.d.f._

The Berry-Esseen Theorem holds if \(C=0.4748\); see Shevstova (2014). The smallest value of \(C\) for which the result holds is unknown, but it is known that it fails for \(C<0.4097\) (van Beek (1972)).

If \(F\) is a fixed distribution with finite third moment and nonzero variance, the right side of (11.14) tends to zero and hence the left side of (11.14) tends to zero uniformly in \(x\). Furthermore, if \(\mathbf{F}\) is the family of distributions \(F\) with

\[\frac{E_{F}[|X-\mu(F)|^{3}]}{\sigma^{3}(F)}<B\, \tag{11.15}\]

for some fixed \(B<\infty\), then this convergence is also uniform in \(F\) as \(F\) varies in \(\mathbf{F}\). Thus, if \(S_{n}\) is the sum of \(n\) i.i.d. variables with distribution \(F_{n}\) in \(\mathbf{F}\), then

\[\sup_{x}\left|P\left\{\frac{S_{n}-n\mu(F_{n})}{n^{1/2}\sigma(F_{n})}\leq x \right\}-\Phi(x)\right|\to 0. \tag{11.16}\]

**Example 11.2.3**: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. Bernoulli trials with probability of success \(p\). Then, \(S_{n}=\sum_{i}X_{i}\) is binomial based on \(n\) trials and success probability \(p\), and the usual Central Limit Theorem asserts that the probability that \((S_{n}-np)/[np(1-p)]^{1/2}\) is less or equal to \(x\) converges to \(\Phi(x)\), if \(p\) is not zero or one. It follows from the Berry-Esseen Theorem that this convergence is uniformin both \(x\) and \(p\) as long as \(p\in[\epsilon,1-\epsilon]\) for some \(\epsilon>0\). To see why, we show that condition (11.15) is satisfied. Observe that

\[E[|X_{1}-p|^{3}]=p(1-p)[(1-p)^{2}+p^{2}]\leq p(1-p)\.\]

Thus,

\[E[|X_{1}-p|^{3}]/[p(1-p)]^{3/2}\leq[\epsilon(1-\epsilon)]^{-1/2}\,\]

so that (11.15) holds with \(B^{2}=\epsilon(1-\epsilon)\). Thus, (11.16) holds, so that if \(S_{n}\) is binomial based on \(n\) trials and success probability \(p_{n}\to p\in(0,1)\), then

\[P\{{S_{n}-np_{n}\over[np_{n}(1-p_{n})]^{1/2}}\leq x_{n}\}\to\,\Phi(x) \tag{11.17}\]

whenever \(x_{n}\to x\).

**Example 11.2.4**: **(The Sample Median)** As an application of the Berry-Esseen Theorem and the previous example, the following result establishes the asymptotic normality of the sample median. Given a sample \(X_{1},\ldots,\)\(X_{n}\) with order statistics \(X_{(1)}\leq\cdots\leq X_{(n)}\), the median \(\tilde{X}_{n}\) is defined to be the middle-order statistic \(X_{(k)}\) if \(n=2k-1\) is odd and the average of \(X_{(k)}\) and \(X_{(k+1)}\) if \(n=2k\) is even.

**Theorem 11.2.8**: _Suppose \(X_{1},\ldots,\)\(X_{n}\) are i.i.d. real-valued random variables with c.d.f. \(F\). Assume \(F(\theta)=1/2\), and that \(F\) is differentiable at \(\theta\) with \(F^{\prime}=f\) and \(f(\theta)>0\). Let \(\tilde{X}_{n}\) denote the sample median. Then_

\[n^{1/2}(\tilde{X}_{n}-\theta)\stackrel{{ d}}{{\to}}{\rm N}(0,\,{1 \over 4f^{2}(\theta)})\.\]

Proof. Assume first that \(n\) tends to \(\infty\) through odd values and, without loss of generality, that \(\theta=0\). Fix any real number \(a\) and let \(S_{n}\) be the number of \(X_{i}\) that exceed \(a/n^{1/2}\). Then the event \(\{\tilde{X}_{n}\leq a/n^{1/2}\}\) is equivalent to the event \(\{S_{n}\leq(n-1)/2\}\). But, \(S_{n}\) is binomial with parameters \(n\) and success probability \(p_{n}=1-F(a/n^{1/2})\). Thus,

\[P\{n^{1/2}\tilde{X}_{n}\leq a\}=\,P\{S_{n}\leq{n-1\over 2}\}=\,P\{{S_{n}-np_{n} \over[np_{n}(1-p_{n})]^{1/2}}\leq x_{n}\}\,\]

where

\[x_{n}={{1\over 2}(n-1)-np_{n}\over[np_{n}(1-p_{n})]^{1/2}}={n^{1/2}({1\over 2 }-p_{n})-1/(2n^{1/2})\over[p_{n}(1-p_{n})]^{1/2}}\.\]

As \(n\to\infty\), \(p_{n}\to 1/2\) and

\[n^{1/2}({1\over 2}-p_{n})=a\cdot{F(a/n^{1/2})-F(0)\over a/n^{1/2}}\to af(0)\,\]which implies \(x_{n}\to 2af(0)\). Therefore, by (11.17),

\[P\{n^{1/2}\tilde{X}_{n}\leq a\}\to\,\Phi[2f(0)a]\,\]

which completes the proof for odd \(n\). For the case of even \(n\), see Problem 11.16.

Another result concerning uniformity in weak convergence is the following theorem of Polya.

**Theorem 11.2.9** (Polya's Theorem): _Suppose \(X_{n}\xrightarrow{d}X\) and \(X\) has a continuous c.d.f \(F_{X}\). Let \(F_{X_{n}}\) denote the c.d.f. of \(X_{n}\). Then, \(F_{X_{n}}(x)\) converges to \(F_{X}(x)\), uniformly in \(x\)._

It is interesting and important to know that weak convergence of \(F_{n}\) to \(F\) can be expressed in terms of \(\rho(F_{n},\,F)\), where \(\rho\) is a metric on the space of distributions. (Some basic properties of metrics are reviewed in the appendix, Section A.2.) To be specific, on the real line, define the Levy distance between distributions \(F\) and \(G\) as follows.

**Definition 11.2.3**: Let \(F\) and \(G\) be distribution functions on the real line. The _Levy distance_ between \(F\) and \(G\), denoted \(\rho_{L}(F,\,G)\) is defined by

\[\rho_{L}(F,\,G)=\inf\{\epsilon>0:\ F(x-\epsilon)-\epsilon\leq G(x)\leq F(x+ \epsilon)+\epsilon\ \ \ \text{for}\ \ \text{all}\ x\}\.\]

The definition implies that \(\rho_{L}(F,\,G)=\rho_{L}(G,\,F)\) and that \(\rho_{L}\) is a metric on the space of distribution functions (Problem 11.21). Moreover, if \(F_{n}\) and \(F\) are distribution functions, then weak convergence of \(F_{n}\) to \(F\) is equivalent to \(\rho_{L}(F_{n},\,F)\to 0\) (Problem 11.23). In this sense, \(\rho_{L}\) metrizes weak convergence.

We shall next consider the implication of weak convergence for the convergence of quantiles. Ideally, the \((1-\alpha)\) quantile \(x_{1-\alpha}\) of a distribution \(F\) is defined by

\[F(x_{1-\alpha})=1-\alpha. \tag{11.18}\]

For the solutions of (11.18), it is necessary to distinguish three cases. First, if \(F\) is continuous and strictly increasing, the equation (11.18) has a unique solution. Second, if \(F\) is not strictly increasing, it may happen that \(F(x)=1-\alpha\) on an interval \([a,\,b)\) or \([a,\,b]\), so that any \(x\) in such an interval could serve as a \(1-\alpha\) quantile. Then, we shall define the \(1-\alpha\) quantile as the left hand endpoint of the interval. Third, if \(F\) has discontinuities, then (11.18) may have no solutions. This happens if \(F(x)>1-\alpha\) and \(\sup\{F(y):\ y<x\}\leq 1-\alpha\), but in this case we would call \(x\) the \(1-\alpha\) quantile of \(F\). A general definition encompassing all these possibilities is given by

\[x_{1-\alpha}=\inf\{x:\ F(x)\geq 1-\alpha\}. \tag{11.19}\]

This is also sometimes written as \(x_{1-\alpha}=F^{-1}(1-\alpha)\) although \(F\) may not have a proper inverse function.

Weak convergence of \(F_{n}\) to \(F\) is not enough to guarantee that \(F_{n}^{-1}(1-\alpha)\) converges to \(F^{-1}(1-\alpha)\), but the following result shows this is true if \(F\) is continuous and strictly increasing at \(F^{-1}(1-\alpha)\).

**Lemma 11.2.1**: _Let \(\{F_{n}\}\) be a sequence of distribution functions on the real line converging weakly to a distribution function \(F\). Assume \(F\) is continuous and strictly increasing at \(y=F^{-1}(1-\alpha)\). Then,_

\[F_{n}^{-1}(1-\alpha)\to F^{-1}(1-\alpha)\.\]

Proof. Fix \(\delta>0\). Let \(y-\epsilon\) and \(y+\epsilon\) be continuity points of \(F\) for some \(0<\epsilon\leq\delta\). Then,

\[F_{n}(y-\epsilon)\to F(y-\epsilon)<1-\alpha\]

and

\[F_{n}(y+\epsilon)\to F(y+\epsilon)>1-\alpha.\]

Hence, for all sufficiently large \(n\),

\[y-\epsilon\leq F_{n}^{-1}(1-\alpha)\leq y+\epsilon\,\]

and so, \(|F_{n}^{-1}(1-\alpha)-y|\leq\delta\) for all sufficiently large \(n\). Since \(\delta\) was arbitrary, the result is proved.

The following result is of fundamental importance.

**Theorem 11.2.10**: **(Continuous Mapping Theorem)** _Suppose \(X_{n}\stackrel{{ d}}{{\to}}X\). Let \(g\) be a (measurable) map from \({\rm I\kern-1.8ptR}^{k}\) to \({\rm I\kern-1.8ptR}^{s}\). Let \(C\) be the set of points in \({\rm I\kern-1.8ptR}^{k}\) for which \(g\) is continuous. If \(P(X\in C)=1\), then \(g(X_{n})\stackrel{{ d}}{{\to}}g(X)\)._

**Example 11.2.5**: Suppose \(X_{n}\) is a sequence of real-valued random variables such that \(X_{n}\stackrel{{ d}}{{\to}}N(0,\sigma^{2})\). By the Continuous Mapping Theorem, it follows that

\[\frac{X_{n}^{2}}{\sigma^{2}}\stackrel{{ d}}{{\to}}\chi_{1}^{2}\,\]

where \(\chi_{k}^{2}\) denotes the Chi-squared distribution with \(k\) degrees of freedom. More generally, suppose \(X_{n}\) is a sequence of \(k\times 1\) vector-valued random variables such that

\[X_{n}\stackrel{{ d}}{{\to}}N(0,\Sigma)\,\]

where \(\Sigma\) is assumed positive definite. Then, there exists a unique positive definite symmetric matrix \(C\) such that \(C\cdot C=\Sigma\) and we write \(C=\Sigma^{1/2}\). (For the construction of the square root of a positive definite symmetric matrix, see Lehmann (1999), p. 306.) By the Continuous Mapping Theorem, it follows that

\[\left|C^{-1}X_{n}\right|^{2}\stackrel{{ d}}{{\to}}\chi_{k}^{2}\.\ \ \blacksquare\]

### Convergence in Probability and Applications

As pointed out earlier, convergence in law of \(X_{n}\) to \(X\) asserts only that the distribution of \(X_{n}\) tends to that of \(X\), but says nothing about \(X_{n}\) itself becoming close to \(X\). The following stronger form of convergence provides that \(X_{n}\) and \(X\) themselves are close for large \(n\).

**Definition 11.3.1**: A sequence of random vectors \(\{X_{n}\}\)_converges in probability_ to \(X\), written \(X_{n}\stackrel{{ P}}{{\to}}X\), if, for every \(\epsilon>0\),

\[P\{|X_{n}-X|>\epsilon\}\to 0\ \ \ \ \ \ \mbox{as $n\to\infty$}.\]

Convergence in probability implies convergence in distribution (Problem 11.32); the converse is false in general. However, if \(X_{n}\) converges in distribution to a distribution assigning probability one to a constant vector \(c\), then \(X_{n}\) converges in probability to \(c\), and conversely. Note that, unlike weak convergence, \(X_{n}\) and \(X\) must be defined on the same probability space in order for Definition 11.3.1 to make sense.

Convergence in probability of a sequence of random vectors \(X_{n}\) is equivalent to convergence in probability of their components. That is, if \(X_{n}=(X_{n,1},\ldots,X_{n,k})^{\top}\) and \(X=(X_{1},\ldots,X_{k})^{\top}\), then \(X_{n}\stackrel{{ P}}{{\to}}X\) iff for each \(i=1\), \(\ldots,k\), \(X_{n,i}\stackrel{{ P}}{{\to}}X_{i}\). Moreover, \(X_{n}\stackrel{{ P}}{{\to}}0\) if and only if \(|X_{n}|\stackrel{{ P}}{{\to}}0\) (Problem 11.33).

A sequence of real-valued random variables \(X_{n}\) converges in probability to infinity, written \(X_{n}\stackrel{{ P}}{{\to}}\infty\) if, for any real number \(B\),

\[P\{X_{n}<B\}\to 0\]

as \(n\to\infty\).

The next result and the later Theorem 11.4.1 deal with the convergence of the average of i.i.d. random variables toward their expectation, and are known as the weak and strong laws of large numbers. The terminology reflects the fact that the strong law asserts a stronger conclusion than the weak law.

**Theorem 11.3.1**: **(Weak Law of Large Numbers)** _Let \(X_{i}\) be i.i.d. real-valued random variables with mean \(\mu\). Then,_

\[\widetilde{X}_{n}\equiv\frac{1}{n}\sum_{i=1}^{n}X_{i}\stackrel{{ P}}{{\to}}\mu\.\]Note that it is possible for \(\bar{X}_{n}\) to converge in probability to a constant even if the mean does not exist (Problem 11.29). Also, if the \(X_{i}\) are nonnegative and the mean is not finite, then \(\bar{X}_{n}\stackrel{{ P}}{{\to}}\infty\) (Problem 11.36).

Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. according to a model \(\{P_{\theta},\ \theta\in\Omega\}\). A sequence of estimators \(T_{n}=T_{n}(X_{1},\ldots,\)\(X_{n})\) is said to be a weakly consistent (or just consistent) estimator sequence of \(g(\theta)\) if, for each \(\theta\in\Omega\),

\[T_{n}\stackrel{{ P}}{{\to}}g(\theta)\.\]

Thus, the consistency of an estimator sequence merely asserts convergence in probability for each value of the parameter. For example, the Weak Law of Large Numbers asserts that the sample mean is a consistent estimator of the population mean whenever the population mean exists.

**Example 11.3.1**: Suppose \(X_{1},\ldots,\)\(X_{n}\) are i.i.d. according to either \(P_{0}\) or \(P_{1}\). If \(p_{i}\) denotes the density of \(P_{i}\) with respect to a dominating measure, then by the Neyman-Pearson Lemma, an optimal test rejects for large values of

\[T_{n}\equiv\frac{1}{n}\sum_{i=1}^{n}\log[p_{1}(X_{i})/p_{0}(X_{i})]\.\]

By the Weak Law of Large Numbers, under \(P_{0}\),

\[T_{n}\stackrel{{ P}}{{\to}}-K(P_{0},\,P_{1})\, \tag{11.20}\]

where \(K(P_{0},\,P_{1})\) is the so-called Kullback-Leibler Information, defined as

\[K(P_{0},\,P_{1})=-E_{P_{0}}[\log(p_{1}(X_{1})/p_{0}(X_{1}))]. \tag{11.21}\]

The convergence (11.20) assumes \(K(P_{0},\,P_{1})\) is well-defined in the sense that the expectation in (11.21) exists. But, by Jensen's inequality (since the negative log is convex),

\[K(P_{0},\,P_{1})\geq-\log[E_{P_{0}}(p_{1}(X_{1})/p_{0}(X_{1}))]\geq 0\.\]

If \(P_{0}\) and \(P_{1}\) are distinct, then the first inequality is strict, so that \(K(P_{0},\,P_{1})\geq 0\) with equality iff \(P_{0}=P_{1}\). Note, however, that \(K(P_{0},\,P_{1})\) may be \(\infty\), but even in this case, the convergence (11.20) holds; see Problem 11.37. Similarly, under the alternative hypothesis \(P_{1}\),

\[T_{n}\stackrel{{ P}}{{\to}}E_{P_{1}}[\log(p_{1}(X_{1})/p_{0}(X_{1} )]=K(P_{1},\,P_{0})\geq 0\.\]

Note that \(K(P_{0},\,P_{1})\) need not equal \(K(P_{1},\,P_{0})\).

In summary, \(T_{n}\) converges in probability, under \(P_{0}\), to a negative constant (possibly \(-\infty\)), while under \(P_{1}\), \(T_{n}\) converges in probability to a positive constant (assuming\(P_{0}\) and \(P_{1}\) are distinct). Therefore, for testing \(P_{0}\) versus \(P_{1}\), the test that rejects when \(T_{n}>0\) is _asymptotically perfect_ in the sense that both error probabilities tend to zero; that is, \(P_{0}\{T_{n}>0\}\to 0\) and \(P_{1}\{T_{n}\leq 0\}\to 0\). It also follows that, for fixed \(\alpha\in(0,1)\), if \(\phi_{n}\) is a most powerful level \(\alpha\) test sequence for testing \(P_{0}\) versus \(P_{1}\) based on \(n\) i.i.d. observations, then the power of \(\phi_{n}\) against \(P_{1}\) tends to one. Thus, if \(P_{0}\) and \(P_{1}\) are fixed with \(n\to\infty\), the problem is degenerate from an asymptotic point of view.

For convergence in probability to a constant, it is not necessary for the \(X_{n}\) to be defined on the same probability space. Suppose \(P_{n}\) is a probability on a probability space (\(\Omega_{n}\), \(\mathcal{F}_{n}\)), and let \(X_{n}\) be a random vector from \(\Omega_{n}\) to \(\mathbb{R}^{k}\). Then, if \(c\) is a fixed constant vector in \(\mathbb{R}^{k}\), we say that \(X_{n}\) converges to \(c\) in \(P_{n}\)-probability if, for every \(\epsilon>0\),

\[P_{n}\{|X_{n}-c|>\epsilon\}\to 0\quad\mbox{ as }n\to\infty\;.\]

Alternatively, we may say \(X_{n}\) converges to \(c\) in probability if it is understood that the law of \(X_{n}\) is determined by \(P_{n}\).

For a sequence of numbers \(x_{n}\) and \(y_{n}\), the notation \(x_{n}=o(y_{n})\) means \(x_{n}/y_{n}\to 0\) as \(n\to\infty\). For random variables \(X_{n}\) and \(Y_{n}\), the notation \(X_{n}=o_{P}(Y_{n})\) means \(X_{n}/Y_{n}\stackrel{{ P}}{{\to}}0\). Similarly, \(X_{n}=o_{P_{n}}(Y_{n})\) means \(X_{n}/Y_{n}\to 0\) in \(P_{n}\)-probability.

The following theorem is very useful for proving limit theorems.

**Theorem 11.3.2**: **(Slutsky's Theorem)** _Suppose \(\{X_{n}\}\) is a sequence of real-valued random variables such that \(X_{n}\stackrel{{ d}}{{\to}}X\). Further, suppose \(\{A_{n}\}\) and \(\{B_{n}\}\) satisfy \(A_{n}\stackrel{{ P}}{{\to}}a\), and \(B_{n}\stackrel{{ P}}{{\to}}b\), where \(a\) and \(b\) are constants. Then, \(A_{n}X_{n}+B_{n}\stackrel{{ d}}{{\to}}aX+b\)._

Proof. By Problem 11.34, it follows that

\[(X_{n},\,A_{n},\,B_{n})\stackrel{{ d}}{{\to}}(X,a,\,b)\;.\]

Apply the Continuous Mapping Theorem (Theorem 11.2.10).

The conclusion in Slutsky's Theorem may be strengthened to convergence in probability if it is assumed that \(X_{n}\stackrel{{ P}}{{\to}}X\). The following corollary to Slutsky's Theorem is also fundamental.

**Corollary 11.3.1**: _Suppose \(\{X_{n}\}\) is a sequence of real-valued random variables such that \(X_{n}\) tends to \(X\) in distribution, where \(X\) has a cumulative distribution function \(F\) which is continuous at \(c\). If \(C_{n}\to c\) in probability, then_

\[P\{X_{n}\leq C_{n}\}\to\,F(c)\;.\]

Corollary 11.3.1 is useful even when \(C_{n}\) are nonrandom constants tending to \(c\). Also, the corollary holds even if \(c=\infty\) or \(c=-\infty\) (Problem 11.40), with the interpretation \(F(\infty)=1\) and \(F(-\infty)=0\).

Note that Slutsky's Theorem holds more generally if the convergence in probability assumptions are replaced by convergence in \(P_{n}\)-probability.

**Example 11.3.2**: **(Local Power Calculation)** Suppose \(S_{n}\) is binomial based on \(n\) trials and success probability \(p\). Consider testing \(p=1/2\) versus \(p>1/2\). The uniformly most powerful test rejects for large values of \(S_{n}\). By Example 11.2.3,

\[Z_{n}\equiv(S_{n}-\frac{n}{2})/\sqrt{n/4}\stackrel{{ d}}{{\to}}N(0,1)\,\]

and so the test that rejects the null hypothesis when this quantity exceeds the normal critical value \(z_{1-\alpha}\) is asymptotically level \(\alpha\). Let \(\beta_{n}(p)\) denote the power of this test against a fixed alternative \(p>1/2\). Then, \((S_{n}-np)/\sqrt{np(1-p)}\) is asymptotically standard normal if \(p\) is the true value. Hence,

\[\beta_{n}(p)=P_{p}\{Z_{n}>z_{1-\alpha}\}=P_{p}\{\frac{S_{n}-np}{\sqrt{np(1-p)} }>d_{n}(p)\}\,\]

where

\[d_{n}(p)=\frac{z_{1-\alpha}}{[4p(1-p)]^{1/2}}+\frac{\sqrt{n}(\frac{1}{2}-p)}{ \sqrt{p(1-p)}}\to-\infty\]

if \(p>1/2\). Thus, \(\beta_{n}(p)\to 1\) as \(n\to\infty\) for any \(p>1/2\), and so the test sequence is pointwise consistent.

This result does not distinguish between alternative values of \(p\). Better discrimination is obtained by considering alternatives for which the power tends to a value less than 1. This is achieved by replacing a fixed alternative \(p\) by a sequence \(p_{n}\) tending to \(1/2\), so that the task of distinguishing between \(1/2\) and \(p_{n}\) becomes more difficult as information accumulates with increasing \(n\). It turns out that the power will tend to a limit less than one but greater than \(\alpha\) if \(p_{n}=1/2+hn^{-1/2}\) if \(h>0\). To see this, note that, by Example 11.2.3, under \(p_{n}\), \((S_{n}-np_{n})/\sqrt{np_{n}(1-p_{n})}\) is asymptotically standard normal. Then,

\[\beta_{n}(p_{n})=P_{p_{n}}\{Z_{n}>z_{1-\alpha}\}=P_{p_{n}}\{\frac{S_{n}-np_{n} }{\sqrt{np_{n}(1-p_{n})}}>d_{n}(p_{n})\}\.\]

But, \(d_{n}(p_{n})\to z_{1-\alpha}-2h\). Hence, if \(Z\) denotes a standard normal variable,

\[\beta_{n}(p_{n})\to P\{Z>z_{1-\alpha}-2h\}=1-\Phi(z_{1-\alpha}-2h)\.\]

Also, note that \(\beta_{n}(p_{n})\to 1\) if \(\sqrt{n}(p_{n}-1/2)\to\infty\) and \(\beta_{n}(p_{n})\to\alpha\) if \(\sqrt{n}(p_{n}-1/2)\to 0\) (Problem 11.41).

The following is another useful result concerning convergence in probability.

**Theorem 11.3.3**: _Suppose \(X_{n}\) and \(X\) are random vectors in \(\mathbbm{R}^{k}\) with \(X_{n}\stackrel{{ P}}{{\to}}X\). Let \(g\) be a continuous function from \(\mathbbm{R}^{k}\) to \(\mathbbm{R}^{s}\). Then, \(g(X_{n})\stackrel{{ P}}{{\to}}g(X)\)._Example (Sample Standard Deviation)Let \(X_{1},\ldots,X_{n}\) be i.i.d. real-valued random variables with common mean \(\mu\) and finite variance \(\sigma^{2}\). The usual unbiased sample variance estimator is given by

\[S_{n}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}\, \tag{11.22}\]

where \(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\) is the sample mean. By the Weak Law of Large Numbers, \(\bar{X}_{n}\to\mu\) in probability and \(n^{-1}\sum_{i=1}^{n}X_{i}^{2}\to E(X_{1}^{2})=\mu^{2}+\sigma^{2}\) in probability. Hence,

\[\frac{n-1}{n}S_{n}^{2}=n^{-1}\sum_{i=1}^{n}X_{i}^{2}-\bar{X}_{n}^{2}\to\sigma^ {2}\]

in probability, by Slutsky's Theorem. Thus, \(S_{n}^{2}\to\sigma^{2}\) in probability, which implies \(S_{n}\to\sigma\) in probability, by Theorem 11.3.3.

Example (Confidence Intervals for a Binomial \(p\))Suppose \(S_{n}\) is binomial based on \(n\) trials and unknown success probability \(p\). Let \(\hat{p}_{n}=S_{n}/n\). By Example 11.2.3, for any \(p\in(0,1)\), \(\sqrt{n}(\hat{p}_{n}-p)\) converges in distribution to \(N(0,\,p(1-p))\). This implies \(\hat{p}_{n}\stackrel{{ p}}{{\to}}p\) and so

\[\sqrt{\hat{p}_{n}(1-\hat{p}_{n})}\stackrel{{ p}}{{\to}}\sqrt{p(1-p)}\]

as well. Therefore, by Slutsky's Theorem, for any \(p\in(0,1)\),

\[\frac{n^{1/2}(\hat{p}_{n}-p)}{\sqrt{\hat{p}_{n}(1-\hat{p}_{n})}}\stackrel{{ d}}{{\to}}N(0,1)\.\]

This implies that the confidence interval

\[\hat{p}_{n}\pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{n}(1-\hat{p}_{n})}{n}} \tag{11.23}\]

is pointwise consistent in level, for any fixed \(p\) in \((0,1)\), where \(z_{\beta}\) is the \(\beta\) quantile of \(N(0,1)\). Note, however, that this confidence interval is not uniformly consistent in level; in fact, for any \(n\), the coverage probability can be arbitrarily close to \(0\) (Problem 11.42).

Unfortunately, an accumulating literature has shown that the coverage of the interval in (11.23) is quite unreliable even for large values of \(n\) or \(np(1-p)\), and varies quite erratically as the sample size increases. To cite just one example, the probability of the interval (11.23) covering the true \(p\) when \(p=0.2\) and \(1-\alpha=0.95\) is \(0.946\) when \(n=30\), and it is \(0.928\) when \(n=98\). This example is taken from Table 1 of Brown Cai and DasGupta (2001), who survey the literature and recommend more reliable alternatives. Because of the great practical importance of the problem, we summarize some of their principal recommendations.

For small \(n\), the authors recommend two procedures. The first, which goes back to Wilson (1927), is based on the quadratic inequality

\[|\hat{p}_{n}-p|\leq z_{1-\frac{n}{2}}\sqrt{\frac{p(1-p)}{n}}\, \tag{11.24}\]

which has probability under \(p\) tending to \(1-\alpha\). So, if we were testing the simple null hypothesis that \(p\) is true, we can invert the test with acceptance region (11.24). Solving for \(p\) in (11.24), one obtains the Wilson interval (Problem 11.43)

\[\tilde{p}_{n}\pm z_{1-\frac{n}{2}}\frac{\sqrt{n}}{\bar{n}}\sqrt{\hat{p}_{n} \hat{q}_{n}+\frac{z_{1-\frac{n}{2}}^{2}}{4n}}\, \tag{11.25}\]

where \(\tilde{p}_{n}=\tilde{S}_{n}/\bar{n}\), \(\tilde{S}_{n}=S_{n}+\frac{1}{2}z_{1-\frac{n}{2}}^{2}\), \(\bar{n}=n+z_{1-\frac{n}{2}}^{2}\), and \(\hat{q}_{n}=1-\hat{p}_{n}\). As an alternative, the authors recommend an equal-tailed Bayes interval based on the Beta prior with \(a=b=1/2\); see Example 5.7.2.

Theoretical and additional numerical support are provided in Brown, Cai and DasGupta (2002). Other approximations are reviewed in Johnson et al. (1992).

An immediate consequence of Slutsky's Theorem is the following. If \(\tilde{T}_{n}\stackrel{{ d}}{{\to}}T\) and if \(E(\tilde{T}_{n}-T_{n})^{2}\to 0\), then \(T_{n}\stackrel{{ d}}{{\to}}T\). A less obvious but useful result is the following, due to Hajek (Problem 11.44).

**Lemma 11.3.1**: _Suppose that, as \(n\to\infty\),_

\[\frac{\tilde{T}_{n}-E(\tilde{T}_{n})}{\sqrt{Var(\tilde{T}_{n})}}\stackrel{{ d}}{{\to}}T\]

_and_

\[\frac{E[(\tilde{T}_{n}-T_{n})^{2}]}{Var(\tilde{T}_{n})}\to 0\.\]

_Then,_

\[\frac{T_{n}-E(T_{n})}{\sqrt{Var(T_{n})}}\stackrel{{ d}}{{\to}}T\.\]

The following method is often used to prove limit theorems, especially asymptotic normality.

**Theorem 11.3.4**: **(Delta Method)** _Suppose \(X_{1},\,X_{2},\,\ldots\) and \(X\) are random vectors in \(\mathbbm{R}^{k}\). Assume \(\tau_{n}(X_{n}-\mu)\stackrel{{ d}}{{\to}}X\) where \(\mu\) is a constant vector and \(\{\tau_{n}\}\) is a sequence of constants \(\tau_{n}\to\infty\).__(i) Suppose \(g\) is a function from \({\bf R}^{k}\) to \({\bf R}\) which is differentiable at \(\mu\) with gradient (vector of first partial derivatives) of dimension \(1\times k\) at \(\mu\) equal to \(\dot{g}(\mu)\).3 Then,_

Footnote 3: When \(k=1\), we may also use the notation \(g^{\prime}(\mu)\) for the ordinary first derivative of \(g\) with respect to \(\mu\), as well as \(g^{\prime\prime}(\mu)\) for the second derivative.

\[\tau_{n}[g(X_{n})-g(\mu)]\stackrel{{ d}}{{\to}}\dot{g}(\mu)X. \tag{11.26}\]

_In particular, if \(X\) is multivariate normal in \({\bf R}^{k}\) with mean vector 0 and covariance matrix \(\Sigma\), then_

\[\tau_{n}[g(X_{n})-g(\mu)]\stackrel{{ d}}{{\to}}N(0,\dot{g}(\mu) \Sigma\dot{g}(\mu)^{\top}). \tag{11.27}\]

_(ii) More generally, suppose \(g=(g_{1},\ldots,g_{q})^{\top}\) is a mapping from \({\bf R}^{k}\) to \({\bf R}^{q}\), where \(g_{i}\) is a function from \({\bf R}^{k}\) to \({\bf R}\) which is differentiable at \(\mu\). Let \(D\) be the \(q\times k\) matrix with \((i,\,j)\) entry equal to \(\partial g_{i}(y_{1},\ldots,y_{k})/\partial y_{j}\) evaluated at \(\mu\). Then,_

\[\tau_{n}[g(X_{n})-g(\mu)]=\tau_{n}[g_{1}(X_{n})-g_{1}(\mu),\ldots,g_{q}(X_{n}) -g_{q}(\mu)]^{\top}\stackrel{{ d}}{{\to}}DX\.\]

_In particular, if \(X\) is multivariate normal in \({\bf R}^{k}\) with mean vector 0 and covariance matrix \(\Sigma\), then_

\[\tau_{n}[g(X_{n})-g(\mu)]\stackrel{{ d}}{{\to}}N(0,\,D\Sigma\,D^{ \top})\.\]

Proof. We prove (i) with (ii) left as an exercise (Problem 11.49). Note that \(X_{n}-\mu=o_{P}(1)\). Differentiability of \(g\) at \(\mu\) implies

\[g(x)=g(\mu)+\dot{g}(\mu)(x-\mu)+R(x-\mu)\,\]

where \(R(y)=o(|y|)\) as \(|y|\to 0\). Now,

\[\tau_{n}[g(X_{n})-g(\mu)]-\dot{g}(\mu)\tau_{n}(X_{n}-\mu)=\tau_{n}R(X_{n}-\mu)\.\]

By Slutsky's Theorem, it suffices to show \(\tau_{n}R(X_{n}-\mu)=o_{P}(1)\). But,

\[\tau_{n}R(X_{n}-\mu)=\tau_{n}|X_{n}-\mu|\cdot h(X_{n}-\mu)\,\]

where \(h(y)=R(y)/|y|\) and \(h(0)\) is defined to be 0, so that \(h\) is continuous at 0. The weak convergence hypothesis and the Continuous Mapping Theorem imply \(\tau_{n}|X_{n}-\mu|\) has a limiting distribution. So, by Slutsky's Theorem, it is enough to show \(h(X_{n}-\mu)=o_{P}(1)\). But, this follows by the Continuous Mapping Theorem as well.

Note that (11.26) and (11.27) remain true if \(\dot{g}(\mu)=0\) with the interpretation that the limit distribution places all its mass at zero, in which case we can conclude

\[\tau_{n}[g(X_{n})-g(\mu)]\stackrel{{ P}}{{\to}}0\.\]

**Example 11.3.5**: **(Binomial Variance)** Suppose \(S_{n}\) is binomial based on \(n\) trials and success probability \(p\). Let \(\hat{p}_{n}=S_{n}/n\). By the Central Limit Theorem,

\[n^{1/2}(\hat{p}_{n}-p)\stackrel{{ d}}{{\rightarrow}}N(0,\,p(1-p))\.\]

Consider estimating \(g(p)=p(1-p)\). By the Delta Method,

\[n^{1/2}[g(\hat{p}_{n})-g(p)]\stackrel{{ d}}{{\rightarrow}}N(0,\,( 1-2p)^{2}p(1-p))\.\]

If \(p=1/2\), then \(\dot{g}(1/2)=0\), so that

\[n^{1/2}[g(\hat{p}_{n})-g(p)]\stackrel{{ P}}{{\rightarrow}}0\.\]

In order to obtain a nondegenerate limit distribution in this case, note that

\[n[g(\hat{p}_{n})-\frac{1}{4}]=-[n^{1/2}(\hat{p}_{n}-\frac{1}{2})]^{2}\.\]

Therefore, by the Continuous Mapping Theorem,

\[n[g(\hat{p}_{n})-\frac{1}{4}]\stackrel{{ d}}{{\rightarrow}}-X^{2}\,\]

where \(X\) is \(N(0,\,1/4)\), or

\[n[g(\hat{p}_{n})-\frac{1}{4}]\stackrel{{ d}}{{\rightarrow}}-\frac {1}{4}\chi_{1}^{2}\,\]

where \(\chi_{1}^{2}\) is a random variable distributed as Chi-squared with one degree of freedom.

In the case \(\dot{g}(\mu)=0\), it is not surprising that the limit distribution is a multiple of a Chi-squared variable with one degree of freedom. Indeed, suppose \(k=1\) and \(g\) is twice differentiable at \(\mu\) with second derivative \(g^{\prime\prime}(\mu)\), so that

\[g(x)=g(\mu)+\frac{1}{2}g^{\prime\prime}(\mu)(x-\mu)^{2}+R(x-\mu)\,\]

where \(R(x-\mu)=o[(x-\mu)^{2}]\) as \(x\rightarrow\mu\). Arguing as in the proof of Theorem 11.3.4 yields

\[\tau_{n}^{2}[g(X_{n})-g(\mu)]-\tau_{n}^{2}\frac{g^{\prime\prime}(\mu)}{2}(X_{n }-\mu)^{2}=\tau_{n}^{2}R(X_{n}-\mu)=o_{P}(1) \tag{11.28}\]

(Problem 11.51). By the Continuous Mapping Theorem,\[\tau_{n}(X_{n}-\mu)\stackrel{{ d}}{{\to}}X\]

implies

\[\tau_{n}^{2}\frac{g^{{}^{\prime\prime}}(\mu)}{2}(X_{n}-\mu)^{2}\stackrel{{ d}}{{\to}}\frac{g^{{}^{\prime\prime}}(\mu)}{2}X^{2}\.\]

By Slutsky's Theorem, \(\tau_{n}^{2}[g(X_{n})-g(\mu)]\) has this same limiting distribution. Of course, if \(X\) is \(N(\mu,\sigma^{2})\), then this limiting distribution is \(\frac{g^{{}^{\prime\prime}}(\mu)\sigma^{2}}{2}\chi_{1}^{2}\).

**Example 11.3.6** (Sample Correlation): Let \((U_{i},\,V_{i})\) be i.i.d. bivariate random vectors in the plane, with both \(U_{i}\) and \(V_{i}\) assumed to have finite nonzero variances. Let \(\sigma_{U}^{2}=Var(U_{i})\), \(\sigma_{V}^{2}=Var(V_{i})\), \(\mu_{U}=E(U_{i})\), \(\mu_{V}=E(V_{i})\) and let \(\rho=Cov(U_{i},\,V_{i})/(\sigma_{U}\sigma_{V})\) be the population correlation coefficient. The usual sample correlation coefficient is given by

\[\hat{\rho}_{n}=\frac{\sum_{i=1}^{n}(U_{i}-\bar{U}_{n})(V_{i}-\bar{V}_{n})/n}{S _{U}S_{V}}\, \tag{11.29}\]

where \(\bar{U}_{n}=\sum U_{i}/n\), \(\bar{V}_{n}=\sum V_{i}/n\), \(S_{U}^{2}=\sum(U_{i}-\bar{U}_{n})^{2}/n\), and \(S_{V}^{2}=\sum(V_{i}-\bar{V}_{n})^{2}/n\). Then, \(n^{1/2}(\hat{\rho}_{n}-\rho)\) is asymptotically normal. The important observation is that \(\hat{\rho}_{n}\) is a smooth function of the vector of means \(\bar{X}_{n}\), where \(X_{i}\) is the vector \(X_{i}=(U_{i},\,V_{i},\,U_{i}^{2},\,V_{i}^{2},\,U_{i}\,V_{i})^{\top}\). In fact, \(\hat{\rho}_{n}=g(\bar{X}_{n})\), where

\[g((y_{1},\,y_{2},\,y_{3},\,y_{4},\,y_{5})^{\top})=\frac{y_{5}-y_{1}y_{2}}{(y_ {3}-y_{1}^{2})^{1/2}(y_{4}-y_{2}^{2})^{1/2}}\.\]

Note that \(g\) is smooth and \(\dot{g}\) is readily computed. Let \(\mu=E(X_{i})\) denote the mean vector. Further assume that \(U_{i}\) and \(V_{i}\) have finite fourth moments. Then, by the multivariate CLT,

\[n^{1/2}(\bar{X}_{n}-\mu)\stackrel{{ d}}{{\to}}N(0,\,\Sigma)\,\]

where \(\Sigma\) is the covariance matrix of \(X_{1}\). For example, the \((1,5)\) component of \(\Sigma\) is \(Cov(U_{1},\,U_{1}V_{1})\). Hence, by the Delta Method,

\[n^{1/2}[g(\bar{X}_{n})-g(\mu)]=n^{1/2}(\hat{\rho}_{n}-\rho)\stackrel{{ d}}{{\to}}N(0,\,\dot{g}(\mu)\,\Sigma\,\dot{g}(\mu)^{\top}). \tag{11.30}\]

As an example, suppose that \((U_{i},\,V_{i})\) is bivariate normal; in this case, (11.30) reduces to (Problem 11.52)

\[n^{1/2}(\hat{\rho}_{n}-\rho)\stackrel{{ d}}{{\to}}N(0,\,(1-\rho^ {2})^{2}). \tag{11.31}\]

This implies \((1-\hat{\rho}_{n}^{2})\stackrel{{ P}}{{\to}}1-\rho^{2}\). Then, by Slutsky's Theorem,

\[n^{1/2}(\hat{\rho}_{n}-\rho)/(1-\hat{\rho}_{n}^{2})\stackrel{{ d}}{{\to}}N(0,\,1)\,\]and so the confidence interval

\[\hat{\rho}_{n}\pm n^{-1/2}z_{1-\frac{\alpha}{2}}(1-\hat{\rho}_{n}^{2})\]

is a pointwise asymptotically level \(1-\alpha\) confidence interval for \(\rho\). The error in this asymptotic approximation derives from both the normal approximation to the distribution of \(\hat{\rho}_{n}\) and the fact that one is approximating the limiting variance. To counter the second of these effects, the following variance stabilization technique can be used. By the Delta Method, if \(h\) is differentiable, then

\[n^{1/2}[h(\hat{\rho}_{n})-h(\rho)]\stackrel{{ d}}{{\to}}N(0,[h^{ \prime}(\rho)]^{2}(1-\rho^{2})^{2})\.\]

The idea is to choose \(h\) so that the limiting variance does not depend on \(\rho\) and is a constant; such a transformation is then called a _variance stabilizing transformation_. The solution is known as Fisher's \(z\)-transformation and is given by

\[h(\rho)=\frac{1}{2}\log(\frac{1+\rho}{1-\rho})=\operatorname{arctanh}(\rho)\.\]

Then,

\[h(\hat{\rho}_{n})\pm n^{-1/2}z_{1-\frac{\alpha}{2}}\]

is a pointwise asymptotically level \(1-\alpha\) confidence interval for \(h(\rho)\). The inverse function of \(h\) is the hyperbolic tangent function

\[\tanh(y)=h^{-1}(y)=\frac{e^{y}-e^{-y}}{e^{y}+e^{-y}}\,\]

so that

\[[\tanh(\operatorname{arctanh}(\hat{\rho}_{n})-n^{-1/2}z_{1-\frac{\alpha}{2}}), \tanh(\operatorname{arctanh}(\hat{\rho}_{n})+n^{-1/2}z_{1-\frac{\alpha}{2}})]\]

is also a pointwise asymptotically level \(1-\alpha\) confidence interval for \(\rho\).4

Footnote 4: For discussion of this transformation, see Mudholkar (1983), Stuart and Ord, Vol. 1 (1987) and Efron and Tibshirani (1993), p. 54. Numerical evidence supports replacing \(n\) by \(n-3\) in (11.32).

Sometimes, \(\{X_{n}\}\) may not have a limiting distribution, but the weaker property of _tightness_ may hold, which only requires that no probability escapes to \(\pm\infty\).

**Definition 11.3.2**.: A sequence of random vectors \(\{X_{n}\}\) is _tight_ (or _uniformly tight_) if \(\forall\epsilon>0\), there exists a constant \(B\) such that

\[\inf_{n}P\{|X_{n}|\leq B\}\geq 1-\epsilon\.\]A bounded sequence of numbers \(\{x_{n}\}\) is sometimes written \(x_{n}=O(1)\); more generally \(x_{n}=O(y_{n})\) if \(x_{n}/y_{n}=O(1)\). If \(\{X_{n}\}\) is tight, we sometimes also say \(X_{n}\) is bounded in probability, and write \(|X_{n}|=O_{P}(1)\). If \(X_{n}\) is tight and \(Y_{n}\stackrel{{ P}}{{\to}}0\) (sometimes written \(Y_{n}=o_{P}(1)\)), then \(|X_{n}Y_{n}|\stackrel{{ P}}{{\to}}0\) (Problem 11.64). The notation \(|X_{n}|=O_{P}(|Y_{n}|)\) means \(|X_{n}|/|Y_{n}|\) is tight.

Tightness of a sequence of random vectors in \({\rm I\kern-1.8ptR}^{k}\) is equivalent to each of the component variables being tight (Problem 11.45). Note that tightness, like convergence in distribution, really refers to the sequence of laws of \(X_{n}\), denoted \({\cal L}(X_{n})\). Thus, we shall interchangeably refer to tightness of a sequence of random variables or the sequence of their distributions.

In a statistical context, suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. according to a model \(\{P_{\theta},\ \theta\in\Omega\}\). Recall that an estimator sequence \(T_{n}\) is a (weakly) consistent estimator of \(g(\theta)\) if, for every \(\theta\in\Omega\),

\[T_{n}-g(\theta)\to 0\]

in probability when \(P_{\theta}\) is true. An estimator sequence \(T_{n}\) is said to be \(\tau_{n}\)-consistent for \(g(\theta)\) if, for every \(\theta\in\Omega\),

\[\tau_{n}[T_{n}-g(\theta)]\]

is tight when \(P_{\theta}\) is true. For example, if the underlying population has a finite variance, it follows from the Central Limit Theorem that the sample mean is a \(n^{1/2}\)-consistent estimator of the population mean.

Whenever \(X_{n}\) converges in distribution to a limit distribution, then \(\{X_{n}\}\) is tight, and the following partial converse is true. Just as any bounded sequence of real numbers has a subsequence which converges, so does any sequence of random variables \(X_{n}\) that is \(O_{P}(1)\). This important result is stated next.

**Theorem 11.3.5**: **(Prohorov's Theorem)** _Suppose \(\{X_{n}\}\) is tight on \({\rm I\kern-1.8ptR}^{k}\). Then, there exists a subsequence \(n_{j}\) and a random vector \(X\) such that \(X_{n_{j}}\stackrel{{ d}}{{\to}}X\)._

### 11.4 Almost Sure Convergence

On occasion, we shall utilize a form of convergence of \(X_{n}\) to \(X\) stronger than convergence in probability.

**Definition 11.4.1**: Suppose \(X_{n}\) and \(X\) are random vectors in \({\rm I\kern-1.8ptR}^{k}\), defined on a common probability space \(({\cal X},\,{\cal F})\). Then, \(X_{n}\) is said to _converge almost surely_ (a.s.) to \(X\) if \(X_{n}(\omega)\to X(\omega)\) on a set of points \(\omega\) which has probability one; that is, if

\[P\{\omega\in{\cal X}:\ \lim_{n\to\infty}|X_{n}(\omega)-X(\omega)|=0\}=1\.\]

This is denoted by \(X_{n}\to X\) a.s..

Equivalently, we say that \(X_{n}\) converges to \(X\) with probability one, since there is a set of outcomes \(\omega\) having probability one such that \(X_{n}(\omega)\to X(\omega)\). If \(X_{n}\) converges almost surely to \(X\), then \(X_{n}\) converges in probability to \(X\), but the converse is false (but see Problem 11.72). Indeed, convergence in probability does not even guarantee \(X_{n}(\omega)\to X(\omega)\) for any outcome \(\omega\). The following provides a classic counterexample.

**Example 11.4.1**: **(Convergence in probability, but not a.s.)** Suppose \(U\) is uniformly distributed on \([0,1)\), so that \(\mathcal{X}\) is [0,1), \(\mathcal{F}\) is the class of Borel sets, \(U=U(\omega)=\omega\), and \(P\) is the uniform probability measure. For \(m=1,2,\ldots\) and \(j=1,\ldots,m\), let \(Y_{m,j}\) be one if \(U\in[(j-1)/m,\,j/m)\) and zero otherwise. For any \(m\), exactly one of the \(Y_{m,j}\) is one and the rest are zero; also, \(P\{Y_{m,j}=1\}=1/m\to 0\) as \(m\to\infty\). String together all the variables so that \(X_{1}=Y_{1}\), \(X_{2}=Y_{2,1}\), \(X_{3}=Y_{2,2}\), \(X_{4}=Y_{3,1}\), \(X_{5}=Y_{3,2}\), etc. Then, \(X_{n}\to 0\) in probability. But \(X_{n}\) does not converge to \(0\) for any outcome \(U\) since \(X_{n}\) oscillates infinitely often between \(0\) and \(1\).

**Theorem 11.4.1**: **(Strong Law of Large Numbers)** _Let \(X_{i}\) be i.i.d. real-valued random variables with mean \(\mu\). Then_

\[\bar{X}_{n}\equiv\tfrac{1}{n}\sum_{i=1}^{n}X_{i}\to\mu\ \ \ \mathrm{a.s.}\]

_Conversely, if \(\overline{X}_{n}\to\mu\), a.s. with \(|\mu|<\infty\), then \(E|X_{1}|<\infty\)._

In a statistical context, suppose \(X_{1},\ldots,X_{n}\) are i.i.d. according to a model \(\{P_{\theta},\ \theta\in\Omega\}\). Suppose, under each \(\theta\), \(T_{n}=T_{n}(X_{1},\ldots,X_{n})\) converges almost surely to \(g(\theta)\). Then, \(T_{n}\) is said to be strongly consistent estimator of \(g(\theta)\).

One of the most fundamental examples of almost sure convergence is provided by the Glivenko-Cantelli Theorem. To state the result, first define the Kolmogorov-Smirnov distance between c.d.f.s \(F\) and \(G\) as

\[d_{K}(F,G)=\sup_{t}|F(t)-G(t)|. \tag{11.33}\]

**Theorem 11.4.2**: **(Glivenko-Cantelli Theorem)** _Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. real-valued random variables with c.d.f. \(F\). Let \(\hat{F}_{n}\) be the empirical c.d.f. defined by_

\[\hat{F}_{n}(t)=\frac{1}{n}\sum_{i=1}^{n}I\{X_{i}\leq t\}. \tag{11.34}\]

_Then,_

\[d_{K}(\hat{F}_{n},F)\to 0\ \ \ a.s.\]

To prove the Glivenko-Cantelli Theorem, note that, for every fixed \(t\), \(\hat{F}_{n}(t)\to F(t)\) almost surely, by the Strong Law of Large Numbers. That this convergence is uniform in \(t\) follows from the fact that \(F\) is monotone (Problem 11.62).

**Example 11.4.2** (Kolmogorov-Smirnov Test): The Glivenko-Cantelli Theorem 11.4.2 forms the basis for the Kolmogorov-Smirnov goodness of fit test, previously introduced in Section 6.13. Specifically, consider the problem of testing the simple null hypothesis that \(F=F_{0}\) versus \(F\neq F_{0}\). The Glivenko-Cantelli Theorem implies that, under \(F\),

\[d_{K}(\hat{F}_{n},\,F_{0})\to d_{K}(F,\,F_{0})\ \ \ a.s.\]

(and hence in probability as well), where the right side is zero if and only if \(F=F_{0}\). Thus, the statistic \(d_{K}(\hat{F}_{n},\,F_{0})\) tends to be small under the null hypothesis and large under the alternative. In order for this statistic to have a nondegenerate limit distribution under \(F_{0}\), we normalize by multiplication of \(n^{1/2}\) and the Kolmogorov-Smirnov goodness of fit test statistic is given by

\[T_{n}\equiv\sup_{t\in{\bf R}}n^{1/2}|\hat{F}_{n}(t)-F_{0}(t)|=n^{1/2}d_{K}(\hat {F}_{n},\,F_{0}). \tag{11.35}\]

The Kolmogorov-Smirnov test rejects the null hypothesis if \(T_{n}>s_{n,1-\alpha}\), where \(s_{n,1-\alpha}\) is the \(1-\alpha\) quantile of the null distribution of \(T_{n}\) when \(F_{0}\) is the uniform \(U(0,\,1)\) distribution. Recall from Section 6.13 that the finite sampling distribution of \(T_{n}\) under \(F_{0}\) is the same for all continuous \(F_{0}\) (also see Problem 11.68), but its exact form is difficult to express. Some approaches to obtaining this distribution are discussed in Durbin (1973) and Section 4.3 of Gibbons and Chakraborti (1992). Values for \(s_{n,1-\alpha}\) have been tabled in Birnbaum (1952). For exact power calculations in both the continuous and discrete case, see Niederhausen (1981) and Gleser (1985).

By the duality of tests and confidence regions, the Kolmogorov-Smirnov test can be inverted to yield uniform confidence bands for \(F\), given by

\[R_{n,1-\alpha}=\{F:\ n^{1/2}\sup_{t}|\hat{F}_{n}(t)-F(t)|\leq s_{n,1-\alpha}\}. \tag{11.36}\]

By construction, \(P_{F}\{F\in R_{n,1-\alpha}\}=1-\alpha\) if \(F\) is continuous; furthermore, the confidence band is conservative if \(F\) is not continuous (Problem 11.69).

The limiting behavior of \(T_{n}\) will be discussed in Section 16.2. In fact, when \(F=F_{0}\), \(T_{n}\) has a continuous strictly increasing limiting distribution with \(1-\alpha\) quantile \(s_{1-\alpha}\) (and so \(s_{n,1-\alpha}\to s_{1-\alpha}\)). It follows that the width of the band (11.36) is \(O(n^{-1/2})\). Alternatives to the Kolmogorov-Smirnov bands that are more narrow in the tails and wider in the middle are discussed in Owen (1995).

The following useful inequality, which holds for finite sample sizes, actually implies the Glivenko-Cantelli Theorem (Problem 11.70).

**Theorem 11.4.3** (Dvoretzky, Kiefer, Wolfowitz Inequality): _Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. real-valued random variables with c.d.f. \(F\). Let \(\hat{F}_{n}\) be the empirical c.d.f. (11.34). Then, for any \(d>0\) and any positive integer \(n\),_

\[P\{d_{K}(\hat{F}_{n},\,F)>d\}\leq C\exp(-2nd^{2})\, \tag{11.37}\]

_where \(C\) is a universal constant._Massart (1990) shows that we can take \(C=2\), which greatly improves the original value obtained by Dvoretzy et al. (1956).

**Example 11.4.3** (Monte Carlo Simulation): Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. observations with common distribution \(P\). Assume \(P\) is known. The problem is to determine the distribution or quantile of some real-valued statistic \(T_{n}(X_{1},\ldots,X_{n})\) for a fixed finite sample size \(n\). Denote this distribution by \(J_{n}(t)\), so that

\[J_{n}(t)=P\{T_{n}(X_{1},\ldots,X_{n})\leq t\}\enspace.\]

This distribution may not have a tractable form or may not be explicitly computable, but the following simulation scheme allows the distribution \(J(t)\) to be estimated to any desired level of accuracy. For \(j=1,\ldots,B\), let \(X_{j,1},\ldots,X_{j,n}\) be a sample of size \(n\) from \(P\); then, one simply evaluates \(T_{n}(X_{j,1},\ldots,X_{j,n})\), and the empirical distribution of these \(B\) values serves as an approximation to the true sampling distribution \(J_{n}(t)\). Specifically, \(J_{n}(t)\) is approximated by

\[\hat{J}_{n,B}(t)=B^{-1}\sum_{j=1}^{B}I\{T_{n}(X_{j,1},\ldots,X_{j,n})\leq t\}\enspace.\]

For large \(B\), \(\hat{J}_{n,B}(t)\) will be a good approximation to the true sampling distribution \(J_{n}(t,P)\). One (though perhaps crude) way of quantifying the closeness of this approximation is the following. By the Dvoretsky, Kiefer, Wolfowitz inequality (11.37) (with \(B\) now taking over the role of \(n\)), there exists a universal constant \(C\) so that

\[P\{d_{K}(\hat{J}_{n,B},J_{n})>d\}\leq C\,\exp(-2Bd^{2}).\]

Hence, if we desire the probability of the supremum distance between \(\hat{J}_{n,B}(\cdot)\) and \(J_{n}(\cdot,P)\) to be greater than \(d\) with probability less than \(\epsilon\), all we need to do is ensure that \(B\) is large enough so that \(C\exp(-2Bd^{2})\leq\epsilon\). Since \(B\), the number of simulations, is determined by the statistician (assuming enough computing power), the desired accuracy can be obtained. Further results on the choice of \(B\) are given in Jockel (1986).

Here, we are tacitly assuming that one can easily accomplish the sampling of observations from \(P\). Of course, when \(P\) corresponds to a cumulative distribution function \(F\) on the real line, one can usually just obtain observations from \(F\) by \(F^{-1}(U)\), where \(U\) is a random variable having the uniform distribution on \((0,1)\). This construction assumes an ability to calculate an inverse function \(F^{-1}(\cdot)\). A sample \(X_{j,1},\ldots,X_{j,n}\) of \(n\) i.i.d. \(F\) variables can then be obtained from \(n\) i.i.d. Uniform (0, 1) observations \(U_{j,1},\ldots,U_{j,n}\) by the prescription \(X_{j,n}=F^{-1}(U_{j,n})\). If \(F^{-1}\) is not tractable, other methods for generating observations with prescribed distributions are available in statistical software packages, such as R, Excel, or Maple.

Note, however, that we have ignored any error from the use of a pseudo-random number generator, which presumably would be needed to generate the Uniform (0, 1) variables. The above idea forms the basis of many approximation schemes; for some general references on Monte Carlo simulation, see Devroye (1986) and Ripley (1987).

Almost sure convergence is the strongest type of convergence we have introduced and it has many consequences. For example, suppose \(X_{n}\to X\) almost surely and \(|X_{n}|\leq 1\) with probability one. Then, \(|X|\leq 1\) with probability one, and so \(E(|X|)\leq 1\); by the Lebesgue-dominated convergence theorem (Theorem 2.2.2), it follows that \(E(X_{n})\to E(X)\). If the assumption that \(X_{n}\to X\) almost surely is replaced by the weaker condition that \(X_{n}\) converges in distribution to \(X\), then the argument to show \(E(X_{n})\to E(X)\) breaks down. However, we shall now show that the result continues to hold since the conclusion pertains only to distributional properties of \(X_{n}\) and \(X\). The argument is based on the following theorem.

**Theorem 11.4.4**: **(Almost Sure Representation Theorem)** _Suppose \(X_{n}\stackrel{{ d}}{{\to}}X\) in \({\rm I\!R}^{k}\). Then, there exist random vectors \(\widetilde{X}_{n}\) and \(\widetilde{X}\) defined on some common probability space such that \(\widetilde{X}_{n}\) has the same distribution as \(X_{n}\) and \(\widetilde{X}_{n}\to\widetilde{X}\) a.s. (and so \(\tilde{X}\) has the same distribution as \(X\))._

**Example 11.4.4**: **(Convergence of Moments)** Suppose \(X_{n}\) and \(X\) are real-valued random variables and \(X_{n}\stackrel{{ d}}{{\to}}X\). If the \(X_{n}\) are uniformly bounded, then \(E(X_{n})\to E(X)\). To see why, construct \(\tilde{X}_{n}\) and \(\tilde{X}\) by the Almost Sure Representation Theorem and then apply the Dominated Convergence Theorem (Theorem 2.2.2) to the \(\tilde{X}_{n}\) to conclude

\[E(X_{n})=E(\tilde{X}_{n})\to E(\tilde{X})=E(X). \tag{11.38}\]

If the \(X_{n}\) are not uniformly bounded, but \(X_{n}\geq 0\), then by Fatou's Lemma (Theorem 2.2.1), we may conclude

\[E(X)=E(\tilde{X})\leq\liminf_{n}E(\tilde{X}_{n})=\liminf_{n}E(X_{n})\.\]

As a final result, suppose \(X_{n}\stackrel{{ d}}{{\to}}X\) and \(|X|\) has distribution \(F\) which is continuous at \(t\). Then, by the Continuous Mapping Theorem,

\[|X_{n}|I\{|X_{n}|\leq t\}\stackrel{{ d}}{{\to}}|X|I\{|X|\leq t\}\.\]

By (11.38), we may conclude

\[E[|X_{n}|I\{|X_{n}|\leq t\}]\to E[|X|I\{|X|\leq t\}]. \tag{11.39}\]

If, in addition, \(E|X_{n}|\to E|X|\), then

\[E[|X_{n}|I\{|X_{n}|>t\}]\to E[|X|I\{|X|>t\}]. \tag{11.40}\]More generally, convergence of moments (which are not truncated) holds under uniform integrability, which we now define.

**Definition 11.4.5**: A sequence of random variables \(X_{1},\,X_{2},\ldots\) is called _uniformly integrable_ if, given any \(\epsilon>0\), there exists \(\lambda<\infty\) such that

\[\sup_{n}E[|X_{n}|I\{|X_{n}|>\lambda\}]<\epsilon\.\]

The sequence is \(\{X_{n}\}\) is called _asymptotically uniformly integrable_ if

\[\lim_{\lambda\to\infty}\limsup_{n\to\infty}E[|X_{n}|I\{|X_{n}|>\lambda\}]=0\.\]

Uniform integrability is slightly stronger than asymptotic uniform integrability. A sufficient condition for uniform integrability of \(\{X_{n}\}\) is, for some \(\delta>0\), \(\sup_{n}E(|X_{n}|^{1+\delta})<\infty\). A useful result is the following.

**Theorem 11.4.5**: _Suppose \(X_{n}\stackrel{{ d}}{{\to}}X\) and \(\{X_{n}\}\) is asymptotically uniformly integrable. Then, \(E(X_{n})\to E(X)\)._

### Problems

#### Section 11.1

**Problem 11.1**: For each \(\theta\in\Omega\), let \(f_{n}(\theta)\) be a real-valued sequence. We say \(f_{n}(\theta)\) converges uniformly (in \(\theta\)) to \(f(\theta)\) if

\[\sup_{\theta\in\Omega}|f_{n}(\theta)-f(\theta)|\to 0\]

as \(n\to\infty\). If \(\Omega\) if a finite set, show that the pointwise convergence \(f_{n}(\theta)\to f(\theta)\) for each fixed \(\theta\) implies uniform convergence. However, show the converse can fail even if \(\Omega\) is countable.

#### Section 11.2

**Problem 11.2**: For a univariate c.d.f. \(F\), show that the set of points of discontinuity is countable.

**Problem 11.3**: Let \(X\) be \(N(0,1)\) and \(Y=X\). Determine the set of continuity points of the bivariate distribution of \((X,\,Y)\).

[MISSING_PAGE_FAIL:562]

1. Show that, under \(H\), the left side of (11.41) converges in distribution to \(N(0,\sigma^{2})\) with \(\sigma^{2}=\operatorname{Var}_{0}[\log r(X_{i})]\), provided \(\sigma<\infty\).
2. From (i) it follows that \(k_{n}\to\sigma z_{1-\alpha},\) where \(z_{\alpha}\) is the \(\alpha\) quantile of \(N(0,1)\).
3. The power of the test (11.41) against \(p_{1}\) tends to \(1\) as \(n\to\infty\). _Hint_: Use Problem 3.41(iv).

**Problem 11.16**: Complete the proof of Theorem 11.2.8 by considering \(n\) even.

**Problem 11.17**: Generalize Theorem 11.2.8 to the case of the \(p\)th sample quantile.

**Problem 11.18**: Let \(X_{1},\ldots,\)\(X_{n}\) be i.i.d. normal with mean \(\theta\) and variance \(1\). Let \(\tilde{X}_{n}\) be the usual sample mean and let \(\tilde{X}_{n}\) be the sample median. Let \(p_{n}\) be the probability that \(\tilde{X}_{n}\) is closer to \(\theta\) than \(\tilde{X}_{n}\) is. Determine \(\lim_{n\to\infty}p_{n}\).

**Problem 11.19**: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. real-valued random variables with c.d.f. \(F\). Assume \(\exists\theta_{1}<\theta_{2}\) such that \(F(\theta_{1})=1/4\), \(F(\theta_{2})=3/4\), and \(F\) is differentiable, with density \(f\) taking positive values at \(\theta_{1}\) and \(\theta_{2}\). Show that the sample inter-quartile range (defined as the difference between the \(0.75\) quantile and \(0.25\) quantile) is a \(\sqrt{n}\)- consistent estimator of the population inter-quartile range (\(\theta_{2}-\theta_{1}\)).

**Problem 11.20**: Prove Polya's Theorem 11.2.9. _Hint:_ First consider the case of distributions on the real line.

**Problem 11.21**: Show that \(\rho_{L}(F,G)\) defined in Definition 11.2.3 is a metric; that is, show \(\rho_{L}(F,G)=\rho_{L}(G,F)\), \(\rho_{L}(F,G)=0\) if and only if \(F=G\), and

\[\rho_{L}(F,G)\leq\rho_{L}(F,H)+\rho_{L}(H,G)\.\]

**Problem 11.22**: For cumulative distribution functions \(F\) and \(G\) on the real line, define the Kolmogorov-Smirnov distance between \(F\) and \(G\) to be

\[d_{K}(F,G)=\sup_{x}|F(x)-G(x)|\.\]

Show that \(d_{K}(F,G)\) defines a metric on the space of distribution functions; that is, show \(d_{K}(F,G)=d_{K}(G,F)\), \(d_{K}(F,G)=0\) implies \(F=G\) and

\[d_{K}(F,G)\leq d_{K}(F,H)+d_{K}(H,G)\.\]

Also, show that \(\rho_{L}(F,G)\leq d_{K}(F,G)\), where \(\rho_{L}\) is the Levy metric. Construct a sequence \(F_{n}\) such that \(\rho_{L}(F_{n},F)\to 0\) but \(d_{K}(F_{n},F)\) does not converge to zero.

**Problem 11.23**: Let \(F_{n}\) and \(F\) be c.d.f.s on \(\mathbb{R}\). Show that weak convergence of \(F_{n}\) to \(F\) is equivalent to \(\rho_{L}(F_{n},F)\to 0\), where \(\rho_{L}\) is the Levy metric.

**Problem 11.24**: Suppose \(F\) and \(G\) are two probability distributions on \(\mathbb{R}^{k}\). Let \(\mathcal{L}\) be the set of (measurable) functions \(f\) from \(\mathbb{R}^{k}\) to \(\mathbb{R}\) satisfying \(|f(x)-f(y)|\leq|x-y|\)and \(\sup_{x}|f(x)|\leq 1\), where \(|\cdot|\) is the usual Euclidean norm. Define the Bounded-Lipschitz Metric as

\[\lambda(F,\,G)=\sup\{|E_{F}f(X)-E_{G}f(X)|:\ f\in{\cal L}\}\.\]

Show that \(F_{n}\stackrel{{ d}}{{\to}}F\) is equivalent to \(\lambda(F_{n},\,F)\to 0\). Thus, weak convergence on \({\rm I\kern-1.8ptR}^{k}\) is metrizable. [See examples 21-22 in Pollard (1984).]

**Problem 11.25**: For a c.d.f. \(F\) with quantile function defined by

\[F^{-1}(u)=\inf\{x:\ F(x)\geq u\}\,\]

show that: (i) \(F(x)\geq u\) is equivalent to \(F^{-1}(u)\leq x\).

(ii) \(F^{-1}(\cdot)\) is nondecreasing and left continuous with right-hand limits.

(iii) \(F(F^{-1}(u))\leq u\) with equality if \(F\) is continuous at \(F^{-1}(u)\).

**Problem 11.26**: (i) Construct a sequence of distribution functions \(\{F_{n}\}\) on the real line such that \(F_{n}\stackrel{{ d}}{{\to}}F\), but the convergence \(F_{n}^{-1}(1-\alpha)\to F^{-1}(1-\alpha)\) fails, even if \(F\) is assumed continuous. (ii) On the other hand, if \(F\) is assumed continuous (but not necessarily strictly increasing), show that

\[F_{n}(F_{n}^{-1}(1-\alpha))\to\,F(F^{-1}(1-\alpha))=1-\alpha\.\]

[Note the left side need not be \(1-\alpha\) since \(F_{n}\) is not assumed continuous.]

**Section 11.3**: **Problem 11.27**: (Markov's Inequality) Let \(X\) be a real-valued random variable with \(X\geq 0\). Show that, for any \(t>0\),

\[P\{X\geq t\}\leq\frac{E[XI\{X\geq t\}]}{t}\leq\frac{E(X)}{t}\ ;\]

here \(I(X\geq t)\) is the indicator variable that is \(1\) if \(X\geq t\) and is \(0\) otherwise.

**Problem 11.28**: (Chebyshev's Inequality) (i) Show that, for any real-valued random variable \(X\) and any constants \(a>0\) and \(c\),

\[E(X-c)^{2}\geq a^{2}P\{|X-c|\geq a\}\.\]

(ii) Hence, if \(X_{n}\) is any sequence of random variables and \(c\) is a constant such that \(E(X_{n}-c)^{2}\to 0\), then \(X_{n}\to c\) in probability. Give a counterexample to show the converse is false.

**Problem 11.29**: Give an example of an i.i.d. sequence of real-valued random variables such that the sample mean converges in probability to a finite constant, yet the mean of the sequence does not exist.

**Problem 11.30**: Prove the following generalization of Lemma 11.2.1. Suppose \(\{\hat{F}_{n}\}\) is a sequence of random distribution functions satisfying \(\hat{F}_{n}(x)\stackrel{{ P}}{{\to}}F(x)\) at all \(x\) which are continuity points of a fixed distribution function \(F\). Assume \(F\) is continuous and strictly increasing at \(F^{-1}(1-\alpha)\). Then,

\[\hat{F}_{n}^{-1}(1-\alpha)\stackrel{{ P}}{{\to}}F^{-1}(1-\alpha)\.\]

**Problem 11.31**: Prove a result analogous to Problem 11.26 if \(\{\hat{F}_{n}\}\) is a random sequence, similar to how Problem 11.30 is a generalization of Lemma 11.2.1.

**Problem 11.32**: Suppose \(X_{n}\) and \(X\) are real-valued random variables (defined on a common probability space). Prove that, if \(X_{n}\) converges to \(X\) in probability, then \(X_{n}\) converges in distribution to \(X\). Show by counterexample that the converse is false. However, show that if \(X\) is a constant with probability one, then \(X_{n}\) converging to \(X\) in distribution implies \(X_{n}\) converges to \(X\) in probability.

**Problem 11.33**: Suppose \(X_{n}\) is a sequence of random vectors.

(i) Show \(X_{n}\stackrel{{ P}}{{\to}}0\) if and only if \(|X_{n}|\stackrel{{ P}}{{\to}}0\) (where the first zero refers to the zero vector and the second to the real number zero).

(ii) Show that convergence in probability of \(X_{n}\) to \(X\) is equivalent to convergence in probability of their components to the respective components of \(X\).

**Problem 11.34**: Assume \(X_{n}\stackrel{{ d}}{{\to}}X\) and \(Y_{n}\stackrel{{ P}}{{\to}}c\), where \(c\) is a constant. Show that \((X_{n},Y_{n})\stackrel{{ d}}{{\to}}(X,c)\).

**Problem 11.35**: Generalize Slutsky's Theorem (Theorem 11.3.2) to the case where \(X_{n}\) is a vector, \(A_{n}\) is a matrix, and \(B_{n}\) is a vector.

**Problem 11.36**: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. real-valued random variables. Write \(X_{i}=X_{i}^{+}-X_{i}^{-}\), where \(X_{i}^{+}=\max(X_{i},0)\). Suppose \(X_{i}^{-}\) has a finite mean, but \(X_{i}^{+}\) does not. Let \(\tilde{X}_{n}\) be the sample mean. Show \(\tilde{X}_{n}\stackrel{{ P}}{{\to}}\infty\). _Hint:_ For \(B>0\), let \(Y_{i}=X_{i}\) if \(X_{i}\leq B\) and \(Y_{i}=B\) otherwise; apply the Weak Law to \(\tilde{Y}_{n}\).

**Problem 11.37**: (i) Let \(K(P_{0},P_{1})\) be the Kullback-Leibler Information, defined in (11.21). Show that \(K(P_{0},P_{1})\geq 0\) with equality iff \(P_{0}=P_{1}\).

(ii) Show the convergence (11.20) holds even when \(K(P_{0},P_{1})=\infty\). _Hint:_ Use Problem 11.36.

**Problem 11.38**: As in Example 11.3.1, consider the problem of testing \(P=P_{0}\) versus \(P=P_{1}\) based on \(n\) i.i.d. observations. The problem is an alternative way to show that a most powerful level \(\alpha\) (\(0<\alpha<1\)) test sequence has limiting power one. If\(P_{0}\) and \(P_{1}\) are distinct, there exists \(E\) such that \(P_{0}(E)\neq P_{1}(E)\). Let \(\hat{p}_{n}\) denote the proportion of observations in \(E\) and construct a level-\(\alpha\) test sequence based on \(\hat{p}_{n}\) which has power tending to one.

**Problem 11.39**: If \(X_{n}\) is a sequence of real-valued random variables, prove that \(X_{n}\to 0\) in \(P_{n}\)-probability if and only if \(E_{P_{n}}[\min(|X_{n}|,\,1)]\to 0\).

**Problem 11.40**: (i) Prove Corollary 11.3.1.

(ii) Suppose \(X_{n}\stackrel{{ d}}{{\to}}X\) and \(C_{n}\stackrel{{ P}}{{\to}}\infty\). Show \(P\{X_{n}\leq C_{n}\}\to 1\).

**Problem 11.41**: In Example 11.3.2, show that \(\beta_{n}(p_{n})\to 1\) if \(n^{1/2}(p_{n}-1/2)\to\infty\) and \(\beta_{n}(p_{n})\to\alpha\) if \(n^{1/2}(p_{n}-1/2)\to 0\).

**Problem 11.42**: In Example 11.3.4, let \(I_{n}\) be the interval (11.23). Show that, for any \(n\),

\[\inf_{p}P_{p}\{p\in\hat{I}_{n}\}=0\.\]

_Hint:_ Consider \(p\) positive but small enough so that the chance that a sample of size \(n\) results in \(0\) successes is nearly \(1\).

**Problem 11.43**: Show how the interval (11.25) is obtained from (11.24).

**Problem 11.44**: Prove Lemma 11.3.1

**Problem 11.45**: Show that tightness of a sequence of random vectors in \(\mathbb{R}^{k}\) is equivalent to each of the component variables being tight.

**Problem 11.46**: Suppose \(P_{n}\) is a sequence of probabilities and \(X_{n}\) is a sequence of real-valued random variables; the distribution of \(X_{n}\) under \(P_{n}\) is denoted \(\mathcal{L}(X_{n}|P_{n})\). Prove that \(\mathcal{L}(X_{n}|P_{n})\) is tight if and only if \(X_{n}/a_{n}\to 0\) in \(P_{n}\)-probability for every sequence \(a_{n}\uparrow\infty\).

**Problem 11.47**: Suppose \(X_{n}\stackrel{{ d}}{{\to}}N(\mu,\,\sigma^{2})\). (i). Show that, for any sequence of numbers \(c_{n}\), \(P(X_{n}=c_{n})\to 0\). (ii). If \(c_{n}\) is any sequence such that \(P(X_{n}>c_{n})\to\alpha\), then \(c_{n}\to\mu+\sigma z_{1-\alpha}\), where \(z_{1-\alpha}\) is the \(1-\alpha\)-quantile of \(N(0,\,1)\).

**Problem 11.48**: Let \(X_{1},\cdots,X_{n}\) be i.i.d. normal with mean \(\theta\) and variance \(1\). Suppose \(\hat{\theta}_{n}\) is a location equivariant sequence of estimators such that, for every fixed \(\theta\), \(n^{1/2}(\hat{\theta}_{n}-\theta)\) converges in distribution to the standard normal distribution (if \(\theta\) is true). Let \(\bar{X}_{n}\) be the usual sample mean. Show that, if \(\theta\) is fixed at the true value, then \(n^{1/2}(\hat{\theta}_{n}-\bar{X}_{n})\) tends to \(0\) in probability under \(\theta\).

**Problem 11.49**: Prove part (ii) of Theorem 11.3.4.

**Problem 11.50**: Suppose \(R\) is a real-valued function on \(\mathbb{R}^{k}\) with \(R(y)=o(|y|^{p})\) as \(|y|\to 0\), for some \(p>0\). If \(Y_{n}\) is a sequence of random vectors satisfying \(|Y_{n}|=o_{P}(1)\), then show \(R(Y_{n})=o_{P}(|Y_{n}|^{p})\). _Hint:_ Let \(g(y)=R(y)/|y|^{p}\) with \(g(0)=0\) so that \(g\) is continuous at \(0\); apply the Continuous Mapping Theorem.

**Problem 11.51**: Use Problem 11.50 to prove (11.28).

**Problem 11.52**: Assume \((U_{i},\,V_{i})\) is bivariate normal with correlation \(\rho\). Let \(\hat{\rho}_{n}\) denote the sample correlation given by (11.29). Verify the limit result (11.31).

**Problem 11.53**: Consider the setting of Problem 6.21, where \((X_{i},\,Y_{i})\) are independent \(N(\mu_{i},\,\sigma^{2})\) for \(i=1,\,\ldots,\,n\). The parameters \(\mu_{1},\,\ldots,\,\mu_{n}\) and \(\sigma^{2}\) are all unknown. For testing \(\sigma=1\) against \(\sigma>1\), determine the limiting power of the UMPI level-\(\alpha\) test against alternatives \(1+hn^{-1/2}\).

**Problem 11.54**: (i) If \(X_{1},\,\ldots,\,X_{n}\) is a sample from a Poisson distribution with mean \(E(X_{i})=\lambda\), then \(\sqrt{n}(\sqrt{X}-\sqrt{\lambda})\) tends in law to \(N(0,\,\frac{1}{4})\) as \(n\to\infty\).

(ii) If \(X\) has the binomial distribution \(b(p,\,n)\), then \(\sqrt{n}[\arcsin\sqrt{X/n}-\arcsin\sqrt{p}]\) tends in law to \(N(0,\,\frac{1}{4})\) as \(n\to\infty\).

_Note_. Certain refinements of variance stabilizing transformations are discussed by Anscombe (1948), Freeman and Tukey (1950), and Hotelling (1953). Transformations of data to achieve approximately a normal linear model are considered by Box and Cox (1964); for later developments stemming from this work see Bickel and Doksum (1981), Box and Cox (1982), and Hinkley and Runger (1984).

**Problem 11.55**: Suppose \((X_{1},\ldots,\,X_{k})\) is multinomial based on \(n\) trials and cell probabilities \((p_{1},\,\ldots,\,p_{k})\). Show that

\[\sqrt{n}\left[\,\sum_{j=1}^{k}\frac{X_{j}}{n}\,\log\left(\frac{X_{j}}{n}\right) -c\,\right]\]

converges in distribution to \(F\), for some constant \(c\) and distribution \(F\). Identify \(c\) and \(F\).

**Problem 11.56**: Suppose \(X_{i,\,j}\) are independently distributed as \(N(\mu_{i},\,\sigma_{i}^{2})\); \(i=1,\,\ldots,\,s\); \(j=1,\,\ldots,\,n_{i}\). Let \(S_{n,i}^{2}=\sum_{j}(X_{i,\,j}-\bar{X}_{i})^{2}\), where \(\bar{X}_{i}=n_{i}^{-1}\,\sum_{j}X_{i,j}\). Let \(Z_{n,i}=\log[S_{n,i}^{2}/(n_{i}-1)]\). Show that, as \(n_{i}\to\infty\),

\[\sqrt{n_{i}-1}[Z_{n,i}-\log(\sigma_{i}^{2})]\stackrel{{ d}}{{\to}}N(0,2)\;.\]

Thus, for large \(n_{i}\), the problem of testing equality of all the \(\sigma_{i}\) can be approximately viewed as testing equality of means of normally distributed variables with known (possibly different) variances. Use Problem 7.12 to suggest a test.

**Problem 11.57**: Let \(X_{1},\,\cdots,\,X_{n}\) be i.i.d. Poisson with mean \(\lambda\). Consider estimating \(g(\lambda)=e^{-\lambda}\) by the estimator \(T_{n}=e^{-\bar{X}_{n}}\). Find an approximation to the bias of \(T_{n}\); specifically, find a function \(b(\lambda)\) satisfying

\[E_{\lambda}(T_{n})=g(\lambda)+n^{-1}b(\lambda)+O(n^{-2})\]as \(n\to\infty\). Such an expression suggests a new estimator \(T_{n}-n^{-1}b(\lambda)\), which has bias \(O(n^{-2})\). But, \(b(\lambda)\) is unknown. Show that the estimator \(T_{n}-n^{-1}b(\bar{X}_{n})\) has bias \(O(n^{-2})\).

**Problem 11.58**: Let \(X_{1},\ldots,X_{n}\) be a random sample from the Poisson distribution with unknown mean \(\lambda\). The uniformly minimum variance unbiased estimator (UMVUE) of \(\exp(-\lambda)\) is known to be \([(n-1)/n]^{T_{n}}\), where \(T_{n}=\sum_{i=1}^{n}X_{i}\). Find the asymptotic distribution of the UMVUE (appropriately normalized). _Hint:_ It may be easier to first find the asymptotic distribution of \(\exp(-T_{n}/n)\).

**Problem 11.59**: Let \(X_{i,j}\), \(1\leq i\leq I\), \(1\leq j\leq n\) be independent with \(X_{i,j}\) Poisson with mean \(\lambda_{i}\). The problem is to test the null hypothesis that the \(\lambda_{i}\) are all the same versus they are not all the same. Consider the test that rejects the null hypothesis iff

\[T\equiv\frac{n\sum_{i=1}^{I}(\bar{X}_{i}-\bar{X})^{2}}{\bar{X}}\]

is large, where \(\bar{X}_{i}=\sum_{j}X_{i,j}/n\) and \(\bar{X}=\sum_{i}\bar{X}_{i}/I\).

(i) How large should the critical values be so that, if the null hypothesis is correct, the probability of rejecting the null hypothesis tends (as \(n\to\infty\) with \(I\) fixed) to the nominal level \(\alpha\).

(ii) Show that the test is pointwise consistent in power against any (\(\lambda_{1}\),..., \(\lambda_{I}\)), as long as the \(\lambda_{i}\) are not all equal.

**Problem 11.60**: Assume \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(0,\sigma^{2})\). Let \(\hat{\sigma}_{n}^{2}\) be the maximum likelihood estimator of \(\sigma^{2}\) given by \(\hat{\sigma}_{n}^{2}=\sum_{i=1}^{n}X_{i}^{2}/n\).

(i) Find the limiting distribution of \(\sqrt{n}(\hat{\sigma}_{n}-\sigma)\).

(ii) For a constant \(c\), let \(T_{n,c}=c\sum_{i=1}^{n}|X_{i}|/n\). For what constant \(c\) is \(T_{n,c}\) a consistent estimator of \(\sigma\)?

(iii) Determine the limiting distribution of \(\sqrt{n}(T_{n,c}-\sigma)\) with \(c\) chosen as your consistent estimator.

(iv) Determine the limiting distribution of \(\sqrt{n}\log(\hat{\sigma}_{n}/T_{n,c})\) (again with \(c\) chosen from (ii) above).

**Problem 11.61**: Suppose \(X_{1},\ldots,X_{I}\) are independent and binomially distributed, with \(X_{i}\sim b(n_{i},\,p_{i})\); that is, \(X_{i}\) is the number of successes in \(n_{i}\) Bernoulli trials. Suppose that \(p_{i}\) satisfies

\[\log[p_{i}/(1-p_{i})]=\theta d_{i}\]

for known constants \(d_{i}\), which implies

\[p_{i}=\frac{e^{d_{i}\theta}}{1+e^{d_{i}\theta}}\]

(Think of \(d_{i}\) as the dose given to \(n_{i}\) subjects, and you observe \(X_{i}\) deaths at the dosage level \(d_{i}\).) Both \(d_{i}\) and \(n_{i}\) are known.

(i) For testing the null hypothesis \(\theta=0\) against \(\theta=1\), find the form of the most powerful level-\(\alpha\) test and show that it rejects for large values of a test statistic \(T\).

(ii) If the null hypothesis is true and the sample sizes \(n_{i}\) are moderately large, what is the approximate distribution of \(T\)?

(iii) If \(I=5\), \(d_{i}=i\) and \(n_{i}=100\), approximate the \(p\)-value of your test if you observe \(X_{1}=40\), \(X_{2}=51\), \(X_{3}=64\), \(X_{4}=73\) and \(X_{5}=80\).

#### _Section 11.4_

**Problem 11.62**: Prove the Glivenko-Cantelli Theorem. _Hint:_ Use the Strong Law of Large Numbers and the monotonicity of \(F\).

**Problem 11.63**: Let \(X_{1}\),..., \(X_{n}\) be i.i.d. \(P\) on \(S\). Suppose \(S\) is countable and let \(\mathcal{E}\) be the collection of _all_ subsets of \(S\). Let \(\hat{P}_{n}\) be the _empirical measure_; that is, for any subset \(E\) of \(\mathcal{E}\), \(\hat{P}_{n}(E)\) is the proportion of observations \(X_{i}\) that fall in \(E\). Prove, with probability one,

\[\sup_{E\in\mathcal{E}}|\hat{P}_{n}(E)-P(E)|\to 0\.\]

**Problem 11.64**: Suppose \(X_{n}\) is a tight sequence and \(Y_{n}\stackrel{{ P}}{{\to}}0\). Show that \(X_{n}Y_{n}\stackrel{{ P}}{{\to}}0\). If it is assumed \(Y_{n}\to 0\) almost surely, can you conclude \(X_{n}Y_{n}\to 0\) almost surely?

**Problem 11.65**: Suppose \(X_{n}\) is a sequence of real-valued random variables.

(i) Assume \(X_{n}\) is Cauchy in probability; that is, for all \(\epsilon>0\),

\[\lim_{\min(m,n)\to\infty}P\{|X_{n}-X_{m}|>\epsilon\}\to 0\.\]

Then, show there exists a random variable \(X\) such that \(X_{n}\stackrel{{ P}}{{\to}}X\), in which case we may write \(X=\lim_{n\to\infty}X_{n}\).

(ii) Assume \(X_{n}\) satisfies \(E(|X_{n}|^{p})<\infty\). Also, assume \(X_{n}\) is Cauchy in \(L_{p}\); that is,

\[\lim_{\min(m,n)\to\infty}E(|X_{n}-X_{m}|^{p})\to 0\.\]

Then, show there exist a random variable \(X\) such that \(E(|X_{n}-X|^{p})\to 0\) and \(E(|X|^{p})<\infty\).

**Problem 11.66**: For a c.d.f. \(F\), define the quantile transformation \(Q\) by

\[Q(u)=\inf\{t:\ F(t)\geq u\}\.\]

(i) Show the event \(\{F(t)\geq u\}\) is the same as \(\{Q(u)\leq t\}\).

(ii) If \(U\) is uniformly distributed on (0, 1), show the distribution of \(Q(U)\) is \(F\).

[MISSING_PAGE_FAIL:570]

[MISSING_PAGE_FAIL:571]

**Problem 11.78**: Assume \(X_{1},\ldots,X_{n}\) are i.i.d. with \(E(|X_{i}|^{p}]<\infty.\) Then, show that

\[n^{-\frac{1}{p}}\max_{1\leq i\leq n}|X_{i}|\stackrel{{ P}}{{ \rightarrow}}0\.\]

**Problem 11.79**: (i) Suppose \(X_{n}\stackrel{{ d}}{{\rightarrow}}X\) and \(Var(X_{n})\to Var(X)<\infty.\) Show \(E(X_{n})\to E(X).\)

(ii) Suppose \((X_{n},Y_{n})\stackrel{{ d}}{{\rightarrow}}(X,Y)\) in the plane, with \(Var(X_{n})\to Var(X)<\infty\) and \(Var(Y_{n})\to Var(Y)<\infty.\) Show that \(Cov(X_{n},Y_{n})\to Cov(X,Y).\)

### Notes

The convergence concepts in this chapter are classical and can be found in most graduate probability texts such as Billingsley (1995) or Dudley (1989). The Central Limit Theory for Bernoulli trials dates back to de Moivre (1733) and for more general distributions to Laplace (1812). Their treatment was probabilistic and did not involve problems in inference. Normal experiments were first treated in Gauss (1809). Further history is provided in Stigler (1986) and Hald (1990, 1998).

## Chapter 12 Extensions of the CLT to Sums of Dependent Random Variables

### 12.1 Introduction

In this chapter, we consider some extensions of the Central Limit Theorem to classes of sums (or averages) of dependent random variables. Many further extensions are possible, but we focus on ones that will be useful in the sequel. Section 12.2 considers sampling without replacement from a finite population. As an application, the potential outcomes framework is introduced in order to study treatment effects. The class of \(U\)-statistics is studied in Section 12.3, with applications to the classical one-sample signed-rank statistic and the two-sample Wilcoxon rank-sum statistic. Section 12.4 considers CLTs for stationary, mixing sequences, which provides a basis for understanding robustness of procedures under dependence, as in Section 13.2.2. These three sections may be read independently of each other. A very general approach is provided by Stein's method in Section 12.5.

### 12.2 Random Sampling Without Replacement

from a Finite Population

Let \(\Pi_{N}=\{x_{N,1},\ldots,x_{N,N}\}\) denote a population consisting of \(N\) real-valued units. Let \(X_{1}\),..., \(X_{n}\) be a random sample taken without replacement from \(\Pi_{N}\) (so \(n\leq N\)), and let \(\bar{X}_{n}=\sum_{i=1}^{n}X_{i}/n\) be the sample mean. Let \(\bar{x}_{N}=\sum_{j=1}^{N}x_{N,i}/N\) be the mean of population \(\Pi_{N}\) and let

\[s_{N}^{2}=\frac{1}{N}\sum_{j=1}^{N}(x_{N,j}-\bar{x}_{N})^{2} \tag{12.1}\]

denote its variance, so that \(Var(X_{i})=s_{N}^{2}\) for simple random sampling. Then, it is easy to check (Problem 12.1) that\[E(\bar{X}_{n})=\bar{x}_{N} \tag{12.2}\]

and

\[Var(\bar{X}_{n})=\frac{s_{N}^{2}}{n}\cdot\frac{N-n}{N-1}. \tag{12.3}\]

Under certain conditions where both \(n\) and \(N\) tend to infinity, one might expect that \((\bar{X}_{n}-\bar{x}_{N})/\sqrt{Var(\bar{X}_{n})}\) is asymptotically \(N(0,1)\). The following theorem gives a sufficient condition.

**Theorem 12.2.1**: _Under the above setup, assume_

\[\frac{1}{\min(n,\,N-n)}\cdot\frac{\max_{1\leq j\leq N}(x_{N,\,j}-\bar{x}_{N})^ {2}}{s_{N}^{2}}\to 0. \tag{12.4}\]

_Then, as \(N\to\infty\),_

\[\frac{(\bar{X}_{n}-\bar{x}_{N})}{\sqrt{Var(\bar{X}_{n})}}\stackrel{{ d}}{{\to}}N(0,1). \tag{12.5}\]

**Example 12.2.1** (Two-Sample Wilcoxon Rank-Sum Test): Consider the case where \(x_{N,j}=j\). In other words, \(\bar{X}_{n}\) is the average of \(n\) numbers taken without replacement from \(\{1,\,\ldots,\,N\}\). Then \(\bar{x}_{N}=(N+1)/2\) and

\[s_{N}^{2}=\frac{N^{2}-1}{12}. \tag{12.6}\]

Therefore, (12.3) reduces to

\[Var(\bar{X}_{n})=\frac{(N+1)(N-n)}{12n}. \tag{12.7}\]

Also,

\[\max_{1\leq j\leq N}\left(j-\frac{N+1}{2}\right)^{2}=\frac{(N-1)^{2}}{4}\]

and so the left side of (12.4) reduces to

\[\frac{1}{\min(n,\,N-n)}\cdot\frac{3(N-1)^{2}}{N^{2}-1}\to 0\.\]

If \(\min(n,\,N-n)\to\infty\), then (12.5) follows.

In a statistical context, assume that \(Y_{1},\,\ldots,\,Y_{m}\) are i.i.d. \(F\) and, independently, \(Z_{1},\,\ldots,\,Z_{n}\) are i.i.d. \(G\). Let \(N=m+n\). Under the null hypothesis \(F=G\) and the assumption that \(F\) is continuous, the \(N\choose n\) assignment of ranks are all equally likely. The Wilcoxon statistic, \(W_{n}\), denotes the sum of the ranks of the \(Y_{i}\)s. So, the distribution of \(W_{n}\) under \(F=G\) is that of the sum of \(n\) numbers taken at random from \(\{1,\ldots,N\}\), or \(W_{n}=n\bar{X}_{n}\) in the statement of Theorem 12.2.1. Therefore, we conclude

\[\frac{W_{n}-\frac{1}{2}n(N+1)}{\sqrt{mn(N+1)/12}}\stackrel{{ d}}{{\to}}N(0,1)\.\]

The necessary and sufficient conditions for (12.5) were obtained from Hajek (1960), stated next.

**Theorem 12.2.2**: _Under the above setup, let_

\[\delta_{N,j}=(x_{N,j}-\bar{x}_{N})/s_{N}\.\]

_Assume \(\min(n,\,N-n)\to\infty\). Then, (12.5) holds if and only if, for every \(\epsilon>0\),_

\[\frac{1}{N}\ \sum_{j:\ |\delta_{N,j}|>\epsilon\sqrt{n(N-n)/N}}\delta_{N,j}^{2}\,\to\,0. \tag{12.8}\]

Proof. We only prove the sufficiency part. Without loss of generality, assume \(\bar{x}_{N}=0\). Write \(\bar{X}_{n}=\sum_{j=1}^{N}H_{j}x_{N,j}/n\), where \((H_{1},\,\ldots,\,H_{N})\) are indicator variables, with \(H_{j}=1\) if and only if item \(j\) is chosen in the sample (so that \(\sum_{j}H_{j}=n\)). Let \(I_{1},\ldots,I_{N}\) be i.i.d. Bernoulli variables with success probability \(n/N\). Let \(\bar{X}_{n}=\sum_{j=1}^{N}I_{j}x_{N,j}/n\). Note that \(\bar{X}_{n}\) is an average of independent random variables, whose limiting distribution (after normalization) can be obtained by the Lindeberg Central Limit Theorem. Indeed, (12.5) holds with \(\bar{X}_{n}\) replaced by \(\bar{X}_{n}\). We will show that \(\bar{X}_{n}\) and \(\bar{X}_{n}\) have the same limiting distributions. We will apply Lemma 11.3.1 to \(S_{n}=n\bar{X}_{n}\) and \(\bar{S}_{n}=n\bar{S}_{n}\), and therefore the result follows once we show that

\[\frac{E[(\bar{S}_{n}-S_{n})^{2}]}{Var(\bar{S}_{n})}\to\,0. \tag{12.9}\]

To do this, we will first construct \(I=(I_{1},\,\ldots,\,I_{N})\) and then construct \(H=(H_{1},\,\ldots,\,H_{N})\) so that \(H\) is uniform over all vectors of length \(N\) with exactly \(n\) ones and \(N-n\) zeroes in which case \(H\) and \(I\) are appropriately close (or "coupled").

First, let \(B_{N}=\sum_{j=1}^{N}I_{j}\), which has the binomial distribution with parameters \(N\) and \(n/N\). If \(B_{N}=n\), just take \(H=I\). If \(B_{N}<n\), then let \(H_{j}=1\) whenever \(I_{j}=1\), which generates \(B_{N}\) out of the required \(n\) observations in the sample. Then, choose \(n-B_{N}\) remaining indices at random without replacement among the remaining \(N-B_{N}\) observations, and set \(H_{j}=1\) for those chosen indices. Similarly, if \(B_{N}>n\), then there are too many \(j\) for which \(I_{j}=1\), so choose a subset of size \(n\) from \(B_{N}\) randomly without replacement.

Next, note that if \(B_{N}>n\), then \[\tilde{S}_{n}-S_{n}=\sum_{j=1}^{N}x_{N,j}I\{I_{j}=1,\,H_{j}=0\}\]

(because the terms \(x_{N,j}\) for which \(I_{j}=H_{j}=1\) cancel and we cannot have \(H_{j}=1\) and \(I_{j}=0\) if \(B_{N}>n\)). Conditional on a value of \(B_{N}>n\), \(\tilde{S}_{n}-S_{n}\) is a sum of \(B_{N}-n\) observations taken without replacement from the \(x_{N,j}s\), and hence has mean \(0\). Moreover, using (12.3) with \(n\) replaced by \(B_{n}-n\),

\[Var(\tilde{S}_{n}-S_{n}|B_{N})=(B_{N}-n)s_{N}^{2}\cdot\frac{N-B_{N}+n}{N-1}\leq |B_{N}-n|s_{N}^{2}\,\]

if \(B_{n}>n\).

Similarly, if \(B_{N}<n\), then \(\tilde{S}_{n}\) is a sum of \(B_{N}\) variables and we need \(n-B_{N}\) more observations to construct the sample of size \(n\). So, \(S_{n}-\tilde{S}_{n}\) is a sum of \(n-B_{N}\) variables, yielding the same bound as in (12.10). Since the bound is clearly true when \(B_{N}=n\), (12.10) holds for all \(B_{N}\).

Therefore, we can conclude that

\[E[(\tilde{S}_{n}-S_{n})^{2}]=E[Var(\tilde{S}_{n}-S_{n})|B_{N}]\leq s_{N}^{2}E( |B_{n}-n|)\]

\[\leq s_{N}^{2}\sqrt{Var(B_{N})}\leq s_{N}^{2}\sqrt{n(1-\frac{n}{N})}\.\]

(Note that if \(\tilde{S}_{n}\) and \(S_{n}\) were independent, this term would be order \(n\), not \(\sqrt{n}\), a consequence of the coupling.) Since

\[Var(\tilde{S}_{n})=s_{N}^{2}n(1-\frac{n}{N})\,\]

the left side of (12.9) is \(1/\tau_{N}\), where

\[\tau_{N}=\sqrt{n(1-\frac{n}{N})}\.\]

But, \(\tau_{N}\to\infty\) as \(\min(n,\,N-n)\to\infty\) (Problem 12.5), as required.

In the above results, the \(x_{N,j}\) are fixed, but in the study of permutation tests later on, they may sometimes be considered as outcomes of random variables. One might then apply Theorems 12.2.1 and 12.2.2 conditional on the outcomes. For this, we develop a simple and perhaps more intuitive sufficient condition. First, let \(\Delta_{N}\) denote a random variable which is uniform on the \(N\) standardized values \((x_{N,j}-\tilde{x}_{N})/s_{N}\), with its distribution denoted by \(G_{N}\). (Note here and below that ties are allowed and \(G_{N}\) is just the distribution of \(\Delta_{N}\).) In an asymptotic framework where the \(x_{N,j}\) are fixed, we may nevertheless envision them settling down in such a way that \(G_{N}\) is getting close to some \(G\).

[MISSING_PAGE_EMPTY:2179]

\[s_{N,u}^{2}\to\sigma_{u}^{2}\quad\text{and}\quad s_{N,v}^{2}\to\sigma_{v}^{2}\.\]

_Finally, assume \(F_{N}\overset{d}{\to}F\), where \(F\) is a bivariate distribution with marginal variances \(\sigma_{u}^{2}\) and \(\sigma_{v}^{2}\). Then, \(s_{N,uv}\) converges to a limit \(\sigma_{uv}\) (which is the covariance of the bivariate distribution \(F\)) and_

\[\sqrt{n}(\bar{U}_{n}-u_{N},\,\bar{V}_{n}-v_{N})^{\top}\overset{d}{\to}N(0,(1-p)\Sigma)\, \tag{12.13}\]

_where \(\Sigma\) is the covariance matrix_

\[\Sigma=\begin{bmatrix}\sigma_{u}^{2}&\sigma_{uv}\\ \sigma_{uv}&\sigma_{v}^{2}\end{bmatrix}.\]

Proof. Since Theorem 12.2.3 applies, (12.5) holds. But

\[n\,Var(\bar{X}_{n})=s_{N}^{2}\cdot\frac{N-n}{N-1}\to\sigma^{2}(1-p)\,\]

and (i) follows. To prove (ii), note the fact that \(s_{N,uv}\to\sigma_{uv}\), where \(\sigma_{uv}\) is the covariance of \(F\), follows by Problem 11.79. The bivariate asymptotic normality follows by using (i) together with the Cramer-Wold Device.

**Example 12.2.2**: **(Potential Outcomes and Treatment Effects)** _Consider an experiment where \(Y_{j}\) is the observed outcome of interest for unit \(j\), for \(j=1,\ldots,\)\(N\). Some units are treated while others serve as controls, and the outcomes could potentially vary under the two scenarios. Denote by \(Y_{j}(1)\) the potential outcome for unit \(j\) if treated and \(Y_{j}(0)\) the potential outcome for unit \(j\) if untreated. Let \(D_{j}\) denote an indicator variable that is 1 if unit \(j\) is treated and 0 if not. The observed outcomes can be expressed in terms of the potential outcomes and treatment assignments by the relationship_

\[Y_{j}=Y_{j}(1)D_{j}+Y_{j}(0)(1-D_{j}). \tag{12.14}\]

_It is assumed that the \(Y_{j}\) and \(D_{j}\) are observed, so that \(Y_{j}(1)\) is observed if \(D_{j}=1\) but \(Y_{j}(0)\) is observed if \(D_{j}=0\). Such a framework dates back to Neyman (1923) and is expanded upon by Rubin (1974)._

_So far, nothing has been assumed about the distribution of the variables introduced, observed or not. We now consider a specialized setting where the potential outcomes are nonrandom, and the randomness comes entirely from the treatment assignment vector \((D_{1},\ldots,D_{N})\). We will assume a fixed number \(n\) is allocated to treatment, with the remaining allocated to control, so that all \({N\choose n}\) combinations of treatment vectors are equally likely. The object of interest is the population average causal effect, \(\theta_{N}\), defined by_\[\theta_{N}=\frac{1}{N}\sum_{j=1}^{N}[Y_{j}(1)-Y_{j}(0)]=\bar{Y}_{N}(1)-\bar{Y}_{N}( 0)\, \tag{12.15}\]

where

\[\bar{Y}_{N}(k)=\frac{1}{N}\sum_{j=1}^{N}Y_{j}(k)\ \ \ \mbox{for}\ k=0,1\.\]

The usual unbiased estimator of the average treatment effect, \(\theta_{N}\), is then

\[\hat{\theta}_{N}=\frac{1}{n}\sum_{j=1}^{N}Y_{j}(1)D_{j}-\frac{1}{N-n}\sum_{j=1} ^{N}Y_{j}(0)(1-D_{j}). \tag{12.16}\]

We would like to determine the limiting distribution of \(\hat{\theta}_{N}\) as \(N\rightarrow\infty\). Assume \(n/N\to p\in(0,1)\). Let the population variances and covariance be denoted by

\[s_{N}^{2}(k)=\frac{1}{N}\sum_{j=1}^{N}[Y_{j}(k)-\bar{Y}_{N}(k)]^{2}\ \ \ \mbox{for}\ k=0,1\]

and

\[s_{N}(0,1)=\frac{1}{N}\sum_{j=1}^{N}[(Y_{j}(1)-\bar{Y}_{N}(1))(Y_{j}(0)-\bar{Y }_{N}(0))]\.\]

Assume \(s_{N}^{2}(k)\to s^{2}(k)\) and \(s_{N}(0,1)\to s(0,1)\). Also, let

\[\tau_{N}^{2}=\frac{1}{N}\sum_{j=1}^{N}[(Y_{j}(1)-Y_{j}(0))-\theta_{N}]^{2}. \tag{12.17}\]

Simple algebra yields

\[\tau_{N}^{2}=s_{N}^{2}(1)+s_{N}^{2}(0)-2s_{N}(0,1)\]

and so

\[\tau_{N}^{2}\rightarrow\tau^{2}=s^{2}(1)+s^{2}(0)-2s(0,1)\.\]

Apply Corollary 12.2.1(ii) by taking \(u_{N,j}=Y_{j}(1)\) and \(v_{N,j}=Y_{j}(0)\). Then, let \(F_{N}\) denote the (empirical) distribution of the values \(\{Y_{j}(1),\,Y_{j}(0)\}\). Assume \(F_{N}\) converges in distribution to \(F\) with covariance matrix \(\Sigma\), and the diagonal elements of the covariance matrix of \(F_{N}\), say \(\Sigma_{N}\), converge to those of \(\Sigma\). So, \(\Sigma\) has diagonal elements \(s^{2}(1)\) and \(s^{2}(0)\) with off-diagonal elements \(s(0,1)\). Then, Corollary 12.2.1(ii) yields that \[\sqrt{n}\left[\frac{1}{n}(\sum_{j=1}^{N}Y_{j}(1)D_{j},\sum_{j=1}^{N}Y_{ j}(0)D_{j})-(\vec{Y}_{N}(1),\,\vec{Y}_{N}(0))\right]\] \[\stackrel{{ d}}{{\to}}N(0,(1-p)\Sigma). \tag{12.18}\]

Let the \(n\) units sampled correspond to those who receive treatment, and those not sampled the controls. Then, \(\hat{\theta}_{N}\) can be expressed as

\[\frac{1}{n}\sum_{j=1}^{N}Y_{j}(1)D_{j}+\frac{n}{N-n}\frac{1}{n}\sum_{j=1}^{N}Y _{j}(0)D_{j}-\frac{1}{N-n}\sum_{j=1}^{N}Y_{j}(0)\.\]

Let \((U,V)\) denote a bivariate normal variable with mean vector \(0\) and covariance matrix \((1-p)\Sigma\). Apply the continuous mapping theorem, noting \(n/(N-n)\to p/(1-p)\), to get

\[\sqrt{n}(\hat{\theta}_{N}-\theta_{N})\stackrel{{ d}}{{\to}}U+ \frac{p}{1-p}V\.\]

All that remains is to calculate the variance on the right-hand side. But,

\[Var(U+\frac{p}{1-p}V) = Var(U)+\frac{p^{2}}{(1-p)^{2}}Var(V)+\frac{2p}{1-p}Cov(U,V)\] \[= (1-p)s^{2}(1)+\frac{p^{2}}{1-p}s^{2}(0)+2ps(0,1)\] \[= (1-p)s^{2}(1)+\frac{p^{2}}{1-p}s^{2}(0)+p[s^{2}(1)+s^{2}(0)-\tau^ {2}]\] \[= s^{2}(1)+\frac{p}{1-p}s^{2}(0)-p\tau^{2}\.\]

Therefore,

\[\sqrt{n}(\hat{\theta}_{N}-\theta_{N})\stackrel{{ d}}{{\to}}N\left(0,s^{2}(1)+\frac{p}{1-p}s^{2}(0)-p\tau^{2}\right)\,\]

(which actually holds even if \(p=0\) as long as \(n\to\infty\)) or equivalently

\[\sqrt{N}(\hat{\theta}_{N}-\theta_{N})\stackrel{{ d}}{{\to}}N \left(0,\frac{s^{2}(1)}{p}+\frac{s^{2}(0)}{1-p}-\tau^{2}\right). \tag{12.19}\]

The above result is summarized in the following theorem.

**Theorem 12.2.4**: _Consider the above setup, where \(F_{N}\) is the distribution of the potential outcomes_

\[\{(Y_{j}(1),\,Y_{j}(0)),\,j=1,\ldots,\,N\},\]

_and \(F_{N}\) has covariance matrix \(\Sigma_{N}\). Suppose \(n\) units are treated, where \(n/N\to p\in(0,1)\). Assume \(F_{N}\stackrel{{ d}}{{\to}}F\), where the diagonal elements of \(\Sigma_{N}\) converge to those of \(\Sigma\), where \(\Sigma\) is the covariance matrix of \(F\). Then, the average treatment effect \(\hat{\theta}_{N}\) defined in (12.16) satisfies (12.19)._

Note that the limiting distribution depends on the limiting value of \(\tau_{N}^{2}\) defined in (12.17). The individual causal effects \(Y_{j}(1)-Y_{j}(0)\), nor the average of their squares, cannot be estimated without further assumptions. Let us consider the implications of Theorem 12.2.4 for inference. First, consider Fisher's "sharp" null hypothesis \(H_{F}\) specified by

\[H_{F}:\ Y_{j}(1)=Y_{j}(0)\,\ \ \ \ j=1,\ldots,N\.\]

One can construct an exact level-\(\alpha\) test by calculating a permutation test based on \(\hat{\theta}_{N}\). That is, consider the permutation distribution defined as the empirical distribution of \(\hat{\theta}_{N}\) recomputed over all \({N\choose n}\) treatment assignments. Theorem 12.2.4 gives its precise limiting behavior under \(H_{F}\), where \(\tau=0\) and \(s_{1}=s_{0}\). Alternatively, one can apply a normal approximation. Under \(H_{F}\), \(\tau^{2}=0\) and the variance in the limiting distribution (12.19) simplifies, and only depends on \(s^{2}(1)\) and \(s^{2}(0)\). But, \(s^{2}(1)\) can be estimated consistently (Problem 12.12) by

\[\hat{s}_{N}^{2}(1)=\frac{1}{n}\sum_{j=1}^{N}Y_{j}^{2}(1)D_{i}-\left[\frac{1}{ n}\sum_{j=1}^{N}Y_{j}(1)D_{i}\right]^{2}\, \tag{12.20}\]

and similarly for \(\hat{s}_{N}^{2}(0)\). Define \(\hat{\sigma}_{N}^{2}\) by

\[\hat{\sigma}_{N}^{2}=\frac{N\hat{s}_{N}^{2}(1)}{n}+\frac{N\hat{s}_{N}^{2}(0)}{ N-n}\xrightarrow{P}\frac{s^{2}(1)}{p}+\frac{s^{2}(0)}{1-p}\.\]

Then, the one-sided test that rejects \(H_{F}\) if \(\sqrt{N}\hat{\theta}_{N}>\hat{\sigma}_{N}z_{1-\alpha}\) has limiting rejection probability equal to \(\alpha\) under \(H_{F}\). (Actually, Fisher proposed an alternative estimator of variance; see Ding (2017).)

On the other hand, consider Neyman's "weak" null hypothesis \(H_{N}\) specified by \(\theta_{N}=0\). In this case, Theorem 12.2.4 applies, but the limiting variance cannot be estimated consistently due to the presence of \(\tau^{2}\). Here, the one-sided test which rejects \(H_{N}\) if \(\sqrt{N}\hat{\theta}_{N}>\hat{\sigma}_{N}z_{1-\alpha}\) has limiting rejection probability under \(H_{N}\) which is \(\leq\alpha\) and is \(<\alpha\) if \(\tau>0\). Thus, this approach, while valid, is conservative. Note, however, that improvements are possible by bounding \(\tau^{2}\), subject to constraints of the marginal distributions; see Aronow et al. (2015).

We can extend Theorem 12.2.4 to the setting where there is randomness, not only from treatment assignment, but also due to sampling. That is, assume there are \(N\) units, with \(N_{0}\) of them sampled at random without replacement. Among the \(N_{0}\) units in the experiment, a random sample of \(n\) are treated and \(N_{0}-n\) serve as controls.

**Theorem 12.2.5**: _Consider a population of size \(N\). Let \(F_{N}\) be the distribution of the potential outcomes_

\[\{(Y_{j}(1),\,Y_{j}(0)),\,j=1,\ldots,N\},\]

_and let \(F_{N}\) have covariance matrix \(\Sigma_{N}\). Assume \(F_{N}\xrightarrow{d}F\), where the diagonal elements of \(\Sigma_{N}\) converge to those of \(\Sigma\), where \(\Sigma\) is the covariance matrix of \(F\) and \(\Sigma\) has diagonal elements \(s^{2}(1)\) and \(s^{2}(0)\). Sample \(N_{0}\) without replacement, and then assign \(n\) at random to be treated and \(N_{0}-n\) to serve as controls. Assume \(N_{0}/N\to f\in[0,\,1]\) and \(n/N_{0}\to p\in(0,\,1)\). Let \(\hat{\theta}_{N_{0}}\) denote the estimated average treatment effect based on the \(N_{0}\) units sampled, and let \(\theta_{N}\) denote the population average treatment effect (for all \(N\) items). Then,_

\[\sqrt{N_{0}}(\hat{\theta}_{N_{0}}-\theta_{N})\stackrel{{ d}}{{ \to}}N\left(0,\,\frac{s^{2}(1)}{p}+\frac{s^{2}(0)}{1-p}-f\tau^{2}\right)\;, \tag{12.21}\]

_where \(\tau^{2}\) is the limit of \(\tau_{N}^{2}\) defined in (12.17)._

Proof. Let \(S_{j}=1\) if item \(j\) is one of the \(N_{0}\) sampled and \(0\) otherwise. Let \(\tilde{\theta}_{N_{0}}\) denote the average treatment effect for the \(N_{0}\) items sampled; that is,

\[\tilde{\theta}_{N_{0}}=\frac{1}{N_{0}}\sum_{j=1}^{N}[Y_{j}(1)-Y_{j}(0)]S_{j}\;.\]

Write

\[\sqrt{N_{0}}(\hat{\theta}_{N_{0}}-\theta_{N})=A_{N}+B_{N}\;,\]

where

\[A_{N}=\sqrt{N_{0}}(\hat{\theta}_{N_{0}}-\tilde{\theta}_{N_{0}})\]

and

\[B_{N}=\sqrt{N_{0}}(\tilde{\theta}_{N_{0}}-\theta_{N})\;.\]

By Theorem 12.2.3 applied to \(x_{N,\,j}=Y_{j}(1)-Y_{j}(0)\), \(j=1\),..., \(N\) and replacing \(n\) there with \(N_{0}\), it follows that

\[B_{n}\stackrel{{ d}}{{\to}}N(0,\,(1-f)\tau^{2})\;.\]

But, conditional on the \(N_{0}\) items sampled, we can apply Theorem 12.2.4 to conclude that (Problem 12.13)

\[A_{N}\stackrel{{ d}}{{\to}}N\left(0,\,\frac{s^{2}(1)}{p}+\frac{s^ {2}(0)}{1-p}-\tau^{2}\right)\;. \tag{12.22}\]

Note that, conditional on the \(N_{0}\) items sampled, \(A_{N}\) and \(B_{N}\) are conditionally independent (since \(A_{N}\) is not even random). Therefore, we can apply Problem 11.73(i) to complete the proof by adding the limiting variances of \(A_{N}\) and \(B_{N}\) to get (12.21).

As expected, when \(N_{0}\) is small relative to \(N\), so that \(f=\)0, the limiting variance no longer depends on \(\tau^{2}\) and one can construct tests for Neyman's hypothesis that are no longer conservative.

### 12.3 U-Statistics

We begin by considering the one-sample case. Assume \(X_{1}\),..., \(X_{n}\) are i.i.d. \(P\) on some general space. Suppose interest focuses on a real-valued parameter of the form

\[\theta(P)=E[h(X_{1},\ldots,X_{b})]\]

for some function \(h(\cdot)\). The function \(h(\cdot)\) is called the kernel of the \(U\)-statistic. It is assumed, without loss of generality, that \(h(\cdot)\) is symmetric in its arguments. If it were not, it could be replaced by the average of \(h\) computed over all permutations of \(X_{1}\),..., \(X_{b}\). We will also generally assume that

\[E[h^{2}(X_{1},\ldots,X_{b})]<\infty\.\]

The corresponding \(U\)-statistic is defined (for \(n\geq b\)) by

\[U_{n}=\frac{1}{{n\choose b}}\sum_{c}h(X_{i_{1}},\ldots,X_{i_{b}})\,\]

where \(\sum_{c}\) denotes summation over the \({n\choose b}\) combinations of \(b\)-tuples \(\{i_{1},\ldots,i_{b}\}\) consisting of \(b\) distinct elements from \(\{1,\ldots,n\}\). Of course, \(U_{n}\) is an unbiased estimator of \(\theta(P)\). Notice that \(U_{n}\) is an average of identically distributed random variables, but the terms are independent only in the case \(b=1\). The goal will be to approximate the distribution of \(U_{n}\), but first we consider some examples.

**Example 12.3.1** (**Averages**) If \(b=1\), \(U_{n}=\sum_{i=1}^{n}h(X_{i})/n\) is indeed an average of i.i.d. random variables, and so \(p\)th sample moments are a special case with \(h(x)=x^{\,p}\). Also, fixing \(t\) and letting \(h(x)=I\{x\leq t\}\) yields the empirical c.d.f. evaluated at \(t\).

**Example 12.3.2** (**Sample Variance**) Consider the kernel

\[h(x_{1},x_{2})=\frac{1}{2}(x_{1}-x_{2})^{2}\.\]

Let \(\sigma^{2}(P)=Var(X_{i})\), assumed finite. Then,

\[\theta(P)=E[\frac{1}{2}(X_{1}-X_{2})^{2}]=\frac{1}{2}[Var(X_{1})+Var(X_{2})]= \sigma^{2}(P)\.\]

Letting \(\bar{X}_{n}=\sum_{i=1}^{n}X_{i}/n\), the corresponding \(U\)-statistic is

\[U_{n}=\frac{1}{2{n\choose 2}}\sum_{i<j}(X_{i}-X_{j})^{2}=\frac{1}{2n(n-1)} \sum_{\text{all }i,j}(X_{i}-X_{j})^{2}=\]\[\frac{1}{2n(n-1)}\sum_{\text{all }i,j}(X_{i}^{2}-2X_{i}X_{j}+X_{j}^{2})=\frac{1}{2n(n-1)} \sum_{i=1}^{n}(2nX_{i}^{2}-2n^{2}\bar{X}_{n}^{2})=\]

\[\frac{1}{n-1}(\sum_{i=1}^{n}X_{i}^{2}-n\bar{X}_{n}^{2})=\frac{1}{n-1}\sum_{i=1} ^{n}(X_{i}-\bar{X}_{n})^{2}\,\]

the usual (unbiased version of the) sample variance.

**Example 12.3.3** (Gini's mean difference): Let \(h(x_{1},x_{2})=|x_{1}-x_{2}|\), so that \(\theta(P)=E(|X_{1}-X_{2}|)\). The corresponding \(U\)-statistic

\[U_{n}=\frac{1}{{n\choose 2}}\sum_{i<j}|X_{i}-X_{j}|\]

is known as Gini's mean difference.

In order to derive the limiting distribution of \(U_{n}\), it is first helpful to derive its variance. Toward this end, for \(1\leq k\leq b\), define functions \(h_{k}\) as follows:

\[h_{k}(x_{1},\ldots,x_{k})=E[h(x_{1},\ldots,x_{k},X_{k+1},\ldots,X_{b})]. \tag{12.25}\]

Of course, \(E[h_{k}(X_{1},\ldots,X_{k})]=\theta(P)\). Then, define

\[\zeta_{k}=Var[h_{k}(X_{1},\ldots,X_{k})]. \tag{12.26}\]

As we will soon see, asymptotic normality depends heavily on \(\zeta_{1}\) (and it being nonzero).

**Example 12.3.4** (Continuation of Example 12.3.2): Here, \(h_{1}(x_{1})\) is given by

\[h_{1}(x_{1})=\frac{1}{2}E[(x_{1}-X_{2})^{2}]=\frac{1}{2}[\sigma^{2}(P)+(x_{1}- \mu(P))^{2}]\.\]

Then,

\[\zeta_{1}=\frac{1}{4}\left\{E[(X-\mu(P))^{4}]-\sigma^{4}(P)\right\}\.\]

Also, \(h_{2}=h\) and

\[\zeta_{2}=Var[h(X_{1},X_{2})]=\frac{1}{4}E(X_{1}-X_{2})^{4}-\sigma^{4}(P)\]

\[=\frac{1}{2}\{E[(X-\mu(P))^{4}]+\sigma^{4}(P)\}\.\ \blacksquare \tag{12.27}\]

Next, we consider a formula for the exact variance of a \(U\)-statistic.

[MISSING_PAGE_EMPTY:2187]

because there are \({n\choose b}\) ways to first pick \(i_{1},\ldots,i_{b}\), then \({b\choose k}\) ways to determine the indices in common, and then there are \({n-b\choose b-k}\) remaining ways to fill out \(j_{1},\ldots,j_{b}\). Putting this together yields

\[Var(U_{n})={n\choose b}^{-2}\sum_{k=1}^{b}{n\choose b}{b\choose k}{n-b\choose b -k}\zeta_{k}\,\]

and the result follows.

For any fixed nonnegative integers \(i\) and \(j\), and \(n\) large enough so that \(n-i>j\),

\[{n-i\choose j}=\frac{1}{j!}(n-i)(n-i-1)\cdots(n-i-j+1)\sim\frac{n^{j}}{j!}\,\]

where \(p_{n}\sim q_{n}\) means \(p_{n}/q_{n}\to 1\) as \(n\to\infty\). It follows that the factor multiplying \(\zeta_{k}\) in the \(k\)th term of (12.28) is equal to

\[\frac{1}{{n\choose b}}{b\choose k}{n-b\choose b-k}\sim\frac{b!}{n^{b}}{b \choose k}\frac{n^{b-k}}{(b-k)!}\sim k!{b\choose k}^{2}\frac{1}{n^{k}}\.\]

Therefore, the following corollary is true.

**Corollary 12.3.1**: _Assume (12.23). Then, the variance of \(U_{n}\) satisfies_

\[Var(U_{n})=\frac{b^{2}}{n}\zeta_{1}+o\left(\frac{1}{n}\right). \tag{12.30}\]

In fact, the error term in (12.30) is at most \(O(1/n^{2})\). Similarly, if \(\zeta_{1}=0\), then

\[Var(U_{n})=2{b\choose 2}^{2}\frac{1}{n^{2}}\zeta_{2}+o\left(\frac{1}{n^{2}} \right)\.\]

Clearly, the rate of convergence of \(Var(U_{n})\) to zero depends on the smallest value of \(j\) for which \(\zeta_{j}>0\). That is, if \(\zeta_{1}=\cdots\zeta_{j-1}=0\) but \(\zeta_{j}>0\), then

\[n^{j}\,Var(U_{n})\to j!{b\choose j}^{2}\zeta_{j}\.\]

We are now in a position to prove asymptotic normality of \(U_{n}\).

**Theorem 12.3.2**: _Assume (12.23) and \(\zeta_{1}>0\). Then,_

\[\sqrt{n}[U_{n}-\theta(P)]-\frac{b}{n}\sum_{i=1}^{n}[h_{1}(X_{i})-\theta(P)] \stackrel{{ P}}{{\to}}0 \tag{12.31}\]_and so_

\[\sqrt{n}[U_{n}-\theta(P)]\stackrel{{ d}}{{\to}}N(0,b^{2}\zeta_{1}) \tag{12.32}\]

Proof. Define \(\hat{U}_{n}\) so that

\[\hat{U}_{n}-\theta(P)=\frac{b}{n}\sum_{i=1}^{n}[h_{1}(X_{i})-\theta(P)]. \tag{12.33}\]

By the Central Limit Theorem

\[\sqrt{n}[\hat{U}_{n}-\theta(P)]\stackrel{{ d}}{{\to}}N(0,b^{2} \zeta_{1})\.\]

The result will follow by Slutsky's Theorem if we can show

\[\sqrt{n}[(U_{n}-\theta(P))-(\hat{U}_{n}-\theta(P))]\stackrel{{ P}}{{\to}}0. \tag{12.34}\]

But, \(U^{\prime}_{n}=U_{n}-\hat{U}_{n}\) is a \(U\)-statistic based on the kernel

\[h^{\prime}(x_{1},\ldots,x_{b})=[h(x_{1},\ldots,x_{b})-\theta(P)]-\sum_{i=1}^{b }[h_{1}(x_{i})-\theta(P)]\.\]

Indeed, averaging \(h^{\prime}\) over all combinations yields

\[U^{\prime}_{n}=\frac{1}{{n\choose b}}\sum_{c}h^{\prime}(X_{i_{1}},\ldots,X_{i_ {b}})\]

\[=U_{n}-\theta(P)-\frac{1}{{n\choose b}}\sum_{c}\sum_{j=1}^{b}[h_{1}(X_{i_{j}}) -\theta(P)]\]

\[=U_{n}-\theta(P)-\frac{{n-1\choose b-1}}{{n\choose b}}\sum_{i=1}^{n}[h_{1}(X_{ i})-\theta(P)]\]

\[=U_{n}-\theta(P)-\frac{b}{n}\sum_{i=1}^{n}[h_{1}(X_{i})-\theta(P)]=U_{n}-\hat{U }_{n}\,\]

as claimed. But, in obvious notation, the \(h^{\prime}_{1}\) corresponding to the kernel \(h^{\prime}\) is zero, and so its variance \(\zeta^{\prime}_{1}\) is 0 as well. By Corollary 12.3.1,

\[Var(U^{\prime}_{n})=Var(U_{n}-\hat{U}_{n})=O(1/n^{2})\,\]

which certainly implies\[E\{[\sqrt{n}(U_{n}-\hat{U}_{n})]^{2}\}\to 0\.\]

By Chebychev's Inequality, (12.34) holds and the result follows.

Note the theorem is true as stated even if \(\zeta_{1}=0\), if \(N(0,\,0)\) is interpreted as point mass at \(0\).

**Example 12.3.5**: **(Continuation of Example 12.3.2)** With \(U_{n}\) the (unbiased) version of the sample variance, we can conclude that

\[\sqrt{n}(U_{n}-\sigma^{2}(P))\stackrel{{ d}}{{\to}}N(0,4\zeta_{1})\,\]

where

\[\zeta_{1}=\frac{1}{4}\left\{E[(X-\mu(P))^{4}]-\sigma^{4}(P)\right\}\.\]

Note that it is possible that \(\zeta_{1}=0\), as occurs when \(X_{i}\) is Bernoulli with success probability \(1/2\). In this case, \(U_{n}=n\hat{p}_{n}(1-\hat{p}_{n})/(n-1)\), where \(\hat{p}_{n}=\bar{X}_{n}\). Then, similar to Example 11.3.5, we can deduce in this case that (Problem 12.18)

\[n\left(U_{n}-\frac{1}{4}\right)\stackrel{{ d}}{{\to}}-\frac{1}{4 }\chi_{1}^{2}+\frac{1}{4}\.\]

**Example 12.3.6**: **(One-Sample Wilcoxon Signed-Rank Statistic)** Assume \(X_{1}\),..., \(X_{n}\) are i.i.d. on the real line with c.d.f. \(F\). Assume \(F\) is continuous (though the argument generalizes if \(F\) is not continuous). Let \(h(x_{1},\,x_{2})=I\{x_{1}+x_{2}>0\}\) and \(\theta(F)=P\{X_{1}+X_{2}>0\}\). Typically, \(U_{n}\) is used as a test of the null hypothesis \(H_{0}\) that the center of the underlying distribution (assumed symmetric) is \(0\), in which case \(\theta(F)=1/2\). Then,

\[h_{1}(x)=1-F(-x)\.\]

Under \(H_{0}\),

\[\zeta_{1}=Var[F(-X)]=VarF[(X)]=1/12\,\]

since \(F(X)\) is distributed as \(U(0,\,1)\). Hence,

\[\sqrt{n}(U_{n}-\theta(F))\stackrel{{ d}}{{\to}}N(0,\frac{1}{3})\.\]

A variation is based on the usual one-sample Wilcoxon statistic \(V_{n}\), which is described as follows. The assumption that \(F\) is continuous implies there are no ties with probability one. Rank \(|X_{1}|\), \(|X_{2}|\),..., \(|X_{n}|\) from smallest to largest and let \(R_{i}\) denote the rank of \(|X_{i}|\). Define

\[V_{n}=\sum_{i=1}^{n}R_{i}\,I\{X_{i}>0\}\.\]Then (Problem 12.19),

\[V_{n}={n\choose 2}U_{n}+S_{n}\,\]

where \(S_{n}\) is the number of positive \(X_{i}\)s. Since \(S_{n}\) is small compared with \(V_{n}\), the limiting distribution of \(V_{n}\) can be obtained from \(U_{n}\) as

\[n^{-3/2}[V_{n}-E(V_{n})]\stackrel{{ d}}{{\to}}N(0,\frac{1}{12})\,\]

where \(E(V_{n})=n(n+1)/4\).

The proof of asymptotic normality of \(U_{n}\) was facilitated by introducing \(\hat{U}_{n}\) in the proof, as defined in (12.33). At this point, this choice of definition may perhaps seem mysterious. But, we now investigate it as a special case of a general projection concept that has great utility. To begin, suppose \(X_{1}\),..., \(X_{n}\) are mutually independent, though not necessarily i.i.d. The basic goal is to study the distribution of some statistic, say \(T_{n}=T_{n}(X_{1},\)..., \(X_{n})\). If one suspects that \(T_{n}\) is asymptotically normal, then it seems plausible that \(T_{n}\) can be approximated by a sum of independent variables of the form \(\sum_{i=1}^{n}g_{i}(X_{i})\). The following result describes an optimal choice of the \(g_{i}\) when minimizing the expected squared difference between \(T_{n}\) and the approximation.

**Theorem 12.3.3**: _Let \(X_{1}\),..., \(X_{n}\) be independent, and \(E(T_{n}^{2})<\infty\). (i) The choice of \(\sum_{i=1}^{n}g_{i}(X_{i})\) minimizing_

\[E\left\{[T_{n}-\sum_{i=1}^{n}g_{i}(X_{i})]^{2}\right\}\]

_is given by_

\[\hat{T}_{n}=\sum_{i=1}^{n}E(T_{n}|X_{i})-(n-1)E(T_{n})\ ;\]

_that is, taking \(g_{i}(X_{i})=E(T_{n}|X_{i})-\frac{n-1}{n}E(T_{n})\) minimizes (12.39). (ii) For this choice, \(E(\hat{T}_{n})=E(T_{n})\) and_

\[E[(T_{n}-\hat{T}_{n})^{2}]=Var(T_{n})-Var(\hat{T}_{n})\.\]

Proof. For any random variable \(Y\) with finite second moment, the choice of \(g\) minimizing \(E\{[Y-g(Z)]^{2}\}\) is \(g(Z)=E(Y|Z)\). Fix \(i\), and apply this to (12.39) with \(Y=T_{n}-\sum_{j\neq i}g_{j}(X_{j})\) and \(Z=X_{i}\). Then, \(g_{i}\) must satisfy

\[g_{i}(X_{i})=E(T_{n}|X_{i})-\sum_{j\neq i}E[g_{j}(X_{j})]\]or

\[g_{i}(X_{i})-E[g_{i}(X_{i})]=E(T_{n}|X_{i})-\sum_{j=1}^{n}E[g_{j}(X_{j})]\.\]

Summing over \(i\) yields

\[\sum_{i=1}^{n}g_{i}(X_{i})=\sum_{i=1}^{n}E(T_{n}|X_{i})-(n-1)\sum_{j=1}^{n}E[g_ {j}(X_{j})]\.\]

But certainly, \(\sum_{j}E[g_{j}(X_{j})]\) must be \(E(T_{n})\) because otherwise one could subtract the difference and further minimize (12.39).

To prove (ii), first calculate

\[Cov(T_{n},\hat{T}_{n})=Cov(T_{n},\sum_{i=1}^{n}E(T_{n}|X_{i}))=\sum_{i=1}^{n}Cov (T_{n},E(T_{n}|X_{i}))\]

\[=\sum_{i=1}^{n}\big{\{}E[T_{n}E(T_{n}|X_{i})]-E^{2}(T_{n})\big{\}}=\sum_{i=1}^{ n}\big{\{}E[E^{2}(T_{n}|X_{i})]-E^{2}(T_{n})\big{\}}\]

\[=\sum_{i=1}^{n}Var[E(T_{n}|X_{i})]=Var(\sum_{i=1}^{n}E(T_{n}|X_{i}))=Var(\hat{ T}_{n})\.\]

Therefore,

\[Var(T_{n}-\hat{T}_{n})=Var(T_{n})+Var(\hat{T}_{n})-2Cov(T_{n},\hat{T}_{n})=Var (T_{n})-Var(\hat{T}_{n})\,\]

yielding (ii).

The function \(\hat{T}_{n}\) is called the projection, because it projects \(T_{n}\) onto the linear space of all functions that are sums of independent random variables.

As a check, when \(T_{n}=U_{n}\) is a U-statistic, then

\[E(U_{n}|X_{i})=\frac{b}{n}h_{1}(X_{i})+(1-\frac{b}{n})\theta(P)\, \tag{12.42}\]

and \(\hat{T}_{n}\) agrees with \(\hat{U}_{n}\) previously introduced in (12.33). Moreover, by Theorem 12.3.3(ii),

\[Var(U_{n}-\hat{U}_{n})=Var(U_{n})-Var(\hat{U}_{n})=\frac{b^{2}}{n}\zeta_{1}+O( \frac{1}{n^{2}})-\frac{b^{2}}{n}\zeta_{1}=O(\frac{1}{n^{2}})\.\]

Therefore, \(\sqrt{n}(U_{n}-\theta(P))\) and \(\sqrt{n}(\hat{U}_{n}-\theta(P))\) have the same normal limiting distribution as obtained before.

Next, we extend one-sample \(U\)-statistics to two-sample \(U\)-statistics. Suppose \(X_{1},\ldots,X_{m}\) are i.i.d. \(P\) and, independently, \(Y_{1},\ldots,Y_{n}\) are i.i.d. \(Q\). The parameter of interest \(\theta=\theta(P,\,Q)\) is given by

\[\theta(P,\,Q)=E[h(X_{1},\ldots,X_{a},\,Y_{1},\ldots,Y_{b})]\,\]

where the kernel \(h\) is a function of \(a+b\) arguments, and assumed symmetric in its first \(a\) and its last \(b\) arguments. Also assume the kernel has a finite second moment. The corresponding \(U\)-statistic is then

\[U_{m,n}=\frac{1}{{m\choose a}{n\choose b}}\sum_{c}\sum_{d}h(X_{i_{1}},\ldots,X_ {i_{a}},\,Y_{j_{1}},\ldots,\,Y_{j_{b}})\.\]

Define

\[h_{1,0}(x)=E[h(x,X_{2},\ldots,X_{a},\,Y_{1},\ldots.Y_{b})]\]

and

\[h_{0,1}(y)=E[h(X_{1},\ldots,X_{a},\,y,\,Y_{2},\ldots,\,Y_{b})]\.\]

Then, one can check (Problem 12.21) that the projection \(\hat{U}_{m,n}\) of \(U_{m,n}\) is given by

\[\hat{U}_{m,n}=\frac{a}{m}\sum_{i=1}^{m}[h_{1,0}(X_{i})-\theta]+\frac{b}{n}\sum _{j=1}^{n}[h_{0,1}(Y_{j})-\theta]+\theta. \tag{12.43}\]

Let \(\zeta_{1,0}=Var[h_{1,0}(X)]\) and \(\zeta_{0,1}=Var[h_{0,1}(Y)]\). If \(\min(m,n)\to\infty\) with \(m/n\to\lambda<\infty\), then by the CLT,

\[\sqrt{m}[\hat{U}_{m,n}-\theta(P,\,Q)]\stackrel{{ d}}{{\to}}N(0,a^ {2}\zeta_{1,0}+\lambda b^{2}\zeta_{0,1}). \tag{12.44}\]

The same is true if \(\hat{U}_{m,n}\) is replaced by \(U_{m,n}\). The argument requires showing that

\[\sqrt{m}(\hat{U}_{m,n}-U_{m,n})\stackrel{{ P}}{{\to}}0\]

and is similar to the one-sample \(U\)-statistics case (Problem 12.22).

**Example 12.3.7** (**Two-Sample Wilcoxon Statistic**): Let \(h(x,\,y)=I\{x\leq y\}\), so that \(\theta(F,\,G)=P\{X\leq Y\}\) when \(X\) and \(Y\) are independent, \(X\) has c.d.f. \(F\) and \(Y\) has c.d.f. \(G\). Then,

\[U_{m,n}=\sum_{i=1}^{m}\sum_{j=1}^{n}I\{X_{i}\leq Y_{j}\}\.\]The statistic \(mnU_{m,n}\) is known as the Mann-Whitney statistic and is closely related to the Wilcoxon rank-sum statistic \(W_{n}\) in Example 12.2.1 (Problem 12.24). Now,

\[h_{1,0}(x)=1-G^{-}(x)\]

and

\[h_{0,1}(y)=F(y)\,\]

where \(G^{-}(x)=P\{Y<x\}\) (so that \(G^{-}=G\) if \(G\) is continuous). Then,

\[\zeta_{1,0}=Var[1-G^{-}(X)]=E\left\{E[I\{X\leq Y\}|X]^{2}\right\}-\theta^{2}(F,G)\]

\[=P\{X_{1}\leq Y_{1}\,\ X_{1}\leq Y_{2}\}-\theta^{2}(F,G)\.\]

If \(F\) is continuous and \(F=G\), \(\zeta_{1,0}=1/3-1/4=1/12\). Similarly,

\[\zeta_{0,1}=P\{X_{1}\leq Y_{1},\ X_{2}\leq Y_{1}\}-\theta^{2}(F,G)\, \tag{12.45}\]

which again reduces to \(1/12\) when \(F\) is continuous and \(F=G\). Note, however, that this method of proving asymptotic normality does not rely on the assumption \(F=G\), unlike the method presented in Example 12.2.1.

Assume \(F\) is continuous. Under \(H_{0}:F=G\), the test that rejects \(H_{0}\) when

\[\sqrt{m}|U_{n}-\frac{1}{4}|\geq\sqrt{\frac{1+\lambda}{12}}z_{1-\frac{\alpha}{2}} \tag{12.46}\]

has null rejection probability tending to \(\alpha\). On the other hand, for testing the null hypothesis \(H_{0}^{\prime}:P\{X\leq Y\}=1/2\) against two-sided alternatives where \(P\{X\leq Y\}\neq 1/2\), the same test does not control the probability of a Type 1 error, even in large samples. That is, there exists \(F\) and \(G\) satisfying \(H_{0}^{\prime}\) such that the probability of a Type 1 error tends to some value \(>\alpha\). Worse yet, the probability of a Type 3 or directional error can be large (Problem 12.25).

### Stationary Mixing Processes

A stochastic process \(\{X_{t},\ t\in I\}\) is a collection of random variables, indexed by \(I\), that are defined on some common probability space. In this section, we consider the case where \(I\) is the set of integers \(\mathbf{Z}\), in which case the process may be referred to as a time series \(\{X_{j},\ j\in\mathbf{Z}\}\). Note that the \(X_{j}\)'s may be random vectors, or more generally they may take values in some space \(S\), though we focus on the case \(S=\mathbf{R}\).

Dependence is typically the norm when considering random variables that evolve in time (or space). Data \(X_{1},\ldots,X_{n}\) may be regarded as a stretch of some time series.

In this section, we discuss some basic asymptotic theory for the normalized sum or average of such dependent random variables. Further references and historical notes are provided at the end of the chapter.

Any attempt to generalize central limit theorems from independent to dependent random variables must in some way rule out strong dependence. Indeed, in the example where \(X_{j}=X_{1}\) for all \(j\), asymptotic normality fails (unless \(X_{1}\) is normal). Therefore, various types of weak dependence conditions have been used to capture the idea that observations separated far in time are approximately independent. In particular, Rosenblatt (1956) suggested the following notion of strong mixing, also called \(\alpha\)-mixing.

**Definition 12.4.1**: For a time series \(X=\{X_{j},\ j\in{\bf Z}\}\), let \({\cal F}_{m}^{n}\) denote the \(\sigma\)-algebra generated by \(\{X_{j},\ m\leq j\leq n\}\). The corresponding mixing coefficients are defined by

\[\alpha_{X}(k)=\sup_{n}\sup_{A,B}|P(A\cap B)-P(A)P(B)|\,\]

where \(A\) and \(B\) vary over \({\cal F}_{-\infty}^{n}\) and \({\cal F}_{n+k}^{\infty}\), respectively. Then, the process \(X\) is called strong mixing (or \(\alpha\)-mixing) if \(\alpha_{X}(k)\to 0\) as \(k\to\infty\).

A special case is the following.

**Definition 12.4.2**: The sequence \(X=\{X_{j},\ j\in{\bf Z}\}\) is \(m\)-dependent if \(\alpha_{X}(k)=0\) for all \(k>m\).

Evidently, a 0-dependent sequence corresponds to a sequence of independent random variables.

Stochastic processes having a probabilistic structure that is invariant to shifts in time are called strictly stationary, or stationary for short.

**Definition 12.4.3**: The sequence \(X=\{X_{j},\ j\in{\bf Z}\}\) is stationary if, for any integers \(c\), \(k\) and \(j_{1},\ldots,j_{k}\), the joint distribution of \((X_{j_{1}},\ldots,X_{j_{k}})\) is the same as that of \((X_{j_{1}+c},\ldots,X_{j_{k}+c})\).

In contrast, processes \(X\) satisfying \(E(X_{j})\) and \(E(X_{j}^{2})\) do not depend on \(j\), as well as \(Cov(X_{j},\,X_{k})\) depends on \((j,k)\) only through \(k-j\), are called weakly stationary or covariance stationary. In the case where \(Cov(X_{j},\,X_{k})=\sigma^{2}I\{j=k\}\), the process \(X\) is sometimes called a white noise process, or simply an uncorrelated sequence. Note that, for a covariance stationary process \(X\), the function

\[R(k)=Cov(X_{1},\,X_{k+1}) \tag{12.47}\]

is called the autocovariance (or just covariance) function of the process \(X\).

**Example 12.4.1**: (**Moving Averages**) Suppose \(\{\epsilon_{j},\ j\in{\bf Z}\}\) is a collection of independent random variables. Then, the sequence

\[X_{j}=h(\epsilon_{j},\,\epsilon_{j+1},\ldots,\epsilon_{j+m})\]is a sequence of \(m\)-dependent random variables, where \(h\) is any (measurable) function from \({\bf R}^{m+1}\) to \({\bf R}\). In the special case where \(h\) is of the form

\[h(\epsilon_{1},\ldots,\epsilon_{m+1})=\sum_{i=1}^{m+1}w_{i}\epsilon_{i}\]

for constants \(w_{1},\ldots w_{m+1}\), the process \(X=\{X_{j},\;\;j\in{\bf Z}\}\) is known as a moving average process of order \(m\). The case \(w_{i}=1/(m+1)\) is a simple moving average process. If the \(\epsilon_{j}\) are also i.i.d., then the process \(X\) is stationary as well. If the \(\epsilon_{j}\) sequence is weakly stationary, then so is \(X\). Moreover, if the \(\epsilon_{j}\) is an uncorrelated sequence with variance \(\sigma_{\epsilon}^{2}\), then an easy calculation (Problem 12.30) gives

\[R(k)=\sigma_{\epsilon}^{2}(w_{1}w_{1+k}+\cdots+w_{m+1-k}w_{m+1})\;\;\;{\rm if} \;\;\;0\leq k\leq m\;, \tag{12.48}\]

and \(R(k)=0\) if \(k>m\).

**Example 12.4.2**: (**Autoregressive Process**) Suppose

\[X_{j}=\rho X_{j-1}+\epsilon_{j}\;, \tag{12.49}\]

where the \(\epsilon_{j}\) are i.i.d. with distribution \(F\) (though one can also consider the case where the \(\epsilon_{j}\) are just weakly stationary). Such a process is known as an autoregressive process of order 1, denoted by AR(1). At this point, it may not be clear that a process \(X\) satisfying the \(X_{j}-\rho X_{j-1}\) are i.i.d. with distribution \(F\) even exists. Assuming its existence for the moment, we may iterate (12.49) to get

\[X_{j}=\rho X_{j-1}+\epsilon_{j}=\rho(\rho X_{j-2}+\epsilon_{j-1})+\epsilon_{j} =\rho^{2}X_{j-2}+\rho\epsilon_{j-1}+\epsilon_{j}\]

and, in general,

\[X_{j}=\rho^{m}X_{j-m}+\sum_{i=0}^{m-1}\rho^{i}\epsilon_{j-i}\;. \tag{12.50}\]

Moreover, (12.50) suggests that, when \(|\rho|<1\), we can define

\[X_{j}=\sum_{i=0}^{\infty}\rho^{i}\epsilon_{j-i}\;, \tag{12.51}\]

where the infinite series is a well-defined random variable if the \(\epsilon_{j}\) have a finite first moment. Indeed,

\[E\left(\sum_{i=0}^{\infty}|\rho^{i}\epsilon_{j-i}|\right)\leq E(|\epsilon_{1} |)\sum_{i=0}^{\infty}|\rho|^{i}<\infty\;,\]which implies that \(\sum_{i=0}^{\infty}|\rho^{j}\epsilon_{j-i}|\) is finite with probability one, and so is the right side of (12.51) with probability one. 1

Footnote 1: Alternatively, the celebrated Kolmogorov Three-Series Theorem may be used to easily show that the series (12.51) converges with probability one; see Billingsley (1995), Theorem 22.8. In addition, if \(Var(\epsilon_{j})<\infty\), we may write, \(X_{j}=\lim_{m\to\infty}X_{m,j}\), where \(X_{m,j}=\sum_{i=0}^{m-1}\rho^{j}\epsilon_{j-i}\), and the limit can be interpreted in the mean-squared sense; see Problem 11.65.

The representation (12.51) may be viewed as a moving average process of infinite order. Furthermore, with \(X_{j}\) defined as in (12.51), one may now check that the random variables \(X_{j}-\rho X_{j-1}=\epsilon_{j}\) are i.i.d. \(F\). It also follows from the representation (12.51) that \(\{X_{j},\ \ j\in{\bf Z}\}\) is stationary.

Assume the distribution \(F\) of the \(\epsilon_{j}\)'s has mean \(\mu_{\epsilon}\) and finite variance \(\sigma_{\epsilon}^{2}\). Then, (12.51) also implies that (Problem 12.31)

\[E(X_{j})=\frac{\mu_{\epsilon}}{1-\rho}\]

and

\[R(k)=\sigma_{\epsilon}^{2}\cdot\frac{\rho^{k}}{1-\rho^{2}}\]

so that the covariances decay geometrically (or exponentially) fast.

Finally, if the distribution \(F\) is absolutely continuous with respect to Lebesgue measure, then it is known that the process \(X\) is \(\alpha\)-mixing, and the mixing coefficients decay geometrically fast as well; see Mokkadem (1988). Surprisingly, a stationary AR(1) process need not be mixing. A well-known counterexample can be obtained with \(\rho=1/2\) and taking \(F\) to be the distribution placing mass \(1/2\) at both \(0\) and \(1\); see Section 2.3.1 of Doukhan (1995).

Strong mixing has important implications concerning the covariance between random variables separated in time.

**Lemma 12.4.1**: _Give a sequence \(X=\{X_{j},\ \ j\in{\bf Z}\}\) with mixing coefficients \(\alpha_{X}(\cdot)\), assume \(U\) and \(V\) are \({\cal F}_{-\infty}^{n}\) and \({\cal F}_{n+k}^{\infty}\) measurable. (i) If \(U\) and \(V\) are bounded by one in absolute value, then_

\[|Cov(U,V)|\leq 4\alpha_{X}(k)\.\]

_(ii) If \(E(|U|^{p})<\infty\) and \(E(|V|^{q})<\infty\) for some \(p\) and \(q\) with \(\frac{1}{p}+\frac{1}{q}<1\), then_

\[|Cov(U,V)|\leq 8[E(|U|^{p})]^{1/p}[E(|V|^{q})]^{1/q}\alpha_{X}^{1-\frac{1}{p}- \frac{1}{q}}(k)\.\]

A proof of Lemma 12.4.1 can be found in the appendix of Hall and Heyde (1980).

As usual, let \(\bar{X}_{n}=\sum_{i=1}^{n}X_{i}/n\). Before considering asymptotic normality of \(\sqrt{n}[\bar{X}_{n}-E(\bar{X}_{n})]\), we consider its variance. If the \(X_{i}\) have a finite variance, then \[Var(\sqrt{n}\bar{X}_{n})=\frac{1}{n}\sum_{i=1}^{n}Var(X_{i})+\frac{2}{n}\sum_{i<j} Cov(X_{i},X_{j}). \tag{12.53}\]

Further assume weak stationarity and recall \(R(k)=Cov(X_{1},X_{k+1})\). Then, (12.53) simplifies to (Problem 12.32)

\[Var(\sqrt{n}\bar{X}_{n})=Var(X_{1})+2\sum_{k=1}^{n-1}(1-\frac{k}{n})R(k). \tag{12.54}\]

If \(R(k)\to 0\) sufficiently fast as \(k\to\infty\), then we can expect

\[Var(\sqrt{n}\bar{X}_{n})\to Var(X_{1})+2\sum_{k=1}^{\infty}R(k)<\infty. \tag{12.55}\]

Certainly, (12.55) holds if the process is also \(m\)-dependent because the infinite sum becomes a finite sum. If the process \(X\) is stationary and bounded in absolute value by one with \(\alpha\)-mixing coefficients \(\alpha_{X}(\cdot)\) that are summable, then by Lemma 12.4.1(i), \(|R(k)|\leq 4\alpha_{X}(k)\). Thus,

\[\sum_{k=1}^{n-1}(1-\frac{k}{n})|R(k)|\to\sum_{k=1}^{\infty}|R(k)|\leq 4\sum_{ k=1}^{\infty}\alpha_{X}(k)<\infty\,\]

and then (12.55) holds. Finally, if \(X\) is stationary with \(E(|X_{1}|^{2+\delta})<\infty\) for some \(\delta>0\), then by Lemma 12.4.1(ii),

\[|R(k)|\leq C\alpha_{X}^{\frac{\delta}{2+\delta}}(k)\,\]

for some constant \(C<\infty\) (which depends on \(\delta\)). Thus, if the mixing coefficients satisfy

\[\sum_{k=1}^{\infty}\alpha_{X}^{\frac{\delta}{2+\delta}}(k)<\infty\, \tag{12.56}\]

then we can also conclude that (12.55) holds (Problem 12.33). Assuming asymptotic normality holds, we can then expect

\[\sqrt{n}[\bar{X}_{n}-E(X_{1})]\stackrel{{ d}}{{\to}}N(0,\sigma_{ \infty}^{2})\,\]

where \(\sigma_{\infty}^{2}\) is given by the right-hand side of (12.55). In the \(m\)-dependent case, the following holds. (References for proofs are provided in the notes at the end of the chapter.)

**Theorem 12.4.1**: _Assume \(X_{1},\,X_{2},\ldots\) is a stationary \(m\)-dependent sequence with mean \(\mu\) and \(Var(X_{1})<\infty\). Then,_

\[\sqrt{n}(\tilde{X}_{n}-\mu)\stackrel{{ d}}{{\to}}N(0,\sigma_{\infty }^{2})\, \tag{12.57}\]

_where_

\[\sigma_{\infty}^{2}=Var(X_{1})+2\sum_{k=1}^{m}R(k). \tag{12.58}\]

**Example 12.4.3**: **(Runs of Bernoulli Trials)** Suppose \(\epsilon_{1}\), \(\epsilon_{2}\), \(\ldots\) is an i.i.d. sequence of Bernoulli trials, each trial having success probability \(p\). Fix \(m\) and let

\[Y_{i,m}=I\{\epsilon_{i}=\cdots=\epsilon_{i+m}=1\}\]

denote the indicator of the event of \(m+1\) heads in a row starting at \(i\). We would like to determine the limiting distribution of \(\sum_{i=1}^{n-m}Y_{i,m}\), suitably normalized. But, the \(Y_{i,m}\) form an \(m\)-dependent strictly stationary sequence, and so Theorem 12.4.1 applies. We need to calculate \(\sigma_{\infty}^{2}\) in (12.58). Note that \(E(Y_{i,m})=p^{m+1}\). Moreover, for \(k\geq 0\),

\[Cov(Y_{1,m},Y_{1+k,m})=E(Y_{1,m}Y_{1+k,m})-E(Y_{1,m})E(Y_{1+k,m})=p^{m+1+k}-p^{ 2m+2}\.\]

Therefore,

\[\sigma_{\infty}^{2}=p^{m+1}(1-p^{m+1})+2\sum_{k=1}^{m}(p^{m+1+k}-p^{2m+2})\]

\[=p^{m+1}-p^{2m+2}+(2\sum_{k=1}^{m}p^{m+1+k})-2mp^{2m+2}\]

\[=p^{m+1}-(2m+1)p^{2m+2}+2p^{m+1}\left(\frac{p(1-p^{m})}{1-p}\right)\]

\[=p^{m+1}-(2m+1)p^{2m+2}+\frac{2p^{m+2}-2p^{2m+2}}{1-p}\.\]

It follows that

\[(n-m)^{-1/2}\sum_{i=1}^{n-m}(Y_{i,m}-p^{m+1})\stackrel{{ d}}{{\to }}N(0,\sigma_{\infty}^{2})\.\]

Statistics like \(\sum_{i=1}^{n-m}Y_{i,m}\) have been used to test alternatives to Bernoulli sequences; see Ritzwoller and Romano (2021) and their analysis of various tests as applied to the so-called hot hand fallacy.

**Example 12.4.4**: In Theorem 12.4.1, it is possible to have \(\sigma_{\infty}^{2}=0\) even when \(Var(X_{1})>0\). To see how, let \(\{\epsilon_{j},\ \ j\in{\bf Z}\}\) be i.i.d. N(0,1) and set \(X_{j}=\epsilon_{j}-\epsilon_{j-1}\). Then, the \(X_{j}\)s form a stationary 1-dependent sequence with mean \(\mu=0\) and

\[\sigma_{\infty}^{2}=Var(X_{1})+2R(1)=2+2Cov(\epsilon_{2}-\epsilon_{1},\epsilon _{1}-\epsilon_{0})=0\.\]

The theorem still holds with the interpretation \(\sqrt{n}\vec{X}_{n}\stackrel{{ p}}{{\to}}0\).

The following theorem, due to Ibragimov (1962), provides a Central Limit Theorem for stationary strong mixing sequences.

**Theorem 12.4.2**: _Suppose \(X=\{X_{j},\ \ j\in{\bf Z}\}\) is stationary, mean \(\mu\), with \(E(|X_{1}|^{2+\delta})<\infty\) for some \(\delta>0\). Assume the mixing coefficients \(\alpha_{X}(\cdot)\) of \(X\) satisfy_

\[\sum_{j=1}^{\infty}[\alpha_{X}(j)]^{\frac{\delta}{2+\delta}}<\infty. \tag{12.59}\]

_Then,_

\[n^{1/2}(\vec{X}_{n}-\mu)\stackrel{{ d}}{{\to}}N(0,\sigma_{\infty} ^{2})\,\]

_where \(\sigma_{\infty}^{2}\) is finite and given by (12.55)._

**Example 12.4.5**: (**Sample Autocovariance**) Let \(X\) be a stationary process with mean \(\mu\) and covariance function \(R(\cdot)\). Assume \(E(|X_{1}|^{4+2\delta})<\infty\) for some \(\delta\), and assume (12.59) as well. Let \(\hat{R}_{n}(1)\) be the sample autocovariance at lag 1; that is,

\[\hat{R}_{n}(1)=\frac{1}{n-1}\sum_{i=1}^{n-1}(X_{i}-\vec{X}_{n})(X_{i+1}-\vec{X }_{n})\.\]

In order to obtain the limiting distribution of \(\sqrt{n}[\hat{R}_{n}(1)-R(1)]\), first consider \(\vec{R}_{n}(1)\) defined by

\[\vec{R}_{n}(1)=\frac{1}{n-1}\sum_{i=1}^{n-1}Y_{i}\,\]

where \(Y_{i}=(X_{i}-\mu)(X_{i+1}-\mu)\). Then, \(Y_{1}\), \(Y_{2}\),...is stationary with mean \(R(1)\). We can apply Theorem 12.4.2 to deduce that

\[\sqrt{n}[\vec{R}_{n}(1)-R(1)]\stackrel{{ d}}{{\to}}N\left(0,Var(Y _{1})+2\sum_{k=1}^{\infty}Cov(Y_{1},Y_{1+k})\right)\.\]Then, by simple algebra and the fact that \(\sqrt{n}(\tilde{X}_{n}-\mu)^{2}\stackrel{{ P}}{{\to}}0\), it follows that (Problem 12.37)

\[\sqrt{n}[\hat{R}_{n}(1)-\bar{R}_{n}(1)]\stackrel{{ P}}{{\to}}0\, \tag{12.60}\]

and so \(\hat{R}_{n}(1)\) and \(\bar{R}_{n}(1)\) have the same limiting distribution.

### Stein's Method

In this section, Stein's (1972) method is introduced as a general technique for approximating the distribution of a sum (or average) of possibly dependent random variables. In particular, we focus on normal approximation, though the method is more general; see the notes at the end of the chapter. As will be seen, the method also produces error bounds, similar to that in the Berry-Esseen Theorem.

A useful starting point is the following characterization of a random variable \(W\) having the standard normal distribution. A random variable \(W\) satisfies

\[E[f^{\prime}(W)]=E[Wf(W)] \tag{12.61}\]

for all "smooth" \(f\) if and only if \(W\) has the standard normal distribution. We will formalize this characterization below. But, note that if \(f\) is bounded and absolutely continuous and \(\phi\) is the standard normal density, then integration by parts yields

\[E[f^{\prime}(W)]=\int_{-\infty}^{\infty}f^{\prime}(w)\phi(w)dw=f(w)\phi(w) \big{|}_{-\infty}^{\infty}-\int_{-\infty}^{\infty}f(w)\phi^{\prime}(w)dw\]

\[=0+\int_{-\infty}^{\infty}f(w)w\phi(w)dw=E[Wf(W)]\.\]

A rough strategy will be to argue that \(W\) is approximately standard normal if

\[E[f^{\prime}(W)]-E[Wf(W)]\approx 0\]

in some sense. Unlike classical Fourier methods, this will be accomplished by using local perturbations of \(W\). In order to get a quick idea of how this may be possible, consider the simple case where \(X_{1}\),..., \(X_{n}\) are i.i.d. with \(E(X_{i})=0\) and \(Var(X_{i})=1\). Let

\[W=\frac{X_{1}+\cdots X_{n}}{\sqrt{n}}\]

and

\[W_{i}=W-\frac{X_{i}}{\sqrt{n}}\ \,\]so that \(W_{i}\) and \(X_{i}\) are independent. It follows that

\[E[X_{i}f(W_{i})]=E(X_{i})E[f(W_{i})]=0\]

and also

\[E[X_{i}f(W)]=E\{X_{i}[f(W)-f(W_{i})]\}\]

\[\approx E[X_{i}(W-W_{i})f^{\prime}(W)]=\frac{1}{\sqrt{n}}E[X_{i}^{2}f^{\prime}( W)]\enspace. \tag{12.62}\]

Therefore,

\[E[Wf(W)]=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}E[X_{i}f(W)]\approx\frac{1}{n}\sum_{i =1}^{n}E[X_{i}^{2}f^{\prime}(W)]\]

\[=E\left[f^{\prime}(W)\cdot\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}\right]\approx E[f^ {\prime}(W)]\enspace,\]

since by the law of large numbers, \(\sum X_{i}^{2}/n\approx 1\). Such an approach will be made rigorous later, and it will also produce error bounds.

A more formal statement of the characterization (12.61) is given by the following lemma. For any function \(g(\cdot)\) of a real-variable, let \(\|g\|=\sup_{w}|g(w)\|\).

**Lemma 12.5.1**: _If \(Z\) has the standard normal distribution, denoted by \(Z\sim N(0,1)\), then \(E[f^{\prime}(Z)]=E[Zf(Z)]\) for all absolutely continuous \(f\) with \(E|f^{\prime}(Z)|<\infty\). Conversely, if a random variable \(W\) satisfies \(E[f^{\prime}(W)]=E[Wf(W)]\) for all absolutely continuous \(f\) with \(\|f^{\prime}\|<\infty\), then \(W\sim N(0,1)\)._

Proof of Lemma 12.5.1. Assume \(Z\sim N(0,1)\) with \(E|f^{\prime}(Z)|<\infty\). By Fubini's Theorem,

\[E[f^{\prime}(Z)]=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f^{\prime}(z)e^{- z^{2}/2}dz\]

\[=\frac{1}{\sqrt{2\pi}}\int_{0}^{\infty}f^{\prime}(z)\int_{z}^{\infty}xe^{-x^{2} /2}dxdz+\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{0}f^{\prime}(z)\int_{-\infty}^{z} -xe^{-x^{2}/2}dxdz\]

\[=\frac{1}{\sqrt{2\pi}}\int_{0}^{\infty}xe^{-x^{2}/2}\int_{0}^{x}f^{\prime}(z) dzdx+\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{0}-xe^{-x^{2}/2}\int_{x}^{0}f^{\prime}(z )dzdx\]

\[=E\{Z[f(Z)-f(0)]\}=E[Zf(Z)]\enspace.\]

The proof of the second part of Lemma 12.5.1 is left as Problem 12.39, but follows easily from Lemma 12.5.2 below.

As usual, let \(\Phi(\cdot)\) denote the standard normal c.d.f.

**Lemma 12.5.2**: _Fix \(x\in{\bf R}\). The unique bounded solution \(f_{x}(\cdot)\) to the differential equation_

\[f_{x}^{\prime}(w)-wf_{x}(w)=I\{w\leq x\}-\Phi(x) \tag{12.63}\]

_is given by_

\[f_{x}(w)=e^{w^{2}/2}\int_{w}^{\infty}e^{-t^{2}/2}[\Phi(x)-I\{t\leq x\}]dt \tag{12.64}\]

\[=-e^{w^{2}/2}\int_{-\infty}^{w}e^{-t^{2}/2}[\Phi(x)-I\{t\leq x\}]dt\ . \tag{12.65}\]

_Hence,_

\[f_{x}(w)=\begin{cases}\sqrt{2\pi}e^{w^{2}/2}\Phi(w)[1-\Phi(x)]\ \ {\rm if}\ w \leq x\\ \sqrt{2\pi}e^{w^{2}/2}\Phi(x)[1-\Phi(w)]\ \ {\rm if}\ w>x\.\end{cases} \tag{12.66}\]

Proof. Multiply both sides of (12.63) by \(e^{-w^{2}/2}\) to get

\[\frac{d(e^{-w^{2}/2}f_{x}(w))}{dw}=e^{-w^{2}/2}[I\{w\leq x\}-\Phi(x)]\.\]

Integration then yields, for an arbitrary constant \(C\),

\[f_{x}(w)=e^{w^{2}/2}\int_{-\infty}^{w}[I\{t\leq x\}-\Phi(x)]e^{-t^{2}/2}dt+Ce^ {t^{2}/2}. \tag{12.67}\]

The only bounded solution requires \(C=0\), and then (12.67) agrees with (12.65). The equivalence of (12.64) and (12.65) is easy to check, as is (12.66) (Problem 12.40).

By replacing \(w\) with a random variable \(W\) in (12.63) and then taking expectations, it follows that

\[|P\{W\leq x\}-\Phi(x)|=|E[f_{x}^{\prime}(W)-Wf_{x}(W)]|\.\]

Therefore, Stein's method is to bound the right side.

In general, normal approximation may be specified by \(E[h(W)]\approx E[h(Z)]\) for certain functions \(h\) in some specified class \({\cal H}\). Define, for random variables \(X\) and \(Y\), the distance \(d_{\cal H}(X,Y)\) by

\[d_{\cal H}(X,Y)=\sup_{h\in{\cal H}}|E[h(X)]-E[h(Y)]|\.\]

Note that \(d\) really is a measure of closeness on the space of distributions of \(X\) and \(Y\) and, depending on the choice of \({\cal H}\), is a metric in the usual sense. For example, the choice

\[{\cal H}=\{I\{\cdot\leq x\}:\ x\in{\bf R}\}\]corresponds to the Kolmogorov metric, denoted by \(d_{K}\). The choice

\[{\cal H}=\{h:{\bf R}\to{\bf R}:\ |h(x)-h(y)|\leq|x-y|\}\]

is the Wasserstein metric, denoted by \(d_{W}\). Finally, the choice

\[{\cal H}=\{I\{\cdot\in A\}:\ A\in\mbox{Borel sets}\}\]

is the total variation metric \(d_{TV}\).

For a random variable \(W\) and \(Z\sim N(0,1)\), in order to compare \(E[h(W)]\) with \(E[h(Z)]\), fix a function \(h\) with \(E|h(Z)|<\infty\). Let \(f=f_{h}\) be the solution to the _Stein equation_ given by

\[f_{h}^{\prime}(w)-wf_{h}(w)=h(w)-E[h(Z)]. \tag{12.68}\]

Then,

\[d_{\cal H}(W,Z)=\sup_{h\in{\cal H}}|E[f_{h}^{\prime}(W)]-E[Wf_{h}(W)]|. \tag{12.69}\]

(Thus, for a real number \(x\), the notation \(f_{x}\) introduced earlier really corresponds to \(f_{h}\) with \(h(\cdot)=I\{\cdot\leq x\}\).)

Before we can exploit (12.69), it is helpful to record certain smoothness properties of the solution \(f_{h}\) to (12.68).

**Lemma 12.5.3**: _The solution \(f_{h}\) to the Stein equation (12.68) can be written as_

\[f_{h}(w)=-e^{w^{2}/2}\int_{w}^{\infty}e^{-t^{2}/2}[h(t)-Eh(Z)]dt \tag{12.70}\]

\[=e^{w^{2}/s}\int_{-\infty}^{w}e^{-t^{2}/2}[h(t)-Eh(Z)]dt. \tag{12.71}\]

_If \(h\) is bounded, i.e. \(\|h\|<\infty\), then_

\[\|f_{h}\|=\sup_{w}|f_{h}(w)|\leq\sqrt{\frac{\pi}{2}}\|h-Eh(Z)\|\]

_and_

\[\|f_{h}^{\prime}\|\leq 2\|h-Eh(Z)\|\.\]

_If \(h\) is absolutely continuous, then_

\[\|f_{h}\|\leq 2\|h^{\prime}\|\,\ \ \ \|f_{h}^{\prime}\|\leq\sqrt{\frac{2}{\pi}} \|h^{\prime}\|\,\ \ \ and\ \ \ \ \|f_{h}^{\prime\prime}\|\leq 2\|h^{\prime}\|. \tag{12.72}\]

The proof of (12.70) and (12.71) is analogous to the proof of (12.64) and (12.65). The rest of the proof is somewhat technical, but an argument can be found in the appendix to Chapter 2 in Chen et al. (2011). At least intuitively, the solution \(f_{h}\) should be smoother than \(h\) since Equation (12.68) equates a function of \(f_{h}\) and \(f_{h}^{\prime}\) to a shift of \(h\). In what follows, the bounds in (12.72) will be important, but otherwise the arguments will be self-contained.

The next goal is to prove a result in the spirit of Berry-Esseen, except that, for simplicity, we work with the metric \(d_{W}\). Of course, when \(W=W_{n}\) is indexed by \(n\), then \(d_{W}(W_{n},Z)\to 0\) implies weak convergence. Results for \(d_{K}\) may be found in Ross (2011) or Chen et al. (2011). The first step is the following lemma, where independence is assumed.

**Lemma 12.5.4**: _Assume \(X_{1},\ldots,X_{n}\) are independent with \(E(X_{i})=0\) and \(Var(X_{i})=1\). Let \(W=n^{-1/2}\sum_{i=1}^{n}X_{i}\). Let \(f\) satisfy \(\|f^{\prime\prime}\|\leq C<\infty\). Then,_

\[|E[Wf(W)]-E[f^{\prime}(W)]|\leq\frac{3C}{2n^{3/2}}\sum_{i=1}^{n}E(|X_{i}|^{3}). \tag{12.73}\]

Proof. Let \(W_{i}=W-n^{-1/2}X_{i}\), so that \(W_{i}\) and \(X_{i}\) are independent. Then \(E[X_{i}f(W_{i})]=0\) and so

\[E[X_{i}f(W)]=E\{X_{i}[f(W)-f(W_{i})]\}=\]

\[E\{X_{i}[f(W)-f(W_{i})]-X_{i}(W-W_{i})f^{\prime}(W_{i})\}+E[X_{i}(W-W_{i})f^{ \prime}(W_{i})]\.\]

Summing over \(i\) and dividing by \(\sqrt{n}\) yields

\[E[Wf(W)]=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}E\{X_{i}[f(W)-f(W_{i})-(W-W_{i})f^{ \prime}(W_{i})]\}\]

\[+\frac{1}{n}\sum_{i=1}^{n}E[X_{i}^{2}f^{\prime}(W_{i})]\]

\[=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}E\{X_{i}[f(W)-f(W_{i})-(W-W_{i})f^{\prime}(W_ {i})]\} \tag{12.74}\]

\[+\frac{1}{n}\sum_{i=1}^{n}E[f^{\prime}(W_{i})]. \tag{12.75}\]

Therefore, to get a bound on the left side of (12.73), we get a bound on the absolute value of (12.74) and a bound on the absolute difference between \(E[f^{\prime}(W)]\) and (12.75). But, by Taylor's Theorem,

\[|X_{i}[f(W)-f(W_{i})-(W-W_{i})f^{\prime}(W_{i})]|\leq|X_{i}|\frac{1}{2}(W-W_{i })^{2}C=\frac{C}{2}\cdot\frac{|X_{i}|^{3}}{n}\.\]Therefore, the absolute value of (12.74) is bounded above by

\[\frac{C}{2n^{3/2}}\sum_{i=1}^{n}E(|X_{i}|^{3})\.\]

Also, by Taylor's Theorem,

\[|f^{\prime}(W_{i})-f^{\prime}(W)|\leq|W_{i}-W|C=\frac{C|X_{i}|}{\sqrt{n}}\,\]

and so the absolute difference between (12.75) and \(E[f^{\prime}(W)]\) is

\[|\frac{1}{n}E[f^{\prime}(W_{i})-f^{\prime}(W)]\leq\frac{C}{n^{3/2}}\sum_{i=1}^ {n}E|X_{i}|\.\]

Combining (12.76) and (12.77) yields

\[|E[Wf(W)-f^{\prime}(W)]|\leq\frac{C}{2n^{3/2}}\sum_{i=1}^{n}E(|X_{i}|^{3})+ \frac{C}{n^{3/2}}\sum_{i=1}^{n}E|X_{i}|\.\]

But, \(E|X_{i}|\leq E(|X_{i}|^{3})\) if \(E(X_{i}^{2})=1\) (Problem 12.45), so that the right side of (12.78) is bounded by the right side of the result (12.73).

**Theorem 12.5.1**: _Under the assumptions of Lemma 12.5.4,_

\[d_{W}(W,Z)\leq\frac{3}{n^{3/2}}\sum_{i=1}^{n}E[|X_{i}|^{3}]\.\]

Proof. The proof follows from Lemma 12.5.4 upon recalling from Lemma 12.5.3 that \(\|f^{\prime\prime}\|\leq C=2\) for any \(f_{h}\) such that \(h\) is Lipschitz (with Lipschtiz constant one).

Next, we consider sums of certain classes of dependent random variables.

**Definition 12.5.1**: We say that \((X_{1},\ldots,X_{n})\) has dependency neighborhoods \(N_{i}\subseteq\{1,\ldots,n\}\) if \(X_{i}\) is independent of \(\{X_{j}\}_{j\notin N_{i}}\).

An immediate example is an \(m\)-dependent time series, where \(N_{i}\) is the set of indices in \(\{1,\ldots,n\}\) such that \(|j-i|\leq m\).

The dependence structure of the \(X_{i}\) may be represented in terms of a graph with vertices \(\{1,\ldots,n\}\), where \(i\) and \(j\) are connected with an edge if \(j\in N_{i}\). If the cardinality of \(N_{i}\), \(|N_{i}|\), is not too big, then a normal approximation can be obtained. One such version is given in Ross (2011), which is stated next.

**Theorem 12.5.2**: _Suppose \((X_{1},\ldots,X_{n})\) has dependency neighborhoods \(N_{i}\) with \(D=\max_{i}|N_{i}|\). Assume \(E(X_{i})=0\), \(E(X_{i}^{4})<\infty\) and set_\[\sigma^{2}=Var\left(\sum_{i=1}^{n}X_{i}\right)=E\left(\sum_{i=1}^{n}\sum_{j\in N_{i }}X_{i}X_{j}\right)\;.\]

_Let \(W=\sum_{i=1}^{n}X_{i}/\sigma\). Then,_

\[d_{W}(W,Z)\leq\frac{D^{2}}{\sigma^{3}}\sum_{i=1}^{n}E(|X_{i}|^{3})+\frac{\sqrt {28}D^{3/2}}{\sqrt{\pi}\sigma^{2}}\sqrt{\sum_{i=1}^{n}E(X_{i}^{4})}\;. \tag{12.79}\]

Proof. From (12.69) and the inequalities (12.72), it is enough to bound \(|E[f^{\prime}(W)-Wf(W)]|\) for \(f\) satisfying \(\|f^{\prime}\|\leq\sqrt{2/\pi}\) and \(\|f^{\prime\prime}\|\leq 2\). Let \(W_{i}=\sum_{j\notin N_{i}}X_{j}/\sigma\). Then, \(X_{i}\) and \(W_{i}\) are independent, and so \(E[X_{i}f(W_{i})]=0\). By the triangle inequality,

\[|E[f^{\prime}(W)-Wf(W)]|\leq A+B\;,\]

where

\[A=\left|E\left\{\frac{1}{\sigma}\sum_{i=1}^{n}X_{i}[f(W)-f(W_{i})-(W-W_{i})f^{ \prime}(W)]\right\}\right|\]

and

\[B=\left|E\left\{f^{\prime}(W)[1-\frac{1}{\sigma}\sum_{i=1}^{n}X_{i}(W-W_{i})] \right\}\right|\;.\]

We claim that these two terms are bounded above by the corresponding two terms on the right side of (12.79). But by bringing the absolute value inside the expectation and applying Taylor's Theorem, it follows that

\[A\leq\frac{1}{\sigma}\sum_{i=1}^{n}E\left|X_{i}\frac{(W-W_{i})^{2}}{2}f^{ \prime\prime}(W_{i}^{*})\right|\;,\]

where \(W_{i}^{*}\) is between \(W_{i}\) and \(W\). Since \(\|f^{\prime\prime}\|\leq 2\),

\[A\leq\frac{1}{\sigma}\sum_{i=1}^{n}|X_{i}(W-W_{i})^{2}|=\frac{1}{\sigma^{3}} \sum_{i=1}^{n}E\left|X_{i}\left(\sum_{j\in N_{i}}X_{j}\right)^{2}\right|\]

\[\leq\frac{1}{\sigma^{3}}\sum_{i=1}^{n}\sum_{j,k\in N_{i}}E|X_{i}X_{j}X_{k}|\]

\[\leq\frac{1}{\sigma^{3}}\sum_{i=1}^{n}\sum_{j,k\in N_{i}}\frac{1}{3}[E(|X_{i} |^{3})+E(|X_{j}|^{3})+E(|X_{k}|^{3})]\;,\]where the last inequality follows from the arithmetic-geometric mean inequality.2 But,

Footnote 2: The arithmetic-geometric mean inequality says that, for \(y_{i}\geq 0\), \((y_{1}+\cdots+y_{k})/k\geq(y_{1}\cdots y_{k})^{1/k}\).

\[\sum_{i=1}^{n}\sum_{j,k\in N_{i}}E(|X_{i}|^{3})\leq D^{2}\sum_{i=1}^{n}E(|X_{i} |^{3})\]

and also

\[\sum_{i=1}^{n}\sum_{j\in N_{i}}\sum_{k\in N_{i}}E(|X_{j}|^{3})=\sum_{j=1}^{n} \sum_{i\in N_{j}}\sum_{k\in N_{i}}E(|X_{j}|^{3})\leq D^{2}\sum_{j=1}^{n}E(|X_{ j}|^{3})\;.\]

Therefore,

\[A\leq\frac{D^{2}}{\sigma^{3}}\sum_{i=1}^{n}E(|X_{i}|^{3})\;,\]

as previously announced. Next,

\[B\leq\frac{\|f^{\prime}\|}{\sigma^{2}}E\left|\sigma^{2}-\sum_{i=1}^{n}\sum_{j \in N_{i}}X_{i}X_{j}\right|\;.\]

By the Cauchy-Schwarz Inequality, this is bounded above by

\[\sqrt{\frac{2}{\pi}}\frac{1}{\sigma^{2}}\sqrt{Var(\sum_{i=1}^{n}\sum_{j\in N_ {i}}X_{j})}\;.\]

The remainder of the proof consists of bounding the variance term in the last expression; see Problem 12.47.

**Example 12.5.1** (**U-statistics**): As in Section 12.3, consider the \(U\)-statistic, \(U_{n}\), given by (12.24) based on a symmetric kernel \(h\). Then, we can write \(W={n\choose b}U_{n}/\sigma\), where \(\sigma^{2}={n\choose b}^{2}Var(U_{n})\). Theorem 12.5.2 applies if \(n\) is changed to \({n\choose b}\) and we identify an index \(i\) in the sum with a particular \(b\)-tuple \(\{i_{1},\ldots,i_{b}\}\) of \(b\) distinct indices. The number of terms in the sum for \(U_{n}\), say \(N_{\{i_{1},\ldots,i_{b}\}}\), that share one of its \(b\) indices in common with \(\{i_{1},\ldots,i_{b}\}\), can be bounded above by \(b{n-1\choose b-1}\), and so we can set \(D\) equal to this bound. Theorem 12.5.2 applies to yield a bound for \(d_{W}\) (Problem 12.48).

**Example 12.5.2** (**Erdos-Renyi Random Graph**) Consider an Erdos-Renyi random graph, constructed as follows. Fix \(0<p<1\). There are \(n\) vertices, and any pair of vertices is connected with an edge with probability \(p\), independently for each pair of edges. For the sake of argument, assume the vertices are labeled from \(1\) to \(n\). Let \(T\) be the number of triangles formed; that is, a particular triple of distinct vertices \(\{i,\,j,\,k\}\) forms a triangle if all three pairs \(i\) and \(j\), \(j\) and \(k\), and \(i\) and \(k\) are connected with an edge. So, each of the \(N={n\choose 3}\) possible triangles occurs with probability \(p^{3}\). Let \(Y_{i}\) be the indicator of the event that the \(i\)th triple of \(N\) possible triangles is formed. Note that \(i\) indexes the possible \(N\) triangles that may be formed (in any specified order). So, \(T=\sum_{i=1}^{N}Y_{i}\). Let \(W=(T-ET)/\sigma\), where \(\sigma^{2}=Var(T)\). Let \(N_{i}\setminus\{i\}\) be the triples of indices which share exactly two edges with those specified by the index \(i\). Then, Theorem 12.5.2 applies to with \(|N_{i}|=3(n-3)+1\), and so \(D=3n-8\). In order to compute \(\sigma^{2}\), let \(X_{i}=Y_{i}-p^{3}\). Suppose \(j\neq i\) and \(j\in N_{i}\). Then,

\[Cov(Y_{i},\,Y_{j})=p^{5}-p^{6}\.\]

To see why, if, for example, \(i\) corresponds to the vertices \(\{1,\,2,\,3\}\) and \(j\) to the vertices \(\{1,\,2,\,4\}\), then both triangles are formed if the \(5\) edges connecting \(1\) and \(2\), \(2\) and \(3\), \(1\) and \(3\), \(1\) and \(4\), and \(2\) and \(4\) are all present and so \(E(Y_{i}Y_{j})=p^{5}\). An easy calculation (Problem 12.51) then gives that, for any positive integer \(k\),

\[E(|X_{i}|^{k})=p^{3}(1-p^{3})[(1-p^{3})^{k-1}+p^{3(k-1)}] \tag{12.80}\]

and

\[\sigma^{2}={n\choose 3}p^{3}[1-p^{3}+3(n-3)p^{2}(1-p)]. \tag{12.81}\]

It follows that

\[d_{W}(W,\,Z)\leq\frac{(3n-8)^{2}}{\sigma^{3}}{n\choose 3}p^{3}(1-p^{3})[(1-p^ {3})^{2}+p^{6}] \tag{12.82}\]

\[+\frac{\sqrt{28}(3n-8)^{3/2}}{\sqrt{\pi}\sigma^{2}}\sqrt{{n\choose 3}p^{3}(1-p^ {3})[(1-p^{3})^{3}+p^{9}]}\.\]

For fixed \(p\), \(\sigma^{2}=O(n^{4})\) and so the bound (12.82) tends to zero and a central limit theorem for \(T\) holds. One may even let \(p\,\rightarrow\,0\) and still derive a normal approximation to the distribution of \(T\); see Problem 12.51.

The power of Stein's method goes significantly beyond the introduction presented here. For further information, see the notes at the end of the chapter.

### Problems

#### Section 12.2

**Problem 12.1**: Show (12.2) and (12.3).

**Problem 12.2**: Show (12.6) and (12.7).

**Problem 12.3**: Use Theorem 12.2.1 to prove an asymptotic normal approximation to the hypergeometric distribution.

**Problem 12.4**: Show why Theorem 12.2.1 is a special case of Theorem 12.2.2.

**Problem 12.5**: Show that \(\tau_{N}\) defined in the proof of Theorem 12.2.3 satisfies \(\tau_{N}\to\infty\) as min(\(n\), \(N-n\)) \(\to\infty\).

**Problem 12.6**: In the context of Example 12.2.1, find the limiting distribution of the \(W_{n}\) using Theorem 12.2.3. Identify \(G_{n}\) and \(G\).

**Problem 12.7**: In Example 12.2.1, rather than considering the sum of the ranks of the \(Y_{i}\)s, consider the statistic given by the sum of the squared ranks of the \(Y_{i}\)s. Find its limiting distribution, properly normalized, under \(F=G\).

**Problem 12.8**: In the setting of Section 12.2, assume \(N=m+n\) and

\[(x_{N,1},\ldots,x_{N,N})=(y_{1},\ldots,y_{m},\,z_{1},\ldots,z_{n})\.\]

Let \(\bar{y}_{m}=\sum_{i=1}^{m}y_{i}/m\) and \(\bar{z}_{n}=\sum_{j=1}^{n}z_{j}/n\). Let \(\bar{x}_{N}=\sum_{j=1}^{N}x_{N,j}/N\). Also let \(s_{m,y}^{2}=\sum_{i=1}^{m}(y_{i}-\bar{y}_{m})^{2}/m\) and similarly define \(s_{n,z}^{2}\). Let \(Y_{1},\ldots,Y_{m}\) denote a sample obtained without replacement from the \(N\) values, with sample mean \(\bar{Y}_{m}\). Assume \(m/n\to\lambda<\infty\). Assume \(\bar{y}_{m}\to\bar{y}\) and \(\bar{z}_{n}\to\bar{z}\), as well as \(s_{m,y}\to s_{y}\) and \(s_{n,z}\to s_{z}\). Finally assume the uniform distribution on \(y_{1},\ldots,y_{m}\) converges weakly to a c.d.f. \(G_{y}\) with variance \(s_{y}^{2}\), and similarly the uniform distribution on \(z_{1},\ldots,z_{n}\) converges weakly to a c.d.f. \(G_{z}\) with variance \(s_{z}^{2}\).

(i) Find the limiting distribution of \(\sqrt{m}(\bar{Y}_{m}-\bar{x}_{N})\).

(ii) If \(Z_{1}\),..., \(Z_{n}\) denote the outcomes in \(\Pi_{N}\) not sampled by \(Y_{1}\),..., \(Y_{m}\), and \(\bar{Z}_{n}=\sum_{j=1}^{n}Z_{j}/n\), then find the limiting distribution of \(\sqrt{m}(\bar{Y}_{m}-\bar{Z}_{n})\).

(iii) Simplify your answers in the case \(\bar{y}_{m}=\bar{z}_{n}\) and so \(\bar{y}=\bar{z}\).

**Problem 12.9**: Complete the proof of Corollary 12.2.1(ii) using the Cramer-Wold Device.

**Problem 12.10**: In the setting of Corollary 12.2.1(ii), find an exact formula for \(Cov(\bar{U}_{n}\), \(\bar{V}_{n})\) and then calculate the limit of \(nCov(\bar{U}_{n}\), \(\bar{V}_{n})\).

**Problem 12.11**: The limiting expression for \(NVar(\hat{\theta}_{N})\) is given in (12.19). Find an exact expression for \(NVar(\hat{\theta}_{N})\) that has a similar representation.

**Problem 12.12**: Consider the estimator \(\hat{s}_{N}^{2}(1)\) defined in (12.20). Show that \(\hat{s}_{N}^{2}(1)\stackrel{{ P}}{{\to}}s^{2}(1)\). State your assumptions.

**Problem 12.13**: Provide the details to show (12.22). _Hint:_ Use Theorem 12.2.4 and Problem 12.12.

**Problem 12.14**: Prove an analogous result to Theorem 12.2.5 when sampling from an infinite population, where the asymptotic variance has the same form as (12.21) with \(f=0\). Assuming \(s^{2}(1)\) and \(s^{2}(0)\) are known, how would you allocate treatment among the \(N_{0}\) units to minimize the asymptotic variance? (The solution is known as Neyman allocation.)

**Problem 12.15**: Prove a Glivenko-Cantelli Theorem (Theorem 11.4.2) for sampling without replacement from a finite population. Specifically, assume \(X_{1},\ldots,X_{n}\) are sampled at random without replacement from the population with \(N=N_{n}\) elements given by \(\{x_{N,1},\ldots,x_{N,N}\}\). Let \(\hat{F}_{n}(t)=n^{-1}\sum_{i=1}^{n}I\{X_{i}\leq t\}\) and let \(F_{N}(t)=N^{-1}\sum_{j=1}^{N}I\{x_{N,j}\leq t\}\). Show that \(\sup_{t}|\hat{F}_{n}(t)-F_{N}(t)|\stackrel{{ P}}{{\to}}0\). (First, consider the case where \(F_{N}\) converges in distribution to some \(F\), but is this needed?)

_Section 12.3_

**Problem 12.16**: Verify (12.27).

**Problem 12.17**: Suppose \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) are i.i.d. \(P\), with \(E(X_{i}^{2})<\infty\) and \(E(Y_{i}^{2})<\infty\). The parameter of interest is \(\theta(P)=Cov(X_{i},Y_{i})\). Find a kernel for which the corresponding \(U\)-statistic \(U_{n}\) is an unbiased estimator of \(\theta(P)\). Under an appropriate moment assumption, find the limiting distribution of \(U_{n}\). _Hint:_ Compute \(E[(X_{1}-X_{2})(Y_{1}-Y_{2})]\).

**Problem 12.18**: Verify (12.35).

**Problem 12.19**: In Example 12.3.6, show (12.37). Verify the limiting distribution of \(V_{n}\) in (12.38).

**Problem 12.20**: Show (12.42).

**Problem 12.21**: Show (12.43)

**Problem 12.22**: Show that (12.44) holds if \(\hat{U}_{n}\) is replaced by \(U_{n}\).

**Problem 12.23**: Verify (12.45).

**Problem 12.24**: Show that \(W_{n}\) in Example 12.2.1 and \(U_{m,n}\) in Example 12.3.7 are related by \(W_{n}=mnU_{m,n}+n(n+1)/2\), at least in the case of no ties in the data.

**Problem 12.25**: In Example 12.3.7, find \(F\) and \(G\) so that \(\zeta_{0,1}\) and \(\zeta_{1,0}\) are not \(1/12\), even when \(P\{X\leq Y\}=1/2\). Explore how large the rejection probability of the test with rejection region (12.46) can be under \(H^{\prime}_{0}\). What does this imply about a Type 3 or directional error? That is, if the test rejects \(H^{\prime}_{0}\) and then declares \(\theta(F,\,G)>1/2\) because

\[\sqrt{m}(U_{n}-1/2)\geq\sqrt{\frac{1+\lambda}{12}}z_{1-\frac{\alpha}{2}}\,\]

then how large can this probability be even if \(P\{X\leq Y\}<1/2\)?

**Problem 12.26**: Consider testing the null hypothesis that a sample \(X_{1},\ldots,\,X_{n}\) is i.i.d. against the alternative that the distributions of the \(X_{i}\) are stochastically increasing. Mann (1945) proposed the test which rejects for large values of \(N\), where \(N\) is the number of pairs \((X_{i},\,X_{j})\) with \(i<j\) and \(X_{i}<X_{j}\). Determine the limiting distribution of \(N\), suitably normalized. How large should we choose the critical value (for large \(n\)) in order to control the Type 1 error at \(\alpha\)?

**Problem 12.27**: Let \(X_{1},\ldots,\,X_{n}\) be i.i.d. \(P\). Consider estimating \(\theta(P)\) defined by

\[\theta(P)=E[h(X_{1},\ldots,\,X_{b})]\,\]

where \(h\) is a symmetric kernel. Assume \(P\) is such that \(E[h(X_{1},\ldots,\,X_{b})|<\infty\), so that \(\theta(P)\) is also well-defined. Show that \(U_{n}\stackrel{{ P}}{{\rightarrow}}\theta(P)\). In fact, \(E[U_{n}-\theta(P)|\to 0\). _Hint: First how \(U_{n}\) is consistent by comparing \(U_{n}\) with a consistent estimator obtained by averaging the kernel over nonoverlapping subsets of the data of size \(b\)_, _and then apply Rao-Blackwell. Then, use Problem_ 11.76.

**Problem 12.28**: Let \(X_{1},\ldots,\,X_{n}\) be i.i.d. \(P\). Consider estimating \(\theta(P)\) defined by

\[\theta(P)=E[h(X_{1},\ldots,\,X_{b})]\,\]

where \(h\) is a symmetric kernel. Assume \(P\) is such that \(E[h^{2}(X_{1},\ldots,\,X_{b})]<\infty\), so that \(\theta(P)\) is also well-defined. Let \(U_{n}\) be the corresponding \(U\)-statistic defined by (12.24). Let \(\hat{P}_{n}\) be the empirical measure, and also consider the estimator

\[\theta(\hat{P}_{n})=\frac{1}{n^{b}}\sum_{i_{1}=1}^{n}\cdots\sum_{i_{b}=1}^{n}h (X_{i_{1}},\ldots,\,X_{i_{b}})\.\]

Do \(\sqrt{n}[U_{n}-\theta(P)]\) and \(\sqrt{n}[\theta(\hat{P}_{n})-\theta(P)]\) converge to the same limiting distribution? If further conditions are needed, state them. Find the limiting behavior of \(n[U_{n}-\theta(\hat{P}_{n})]\). Again, state any conditions you might need.

**Problem 12.29**: Consider a \(U\)-statistic of degree 2, based on a kernel \(h\). Let \(h_{1}(x)=E[h(x,\,X_{2})]\) and \(\zeta_{1}=Var[h_{1}(X_{1})]\). Assume \(\zeta_{1}>0\), so that we know that \(\sqrt{n}[U_{n}-\theta(P)]\) converges in distribution to the normal distribution with mean 0 and variance\(4\zeta_{1}\). Consider estimating the limiting variance \(4\zeta_{1}\). Since \(U_{n}\) averages \(h(X_{i},\,X_{j})\) over the \({n\choose 2}\) pairs \((X_{i},\,X_{j})\) with \(i\neq j\), one might use the sample variance of these \({n\choose 2}\) pairs as an estimator. That is, define

\[S_{n}^{2}=\frac{1}{{n\choose 2}}\sum_{i<j}[h(X_{i},\,X_{j})-U_{n}]^{2}\.\]

Determine whether or not \(S_{n}^{2}\) is a consistent estimator. State any added conditions you might need. Generalize to \(U\)-statistics of degree \(b\).

_Section 12.4_

**Problem 12.30**: Verify (12.48).

**Problem 12.31**: In Example 12.4.2 with the \(\epsilon_{j}\) having finite variance, derive the formulae for the mean and covariance (12.52) of the process.

**Problem 12.32**: Verify (12.54).

**Problem 12.33**: Assume \(X\) is stationary, \(E(|X_{1}|^{2+\delta})<\infty\) for some \(\delta>0\) and (12.56) holds. Show that (12.55) holds, and hence \(R(k)\to 0\) as \(k\to\infty\).

**Problem 12.34**: Suppose \(X\) is a stationary process with mean \(\mu\) and covariance function \(R(k)\). Assume \(R(k)\to 0\) as \(k\to\infty\). Show \(\bar{X}_{n}\stackrel{{ P}}{{\to}}\mu\). (A sufficient condition for \(R(k)\to 0\) is \(X\) is strongly mixing with \(E(|X_{1}|^{2+\delta})<\infty\); see Problem 12.33.)

**Problem 12.35**: Generalize Theorem 12.4.1 to the case where the \(X_{i}\) are vector-valued.

**Problem 12.36**: Consider the setup of Example 12.4.3.

(i) Find the joint limiting distribution of \(\sum_{i=1}^{n-1}(Y_{i,1},\,Y_{i,0})^{\top}\), suitably normalized.

(ii) Let \(\hat{R}_{n}=\sum_{i=1}^{n-1}Y_{i,\,1}/\sum_{i=1}^{n-1}Y_{i,\,0}\), which is the proportion of successes following a success. Show that \(\sqrt{n}(\hat{R}_{n}-p)\stackrel{{ d}}{{\to}}N(0,\,1-p)\).

**Problem 12.37**: In Example 12.4.5, show the convergence (12.60).

_Section 12.5_

**Problem 12.38**: If \(W\sim N(0,\,\sigma^{2})\) with \(\sigma\neq 1\), what is the generalization of the characterization (12.61)?

**Problem 12.39**: Complete the proof of the converse in Lemma 12.5.1. _Hint: Use Lemma 12.5.2._

**Problem 12.40**: Complete the proof of Lemma 12.5.2 by showing that (12.64) and (12.65) are equivalent, and then showing that (12.66) follows.

**Problem 12.41**: Show that, for \(w>0\),

\[1-\Phi(w)\leq\min\left(\frac{1}{2},\,\frac{1}{w\sqrt{2\pi}}\right)e^{-w^{2}/2}\.\]

Show that this inequality implies \(\|f_{x}\|\leq\sqrt{\pi/2}\) and \(\|f_{x}^{\prime}\|\leq 2\).

**Problem 12.42**: Show that the Wasserstein metric implies weak convergence; that is, if \(d_{W}(X_{n},\,X)\to 0\), then \(X_{n}\overset{d}{\to}X\). Give a counterexample to show the converse is false. Prove or disprove the following claim: For random variables \(X_{n}\) and \(X\) with finite first moments, show that \(d_{W}(X_{n},\,X)\to 0\) if and only if \(X_{n}\overset{d}{\to}X\) and \(E(X_{n})\to E(X)\).

**Problem 12.43**: Investigate the relationships between \(d_{W}\), \(d_{K}\) and \(d_{TV}\), as well as the bounded Lipschitz metric introduced in Problem 11.24. Does convergence of one of them imply convergence of any of the others? If not, illustrate by finding counterexamples.

**Problem 12.44**: If \(Z\) is a real-valued random variable with density bounded by \(C\), then show that, for any random variable \(W\),

\[d_{K}(W,\,Z)\leq\sqrt{2Cd_{W}(W,\,Z)}\,\]

where \(d_{K}\) is the Kolmogorov-Smirnov (or sup or uniform) metric between distribution functions, and \(d_{W}\) is the Wasserstein metric.

**Problem 12.45**: Show that, if \(E(X^{2})=1\), then \(E|X|\leq E(|X|^{3})\).

**Problem 12.46**: Theorem 12.5.1 provides a bound for \(d_{W}(W,\,Z)\) where \(W=n^{-1/2}\sum_{i=1}^{n}X_{i}\) and the \(X_{i}\) are independent with mean \(0\) and variance one. Extend the result so that \(Var(X_{i})=\sigma_{i}^{2}\) may depend on \(i\).

**Problem 12.47**: Finish the proof of Theorem 12.5.2 by showing

\[Var\left(\sum_{i=1}^{n}\sum_{j\in N_{i}}X_{i}X_{j}\right)\leq 14D^{3}\sum_{i=1}^ {n}E(|X_{i}|^{4})\.\]

_Hint_: Use the arithmetic-geometric mean inequality.

**Problem 12.48**: Complete the details in Example 12.5.1 to get an explicit bound from Theorem 12.5.2 for \(d_{W}\). What conditions are you assuming?

**Problem 12.49**: Use Theorem 12.5.2 to derive a Central Limit Theorem for the sample mean of an \(m\)-dependent stationary process. State your assumptions and compare with Theorem 12.4.1.

**Problem 12.50**: An alternative characterization of the Wasserstein metric is the following (which you do not have to show): \(d_{W}(X,Y)\) is the infimum of \(E|X^{\prime}-Y^{\prime}|\) over all possible joint distributions of \((X^{\prime},Y^{\prime})\) such that the marginal distributions of \(X^{\prime}\) and \(Y^{\prime}\) are those of \(X\) and \(Y\), respectively. However, do show that

\[d_{W}(X,Y)=\int_{0}^{1}|F^{-1}(u)-G^{-1}(u)|du=\int_{-\infty}^{\infty}|F(x)-G(x )|dx\,\]

where \(F\) and \(G\) are the c.d.f.s of \(X\) and \(Y\), respectively.

**Problem 12.51**: Verify (12.80), (12.81) and (12.82). Based on the bound (12.82), consider an asymptotic regime where \(p\sim n^{-\beta}\) for some \(\beta\geq 0\). For what \(\beta\) does the bound tend to zero, so that a central limit theorem for \(T\) holds?

**Problem 12.52**: Consider points on a lattice of the form \((i,j)\) where \(i\) and \(j\) are integers from \(0\) to \(n\). Each of these \((n+1)^{2}\) points can be considered a vertex of a graph. Consider connecting edges adjoining \((i,j)\) and \((i+1,j)\) or \((i,j)\) and \((i,j+1)\), so that only edges between nearest vertices are considered in the graph (and each edge is either horizontal or vertical). Suppose each edge appears with probability \(p\), independently of all other edges. For each little square of area one on the lattice, the square is colored red if all four edges appear. So for example, the region in the square with vertices \((0,0)\), \((1,0)\), \((0,1)\) and \((1,1)\) is colored red if all four edges appear, which has probability \(p^{4}\), meaning the edge connecting \((0,0)\) and \((1,0)\), the edge connecting \((1,0)\) and \((1,1)\), the edge connecting \((0,1)\) and \((1,1)\), and the edge connecting \((0,0)\) and \((0,1)\). Let \(\hat{\theta}_{n}\) be the proportion of the big square with area \(n^{2}\) that is colored red. Find the limiting distribution of \(\hat{\theta}_{n}\), suitably normalized.

### Notes

Theorem 12.2.2 is due to Hajek (1960). For further discussion on the literature of CLTs for sampling from a finite population, see Li and Ding (2017). An interesting comparison of Fisher's sharp null versus Neyman's weak null is studied in Ding (2017). The potential outcomes framework has been used extensively in causal inference; Imbens and Rubin (2015), and the references therein.

The foundational paper on \(U\)-statistics is due to Hoeffding (1948). Further results on "projections" are due to Hajek (1968). Full length treatments on \(U\)-statistics can be found in Lee (1990) and Kowalski and Tu (2008).

The notion of \(\alpha\)-mixing or strong mixing is due to Rosenblatt (1956). Various types of mixing are discussed in Doukhan (1995), Bradley (2007), and Dedecker, et al. (2007). Proofs of the mixing inequalities in Lemma 12.4.1 can be found in the appendix in Hall and Heyde (1980), though the results date back to Wolkonoski and Rozanov (1959) and Davydov (1979). A Central Limit Theorem under \(m\)-dependence appeared in Hoeffding and Robbins (1948) (assuming \(2+\delta\) moments), and under mixing in Ibragimov (1962). A quite general Central Limit Theorem under weak dependence, as well as references to others, is given in Neumann (2013). Theorem 12.4.1 is included as a special case of Neumann's result.

Section 12.5 on Stein's method was inspired by Stein (1972, 1986), Chen et al. (2011), and Ross (2011). See also the survey by Chatterjee (2014). The scope of application of Stein's method has been ever-expanding. Stein's method is actually a collection of tools, some of which are based on what Stein (1986) calls "auxiliary randomization", exchangeable pairs, and zero bias coupling. In particular, one can develop approximations in the Kolmogorov-Smirnov metric, and even in high dimensions. Moreover, Stein's method applies to distributional approximation beyond normality; see Ross (2011).

## Chapter 13 Applications to Inference

### 13.1 Introduction

In this chapter, we apply some basic asymptotic concepts and results to gain insight into the large-sample behavior of some fundamental tests. Section 13.2 considers the robustness of some classical tests, like the \(t\)-test, when the underlying assumptions may not hold. For example, for testing the null hypothesis that the underlying mean is zero, it is seen that the \(t\)-test is pointwise asymptotically level \(\alpha\) for any distribution \(F\) with finite nonzero variance. However, such a robustness of validity result does not always extend to other parametric procedures, as will be seen. Although the probability of a Type 1 error tends to the nominal level as \(n\to\infty\), it is also important to investigate the speed of convergence. For this reason, Edgeworth expansions are discussed in Section 13.3. Further results are developed for testing a univariate mean in a nonparametric setting in Section 13.4. The question of whether or not the \(t\)-test is uniformly asymptotically level \(\alpha\) is investigated, where uniformity refers to some broad nonparametric family. In order to obtain uniformity, some restrictions are needed for any method, as demonstrated by a result of Bahadaur and Savage. Section 13.5 serves as an introduction to testing many means in a high-dimensional setting.

### 13.2 Robustness of Some Classical Tests

Optimality theory postulates a statistical model and then attempts to determine a best procedure for that model. Since model assumptions tend to be unreliable, it is necessary to go a step further and ask how sensitive the procedure and its optimality are to the assumptions. In the normal models of Chapters 4-7, three assumptions are made: independence, identity of distribution, and normality. In the two-sample \(t\)-test, there is the additional assumption of equality of variance. We shall considerthe effects of nonnormality and inequality of variance in the first subsection, and that of dependence in the next subsection.

The natural first question to ask about the robustness of a test concerns the behavior of the significance level. If an assumption is violated, is the significance level still approximately valid? Such questions concerning robustness of validity are typically answered by combining two methods of attack. The actual significance level under some alternative distribution is either calculated exactly or, more usually, estimated by simulation. In addition, asymptotic results are obtained which provide approximations to the true significance level for a wide variety of models. We here restrict ourselves to a brief sketch of the latter approach.

##### Effect of Distribution

Consider the one-sample problem where \(X_{1},\ldots,X_{n}\) are independently distributed as \(N(\xi,\sigma^{2})\). Tests of \(H:\xi=\xi_{0}\) are based on the test statistic

\[t_{n}=t_{n}(X_{1},\ldots,X_{n})=\frac{\sqrt{n}(\tilde{X}_{n}-\xi_{0})}{S_{n}}= \frac{\sqrt{n}(\tilde{X}_{n}-\xi_{0})}{\sigma}\bigg{/}\frac{S_{n}}{\sigma}, \tag{13.1}\]

where

\[S_{n}^{2}=\sum(X_{i}-\tilde{X}_{n})^{2}/(n-1)\;;\]

see Section 5.2. When \(\xi=\xi_{0}\) and the \(X\)'s are normal, \(t_{n}\) has the \(t\)-distribution with \(n-1\) degrees of freedom. Suppose, however, that the normality assumption fails and the \(X\)'s instead are distributed according to some other distribution \(F\) with mean \(\xi_{0}\) and finite variance. Then by the Central Limit Theorem, \(\sqrt{n}(\tilde{X}_{n}-\xi_{0})/\sigma\) has the limit distribution \(N(0,1)\); furthermore \(S_{n}/\sigma\) tends to \(1\) in probability by Example 11.3.3. Therefore, by Slutsky's Theorem, \(t_{n}\) has the limit distribution \(N(0,1)\) regardless of \(F\). This shows in particular that the \(t\)-distribution with \(n-1\) degrees of freedom tends to \(N(0,1)\) as \(n\to\infty\).

To be specific, consider the one-sided \(t\)-test which rejects when \(t_{n}\geq t_{n-1,1-\alpha}\), where \(t_{n-1,1-\alpha}\) is the \(1-\alpha\) quantile of the \(t\)-distribution with \(n-1\) degrees of freedom. It follows from Corollary 11.3.1 and the asymptotic normality of the \(t\)-distribution that (see Problem 11.47 (ii))

\[t_{n-1,1-\alpha}\to z_{1-\alpha}=\Phi^{-1}(1-\alpha)\;.\]

In fact, the difference \(t_{n-1,1-\alpha}-z_{1-\alpha}\) is \(O(n^{-1})\), as will be seen in Section 13.3.

Let \(\alpha_{n}(F)\) be the true probability of the rejection region \(t_{n}\geq t_{n-1,1-\alpha}\) when the distribution of the \(X\)'s is \(F\). Then

\[\alpha_{n}(F)=P_{F}\{t_{n}\geq t_{n-1,1-\alpha}\}\]has the same limit as \(P_{\Phi}[t_{n}\geq z_{1-\alpha}]\), which is \(\alpha\). Thus, the \(t\)-test is pointwise asymptotically level \(\alpha\), assuming the underlying distribution has a finite nonzero variance. However, the \(t\)-test is not uniformly asymptotically level \(\alpha\). This issue will be studied more closely in Section 13.4. For sufficiently large \(n\), the actual rejection probability \(\alpha_{n}(F)\) will be close to the nominal level \(\alpha\); how close depends on \(F\) and \(n\). For entries to the literature dealing with this dependence, see Cressie (1980), Tan (1982), Benjamini (1983), and Edelman (1990). Other robust approaches for testing the mean are discussed in Sutton (1993) and Chen (1995). The use of permutation and resampling methods will be deferred to Chapters 17 and 18.

To study the corresponding test of variance, suppose first that the mean \(\xi\) is 0. When \(F\) is normal, the UMP test of \(H:\sigma=\sigma_{0}\) against \(\sigma>\sigma_{0}\) rejects when \(\sum X_{i}^{2}/\sigma_{0}^{2}\) is too large, where the null distribution of \(\sum X_{i}^{2}/\sigma_{0}^{2}\) is \(\chi_{n}^{2}\). By the Central Limit Theorem,

\[\frac{1}{\sqrt{n}}(\sum X_{i}^{2}-n\sigma_{0}^{2})\stackrel{{ d}}{{\to}}N(0,2\sigma_{0}^{4})\]

as \(n\to\infty\), since \(\mbox{Var}(X_{i}^{2})=2\sigma_{0}^{4}\). If the rejection region is written as

\[\frac{\sum X_{i}^{2}-n\sigma_{0}^{2}}{\sqrt{2n}\sigma_{0}^{2}}\geq C_{n}\,\]

it follows that \(C_{n}\to z_{1-\alpha}\).

Suppose now instead that the \(X\)'s are distributed according to a distribution \(F\) with \(E(X_{i})=0\), \(E(X_{i}^{2})=Var(X_{i})=\sigma^{2}\), and \(Var(X_{i}^{2})=\gamma^{2}\). Then,

\[\sum(X_{i}^{2}-n\sigma_{0}^{2})/\sqrt{n}\stackrel{{ d}}{{\to}}N(0, \gamma^{2})\]

when \(\sigma=\sigma_{0}\), and the rejection probability \(\alpha_{n}(F)\) of the test tends to

\[\lim\,P\left\{\frac{\sum X_{i}^{2}-n\sigma_{0}^{2}}{\sqrt{2n}\sigma_{0}^{2}} \geq z_{1-\alpha}\right\}=1-\Phi\left(\frac{z_{1-\alpha}\sqrt{2}\sigma_{0}^{2} }{\gamma}\right).\]

Depending on \(\gamma\), which can take on any positive value, the sequence \(\alpha_{n}(F)\) can thus tend to any limit\(<\!\frac{1}{2}\). Even asymptotically and under rather small departures from normality (if they lead to big changes in \(\gamma\)), the size of the \(\chi^{2}\)-test is thus completely uncontrolled.

For sufficiently large \(n\), the difficulty can be overcome by Studentization,1 where one divides the test statistic by a consistent estimate of the asymptotic standard deviation. Letting \(Y_{i}=X_{i}^{2}\) and \(E(Y_{i})=\eta=\sigma^{2}\), the test statistic then reduces to \(\sqrt{n}(\tilde{Y}-\eta_{0})\). To obtain an asymptotically valid test, it is only necessary to divide by a suitable estimator of \(\sqrt{VarY_{i}}\) such as \(\sqrt{\sum(Y_{i}-\tilde{Y})^{2}/n}\). (However, since \(Y_{i}^{2}=X_{i}^{4}\)small changes in the tail of \(X_{i}\) may have large effects on \(Y_{i}^{2}\), and \(n\) may have to be rather large for the asymptotic result to give a good approximation.)

When \(\xi\) is unknown, the normal theory test for \(\sigma^{2}\) is based on \(\sum(X_{i}-\bar{X}_{n})^{2}\), and the sequence

\[\frac{1}{\sqrt{n}}\left[\sum(X_{i}-\bar{X}_{n})^{2}-n\sigma_{0}^{2}\right]= \frac{1}{\sqrt{n}}\left(\sum X_{i}^{2}-n\sigma_{0}^{2}\right)-\frac{1}{\sqrt{n }}n\bar{X}^{2}\]

again has the limit distribution \(N(0,\gamma^{2})\). To see this, note that the distribution of \(\sum(X_{i}-\bar{X}_{n})^{2}\) is independent of \(\xi\) and put \(\xi=0\). Since \(\sqrt{n}\bar{X}\) has a (normal) limit distribution, \(n\bar{X}^{2}\) is bounded in probability and so \(n\bar{X}^{2}/\sqrt{n}\) tends to zero in probability. The result now follows from that for \(\xi=0\) and Slutsky's Theorem.

The above results carry over to the corresponding two-sample problems that were considered in Section 5.3. Consider the two-sample \(t\)-statistic given by (5.28). An extension of the one-sample argument shows that as \(m\), \(n\to\infty\),

\[\frac{\bar{Y}_{n}-\bar{X}_{m}}{\sigma\sqrt{1/m+1/n}}\stackrel{{ d}}{{\to}}N(0,1)\]

while

\[\frac{\sum(X_{i}-\bar{X}_{m})^{2}+\sum(Y_{j}-\bar{Y}_{n})^{2}}{(m+n-2)\sigma^{ 2}}\stackrel{{ p}}{{\to}}1\]

for samples \(X_{1},\ldots,X_{m}\); \(Y_{1},\ldots,Y_{n}\) from any common distribution \(F\) with finite variance. Thus, the rejection probability \(\alpha_{m,n}(F)\) tends to \(\alpha\) for any such \(F\). As will be seen in Section 13.2.3, the same robustness property for the UMP invariant test of equality of \(s\) means also holds.

On the other hand, the \(F\)-test for variances, just like the one-sample \(\chi^{2}\)-test, is extremely sensitive to the assumption of normality. To see this, express the rejection region in terms of \(\log S_{Y}^{2}-\log S_{X}^{2}\), where

\[S_{X}^{2}=\frac{\sum(X_{i}-\bar{X}_{m})^{2}}{m-1}\]

and

\[S_{Y}^{2}=\frac{\sum(Y_{j}-\bar{Y}_{n})^{2}}{n-1}\.\]

Also, suppose that as \(m\) and \(n\to\infty\), \(m/(m+n)\) remains fixed at \(\rho\). By the result for the one-sample problem and the delta method with \(g(u)=\log u\) (Theorem 11.3.4), it is seen that \(\sqrt{m}[\log S_{X}^{2}-\log\sigma^{2}]\) and \(\sqrt{n}[\log S_{Y}^{2}-\log\sigma^{2}]\) both tend in law to \(N(0,\gamma^{2}/\sigma^{4})\) when the \(X\)'s and \(Y\)'s are distributed as \(F\), and hence that \(\sqrt{m+n}[\log S_{Y}^{2}-\log S_{X}^{2}]\) tends in law to the normal distribution with mean 0 and variance \[\frac{\gamma^{2}}{\sigma^{4}}\left(\frac{1}{\rho}+\frac{1}{1-\rho}\right)=\frac{ \gamma^{2}}{\rho(1-\rho)\sigma^{4}}\.\]

In the particular case that \(F\) is normal, \(\gamma^{2}=2\sigma^{4}\) and the variance of the limit distribution is \(2/\rho(1-\rho)\). For other distributions \(\gamma^{2}/\sigma^{4}\) can take on any positive value and, as in the one-sample case, \(\alpha_{n}(F)\) can tend to any limit less than \(\frac{1}{2}\). [For an entry into the extensive literature on more robust alternatives, see for example Conover et al. (1981), Tiku and Balakrishnan (1984), Boos and Brownie (1989), Baker (1995), Hall and Padmanabhan (1997), and Section 2.10 of Hettmansperger and McKean (1998)].

Having found that the rejection probability of the one- and two-sample \(t\)-tests is relatively insensitive to nonnormality (at least for large samples), let us turn to the corresponding question concerning the power of these tests. By similar asymptotic calculations, it can be shown that the same conclusion holds. Power values of the \(t\)-tests obtained under normality are asymptotically valid also for all other distributions with finite variance. This is a useful result if it has been decided to employ a \(t\)-test and one wishes to know what power it will have against a given alternative \(\xi/\sigma\) or \((\eta-\xi)/\sigma\), or what sample sizes are required to obtain a given power.

Recall that there exists a modification of the \(t\)-test, the _permutation_ version of the \(t\)-test discussed in Section 5.9, whose size is independent of \(F\) not only asymptotically but exactly. Moreover, we will see in Section 17.2 that its asymptotic power is equal to that of the \(t\)-test. It may seem that the permutation \(t\)-test has all the properties one could hope for. However, this overlooks the basic question of whether the \(t\)-test itself, which is optimal under normality, will retain a high standing with respect to its competitors under other distributions. The \(t\)-tests are in fact not robust in this sense. Some tests which are preferable when a broad spectrum of distributions \(F\) is considered possible were discussed in Section 6.9. A permutation test with this property has been proposed by Lambert (1985).

As a last problem, consider the level of the two-sample \(t\)-test when the variances \({\rm Var}(X_{i})=\sigma^{2}\) and \({\rm Var}(Y_{j})=\tau^{2}\) may differ (as in the Behrens-Fisher problem), and the assumption of normality may fail as well. As before, one finds that \((\bar{Y}_{m}-\bar{X}_{n})/\sqrt{\sigma^{2}/m+\tau^{2}/n}\) tends in law to \(N(0,1)\) as \(m\), \(n\to\infty\), while \(S_{X}^{2}=\sum(X_{i}-\bar{X}_{m})^{2}/(m-1)\) and \(S_{Y}^{2}=\sum(Y_{i}-\bar{Y}_{n})^{2}/(n-1)\), respectively, tend to \(\sigma^{2}\) and \(\tau^{2}\) in probability. If \(m\) and \(n\) tend to \(\infty\) through a sequence with fixed proportion \(m/(m+n)=\rho\), the squared denominator of the \(t\)-statistic,

\[D^{2}=\frac{m-1}{m+n-2}S_{X}^{2}+\frac{n-1}{m+n-2}S_{Y}^{2}\,\]

tends in probability to \(\rho\sigma^{2}+(1-\rho)\tau^{2}\), and the limit of

\[t=\frac{1}{\sqrt{\frac{1}{m}+\frac{1}{n}}}\left(\frac{\bar{Y}_{n}-\bar{X}_{m}} {\sqrt{\frac{\sigma^{2}}{m}+\frac{\tau^{2}}{n}}}\cdot\frac{\sqrt{\frac{\sigma ^{2}}{m}+\frac{\tau^{2}}{n}}}{D}\right)\]is normal with mean zero and variance

\[\frac{(1-\rho)\sigma^{2}+\rho\tau^{2}}{\rho\sigma^{2}+(1-\rho)\tau^{2}}. \tag{13.2}\]

The ratio (13.2) is exactly one if and only if \(\rho=\frac{1}{2}\) or \(\sigma=\tau\). When \(m=n\), so that \(\rho=\frac{1}{2}\), the \(t\)-test thus has approximately the right level even if \(\sigma\) and \(\tau\) are far apart. The accuracy of this approximation for different values of \(m=n\) and \(\tau/\sigma\) is discussed by Ramsey (1980) and Posten et al. (1982). However, when \(\rho\neq\frac{1}{2}\), the actual size of the test can differ greatly from the nominal level \(\alpha\) even for large \(m\) and \(n\). An approximate test of the hypothesis \(H:\eta=\xi\) when \(\sigma,\tau\) are not assumed equal, which asymptotically is free of this difficulty, can be obtained through Studentization, i.e., by replacing \(D^{2}\) with \((1/m)S_{X}^{2}+(1/n)S_{Y}^{2}\) and referring the resulting statistic to the standard normal distribution. This approximation is very crude, and not reliable unless \(m\) and \(n\) are fairly large. A refinement, the _Welch approximate \(t\)-test_, refers the resulting statistic not to the standard normal but to the \(t\)-distribution with a random number of degrees of freedom \(f\) given by

\[\frac{1}{f}=\left(\frac{R}{1+R}\right)^{2}\frac{1}{m-1}+\frac{1}{(1+R)^{2}} \cdot\frac{1}{n-1}\,\]

where \(R=(nS_{X}^{2})/(mS_{Y}^{2})\).2 When the \(X\)'s and \(Y\)'s are normal, the actual level of this test has been shown to be quite close to the nominal level for sample sizes as small as \(m=4\), \(n=8\) and \(m=n=6\) [see Wang (1971)]. A further refinement will be mentioned in Section 18.5. A simple but crude approach that controls the level is to use as degrees of freedom the smaller of \(n-1\) and \(m-1\), as remarked by Scheffe (1970). Two-sample permutation tests will be studied in Section 17.3.

Footnote 2: For a variant see Fenstad (1983).

The robustness of the level of Welch's test against nonnormality is studied by Yuen (1974), who shows that for heavy-tailed distributions the actual level tends to be considerably smaller than the nominal level (which leads to an undesirable loss of power), and who proposes an alternative. Some additional results are discussed in Scheffe (1970) and in Tiku and Singh (1981). The robustness of some quite different competitors of the \(t\)-test is investigated in Pratt (1964).

For testing the equality of \(s\) normal means with \(s>2\), the classical test based on the \(F\)-statistic (7.19) is not robust, even if all the observations are normally distributed, regardless of the sample sizes (Scheffe (1959), Problem 13.25); again, the problem is due to the assumption of a common variance. More appropriate tests for this generalized Behrens-Fisher problem have been proposed by Welch (1951), James (1951), and Brown and Forsythe (1974a), and is further discussed by Clinch and Kesselman (1982), Hettmansperger and McKean (1998) and Chapter 10 of Pesarin (2001). The corresponding robustness problem for more general linear hypotheses is treated by James (1954a, 1954b) and Johansen (1980); see also Rothenberg (1984).

#### Effect of Dependence

The one-sample \(t\)-test arises when a sequence of measurements \(X_{1},\ldots,X_{n}\), is taken of a quantity \(\xi\), and the \(X\)'s are assumed to be independently distributed as \(N(\xi,\sigma^{2})\). The effect of nonnormality on the level of the test was discussed in the preceding subsection. Independence may seem like a more innocuous assumption. However, it has been found that observations occurring close in time or space are often positively correlated [Student (1927), Hotelling (1961), Cochran (1968)]. The present section will therefore be concerned with the effect of this type of dependence.

**Lemma 13.2.1**: _Let \(X_{1},\ldots,X_{n}\) be jointly normally distributed with common marginal distribution \(N(0,\sigma^{2})\) and with correlation coefficients \(\rho_{i,j}=\mathrm{corr}(X_{i},X_{j})\). Assume that_

\[\frac{1}{n}\sum_{i\neq j}\rho_{i,j}\to\gamma \tag{13.3}\]

_and_

\[\frac{1}{n^{2}}\sum_{i\neq j}\rho_{i,j}^{2}\to 0 \tag{13.4}\]

_as \(n\to\infty\). Then, (i) the distribution of the \(t\)-statistic \(t_{n}\) defined in Equation (13.1) (with \(\xi_{0}=0\)) tends to the normal distribution \(N(0,1+\gamma)\); (ii) if \(\gamma\neq 0\), the level of the \(t\)-test is not robust even asymptotically as \(n\to\infty\). Specifically, if \(\gamma>0\), the asymptotic level of the \(t\)-test carried out at nominal level \(\alpha\) is_

\[1-\Phi\left(\frac{z_{1-\alpha}}{\sqrt{1+\gamma}}\right)>1-\Phi(z_{1-\alpha})= \alpha\;.\]

Proof. (i): Since the \(X_{i}\) are jointly normal, the numerator \(\sqrt{n}\tilde{X}_{n}\) of \(t_{n}\) is also normal, with mean zero and variance

\[Var\big{(}\sqrt{n}\tilde{X}\big{)}=\sigma^{2}\left[1+\frac{1}{n}\sum_{i\neq j} \rho_{i,j}\right]\to\sigma^{2}(1+\gamma)\;, \tag{13.5}\]

and hence tends in law to \(N(0,\sigma^{2}(1+\gamma))\). The denominator of \(t_{n}\) is the square root of

\[S_{n}^{2}=\frac{1}{n-1}\sum X_{i}^{2}-\frac{n}{n-1}\tilde{X}_{n}^{2}\;.\]

By (13.5), \(Var(\tilde{X}_{n})\to 0\) and so \(\tilde{X}_{n}\stackrel{{ P}}{{\to}}0\). A calculation similar to (13.5) shows that \(Var(n^{-1}\sum_{i=1}^{n}X_{i}^{2})\to 0\) (Problem 13.4). Thus, \(n^{-1}\sum_{i=1}^{n}X_{i}^{2}\stackrel{{ P}}{{\to}}\sigma^{2}\) and so \(S_{n}\stackrel{{ P}}{{\to}}\sigma\). By Slutsky's Theorem, the distribution of \(t_{n}\) therefore tends to \(N(0,1+\gamma)\).

The implications (ii) are obvious.

Under the assumptions of Lemma 13.2.1, the joint distribution of the \(X\)'s is determined by \(\sigma^{2}\) and the correlation coefficients \(\rho_{i,\,j}\), with the asymptotic level of the \(t\)-test depending only on \(\gamma\). The following examples illustrating different correlation structures show that even under rather weak dependence of the observations, the assumptions of Lemma 13.2.1 are satisfied with \(\gamma\neq 0\), and hence that the level of the \(t\)-test is quite sensitive to the assumption of independence.

Model A. (Cluster Sampling) Suppose the observations occur in \(s\) groups (or clusters) of size \(m\), and that any two observations within a group have a common correlation coefficient \(\rho\), while those in different groups are independent. (This may be the case, for instance, when the observations within a group are those taken on the same day or by the same observer, or involve some other common factor.) Then (Problem 13.6),

\[Var(\bar{X})=\frac{\sigma^{2}}{ms}[1+(m-1)\rho]\,\]

which tends to zero as \(s\to\infty\). The conditions of the lemma hold with \(\gamma=(m-1)\rho\), and the level of the \(t\)-test is not asymptotically robust as \(s\to\infty\). In particular, the test overstates the significance of the results when \(\rho>0\).

To provide a specific structure leading to this model, denote the observations in the \(i\)th group by \(X_{i,\,j}\) (\(j=1,\ldots,m\)), and suppose that \(X_{i,\,j}=A_{i}+U_{i,\,j}\), where \(A_{i}\) is a factor common to the observations in the \(i\)th group. If the \(A\)'s and \(U\)'s (none of which are observable) are all independent with normal distributions \(N(\xi,\,\sigma_{A}^{2})\) and \(N(0,\,\sigma_{0}^{2})\), respectively, then the joint distribution of the \(X\)'s is that prescribed by Model A with \(\sigma^{2}=\sigma_{A}^{2}+\sigma_{0}^{2}\) and \(\rho=\sigma_{A}^{2}/\sigma^{2}\).

Model B. (Moving- Average Process) When the dependence of nearby observations is not due to grouping as in Model A, it is often reasonable to assume that \(\rho_{i,\,j}\) depends only on \(|j-i|\) and is nonincreasing in \(|j-i|\). Let \(\rho_{i,i+k}\) then be denoted by \(\rho_{k}\), and suppose that the correlation between \(X_{i}\) and \(X_{i+k}\) is negligible for \(k>m\) (\(m\) an integer \(<n\)), so that one can put \(\rho_{k}=0\) for \(k>m\). Then the conditions for Lemma 13.2.1 are satisfied with

\[\gamma=2\sum_{k=1}^{m}\rho_{k}\.\]

In particular, if \(\rho_{1},\,\ldots,\,\rho_{m}\) are all positive, the \(t\)-test is again too liberal.

A specific structure leading to Model B is given by the moving-average process

\[X_{i}=\xi+\sum_{j=0}^{m}\beta_{j}U_{i+j}\,\]

where the \(U\)'s are independent \(N(0,\,\sigma_{0}^{2})\). Such a process was discussed in Example 12.4.1 and Theorem 12.4.1. The variance \(\sigma^{2}\) of the \(X\)'s is then \(\sigma^{2}=\sigma_{0}^{2}\sum_{j=0}^{m}\beta_{j}^{2}\) and \[\rho_{k}=\left\{\begin{array}{ll}\sum\limits_{i=0}^{n-k}\beta_{i}\beta_{i+k}\\ \sum\limits_{j=0}^{m}\beta_{j}^{2}\\ 0\end{array}\right.\text{for}&k\leq m,\\ 0&\text{for}&k>m.\end{array}\]

Model C. (First- Order Autoregressive Process) A simple model for dependence in which the \(|\rho_{k}|\) are decreasing in \(k\) but \(\neq 0\) for all \(k\) is the _first-order autoregressive process_ previously introduced in Example 12.4.2. Here, we assume the underlying distribution of the observations is normal. Define

\[X_{i+1}=\xi+\beta(X_{i}-\xi)+U_{i+1},\ \ \ \ \ |\beta|<1,\ \ \ i=1,\ldots,n\,\]

with the \(U_{i}\) independent \(N(0,\sigma_{0}^{2})\). If \(X_{1}\) is \(N(\xi,\tau^{2})\), the marginal distribution of \(X_{i}\) for \(i>1\) is normal with mean \(\xi\) and variance \(\sigma_{i}^{2}=\beta^{2}\sigma_{i-1}^{2}+\sigma_{0}^{2}\). The variance of \(X_{i}\) will thus be independent of \(i\) provided \(\tau^{2}=\sigma_{0}^{2}/(1-\beta^{2})\). For the sake of simplicity we shall assume this to be the case, and take \(\xi\) to be zero. From

\[X_{i+k}=\beta^{k}X_{i}+\beta^{k-1}U_{i+1}+\beta^{k-2}U_{i+2}+\cdots+\beta U_{i +k-1}+U_{i+k}\]

it then follows that \(\rho_{k}=\beta^{k}\), so that the correlation between \(X_{i}\) and \(X_{j}\) decreases exponentially with increasing \(|j-i|\). The assumptions of Lemma 13.2.1 are again satisfied, and \(\gamma=2\beta/(1-\beta)\). Thus, in this case too, the level of the \(t\)-test is not asymptotically robust. [Some values of the actual asymptotic level when the nominal level is 0.05 or 0.01 are given by Gastwirth and Rubin (1971).]

In Models A, B, and C, we have seen that the null rejection probability may be far from the nominal level, even in large samples and when the underlying distributions are normal. One can consider alternatives to both normality and independence simultaneously, using the results in Section 12.4, but the general conclusions remain the same. In summary, the effect of dependence on the level of the \(t\)-test is more serious than that of nonnormality.

In order to robustify the test against general dependence through studentization (as was done in the two-sample case with unequal variances), it is necessary to consistently estimate \(\gamma\), which implicitly depends on estimation of all the \(\rho_{i,j}\). Unfortunately, the number of parameters \(\rho_{i,j}\) exceeds the number of observations. However, robustification is possible against some types of dependence. For example, it may be reasonable to assume a model such as A-C so that it is only required to estimate a reduced number of correlations.3 Some specific procedures of this type are discussed by Albers (1978), [and for an associated sign test by Falk and Kohne (1984)]. Such robust procedures will in fact often also be insensitive to the assumption of normality, as can be shown by appealing to an appropriate Central Limit Theorem for dependent variables, such as those in Section 12.4. The validity of these procedures is of course limited to the particular model assumed, including the value of a parameter such as \(m\) in Models A and B. In fact, robustification is achievable for fairly general classes of models with dependence by using an appropriate bootstrap method; see Problem 18.19 and Lahiri (2003). Alternatively, one can use subsampling, as in Romano and Thombs (1996); see Section 18.7.

The results of the present section easily extend to the case of the two-sample \(t\)-test, when each of the two series of observations shows dependence of the kind considered here.

#### Robustness in Linear Models

In this section, we consider the large-sample robustness properties of some of the linear model tests discussed in Chapter 7. As in Section 13.2.1, we focus on the effect of distribution.

A large class of these testing situations is covered by the following general model, which was discussed in Problem 7.8. Let \(X_{1}\),..., \(X_{n}\) be independent with \(E(X_{i})=\xi_{i}\) and \(Var(X_{i})=\sigma^{2}<\infty\), where we assume the vector \(\xi\) to lie in an \(s\)-dimensional subspace \(\Pi_{\Omega}\) of \(\mathbb{R}^{n}\), defined by the following parametric set of equations

\[\xi_{i}=\sum_{j=1}^{s}a_{i,j}\beta_{j}\,\ \ \ \ \ i=1,\ldots,n. \tag{13.6}\]

Here the \(a_{i,j}\) are known coefficients and the \(\beta_{j}\) are unknown parameters. In matrix form, the \(n\times 1\) vector \(\xi\) with \(i\)th component \(\xi_{i}\) satisfies

\[\xi=A\beta\, \tag{13.7}\]

where \(A\) is an \(n\times s\) matrix having (\(i\), \(j\)) entry \(a_{i,j}\) and \(\beta\) is an \(s\times 1\) vector with \(j\)th component \(\beta_{j}\). It is assumed \(A\) is known and of rank \(s\). In the asymptotics below, the \(a_{i,j}\) may depend on \(n\), as will \(\xi\), but \(s\) remains fixed. Throughout, the notation will suppress this dependence on \(n\). The parameter vector \(\beta\) does not change with \(n\).

The least squares estimators \(\hat{\xi}_{1}\),..., \(\hat{\xi}_{n}\) of \(\xi_{1}\),..., \(\xi_{n}\) are defined as the values of \(\xi_{i}\) minimizing

\[\sum_{i=1}^{n}(X_{i}-\xi_{i})^{2}\]

subject to \(\xi\in\Pi_{\Omega}\), where \(\Pi_{\Omega}\) is the space spanned by the \(s\) columns of \(A\). Correspondingly, the least squares estimators \(\hat{\beta}_{1}\),..., \(\hat{\beta}_{s}\) of \(\beta_{1}\),..., \(\beta_{s}\) are the values of \(\beta_{j}\) minimizing

\[\sum_{i=1}^{n}(X_{i}-\sum_{j=1}^{s}a_{i,j}\beta_{j})^{2}\.\]By taking partial derivatives of this last expression with respect to the \(\beta_{j}\), it is seen that the \(\hat{\beta}_{j}\) are solutions of the equations

\[A^{\top}A\beta=A^{\top}X\]

and so

\[\hat{\beta}=(A^{\top}A)^{-1}A^{\top}X\.\]

(The fact that \(A^{\top}A\) is nonsingular follows from Problem 6.3.) Thus,

\[\hat{\xi}=PX\,\]

where

\[P=A(A^{\top}A)^{-1}A^{\top}. \tag{13.8}\]

In fact, \(\hat{\xi}\) is the projection of \(X\) into the space \(\Pi_{\Omega}\). (These estimators formed the basis of optimal invariant tests studied in Chapter 7.) Some basic properties of \(P\) and \(\hat{\xi}\) are recorded in the following lemma.

**Lemma 13.2.2**: _(i) The matrix \(P\) defined by (13.8) is symmetric (\(P=P^{\top}\)) and idempotent (\(P^{2}=P\)). (ii) \(X-\hat{\xi}\) is orthogonal to \(\hat{\xi}\); that is,_

\[\hat{\xi}^{\top}(X-\hat{\xi})=0\.\]

Proof. The proof of (i) follows by matrix algebra (Problem 13.10). To prove (ii), note that

\[\hat{\xi}^{\top}(X-\hat{\xi})=(PX)^{\top}(X-PX)=X^{\top}P^{\top}(X-PX)\]

\[=X^{\top}P^{\top}X-X^{\top}P^{\top}PX=0\,\]

since by (i) \(P^{\top}P=P^{\top}\).

Note that \(\hat{\beta}_{j}\) is a linear combination of the \(X_{i}\). Thus, if the \(X_{i}\) are normally distributed, so are the \(\hat{\beta}_{j}\). However, we would like to understand their properties when then the \(X_{i}\) are not normally distributed. We shall now suppose that the model (13.6) is embedded in a sequence of such models defined by matrices \(A^{(n)}_{i,j}\), with \(s\) fixed and \(n\to\infty\). Suppose that the \(X\)s are not normal but given by

\[X_{i}=U_{i}+\xi_{i}\,\]

where the \(U\)s are i.i.d. according to a distribution \(F\) with mean 0 and variance \(\sigma^{2}<\infty\). Since \(E(X)=\xi=A\beta\), the least squares estimator \(\hat{\beta}\) is unbiased; that is,

\[E(\hat{\beta})=(A^{\top}A)^{-1}A^{\top}E(X)=\beta. \tag{13.9}\]Without the assumption of normality, the asymptotic normality of \(\hat{\beta}_{j}\) can be established by the following lemma, which can be obtained as a consequence of the Lindeberg Central Limit Theorem (Problem 13.11).

**Lemma 13.2.3**: _Let \(Y_{1}\), \(Y_{2},\ldots\) be independent and identically distributed with mean zero and finite variance \(\sigma^{2}\). (i) Let \(c_{1}\), \(c_{2},\ldots\) be a sequence of constants. Then a sufficient condition for_

\[\frac{\sum_{i=1}^{n}c_{i}Y_{i}}{\sqrt{\sum c_{i}^{2}}}\stackrel{{ d}}{{\to}}N(0,\sigma^{2})\]

_is that_

\[\frac{\max_{i=1,\ldots,n}c_{i}^{2}}{\sum\limits_{j=1}^{n}c_{j}^{2}}\to 0\quad as\ \ n\to\infty. \tag{13.10}\]

_(ii) More generally, suppose \(C_{n,1},\ldots,C_{n,n}\) is a sequence of random variables, independent of \(Y_{1}\),..., \(Y_{n}\). Then, a sufficient condition for_

\[\frac{\sum_{i=1}^{n}C_{n,i}Y_{i}}{\sqrt{\sum C_{n,i}^{2}}}\stackrel{{ d}}{{\to}}N(0,\sigma^{2})\]

_is_

\[\frac{\max_{i=1,\ldots,n}C_{n,i}^{2}}{\sum\limits_{j=1}^{n}C_{n,j}^{2}} \stackrel{{ P}}{{\to}}0\quad as\ \ n\to\infty. \tag{13.11}\]

_Moreover, (13.11) implies that_

\[P\left\{\frac{C_{n,i}Y_{i}}{\sqrt{\sum C_{n,i}^{2}}}\leq z|C_{n,1},\ldots,C_{n,n}\right\}\stackrel{{ P}}{{\to}}\Phi(\frac{z}{\sigma})\.\]

Condition (13.10) prevents the \(c\)'s from increasing so fast that the last term essentially dominates the sum, in which case there is no reason to expect asymptotic normality.

**Example 13.2.1**: Suppose \(U_{1}\), \(U_{2}\),... are i.i.d. with mean 0 and finite nonzero variance \(\sigma^{2}\). Consider the simple regression model

\[X_{i}=\alpha+\beta t_{i}+U_{i}\,\]

where the \(t_{i}\) are known and not all equal. The least squares estimator \(\hat{\beta}\) of \(\beta\) satisfies\[\hat{\beta}-\beta=\frac{\sum(X_{i}-\alpha-\beta t_{i})(t_{i}-\bar{t})}{\sum(t_{i}- \bar{t})^{2}}\.\]

By Lemma 13.2.3,

\[\frac{(\hat{\beta}-\beta)\sqrt{\sum(t_{i}-\bar{t})^{2}}}{\sigma}\stackrel{{ d}}{{\rightarrow}}N(0,1)\]

provided

\[\frac{\max(t_{i}-\bar{t})^{2}}{\sum(t_{j}-\bar{t})^{2}}\to 0. \tag{13.12}\]

Condition (13.12) holds in the case of equal spacing \(t_{i}=a+i\Delta\), but not when the \(t\)'s grow exponentially, for example, when \(t_{i}=2^{i}\) (Problem 13.12).

Consider the hypothesis

\[H:\theta=\sum_{j=1}^{s}b_{j}\beta_{j}=0\, \tag{13.13}\]

where the \(b\)'s are known constants with \(\sum b_{j}^{2}=1\). Assume without loss of generality that \(A^{\top}A=I\), the identity matrix, so that the columns of \(A\) are mutually orthogonal and of length one. The least squares estimator of \(\theta\) is given by

\[\hat{\theta}=\sum_{j=1}^{s}b_{j}\hat{\beta}_{j}=\sum_{i=1}^{n}d_{i}X_{i}\, \tag{13.14}\]

where by (13.7)

\[d_{i}=\sum_{j=1}^{s}a_{i,j}b_{j} \tag{13.15}\]

(Problem 13.13). By the assumption that the columns of \(A\) are orthogonal, \(\sum d_{i}^{2}=\sum b_{j}^{2}=1\). So, under \(H\),

\[E(\hat{\theta})=\sum_{j=1}^{s}E(b_{j}\hat{\beta}_{j})=\sum_{j=1}^{s}b_{j}\beta _{j}=0\]

and

\[Var(\hat{\theta})=Var(\sum_{i=1}^{n}d_{i}X_{i})=\sigma^{2}\sum_{i=1}^{n}d_{i}^ {2}=\sigma^{2}\.\]

The uniformly most powerful invariant test rejects \(H\) when the \(t\)-statistic satisfies

[MISSING_PAGE_FAIL:628]

\[d_{i}^{2}=\left(\sum_{j}a_{i,j}b_{j}\right)^{2}\leq\sum_{j=1}^{s}a_{i,j}^{2}\,\]

with equality holding when \(b_{j}=a_{i,j}/\sqrt{\sum_{k}a_{i,k}^{2}}\). The desired maximum of \(d_{i}^{2}\) is therefore \(\sum_{j}a_{i,j}^{2}\), and

\[\max_{i}\sum_{j=1}^{s}a_{i,j}^{2}\to 0\quad as\ \ n\to\infty \tag{13.18}\]

is a sufficient condition for the asymptotic normality of every \(\hat{\theta}\) of the form (13.14).

Condition (13.18) depends on the particular parametrization (13.6) chosen for \(\Pi_{\Omega}\). Note however that

\[\sum_{j=1}^{s}a_{i,j}^{2}=\Pi_{i,i}\, \tag{13.19}\]

where \(\Pi_{i,j}\) is the \((i,\,j)\) element of the projection matrix \(P\).

This shows that the value of \(\Pi_{i,i}\) is coordinate free, i.e., it is unchanged by an arbitrary change of coordinates \(\beta^{*}=B^{-1}\beta\), where \(B\) is a nonsingular matrix, since

\[\xi=A\beta=AB\beta^{*}=A^{*}\beta^{*}\]

with \(A^{*}=AB\), and

\[P^{*}=AB(B^{\top}A^{\top}AB)^{-1}B^{\top}A^{\top}=ABB^{-1}(A^{\top}A)^{-1}(B^{ \top})^{-1}BA=P\.\]

Hence, (13.18) is equivalent to the coordinate-free Huber condition

\[\max_{i}\Pi_{i,i}\to 0\quad as\ \ n\to\infty. \tag{13.20}\]

For evaluating \(\Pi_{i,i}\), it is helpful to note that

\[\hat{\xi}_{i}=\sum_{j=1}^{n}\Pi_{i,j}X_{j}\quad(i=1,\ldots,n),\]

so that \(\Pi_{i,i}\) is simply the coefficient of \(X_{i}\) in \(\hat{\xi}_{i}\), which must be calculated in any case to carry out the test.

If \(\Pi_{i,i}\leq M_{n}\) for all \(i=1,\ldots,n\), then also \(\Pi_{i,j}\leq M_{n}\) for all \(i\) and \(j\). This follows from the fact that there exists a nonsingular \(E\) with \(P=EE^{\top}\), on applying the Cauchy-Schwarz inequality to the \((i,\,j)\) element of \(EE^{\top}\). Condition (13.20) is therefore equivalent to \[\max_{i,j}\,\Pi_{i,j}\to 0\quad as\;\;n\to\infty\;. \tag{13.21}\]

**Example 13.2.2**: (**Example 13.2.1, continued**) In Example 13.2.1, the coefficient of \(X_{i}\) in \(\hat{\xi}_{i}=\hat{\alpha}+\hat{\beta}t_{i}\) is

\[\Pi_{i,i}=\frac{1}{n}+\frac{(t_{i}-\tilde{t})^{2}}{\sum(t_{j}-\tilde{t})^{2}}\]

and the Huber condition reduces to Condition (13.12) found earlier.

**Example 13.2.3**: (**Two-way Layout**) Consider the two-way layout with \(m\) observations per cell and the additive model

\[\xi_{i,j,k}=E(X_{i,j,k})=\mu+\alpha_{i}+\beta_{j}\]

with

\[\sum_{i}\alpha_{i}=\sum_{j}\beta_{j}=0\;,\]

\(i=1,\ldots,a;\;\;j=1,\ldots b;\;\;k=1,\ldots m\). It is easily seen (Problem 13.14) that, for fixed \(a\) and \(b\), the Huber condition is satisfied as \(m\to\infty\).

Let us next generalize the hypothesis (13.13) to hypotheses which impose several linear constraints. Without loss of generality, choose the parametrization in (13.6) in such a way that the \(s\) columns of \(A\) are orthogonal and of length one and make the transformation

\[Y=CX\]

(as used in (7.1)), where \(C\) is orthogonal and the first \(s\) rows of \(C\) are equal to those of \(A^{\top}\), say

\[C=\left(\begin{array}{c}A^{\top}\\ D\end{array}\right) \tag{13.22}\]

for some \((n-s)\times n\) matrix \(D\). If \(\eta_{i}=E(Y_{i})\), we then have that

\[\eta=\left(\begin{array}{c}A^{\top}\\ D\end{array}\right)A\beta=(\beta_{1},\ldots,\beta_{s},0,\ldots,0)^{\top}\;. \tag{13.23}\]

By the orthogonality of \(C\), the \(Y_{i}\) are independent with \(Y_{i}\) distributed as \(N(\eta_{i},\sigma^{2})\), where \(\eta_{i}=\beta_{i}\) for \(i=1,\ldots,s\) and \(\eta_{i}=0\) for \(i=s+1,\ldots,n\). We want to test

\[H:\;\;\sum_{j=1}^{s}\alpha_{i,j}\eta_{j}=0\;;\quad i=1,\ldots,r\]

where we shall assume that the \(r\) vectors \((\alpha_{i,1},\ldots,\alpha_{i,s})^{\top}\) are orthogonal and of length one. Then the variables\[Z_{i}=\begin{cases}\sum_{j=1}^{n}\alpha_{i,j}Y_{j}&i=1,\ldots,r\\ Y_{i}&i=s+1,\ldots,n\end{cases} \tag{13.24}\]

are independent \(N(\zeta_{i},\sigma^{2})\) with

\[\zeta_{i}=\begin{cases}\sum_{j=1}^{s}\alpha_{i,j}\eta_{j}&i=1,\ldots,r\\ \eta_{i}&i=r+1,\ldots,s\\ 0&i=s+1,\ldots,n\end{cases} \tag{13.25}\]

The standard UMPI test of \(H:\ \zeta_{1}=\cdots=\zeta_{r}=0\) rejects when

\[\frac{\sum_{i=1}^{r}Z_{i}^{2}/r}{\sum_{j=s+1}^{n}Z_{j}^{2}/(n-s)}>k\, \tag{13.26}\]

where \(k\) is determined so that the probability of (13.26) is \(\alpha\) when the \(Z\)s are normal and \(H\) holds.

As before, suppose that the model (13.6) is embedded in a sequence of models defined by matrices \(A_{i,j}^{(n)}\), with \(s\) fixed and \(n\to\infty\). Suppose that the \(X\)s satsify

\[X_{i}=U_{i}+\xi_{i}\,\]

where the \(U\)s are i.i.d. according to a distribution \(F\) with mean \(0\) and variance \(\sigma^{2}<\infty\). We then have the following robustness result.

**Theorem 13.2.1**: _Let \(\alpha_{n}(F)\) denote the rejection probability of the test (13.26) when the Us have distribution \(F\) and the null hypothesis constraints are satisfied. Then, \(\alpha_{n}(F)\to\alpha\) provided_

\[\max_{i}\sum_{j=1}^{s}(a_{i,j}^{(n)})^{2}\to 0 \tag{13.27}\]

_or equivalently_

\[\max\,\Pi_{i,i}^{(n)}\to 0\,\]

_where \(\Pi_{i,i}^{(n)}\) is the \(i\)th diagonal element of \(P=A(A^{\top}A)^{-1}A^{\top}\)._

Proof. We must show that the limiting distribution of (13.26) is the same as when \(F\) is normal. First, we shall show that the denominator of (13.26) satisfies

\[\frac{1}{n-s}\sum_{j=s+1}^{n}Z_{j}^{2}\stackrel{{ P}}{{\to}} \sigma^{2}. \tag{13.28}\]

Note that \(X=C^{\top}Y\) and \(Y=QZ\) where \(C^{\top}\) and \(Q\) are both orthogonal. Therefore,\[\frac{1}{n-s}\sum_{j=s+1}^{n}Z_{j}^{2}=\frac{n}{n-s}\left[\frac{1}{n}\sum_{i=1}^{n }Z_{i}^{2}\right]-\frac{1}{n-s}\sum_{i=1}^{s}Z_{i}^{2}\]

\[=\frac{n}{n-s}\cdot\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-\frac{1}{n-s}\sum_{i=1}^{ s}Z_{i}^{2}\.\]

To see that this tends to \(\sigma^{2}\) in probability, we first show that

\[\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}\stackrel{{ P}}{{\to}}\sigma^{2}\.\]

But,

\[\frac{\sum_{i=1}^{n}X_{i}^{2}}{n}=\frac{\sum_{i=1}^{n}(X_{i}-\xi_{i})^{2}}{n}+ \frac{2\sum_{i=1}^{n}\xi_{i}X_{i}}{n}-\frac{\sum_{i=1}^{n}\xi_{i}^{2}}{n}\.\]

The first term on the right tends to \(\sigma^{2}\) in probability, by the Weak Law of Large Numbers. By the orthogonality of \(C\), the last term is equal to \(\sum_{i=1}^{s}\beta_{i}^{2}/n\), which tends to 0 since \(s\) is fixed. It is easily checked that the middle term has a mean and variance which tend to 0. Hence, \(\sum X_{i}^{2}/n\) tends in probability to \(\sigma^{2}\). Next, we show that

\[\frac{\sum_{i=1}^{s}Z_{i}^{2}}{n}\stackrel{{ P}}{{\to}}0\.\]

It suffices to show

\[\frac{\sum_{i=1}^{s}E(Z_{i}^{2})}{n}=\frac{\sum_{i=1}^{s}Var(Z_{i})}{n}+\frac{ \sum_{i=1}^{s}[E(Z_{i})]^{2}}{n}\to 0\.\]

Since \(s\) is fixed and \(Var(Z_{i})=\sigma^{2}\), we only need to show

\[\frac{\sum_{i=1}^{s}[E(Z_{i})]^{2}}{n}\to 0\.\]

For \(i\leq r\),

\[E(Z_{i})=\sum_{j=1}^{s}\alpha_{i,j}\eta_{j}=\sum_{j=1}^{s}\alpha_{i,j}\beta_{j}\]

and

\[[E(Z_{i})]^{2}\leq\sum_{j=1}^{s}\alpha_{i,j}^{2}\sum_{j=1}^{s}\beta_{j}^{2}= \sum_{j=1}^{s}\beta_{j}^{2}\.\]

For \(r+1\leq i\leq s\), \(E(Z_{i})=\beta_{i}\), in which case the same bound holds. Therefore,\[\frac{\sum_{i=1}^{s}[E(Z_{i})]^{2}}{n}\leq\frac{s\sum_{j=1}^{s}\beta_{j}^{2}}{n} \to 0\,\]

and the result (13.28) follows.

Next, we consider the numerator of (13.26). We show the joint asymptotic normality of \((Z_{1},\,\ldots,\,Z_{r})\). By the Cramer-Wold device, it suffices to show that, for any constants \(\gamma_{1},\,\ldots,\,\gamma_{r}\) with \(\sum_{i}\gamma_{i}^{2}=1\),

\[\sum_{i=1}^{r}\gamma_{i}\,Z_{i}\,\stackrel{{ d}}{{\to}}\,N(0,\, \sigma^{2})\.\]

Indeed, since the columns of \(A\) are orthogonal, \(\hat{\beta}_{i}=Y_{i}\) for \(1\leq i\leq s\) and so \(Z_{i}\) is a linear combination of \(\hat{\beta}_{1},\,\ldots,\hat{\beta}_{s}\). But then so is \(\sum_{i}\gamma_{i}\,Z_{i}\) and asymptotic normality follows from the argument for \(\hat{\theta}\) of the form (13.14).

**Example 13.2.4**: **(Test of Homogeneity)** Let \(X_{i,\,j}\) (\(j=1,\,\ldots n_{i};\,\,i=1,\,\ldots,\,s\)) be independently distributed as \(N(\mu_{i},\,\sigma^{2})\). The problem is to test the null hypothesis

\[H:\,\,\mu_{1}=\cdots=\mu_{s}\.\]

In this case, the test (13.26) is UMP invariant and reduces to

\[W^{*}=\frac{\sum n_{i}(X_{i.}-X_{..})^{2}/(s-1)}{\sum\sum(X_{i.,j}-X_{i.})^{2} /(n-s)}\, \tag{13.29}\]

where

\[X_{i.}=\sum_{j}X_{i.,j}/n_{i}\,\quad X_{..}=\sum_{i}\sum_{j}X_{i.,j}/n\]

and \(n=\sum_{i}n_{i}\). If instead of \(X_{i.,j}\) being \(N(\mu_{i},\,\sigma^{2})\), assume that \(X_{i.,j}\) has a distribution \(F(x-\mu_{i})\), where \(F\) is an arbitrary distribution with finite variance. Then, the theorem implies that, if \(\min_{i}n_{i}\to\infty\), then the rejection probability tends to \(\alpha\). In fact, the distributions may even vary within each sample, but it is important that the different samples have a common variance or the result fails; see Problems 13.24 and 13.25.

### Edgeworth Expansions

Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. with c.d.f. \(F\). Let \(\mu(F)\) denote the mean of \(F\), and consider the problem of testing \(\mu(F)=0\). As in Section 13.2.1, let \(\alpha_{n}(F)\) denote the actual rejection probability of the one-sided \(t\)-test under \(F\). It was seen that the \(t\)-test is pointwise consistent in level in the sense that \(\alpha_{n}(F)\to\alpha\) whenever \(F\) has a finite nonzero variance \(\sigma^{2}(F)\). We shall now examine the rate at which the difference \(\alpha_{n}(F)-\alpha\) tends to 0.

In order to study this problem, we will consider expansions of the distribution function of the sample mean, as well as its studentized version. Such expansions are known as _Edgeworth expansions_. Let \(\Phi(\cdot)\) denote the standard normal c.d.f. and \(\varphi(\cdot)\) the standard normal density. Also let

\[\gamma=\gamma(F)=\frac{E_{F}[(X_{i}-\mu(F))^{3}]}{\sigma^{3}(F)}\]

and

\[\kappa=\kappa(F)=\frac{E_{F}[X_{i}-\mu(F))^{4}]}{\sigma^{4}(F)}-3\.\]

The values \(\gamma\) and \(\kappa\) are known as the _skewness_ and _kurtosis_ of \(F\), respectively.

**Theorem 13.3.1**: _Assume \(E_{F}(|X_{i}|^{k+2})<\infty\). Let \(\psi_{F}\) denote the characteristic function of \(F\), and assume_

\[\limsup_{|s|\to\infty}|\psi_{F}(s)|<1. \tag{13.30}\]

_Then,_

\[P_{F}\{\frac{n^{1/2}[\tilde{X}_{n}-\mu(F)]}{\sigma(F)}\leq x\}=\Phi(x)+\sum_{ j=1}^{k}n^{-j/2}\varphi(x)p_{j}(x,\,F)+r_{n}(x,\,F)\, \tag{13.31}\]

_where \(r_{n}(x,\,F)=o(n^{-k/2})\) and \(p_{j}(x,\,F)\) is a polynomial in \(x\) of degree \(3j-1\) which depends on \(F\) through its first \(j+2\) moments. In particular,_

\[p_{1}(x,\,F)=-\frac{1}{6}\gamma(x^{2}-1)\, \tag{13.32}\]

_and_

\[p_{2}(x,\,F)=-x\left[\frac{1}{24}\kappa(x^{2}-3)+\frac{1}{72}\gamma^{2}(x^{4}- 10x^{2}+15)\right]\,. \tag{13.33}\]

_Moreover, the expansion holds uniformly in \(x\) in the sense that, for fixed \(F\),_

\[n^{k/2}\sup_{x}|r_{n}(x,\,F)|\to 0\ \ \ {\rm as}\ n\to\infty.\]

Assumption (13.30) is known as _Cramer's condition_ and can be viewed as a smoothness assumption on \(F\). It holds, for example, if \(F\) is absolutely continuous (or more generally is nonsingular) but fails if \(F\) is a lattice distribution, i.e., \(X_{1}\) can only take on values of the form \(a+jb\) for some fixed \(a\) and \(b\) as \(j\) varies through the integers. A proof of Theorem 13.3.1 can be found in Feller (1971, Section XVI.4)or Bhattacharya and Rao (1976), who also provide formulae for the \(p_{j}(x,F)\) when \(j>2\). The proofs hinge on expansions of characteristic functions.

Note that the term of order \(n^{-1/2}\) is zero if and only if the underlying skewness \(\gamma(F)\) is zero. This shows that the dominant error in using a standard normal approximation to the distribution of the standardized sample mean is due to skewness of the underlying distribution. Expansions such as these hold for many classes of statistics and provide more information than a weak convergence result, such as that provided by the Central Limit Theorem. As an example, the following result provides an Edgeworth expansion for the studentized sample mean. Let \(S_{n}^{2}=\sum_{i}(X_{i}-\bar{X}_{n})^{2}/(n-1)\).

**Theorem 13.3.2**: _Assume \(E_{F}(|X_{i}|^{k+2})<\infty\) and that \(F\) is absolutely continuous.4 Then, uniformly in \(t\),_

Footnote 4: Alternatively, one can assume \(E_{F}(|X_{i}|^{2j+2})<\infty\) and the distribution of \((X_{i},\,X_{i}^{2})\) satisfies the multivariate analogue of Cramer’s condition; see Hall (1992), Chapter 2.

\[P_{F}\{\frac{n^{1/2}[\bar{X}_{n}-\mu(F)]}{S_{n}}\leq t\}=\Phi(t)+\sum_{j=1}^{k} n^{-j/2}\varphi(t)q_{j}(t,\,F)+\bar{r}_{n}(t,\,F)\, \tag{13.34}\]

_where \(n^{k/2}\sup_{t}|\bar{r}_{n}(t,\,F)|\to 0\) and \(q_{j}(t,\,F)\) is a polynomial which depends on \(F\) through its first \(j+2\) moments. In particular,_

\[q_{1}(t,\,F)=\frac{1}{6}\gamma(2t^{2}+1)\, \tag{13.35}\]

_and_

\[q_{2}(t,\,F)=t\left[\frac{1}{12}\kappa(t^{2}-3)-\frac{1}{18}\gamma^{2}(t^{4}+2 t^{2}-3)-\frac{1}{4}(t^{2}+1)\right]\,. \tag{13.36}\]

Note that some authors prefer to provide an Edgeworth expansion as in (13.34). except that \(S_{n}^{2}\) replaces its denominator \(n-1\) with \(n\); then, \(q_{2}(t,\,F)\) would have to be modified as well.

**Example 13.3.1**: (**Expansion for the t-distribution**) Suppose \(F\) is normal \(N(\mu,\,\sigma^{2})\). Let \(t_{n}=n^{1/2}(\bar{X}_{n}-\mu)/S_{n}\). Then, \(\gamma(F)=\kappa(F)=0\). By Theorem 13.3.2,

\[P_{F}\{t_{n}\leq t\}=\Phi(t)-\frac{1}{4n}(t+t^{3})\varphi(t)+o(n^{-1}). \tag{13.37}\]

This result implies a corresponding expansion for the quantiles of the \(t\)-distribution, known as a _Cornish-Fisher expansion_. Specifically, let \(t=t_{n-1,\,1-\alpha}\) be the \(1-\alpha\) quantile of the \(t\)-distribution with \(n-1\) degrees of freedom. We would like to determine \(c=c_{1-\alpha}\) such that

\[t_{n-1,1-\alpha}=z_{1-\alpha}+\frac{c_{1-\alpha}}{n}+o(n^{-1})\.\]

[MISSING_PAGE_EMPTY:2238]

\[\alpha^{t}_{n,1}(F)-\alpha=\Phi(t_{n-1,\alpha})-\alpha+n^{-1/2}\varphi(t_{n-1,1- \alpha})q_{1}(t_{n-1,1-\alpha},F)+O(n^{-1})\.\]

Then, applying (13.38), \(t_{n-1,1-\alpha}-z_{1-\alpha}=O(n^{-1})\), so that a Taylor's expansion yields

\[\alpha^{t}_{n,1}(F)-\alpha=n^{-1/2}\varphi(z_{1-\alpha})q_{1}(z_{1-\alpha},F)+O (n^{-1})\.\]

Therefore,

\[\alpha^{t}_{n,1}(F)-\alpha=O(n^{-1/2})\,\]

but the error in rejection probability is \(O(n^{-1})\) if \(\gamma(F)=0\).

_The Two-sided Case._ Let \(z=z_{1-\frac{\alpha}{2}}\). Then, using the fact that \(\varphi(z)=\varphi(-z)\),

\[\alpha^{z}_{n,2}(F)=P_{F}\{|t_{n}|\geq z\}=1-[P_{F}\{t_{n}\leq z\}-P_{F}\{t_{ n}\leq-z\}]\]

\[=\alpha+n^{-1/2}\varphi(z)[q_{1}(z,F)-q_{1}(-z,F)]+O(n^{-1})\.\]

But, \(q_{1}(\cdot,F)\) is an even function, which implies

\[\alpha^{z}_{n,2}(F)-\alpha=O(n^{-1})\,\]

even if \(\gamma(F)\) is not zero. Similarly, it can be shown that (Problem 13.30)

\[\alpha^{t}_{n,2}(F)-\alpha=O(n^{-1})\.\]

### Nonparametric Inference for the Mean

#### 13.4.1 Uniform Behavior of \(t\)-test

It was seen in Section 13.2.1 that the classical \(t\)-test of the mean is asymptotically pointwise consistent in level for the class \(\mathbf{F}\) of all distributions with finite nonzero variance. In Section 13.3, the orders of error in rejection probability were obtained for a given \(F\). However, these results are not reassuring unless the convergence is uniform in \(F\). If it is not, then for any \(n\), no matter how large, there will exist \(F\) in \(\mathbf{F}\) for which the rejection probability under \(F\), \(\alpha_{n}(F)\), is not even close to \(\alpha\). We shall show below that the convergence is not uniform and that the situation is even worse than what this negative result suggests. Namely, we shall show that for any \(n\), there exist distributions \(F\) for which \(\alpha_{n}(F)\) is arbitrarily close to 1; that is, the size of the \(t\)-test is 1.

Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. real-valued random variables with unknown c.d.f. \(F\in\mathbf{F}\), where \(\mathbf{F}\) is a large nonparametric class of distributions. Let \(\mu(F)\) denote the mean of \(F\) and \(\sigma^{2}(F)\) the variance of \(F\). The goal is to test the null hypothesis \(\mu(F)=0\) versus \(\mu(F)>0\), or perhaps the two-sided alternative \(\mu(F)\neq 0\).

**Theorem 13.4.1**: _For every \(n\), the size of the \(t\)-test is 1 for the family \(\mathbf{F_{0}}\) of all distributions with finite variance._

Proof. Let \(c\) be an arbitrary positive constant less than one and let \(p_{n}=1-c^{1/n}\) so that \((1-p_{n})^{n}=c\). Let \(F=F_{n,c}\) be the distribution that places mass \(1-p_{n}\) at \(p_{n}\) and mass \(p_{n}\) at \(p_{n}-1\), so that \(\mu(F)=0\). With probability \(c\), we have all observations equal to \(p_{n}\). For such a sample, the numerator \(n^{1/2}\tilde{X}_{n}\) of the \(t\)-statistic is \(n^{1/2}p_{n}>0\) while the denominator is 0. Thus, the \(t\)-statistic blows up and the hypothesis will be rejected. The probability of rejection is therefore \(\geq c\), and by taking \(c\) arbitrarily close to 1 the theorem is proved. (Note that one can modify the distributions \(F_{n,c}\) used in the proof to be continuous rather than discrete.)

It follows that the \(t\)-test is not even uniformly asymptotically level \(\alpha\) for the family \(\mathbf{F}_{0}\).

Instead of \(\mathbf{F}_{0}\), one may wish to consider the behavior of the \(t\)-test against other nonparametric families. If \(\mathbf{F}_{2}\) is the family of all symmetric distributions with finite variance, it turns out that the \(t\)-test is still not uniformly level \(\alpha\), and this is true even if the symmetric distributions have their support on \((-1,1)\) or any other fixed compact set; see Romano (2004). In fact, the size of the \(t\)-test under symmetry is one for moderate values of \(\alpha\); see Basu and DasGupta (1995). However, it can be shown that the size of the \(t\)-test is bounded away from 1 for small values of \(\alpha\), by a result of Edelman (1990). Basu and DasGupta (1995) also show that if \(\mathbf{F}_{3}\) is the family of all symmetric unimodal distributions (with no moment restrictions), then the largest rejection probability under \(F\) of the \(t\)-test occurs when \(F\) is uniform on \([-1,1]\), at least in the case of very small \(\alpha\).

On the other hand, we will now show that the \(t\)-test is uniformly consistent over certain large subfamilies of distributions with two finite moments. For this purpose, consider a family of distributions \(\mathbf{\tilde{F}}\) on the real line satisfying

\[\lim_{\lambda\to\infty}\sup_{F\in\mathbf{\tilde{F}}}E_{F}\left[\frac{|X-\mu(F) |^{2}}{\sigma^{2}(F)}I\left\{\frac{|X-\mu(F)|}{\sigma(F)}>\lambda\right\} \right]=0. \tag{13.40}\]

For example, for any \(\epsilon>0\) and \(b>0\), let \(\mathbf{F}_{b}^{2+\epsilon}\) be the set of distributions satisfying

\[E_{F}\left[\frac{|X-\mu(F)|^{2+\epsilon}}{\sigma^{2+\epsilon}(F)}\right]\leq b\.\]

Then, \(\mathbf{\tilde{F}}=\mathbf{F}_{b}^{2+\epsilon}\) satisfies (13.40). To see why, take expectations of both sides of the inequality

\[\lambda^{\epsilon}Y^{2}I\{|Y|>\lambda\}\leq|Y|^{2+\epsilon}\.\]

**Lemma 13.4.1**: _Suppose \(X_{n,1},\ldots,X_{n,n}\) are i.i.d. \(F_{n}\) with \(F_{n}\in\bar{\bf F}\), where \(\bar{\bf F}\) satisfies (13.40). Let \(\tilde{X}_{n}=\sum_{i=1}^{n}X_{n,i}/n\). Then, under \(F_{n}\),_

\[\frac{n^{1/2}[\tilde{X}_{n}-\mu(F_{n})]}{\sigma(F_{n})}\stackrel{{ d}}{{\to}}N(0,1)\.\]

Proof. Let \(Y_{n,i}=[X_{n,i}-\mu(F_{n})]/\sigma(F_{n})\). We verify the Lindeberg Condition (11.11), which in the case of \(n\) i.i.d. variables reduces to showing

\[\limsup_{n}E[Y_{n,i}^{2}I\{|Y_{n,i}|>\epsilon n^{1/2}\}]=0\]

for every \(\epsilon>0\). But, for every \(\lambda>0\),

\[\limsup_{n}E[Y_{n,i}^{2}I\{|Y_{n,i}|>\epsilon n^{1/2}\}]\leq\limsup_{n}E[Y_{n, i}^{2}I\{|Y_{n,i}|>\lambda\}]\.\]

Let \(\lambda\to\infty\) and the right side tends to zero.

**Lemma 13.4.2**: _Let \(Y_{n,1},\ldots,Y_{n,n}\) be i.i.d. with c.d.f. \(G_{n}\) and finite mean \(\mu(G_{n})\) satisfying_

\[\lim_{\beta\to\infty}\limsup_{n\to\infty}E_{G_{n}}\left[|Y_{n,i}-\mu(G_{n})|I \{|Y_{n,i}-\mu(G_{n})|\geq\beta\}\right]=0. \tag{13.41}\]

_Let \(\tilde{Y}_{n}=\sum_{i=1}^{n}Y_{n,i}/n\). Then, under \(G_{n}\), \(\tilde{Y}_{n}-\mu(G_{n})\to 0\) in probability._

Proof. Without loss of generality, assume \(\mu(G_{n})=0\). Define

\[Z_{n,i}=Y_{n,i}I\{|Y_{n,i}|\leq n\}\.\]

Let \(m_{n}=E(Z_{n,i})\) and \(\tilde{Z}_{n}=\sum_{i=1}^{n}Z_{n,i}/n\). Then, the event \(\{|\tilde{Y}_{n}-m_{n}|>\epsilon\}\) implies either \(\{|\tilde{Z}_{n}-m_{n}|>\epsilon\}\) occurs or \(\{\tilde{Y}_{n}\neq\tilde{Z}_{n}\}\) occurs. Hence, for any \(\epsilon>0\),

\[P\{|\tilde{Y}_{n}-m_{n}|>\epsilon\}\leq P\{|\tilde{Z}_{n}-m_{n}|>\epsilon\}+P \{\tilde{Y}_{n}\neq\tilde{Z}_{n}\}. \tag{13.42}\]

The last term is bounded above by

\[P\{\bigcup_{i=1}^{n}\{Y_{n,i}\neq Z_{n,i}\}\}\leq\sum_{i=1}^{n}P\{Y_{n,i}\neq Z _{n,i}\}=nP\{|Y_{n,i}|>n\}\.\]

The first term on the right side of (13.42) can be bounded by Chebyshev's inequality, so that

\[P\{|\tilde{Y}_{n}-m_{n}|>\epsilon\}\leq(n\epsilon^{2})^{-1}E(Z_{n,1}^{2})+nP\{ |Y_{n,1}|>n\}. \tag{13.43}\]

For \(t>0\), let\[\tau_{n}(t)=t[1-G_{n}(t)+G_{n}(-t)]\]

and

\[\kappa_{n}(t)=\frac{1}{t}\int_{-t}^{t}x^{2}dG_{n}(t)=-\tau_{n}(t)+\frac{2}{t}\int _{0}^{t}\tau_{n}(x)dx\ ; \tag{13.44}\]

the last equality follows by integration by parts (Problem 13.37) and corrects (7.7), p. 235 of Feller (1971). Hence,

\[P\{|\tilde{Y}_{n}-m_{n}|>\epsilon\}\leq\epsilon^{-2}\kappa_{n}(n)+\tau_{n}(n). \tag{13.45}\]

But, for any \(t>0\),

\[\tau_{n}(t)\leq E[|Y_{n,1}|I\{|Y_{n,1}|\geq t\}]\,\]

so \(\tau_{n}(n)\to 0\) by (13.41). Fix any \(\delta>0\) and let \(\beta_{0}\) be such that

\[\limsup_{n}E\left[|Y_{n,1}|I\{|Y_{n,1}|>\beta_{0}\}\right]<\frac{\delta}{4}\.\]

Then, there is an \(n_{0}\) such that, for all \(n\geq n_{0}\),

\[E\left[|Y_{n,1}|I\{|Y_{n,1}|>\beta_{0}\}\right]<\frac{\delta}{2}\,\]

and so

\[E|Y_{n,1}|\leq\beta_{0}+\frac{\delta}{2}\]

for all \(n\geq n_{0}\) as well. Then, if \(n\geq n_{0}>\beta_{0}\),

\[\frac{1}{n}\int_{0}^{n}\tau_{n}(x)dx\leq\frac{1}{n}\int_{0}^{n}E\left[|Y_{n,1} |I\{|Y_{n,1}|\geq x\}\right]dx\]

\[\leq\frac{1}{n}\int_{0}^{\beta_{0}}E|Y_{n,1}|dx+\frac{1}{n}\int_{\beta_{0}}^{n }\frac{\delta}{2}dx\leq\frac{\beta_{0}(\beta_{0}+\frac{\delta}{2})}{n}+\frac{ \delta}{2}\,\]

which is less than \(\delta\) for all sufficiently large \(n\). Thus, \(\kappa_{n}(n)\to 0\) as \(n\to\infty\) and so (13.45) tends to \(0\) as well. Therefore, \(\tilde{Y}_{n}-m_{n}\to 0\) in probability. Finally, \(m_{n}\to 0\); to see why, observe

\[0=E(Y_{n,i})=m_{n}+E\left[Y_{n,1}I\{|Y_{n,1}|>n\}\right]\,\]

so that

\[|m_{n}|\leq E\left[|Y_{n,1}|I\{|Y_{n,1}|>n\}\right]\to 0\,\]

by assumption (13.41).

**Lemma 13.4.3**: _Let \(\widetilde{\mathbf{F}}\) be a family of distributions satisfying (13.40). Suppose \(X_{n,1},\ldots,\)\(X_{n,n}\) are i.i.d. \(F_{n}\in\widetilde{\mathbf{F}}\) and \(\mu(F_{n})=0\). Then, under \(F_{n}\),_

\[\frac{\frac{1}{n}\sum_{i=1}^{n}X_{n,i}^{2}}{\sigma^{2}(F_{n})}\to 1\quad\text{ in }\,\,probability.\]

Proof. Apply Lemma 13.4.2 to \(Y_{n,i}=[X_{n,i}^{2}/\sigma^{2}(F_{n})]-1\). To see that Lemma 13.4.2 applies, note that if \(\beta>1\), then the event \(\{|Y_{n,i}|>\beta\}\) implies \(X_{n,i}^{2}/\sigma^{2}(F_{n})>\beta+1\) (since \(X_{n,i}^{2}/\sigma^{2}(F_{n})>0\)) and also \(|Y_{n,i}|<X_{n,i}^{2}/\sigma^{2}(F_{n})\). Hence, for \(\beta>1\),

\[E\left[|Y_{n,i}|\,I\{|Y_{n,i}|\geq\beta\}\right]\leq E\left[\frac{X_{n,i}^{2}} {\sigma^{2}(F_{n})}I\{\frac{|X_{n,i}|}{\sigma(F_{n})}>\sqrt{\beta+1}\}\right]\,.\]

The sup over \(n\) then tends to \(0\) as \(\beta\to\infty\) by the assumption \(F_{n}\in\widetilde{\mathbf{F}}\).

We are now in a position to study the behavior of the \(t\)-test uniformly across a fairly large class of distributions.

**Theorem 13.4.2**: _Let \(F_{n}\in\widetilde{\mathbf{F}}\), where \(\widetilde{\mathbf{F}}\) satisfies (13.40). Assume_

\[n^{1/2}\mu(F_{n})/\sigma(F_{n})\to\delta\,\,\,\,\,\,\text{as}\,\,n\to\infty\]

_(where \(|\delta|\) is allowed to be \(\infty\)). Let \(X_{1},\ldots,\)\(X_{n}\) be i.i.d. with \(c.d.f\,F_{n}\), and consider the \(t\)-statistic_

\[t_{n}=n^{1/2}\bar{X}_{n}/S_{n}\,\]

_where \(\bar{X}_{n}\) is the sample mean and \(S_{n}^{2}\) is the sample variance. If \(|\delta|<\infty\), then under \(F_{n}\),_

\[t_{n}\stackrel{{ d}}{{\to}}N(\delta,1)\.\]

_If \(\delta\to\infty\) (respectively, \(-\infty\)), then \(t_{n}\to\infty\) (respectively, \(-\infty\)) in probability under \(F_{n}\)._

Proof. Write

\[t_{n}=\frac{n^{1/2}[\bar{X}_{n}-\mu(F_{n})]}{S_{n}}+\frac{n^{1/2}\mu(F_{n})/ \sigma(F_{n})}{S_{n}/\sigma(F_{n})}\.\]

The proof will follow if we show \(S_{n}/\sigma(F_{n})\to 1\) in probability under \(F_{n}\) and if

\[\frac{n^{1/2}[\bar{X}_{n}-\mu(F_{n})]}{\sigma(F_{n})}\stackrel{{ d}}{{\to}}N(0,1). \tag{13.46}\]

But the latter follows by Lemma 13.4.1. To show \(S_{n}^{2}/\sigma^{2}(F_{n})\to 1\) in probability, use Lemma 13.4.3 (Problem 13.34).

Theorem 13.4.2 now allows us to deduce that the \(t\)-test is uniformly consistent in level, and it also yields a limiting power calculation.

**Theorem 13.4.3**: _Let \(\mathbf{\tilde{F}}\) satisfy (13.40) and let \(\mathbf{\tilde{F}}_{0}\) be the set of \(F\) in \(\mathbf{\tilde{F}}\) with \(\mu(F)=0\). For testing \(\mu(F)=0\) versus \(\mu(F)>0\), the \(t\)-test that rejects when \(t_{n}>z_{1-\alpha}\) (or \(t_{n-1,1-\alpha}\)) is uniformly asymptotically level \(\alpha\) over \(\mathbf{\tilde{F}}_{0}\); that is,_

\[|\sup_{F\in\mathbf{\tilde{F}}_{0}}P_{F}\{t_{n}>z_{1-\alpha}\}-\alpha|\to 0 \tag{13.47}\]

_as \(n\to\infty\). Also, the limiting power against \(F_{n}\in\mathbf{\tilde{F}}\) with \(n^{1/2}\mu(F_{n})/\sigma(F_{n})\to\delta\) is given by_

\[\lim_{n}P_{F_{n}}\{t_{n}>z_{1-\alpha}\}=1-\Phi(z_{1-\alpha}-\delta). \tag{13.48}\]

_Furthermore,_

\[\inf_{\{F\in\mathbf{\tilde{F}}:\ n^{1/2}\mu(F)/\sigma(F)\geq\delta\}}P_{F}\{t_ {n}>z_{1-\alpha}\}\to 1-\Phi(z_{1-\alpha}-\delta). \tag{13.49}\]

Proof. To prove (13.47), if the result failed, one could extract a subsequence \(\{F_{n}\}\) with \(F_{n}\in\mathbf{\tilde{F}}_{0}\) such that

\[P_{F_{n}}\{t_{n}>z_{1-\alpha}\}\to\beta\neq\alpha\.\]

But this contradicts Theorem 13.4.2 since \(t_{n}\) is asymptotically standard normal under \(F_{n}\). The proof of (13.48) follows from Theorem 13.4.2 as well. To prove (13.49), again argue by contradiction and assume there exists a subsequence \(\{F_{n}\}\) with \(n^{1/2}\mu(F_{n})/\sigma(F_{n})\geq\delta\) such that

\[P_{F_{n}}\{t_{n}>z_{1-\alpha}\}\to\gamma<1-\Phi(z_{1-\alpha}-\delta)\.\]

The result follows from (13.48) if \(n^{1/2}\mu(F_{n})/\sigma(F_{n})\) has a limit; otherwise, pass to any convergent subsequence and apply the same argument.

Note that (13.49) does not hold if \(\mathbf{\tilde{F}}\) is replaced by all distributions with finite second moments or finite fourth moments, or even the more restricted family of distributions supported on a compact set. In fact, there exists a sequence of distributions \(\{F_{n}\}\) supported on a fixed compact set and satisfying \(n^{1/2}\mu(F_{n})/\sigma(F_{n})\geq\delta\) such that the limiting power of the \(t\)-test against this sequence of alternatives is \(\alpha\); see Problem 13.38 for a construction. Nevertheless, the \(t\)-test behaves well for typical distributions, as demonstrated in Theorem 13.4.3. However, it is important to realize the \(t\)-test does not behave uniformly well across distributions with large skewness, as the limiting normal theory fails.

#### A Result of Bahadur and Savage

The negative results for the \(t\)-test under the families of all distributions with finite variance, or even the family of symmetric distributions with infinitely many moments are perhaps unexpected in view of the fact that the \(t\)-test is pointwise consistent in level for any distribution with finite (nonzero) variance, but they should not really be surprising. After all, the \(t\)-test was designed for the family of normal distributions and not for nonparametric families. This raises the question whether there do exist more satisfactory tests of the mean for nonparametric families.

For the family of distributions with finite variance and for some related families, this question was answered by Bahadur and Savage (1956). The desired results follow from the following basic lemma.

**Lemma 13.4.4**: _Let \({\bf F}\) be a family of distributions on \({\rm I\kern-1.8ptR}\) satisfying:_

1. _For every_ \(F\in{\bf F}\)_,_ \(\mu(F)\) _exists and is finite._
2. _For every real_ \(m\)_, there is an_ \(F\in{\bf F}\) _with_ \(\mu(F)=m\)_._
3. _The family_ \({\bf F}\) _is convex in the sense that, if_ \(F_{i}\in{\bf F}\) _and_ \(\gamma\in[0,1]\)_, then_ \(\gamma F_{1}+(1-\gamma)F_{2}\in{\bf F}\)_._

_Let \(X_{1},\,\ldots,\,X_{n}\) be i.i.d. \(F\in{\bf F}\) and let \(\phi_{n}=\phi_{n}(X_{1},\,\ldots,\,X_{n})\) be any test function. Let \({\bf G}_{m}\) denote the set of distributions \(F\in{\bf F}\) with \(\mu(F)=m\). Then,_

\[\inf_{F\in{\bf G}_{m}}E_{F}(\phi_{n})\ \ \mbox{\rm and}\ \ \ \sup_{F\in{\bf G}_ {m}}E_{F}(\phi_{n})\]

_are independent of \(m\)._

Proof. To show the result for the sup, fix \(m_{0}\) and let \(F_{j}\in{\bf G}_{m_{0}}\) be such that

\[\lim_{j}E_{F_{j}}(\phi_{n})=\sup_{F\in{\bf G}_{m_{0}}}E_{F}(\phi_{n})\equiv s\.\]

Fix \(m_{1}\). The goal is to show

\[\sup_{F\in{\bf G}_{m_{1}}}E_{F}(\phi_{n})=s\.\]

Let \(H_{j}\) be a distribution in \({\bf F}\) with mean \(h_{j}\) satisfying

\[m_{1}=(1-\frac{1}{j})m_{0}+\frac{1}{j}h_{j}\]

and define

\[G_{j}=(1-\frac{1}{j})F_{j}+\frac{1}{j}H_{j}\.\]

Thus, \(G_{j}\in{\bf G}_{m_{1}}\). An observation from \(G_{j}\) can be obtained through a two-stage procedure. First, a coin is flipped with probability of heads \(1/j\). If the outcome is ahead, then the observation has the distribution \(H_{j}\); otherwise, the observation is from \(F_{j}\). So, with probability \([1-(1/j)]^{n}\), a sample of size \(n\) from \(G_{j}\) is just a sample from \(F_{j}\). Then,

\[\sup_{G\in{\bf G}_{m_{1}}}E_{G}(\phi_{n})\geq E_{G_{j}}(\phi_{n})\geq(1-\frac{1 }{j})^{n}E_{F_{j}}(\phi_{n})\to s\]

as \(j\to\infty\). Thus,

\[\sup_{G\in{\bf G}_{m_{1}}}E_{G}(\phi_{n})\geq\sup_{G\in{\bf G}_{m_{0}}}E_{G}( \phi_{n})\.\]

Interchanging the roles of \(m_{0}\) and \(m_{1}\) and applying the same argument makes the last inequality an equality. The result for the inf can be obtained by applying the argument to \(1-\phi_{n}\).

**Theorem 13.4.4**: _Let \({\bf F}\) satisfy (i)-(iii) of Lemma 13.4.4. (i) Any test of \(H:\ \mu(F)=0\) which has size \(\alpha\) for the family \({\bf F}\) has power \(\leq\alpha\) for any alternative \(F\) in \({\bf F}\). (ii) Any test of \(H:\ \mu(F)=0\) which has power \(\beta\) against some alternative \(F\) in \({\bf F}\) has size \(\geq\beta\)._

Among the families satisfying (i)-(iii) of Lemma 13.4.4 is the family \({\bf F}_{0}\) of distributions with finite second moment and that with infinitely many moments. Part (ii) of the above theorem provides an alternative proof of Theorem 13.4.1 since the power of the \(t\)-test against the normal alternatives \(N(\mu,1)\) tends to 1 as \(\mu\to\infty\). Theorem 13.4.4 now shows that the failure of the \(t\)-test for the family of all distributions with finite variance is not the fault of the \(t\)-test; in this setting, there exists no reasonable test of the mean. The reason is that slight changes in the tails of the distribution can result in enormous changes in the mean.

#### Alternative Tests

Another family satisfying conditions (i)-(iii) of Theorem 13.4.4 is the family of all distributions with compact support. However, the family of all distributions on a fixed compact set is excluded because it does not satisfy Condition (ii). In fact, the following construction due to Anderson (1967) shows that reasonable tests of the mean do exist if we assume the family of distributions is supported on a specified compact set. Specifically, let \({\bf G}\) be the family of distributions supported on \([-1,1]\), and let \({\bf G}_{0}\) be the set of distributions on \([-1,1]\) having mean 0. We will exhibit a test that has size \(\alpha\) for any fixed sample size \(n\) and all \(F\in{\bf G}_{0}\), and is pointwise consistent in power. First, recall the Kolmogorov-Smirnov confidence band \(R_{n,1-\alpha}\) given by (11.36). This leads to a conservative confidence interval \(I_{n,1-\alpha}\) for \(\mu(F)\) as follows. Include the value \(\mu\) in \(I_{n,1-\alpha}\) if and only if there exists some \(G\) in \(R_{n,1-\alpha}\) with \(\mu(G)=\mu\). Then,\[\{F\in R_{n,1-\alpha}\}\subseteq\{\mu(F)\in I_{n,1-\alpha}\}\]

and so

\[P_{F}\{\mu(F)\in I_{n,1-\alpha}\}\geq P_{F}\{F\in R_{n,1-\alpha}\}\geq 1-\alpha\,\]

where the last inequality follows by construction of the Kolmogorov-Smirnov confidence bands. Finally, for testing \(\mu(F)=0\) versus \(\mu(F)\neq 0\), let \(\phi_{n}\) be the test that accepts the null hypothesis if and only if the value \(0\) falls in \(I_{n,1-\alpha}\). By construction,

\[\sup_{F\in\mathbf{G}_{0}}E_{F}(\phi_{n})\leq\alpha\.\]

We claim that

\[I_{n,1-\alpha}\subseteq\bar{X}_{n}\pm 2n^{-1/2}s_{n,1-\alpha}\,\]

where \(s_{n,1-\alpha}\) is the \(1-\alpha\) quantile of the null distribution of the Kolmogorov-Smirnov test statistic. The result (13.50) follows from the following lemma.

**Lemma 13.4.5**: _Suppose \(F\) and \(G\) are distributions on \([-1,\,1]\) with_

\[\sup_{t}|F(t)-G(t)|\leq\epsilon\.\]

_Then, \(|\mu(F)-\mu(G)|\leq 2\epsilon\)._

For a proof, see Problem 13.35. The result (13.50) now follows by applying the lemma to \(F\) and the empirical cdf \(\hat{F}_{n}\).

Let \(F\) be a distribution with mean \(\mu(F)\neq 0\). Suppose without loss of generality that \(\mu(F)>0\). Also, let \(L_{n,1-\alpha}\) be the lower endpoint of the interval \(I_{n,1-\alpha}\). Then,

\[E_{F}(\phi_{n})\geq P_{F}\{L_{n,1-\alpha}>0\}\geq P_{F}\{\bar{X}_{n}>2n^{-1/2} s_{n,1-\alpha}\}\to 1\,\]

by Slutsky's Theorem, since \(\bar{X}_{n}\to\mu(F)>0\) and \(n^{-1/2}s_{n,1-\alpha}\to 0\). Thus, the test is pointwise consistent in power against any distribution in \(\mathbf{G}\) having nonzero mean. In fact, if \(\{F_{n}\}\) is such that \(|n^{1/2}\mu(F_{n})|\to\infty\), then the limiting power against such a sequence is one (Problem 13.36).

While Anderson's method controls the level and is pointwise consistent in power, it is not efficient; an efficient test construction which is of exact level \(\alpha\) can be based on the confidence interval construction of Romano and Wolf (2000).

Let us next consider the family of symmetric distributions. Here the mean coincides with the center of symmetry, and reasonable level \(\alpha\) tests for this center exist. They can, for example, be based on the signed ranks. The one-sample Wilcoxon test is an example, studied in Examples 12.3.6 and 14.3.11. A large family of randomization tests that control the level is discussed in Section 17.2.

Finally, we mention a quite different approach to the problem considered in this section concerning the validity of the \(t\)-test in a nonparametric setting. Originally,the \(t\)-test was derived for testing the mean, \(\mu\), on the basis of a sample \(X_{1}\),..., \(X_{n}\) from \(N(\mu,\sigma^{2})\). But, \(\mu\) is not only the mean of the normal distribution but it is also, for example, its median. Instead of embedding the normal family in the family of all distributions with finite mean (and perhaps finite variance), we could obtain a different viewpoint by embedding it in the family of all continuous distributions \(F\), and then test the hypothesis that the median of \(F\) is 0. A suitable test is then the sign test.

### Testing Many Means: The Gaussian Sequence Model

In this section, the problem of testing many normal means is considered. Assume \(X_{1}\), \(X_{2}\),...\(X_{n}\) are independent with \(X_{i}\sim N(\mu_{i},1)\). The problem is to test the global null hypothesis \(H_{0}:\,\mu_{i}=0\,\mbox{for}\,i=1,\,\ldots,n\) against some class of alternatives. There is no UMP test, nor is there a UMPU test if \(n>1\). However, there do exist UMPI and maximin tests, which depend on the choice of group and the class of alternatives, respectively. Since the procedures depend on the choice of optimality criteria, we consider the high-dimensional situation where the number of parameters \(n\) tends to infinity. Such an approach clarifies the type of alternatives where the procedures offer good power. We first review the Chi-squared test, and then consider some alternatives.

#### Chi-Squared Test

Let \(T_{n}=\sum_{i=1}^{n}X_{i}^{2}\). The problem is invariant with respect to the group of orthogonal transformations, resulting in the UMPI test that rejects when \(T_{n}>c_{n,1-\alpha}\), where \(c_{n,1-\alpha}\) is the \(1-\alpha\) quantile of the Chi-squared distribution with \(n\) degrees of freedom. In addition, for any fixed \(\delta>0\), this test is maximin against alternatives defined by

\[\omega_{1}=\{(\mu_{1},\ldots,\mu_{n}):\,\,\sum_{i=1}^{n}\mu_{i}^{2}=\delta^{2} \}\, \tag{13.52}\]

as well as

\[\omega_{2}=\{(\mu_{1},\ldots,\mu_{n}):\,\,\sum_{i=1}^{n}\mu_{i}^{2}\geq\delta^{ 2}\}. \tag{13.53}\]

Moreover, the test maximizes average power with respect to the uniform distribution on \(\omega_{1}\).

By Problem 11.13, we can calculate its limiting power against alternatives for which \(\delta_{n}^{2}/\sqrt{2n}\to h\), where \(\delta_{n}^{2}=\sum_{i=1}^{n}\mu_{i}^{2}\). In particular, under such a sequence of alternatives,

\[P\{T_{n}>c_{n,1-\alpha}\}\to 1-\Phi(z_{1-\alpha}-h)\.\]Therefore, \(\delta_{n}^{2}/\sqrt{2n}\) must not tend to 0 in order to get the limiting power to exceed \(\alpha\), i.e., \(\delta_{n}^{2}\) must be of strict order \(\sqrt{n}\). Note that if \(\delta_{n}^{2}/\sqrt{2n}\to\infty\), then the limiting power tends to one; see Problem 13.39. Therefore, in the special case that \(\mu_{i}=\mu\) is constant and nonzero, then the power of the test tends to one. Or, if \(p\) of the \(n\) means has constant value \(\mu\), then \(p\) must be of strict order \(\sqrt{n}\) in order to achieve nontrivial power. More generally, the Chi-squared test performs reasonably well when, roughly, there are many contributions from many of the \(\mu_{i}\), resulting in a larger value of \(\delta_{n}^{2}\).

On the other hand, the Chi-squared test's ability to detect sparse alternatives where a great majority of the \(\mu_{i}\) are zero is poor. In the extreme case where only one \(\mu_{i}\) is nonzero and is equal to \(\mu\), then \(\mu\) must be of strict order at least \(n^{1/4}\). As we will soon see, there are tests that can detect a much smaller value of \(\mu\).

#### Maximin Test for Sparse Alternatives

Fix \(\delta>0\). Consider the maximin test, not for alternatives (13.52) and (13.53), but for alternatives

\[\omega_{3}=\{(\mu_{1},\ldots,\mu_{n}):\text{ exactly one }\mu_{i}=\delta\text{ and remaining }\mu_{j}=0\} \tag{13.54}\]

or

\[\omega_{4}=\{(\mu_{1},\ldots,\mu_{n}):\text{ max}_{i}\mu_{i}\geq\delta\}. \tag{13.55}\]

In such a sparse setting where only one mean is nonzero, the problem is sometimes referred to as the problem of detecting the "needle" in a "haystack." The least favorable distribution places equal mass on the \(n\) points in \(\omega_{3}\) and the maximin test rejects for large values of the (average) likelihood ratio \(L_{n}\) given by

\[L_{n}=\frac{1}{n}\sum_{i=1}^{n}\exp(\delta X_{i}-\frac{\delta^{2}}{2})\ ; \tag{13.56}\]

see Problem 8.24.

The question we now address is the following. If one of the \(\mu_{i}=\delta\) and the remaining are zero (but it is not known for which \(i\) that \(\mu_{i}\) is the nonzero value), what is the order of the smallest value of \(\delta\) for which the test rejects \(H_{0}\) with probability tending to one. First, the following lemma is needed.

**Lemma 13.5.1**: _Assume the above Gaussian setup. Fix \(r>0\) and let_

\[\delta=\delta_{n}=\sqrt{2r\log n}. \tag{13.57}\]_Under \(H_{0}\) and \(r<1\),_

\[L_{n}\stackrel{{ P}}{{\to}}1. \tag{13.58}\]

Proof. Note that \(L_{n}\) is an average of i.i.d. random variables with mean 1, so the result is somewhat expected. However, \(Var(L_{n})\) need not tend to 0 (depending on the value of \(r\)), and so a careful argument is required. A proof based on truncation is given in Problem 13.43, or one can apply the triangular array law of large numbers stated in Lemma 13.4.2; see Problem 13.44.

We now claim that if \(0<r<1\) in (13.57), then the limiting power of the optimal maximin test based on \(L_{n}\) against an alternative where exactly one \(X_{i}\) has mean \(\delta_{n}\) and the remaining have mean 0 tends to \(\alpha\). In other words, the test is essentially no better than the randomized test which rejects with probability \(\alpha\). Let \(d_{n,1-\alpha}\) be the \(1-\alpha\) quantile of the distribution of \(L_{n}\) under \(H_{0}\).

**Theorem 13.5.1**: _Assume the above Gaussian setup with \(\mu_{1}=\delta_{n}\) specified by (13.57) and the remaining \(\mu_{i}=0\). If \(0<r<1\), then_

\[P\{\mbox{\rm reject }H_{0}\}=P\{L_{n}\geq d_{n,1-\alpha}\}\to\alpha\.\]

Proof. Let \(P_{n,1}\) denote the joint distribution of \((X_{1},\ldots,X_{n})\) specified by the mixture distribution where, with probability \(1/n\), the mean vector has \(\delta_{n}\) in the \(i\)th component and 0 in all others. Also, let \(P_{n,0}\) denote the joint distribution when all \(X_{i}\) are i.i.d. \(N(0,1)\). Then, the likelihood ratio is \(d\,P_{n,1}/dP_{n,0}=L_{n}\). Moreover, the power under \(P_{n,1}\) is the same as the power when \(\mu_{1}\) is the nonzero mean. But, the power under \(P_{n,1}\) is given by

\[1-P_{n,1}\{L_{n}<d_{n,1-\alpha}\}=1-\int I\{L_{n}<d_{n,1-\alpha}\}\frac{dP_{n, 1}}{dP_{n,0}}d\,P_{n,0}\]

\[=1-\int I\{L_{n}<d_{n,1-\alpha}\}L_{n}d\,P_{n,0}\]

\[=\alpha-\int I\{L_{n}<d_{n,1-\alpha}\}(L_{n}-1)d\,P_{n,0}. \tag{13.59}\]

By Lemma 13.5.1, under \(P_{n,0}\), \(L_{n}\stackrel{{ P}}{{\to}}1\), so that

\[I\{L_{n}<d_{n,1-\alpha}\}(L_{n}-1)\stackrel{{ P}}{{\to}}0. \tag{13.60}\]

Moreover, \(d_{n,1-\alpha}\) is bounded (by Problem 11.67), so that the left side of (13.60) is bounded. By bounded convergence, the last integral in (13.59) tends to 0.

Therefore, for testing against the alternatives \(\omega_{3}\) or \(\omega_{4}\), no test can have better limiting maximin power than \(\alpha\) if \(\delta_{n}=\sqrt{2r\log n}\) and \(r<1\). We will soon see that a test based on \(\max_{i}X_{i}\) does have limiting power one when \(r>1\), and therefore so must the maximin test based on \(L_{n}\). But, to be clear, a test based on \(L_{n}\) must specify \(\delta_{n}\), whereas the test based on \(\max_{i}X_{i}\) does not.

##### Test Based on Maximum and Bonferroni

As in the sparse setting of the previous section where one of the means is nonzero but positive, an intuitive test is one that rejects for large values of

\[M_{n}=\max(X_{1},\ldots,X_{n})\.\]

(Or course, if the nonzero mean could also be negative, then one could based a test on \(|M_{n}|\).) It is worth noting that, under \(H_{0}\), \(M_{n}\) has a limiting distribution; see Galambos (1977).

**Theorem 13.5.2**.: _Assume \(X_{1}\), \(X_{2}\), \(\ldots\) are i.i.d. \(N(0,1)\) and \(M_{n}=\max(X_{1},\ldots,X_{n})\). Then, for \(-\infty<t<\infty\),_

\[P\left\{\sqrt{2\log n}\left(M_{n}-\sqrt{2\log n}+\frac{\log\log n+\log 4\pi}{2 \sqrt{2\log n}}\right)\leq t\right\}\to G(t)=e^{-e^{-t}}\,\]

_where the c.d.f. \(G\) is the Gumbel distribution._

Let \(m_{n,1-\alpha}\) be the \(1-\alpha\) quantile of the distribution of \(M_{n}\) under \(H_{0}\). Then, it is easy to check (Problem 13.45) that

\[m_{n,1-\alpha}=z_{(1-\alpha)^{1/n}}. \tag{13.61}\]

Alternatively, an approximate conservative critical value may be used in place of \(m_{n,1-\alpha}\). Under \(H_{0}\), by Bonferroni,

\[P\{M_{n}\geq c\}=P\{\bigcup_{i=1}^{n}\{X_{i}\geq c\}\}\leq\sum_{i=1}^{n}P\{X_{i }\geq c\}=n[1-\Phi(c)]\.\]

In order for the right-hand side to be no bigger than the nominal level \(\alpha\), we should take \(c=z_{1-\frac{\alpha}{n}}\). This method which reject when \(M_{n}\geq z_{1-\frac{\alpha}{n}}\) is then called the Bonferroni method. Note, by Problem 13.40,

\[z_{1-\frac{\alpha}{n}}\sim\sqrt{2\log n}\,\]

where \(a_{n}\sim b_{n}\) means \(a_{n}/b_{n}\to 1\). In fact, \(\sqrt{2\log n}\) is, for any fixed \(\alpha\in(0,1)\) an upper bound for all large \(n\) (even though this approximation does not depend on \(\alpha\)).

An alternative description of the Bonferroni method is based on \(p\)-values computed from each \(X_{i}\). To that end, when testing \(\mu_{i}=0\) against a positive alternative based on \(X_{i}\), the resulting \(p\)-value is \(\hat{p}_{i}=1-\Phi(X_{i})\). Then, the Bonferroni method described above is equivalent to the test that rejects \(H_{i}\) if \(\min\hat{p}_{i}\leq\alpha/n\) (Problem 13.46).

We now show that the Bonferroni test has power tending to one when one of the \(\mu_{i}\) is as large as \(\delta_{n}=\sqrt{2r\log n}\), if \(r>1\). Hence, the same is true of the test that rejects when \(M_{n}\geq m_{n,1-\alpha}\) (as well as the minimax test in Section 13.5.2).

**Theorem 13.5.3**: _Assume the Gaussian setup where one of the \(\mu_{i}=\delta_{n}\) specified by (13.57) and the remaining \(\mu_{i}=0\). If \(r>1\), then the Bonferroni test has power satisfying_

\[P\{\mbox{\rm reject }H_{0}\}=P\{M_{n}\geq z_{1-\frac{\alpha}{n}}\}\to 1\.\]

Proof. Without loss of generality, assume \(\mu_{1}=\delta_{n}\) and \(\mu_{i}=0\) for \(i>1\). By Problem 13.40, \(z_{1-\frac{\alpha}{n}}\leq\sqrt{2\log n}\) for sufficiently large \(n\). Therefore, for sufficiently large \(n\),

\[P\{M_{n}\geq z_{1-\frac{\alpha}{n}}\}\geq P\{M_{n}\geq\sqrt{2\log n}\}\.\]

But,

\[P\{M_{n}\geq\sqrt{2\log n}\}\geq P\{X_{1}\geq\sqrt{2\log n}\}=\]

\[1-\Phi(\sqrt{2\log n}-\delta_{n})=1-\Phi((1-\sqrt{r})\sqrt{2\log n})\to 1\.\]

To summarize, by Theorem 13.5.1, no level \(\alpha\) test can have minimum power tending to one against all alternatives in \(\omega_{3}\) (or \(\omega_{4}\)), where at least one of the means is \(\delta_{n}=\sqrt{2r\log n}\) and \(r<1\). On the other hand, the Bonferroni test, or the test based on \(M_{n}\), has power tending to one when \(r>1\). Therefore, \(\sqrt{2\log n}\) can be viewed as a sharp threshold for detecting the nonzero mean.

#### Some Comparisons and the Higher Criticism

The following comparisons between the Chi-squared test and the Bonferroni test can be made. In order for the Chi-squared test to be powerful, the quantity \(\sum_{i}\mu_{i}^{2}/\sqrt{2n}\) needs to be large. As an example, if all of the \(\mu_{i}\) are equal to \(\mu=Cn^{-1/4}\) with \(C\) large, then the Chi-squared test has large power. However, in this setting, the power of the Bonferroni test is poor. On the other hand, in the sparse setting where \(o(n^{1/2})\) of the means are as large as \(\sqrt{2r\log n}\) with \(r>1\), Bonferroni is powerful (as long as at least one of the means is \(\sqrt{2r\log n}\)). But, the Chi-squared test has poor limiting power in this setting. In summary, we can say that roughly the Chi-squared test performs well for many (possibly) small effects while Bonferroni is better for a smaller number of large effects.

Fortunately, there exists a method that performs well in both settings, dating back to Tukey (1953). The test is based on Tukey's Higher Criticism statistic, which we now motivate. First, recall _p_-values \(\hat{p}_{1}\),..., \(\hat{p}_{n}\), where in the context of many means, \(\hat{p}_{i}=1-\Phi(X_{i})\). (Note that the Higher Criticism approach applies more generally whenever one has _p_-values that are i.i.d. \(U(0,\,1)\) under a global null hypothesis \(H_{0}\).) Let \(\hat{F}_{n}(\cdot)\) be the empirical distribution of the _p_-values, so that

\[\hat{F}_{n}(t)=\frac{1}{n}\sum_{i=1}^{n}I\{\hat{p}_{i}\leq t\}\.\]

For a given level of significance \(\beta\) and under \(H_{0}\), \(n\hat{F}_{n}(\beta)\) is distributed as binomial based on \(n\) trials and success probability \(\beta\). So, \(H_{0}\) can be rejected for large values of \(n\hat{F}_{n}(\beta)\), or equivalently, large values of

\[\frac{\sqrt{n}[\hat{F}_{n}(\beta)-\beta]}{\sqrt{\beta(1-\beta)}}\.\]

Such a binomial test can be traced back to Clopper and Pearson (1934). But, rather than using a fixed pre-specified level of significance \(\beta\), the Higher Criticism statistic rejects for large values of \(HC_{n}\) defined by

\[HC_{n}=\sup_{0<\beta<\beta_{0}}\left[\frac{\sqrt{n}[\hat{F}_{n}(\beta)-\beta]} {\sqrt{\beta(1-\beta)}}\right]\,\]

where \(\beta_{0}\) is a tuning parameter. A value of \(\beta_{0}=0.5\) is suggested in Donoho and Jin (2015).

We now briefly describe the optimality of \(HC_{n}\). As before, \(H_{0}\) specifies that \(X_{1}\),..., \(X_{n}\) are i.i.d. \(N(0,\,1)\). The alternative \(H_{1}\) specifies a mixture model where the \(X_{1},\ldots,X_{n}\) are i.i.d. according to the mixture distribution \((1-\epsilon_{n})N(0,\,1)+\epsilon_{n}N(\delta,\,1)\). Let

\[\epsilon_{n}=n^{-\gamma}\quad\frac{1}{2}<\gamma<1\]

and

\[\delta_{n}=\sqrt{2r\log n}\quad 0<r<1\.\]

Note the needle in haystack problem essentially corresponds to \(\gamma=1\) and \(r=1\) while the many small effects case corresponds to \(\gamma=1/2\). Let

\[\rho^{*}(\gamma)=\left\{\begin{array}{rl}\gamma-\frac{1}{2}&\mbox{for }\frac{1}{2}<\gamma\leq\frac{3}{4}\\ (1-\sqrt{1-\gamma})^{2}&\mbox{for }\frac{3}{4}\leq\gamma\leq 1\end{array}\right. \tag{13.62}\]

Ingster (1999) and Jin (2003) showed that for \(r>\rho^{*}(\gamma)\), there exists a test sequence such that the probabilities of both Type 1 and Type 2 errors tend to 0.

On the other hand, they also showed that, for \(r<\rho^{*}(\gamma)\), the limiting sum of the probabilities of Type 1 and Type 2 errors is bounded below by 1. Thus, the function \(\rho^{*}(\gamma)\) gives the threshold values of \(r\) for detecting \(H_{1}\). Moreover, Donoho and Jin (2004) proved that, for \(r>\rho^{*}(\gamma)\), the test based on \(HC_{n}\) is optimal in that a critical value can be chosen (such as \(\sqrt{2\log\log n}\)), so that the sum of error probabilities tends to 0. Importantly, the test does not require knowledge of \(\epsilon_{n}\) or \(\delta_{n}\), or equivalently \(\gamma\) and \(r\), and thus is optimal across a broad range of sparse alternatives.

### Problems

#### Section 13.2

**Problem 13.1**: (i) Let \(X_{1},\ldots,X_{n}\) be a sample from \(N(\xi,\sigma^{2})\). For testing \(\xi=0\) against \(\xi>0\), show that the power of the one-sided one-sample \(t\)-test against a sequence of alternatives \(N(\xi_{n},\sigma^{2})\) for which \(n^{1/2}\xi_{n}/\sigma\rightarrow\delta\) tends to \(1-\Phi(z_{1-\alpha}-\delta)\).

(ii) The result of (i) remains valid if \(X_{1},\ldots,X_{n}\) are a sample from any distribution with mean \(\xi\) and finite variance \(\sigma^{2}\).

**Problem 13.2**: _Generalize the previous problem to the two-sample \(t\)-test._

**Problem 13.3**: _Let \((Y_{i},Z_{i})\) be i.i.d. bivariate random vectors in the plane, with both \(Y_{i}\) and \(Z_{i}\) assumed to have finite nonzero variances. Let \(\mu_{Y}=E(Y_{1})\) and \(\mu_{Z}=E(Z_{1})\), let \(\rho\) denote the correlation between \(Y_{1}\) and \(Z_{1}\), and let \(\hat{\rho}_{n}\) denote the sample correlation, as defined in (11.29)._

(i). Under the assumption \(\rho=0\), show directly (without appealing to Example 11.3.6) that \(n^{1/2}\hat{\rho}_{n}\) is asymptotically normal with mean 0 and variance

\[\tau^{2}=Var[(Y_{1}-\mu_{Y})(Z_{1}-\mu_{Z})]/Var(Y_{1})Var(Z_{1}).\]

(ii). For testing that \(Y_{1}\) and \(Z_{1}\) are independent, consider the test that rejects when \(n^{1/2}|\hat{\rho}_{n}|>z_{1-\frac{\alpha}{2}}\). Show that the asymptotic rejection probability is \(\alpha\), without assuming normality, but under the sole assumption that \(Y_{1}\) and \(Z_{1}\) have arbitrary distributions with finite nonzero variances.

(iii). However, for testing \(\rho=0\), the above test is not asymptotically robust. Show that there exist bivariate distributions for \((Y_{1},Z_{1})\) for which \(\rho=0\) but the limiting variance \(\tau^{2}\) can take on any given positive value.

(iv). For testing \(\rho=0\) against \(\rho>0\), define a denominator \(D_{n}\) and a critical value \(c_{n}\) such that the rejection region \(n^{1/2}\hat{\rho}_{n}/D_{n}\geq c_{n}\) has probability tending to \(\alpha\), under any bivariate distribution with \(\rho=0\) and finite, nonzero marginal variances.

**Problem 13.4**: _Under the assumptions of Lemma 13.2.1, compute \(Cov(X_{i}^{2},X_{j}^{2})\) in terms of \(\rho_{i,j}\) and \(\sigma^{2}\). Show that \(Var(n^{-1}\sum_{i=1}^{n}X_{i}^{2})\rightarrow0\) and hence \(n^{-1}\sum_{i=1}^{n}X_{i}^{2}\stackrel{{ P}}{{\rightarrow}}\sigma ^{2}\)._

**Problem 13.5**: (i) Given \(\rho\), find the smallest and largest value of (13.2) as \(\sigma^{2}/\tau^{2}\) varies from 0 to \(\infty\).

(ii) For nominal level \(\alpha=0.05\) and \(\rho=0.1,\,0.2,\,0.3,\,0.4\), determine the smallest and the largest asymptotic level of the \(t\)-test as \(\sigma^{2}/\tau^{2}\) varies from 0 to \(\infty\).

**Problem 13.6**: Verify the formula for \(Var(\bar{X})\) in Model A.

**Problem 13.7**: In Model A, suppose that the number of observations in group \(i\) is \(n_{i}\). if \(n_{i}\leq M\) and \(s\to\infty\) show that the assumptions of Lemma 13.2.1 are satisfied and determine \(\gamma\).

**Problem 13.8**: Show that the conditions of Lemma 13.2.1 are satisfied and \(\gamma\) has the stated value: (i) in Model B; (ii) in Model C.

**Problem 13.9**: Determine the maximum asymptotic level of the one-sided \(t\)-test when \(\alpha=.05\) and \(m=2\), 4, 6: (i) in Model A; (ii) in Model B.

**Problem 13.10**: Prove (i) of Lemma 13.2.2.

**Problem 13.11**: Prove Lemma 13.2.3. _Hint:_ For part (ii), use Problem 11.72.

**Problem 13.12**: Verify the claims made in Example 13.2.1.

**Problem 13.13**: Verify (13.15).

**Problem 13.14**: In Example 13.2.3, verify the Huber Condition holds.

**Problem 13.15**: Let \(X_{ijk}\) (\(k=1,\,\ldots,n_{ij}\); \(i=1,\,\ldots,a\); \(j=1,\,\ldots,b\)) be independently normally distributed with mean \(E(X_{ijk})=\xi_{ij}\) and variance \(\sigma^{2}\). Then the test of any linear hypothesis concerning the \(\xi_{ij}\) has a robust level provided \(n_{ij}\to\infty\) for all \(i\) and \(j\).

**Problem 13.16**: In the two-way layout of the preceding problem give examples of submodels \(\Pi_{\Omega}^{(1)}\) and \(\Pi_{\Omega}^{(2)}\) of dimensions \(s_{1}\) and \(s_{2}\), both less than \(ab\), such that in one case Condition (13.20) continues to require \(n_{ij}\to\infty\) for all \(i\) and \(j\) but becomes a weaker requirement in the other case.

**Problem 13.17**: Suppose (13.20) holds for some particular sequence \(\Pi_{\Omega}^{(n)}\) with fixed \(s\). Then it holds for any sequence \(\Pi_{\Omega}^{\prime}{}^{(n)}\subseteq\Pi_{\Omega}^{(n)}\) of dimension \(s^{\prime}<s\).

_Hint:_ If \(\Pi_{\Omega}\) is spanned by the \(s\) columns of \(A\), let \(\Pi_{\Omega}^{\prime}\) be spanned by the first \(s^{\prime}\) columns of \(A\).

**Problem 13.18**: Show that (13.10) holds whenever \(c_{n}\) tends to a finite nonzero limit, but the condition need not hold if \(c_{n}\to 0\).

**Problem 13.19**: Let \(\{c_{n}\}\) and \(\{c_{n}^{\prime}\}\) be two increasing sequences of constants such that \(c_{n}^{\prime}/c_{n}\to 1\) as \(n\to\infty\). Then \(\{c_{n}\}\) satisfies (13.10) if and only if \(\{c_{n}^{\prime}\}\) does.

**Problem 13.20**: Let \(c_{n}=u_{0}+u_{1}n+\cdots+u_{k}n^{k}\), \(u_{i}\geq 0\) for all \(i\). Then \(c_{n}\) satisfies (13.10). What if \(c_{n}=2^{n}\)? _Hint:_ Apply Problem 13.19 with \(c_{n}^{\prime}=n^{k}\).

**Problem 13.21**: If \(\xi_{i}=\alpha+\beta t_{i}+\gamma u_{i}\), express Condition (13.20) in terms of the \(t\)'s and \(u\)'s.

**Problem 13.22**: If \(\Pi_{i,i}\) are defined as in (13.19), show that \(\sum_{i=1}^{n}\Pi_{i,i}^{2}=s\).

_Hint:_ Since the \(\Pi_{i,i}\) are independent of \(A\), take \(A\) to be orthogonal.

**Problem 13.23**: The size of each of the following tests is robust against nonnormality:

1. the test (7.24) as \(b\rightarrow\infty\),
2. the test (7.26) as \(mb\rightarrow\infty\),
3. the test (7.28) as \(m\rightarrow\infty\).

**Problem 13.24**: For \(i=1,\ldots,s\) and \(j=1,\ldots,n_{i}\), let \(X_{i,j}\) be independent, with \(X_{i,j}\) having distribution \(F_{i}\), where \(F_{i}\) is an arbitrary distribution with mean \(\mu_{i}\) and finite common variance \(\sigma^{2}\). Consider testing \(\mu_{1}=\cdots=\mu_{s}\) based on the test statistic (13.29), which is UMPI under normality. Show the test remains robust with respect to the rejection probability under \(H_{0}\) even if the \(F_{i}\) differ and are not normal.

**Problem 13.25**: In the preceding problem, investigate the rejection probability when the \(F_{i}\) have different variances. Assume \(\min n_{i}\rightarrow\infty\) and \(n_{i}/n\rightarrow\rho_{i}\).

**Problem 13.26**: Show that the test derived in Problem 11.56 is not robust against nonnormality.

**Problem 13.27**: Let \(X_{1}\),..., \(X_{n}\) be a sample from \(N(\xi,\sigma^{2})\), and consider the UMP invariant level-\(\alpha\) test of \(H:\xi/\sigma\leq\theta_{0}\) (Section 6.4). Let \(\alpha_{n}(F)\) be the actual significance level of this test when \(X_{1}\),..., \(X_{n}\) is a sample from a distribution \(F\) with \(E(X_{i})=\xi\), \(Var(X_{i})=\sigma^{2}<\infty\). Then the relation \(\alpha_{n}(F)\rightarrow\alpha\) will not in general hold unless \(\theta_{0}=0\). _Hint:_ First find the limiting joint distribution of \(\sqrt{n}(\bar{X}-\xi)\) and \(\sqrt{n}(S^{2}-\sigma^{2})\).

### _Section 13.3_

**Problem 13.28**: When sampling from a normal distribution, one can derive an Edgeworth expansion for the \(t\)-statistic as follows. Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\) and let \(t_{n}=n^{1/2}(\bar{X}_{n}-\mu)/S_{n}\), where \(S_{n}^{2}\) is the usual unbiased estimate of \(\sigma^{2}\). Let \(\Phi\) be the standard normal c.d.f. and let \(\Phi^{\prime}=\varphi\). Show

\[P\{t_{n}\leq t\}=\Phi(t)-\frac{1}{4n}(t+t^{3})\varphi(t)+O(n^{-2}) \tag{13.63}\]

as follows. It suffices to let \(\mu=0\) and \(\sigma=1\). By conditioning on \(S_{n}\), we can write

\[P\{t_{n}\leq t\}=E\{\Phi[t(1+S_{n}^{2}-1)^{1/2}]\}\.\]By Taylor expansion inside the expectation, along with moments of \(S_{n}^{2}\), one can deduce (13.63).

**Problem 13.29**: In Theorem 13.3.2, suppose \(S_{n}^{2}\) is defined with its denominator \(n\,-\,1\) replaced by \(n\). Derive the explicit form for \(q_{2}(t,\,F)\) in the corresponding Edgeworth expansion.

**Problem 13.30**: Assuming \(F\) is absolutely continuous with 4 moments, verify (13.39).

**Problem 13.31**: Let \(\phi_{n}\) be the classical \(t\)-test for testing the mean is zero versus the mean is positive, based on \(n\) i.i.d. observations from \(F\). Consider the power of this test against the distribution \(N(\mu,\,1)\). Show the power tends to one as \(\mu\to\infty\).

### _Section 13.4_

**Problem 13.32**: In Lemma 13.4.2, show that Condition (13.41) can be replaced by the assumption that, for some \(\beta_{n}=o(n^{1/2})\),

\[\limsup_{n\to\infty}E_{G_{n}}[|Y_{n,i}-\mu(G_{n})|I\{|Y_{n,i}-\mu(G_{n})|\geq \beta_{n}\}]=0.\]

Moreover, this condition only needs to hold if \(\beta_{n}=o(n)\) if it is also known that \(\sup_{n}E_{G_{n}}|Y_{n,i}-\mu(G_{n})|<\infty\).

**Problem 13.33**: Suppose \(\mathbf{F}\) satisfies the conditions of Theorem 13.4.4. Assume there exists \(\phi_{n}\) such that

\[\sup_{F\in\mathbf{F}:\ \mu(F)=0}E_{F}(\phi_{n})\to\alpha\.\]

Show that

\[\limsup_{n}E_{F}(\phi_{n})\leq\alpha\]

for every \(F\in\mathbf{F}\).

**Problem 13.34**: In the proof of Theorem 13.4.2, prove \(S_{n}/\sigma(F_{n})\to 1\) in probability.

**Problem 13.35**: Prove Lemma 13.4.5.

**Problem 13.36**: Consider the problem of testing \(\mu(F)=0\) versus \(\mu(F)\neq 0\), for \(F\in\mathbf{F_{0}}\), the class of distributions supported on [0, 1]. Let \(\phi_{n}\) be Anderson's test.

(i) If

\[|n^{1/2}\mu(F_{n})|\geq\delta>2s_{n,1-\alpha}\,\]

then show that \[E_{F_{n}}(\phi_{n})\geq 1-\frac{1}{2(2s_{n,1-\alpha}-\delta)^{2}}\,\]

where \(s_{n,1-\alpha}\) is the \(1-\alpha\) quantile of the null distribution of the Kolmogorov-Smirnov statistic. _Hint:_ Use (13.51) and Chebyshev's inequality.

(ii) Deduce that the minimum power of \(\phi_{n}\) over \(\{F:\ n^{1/2}\mu(F)|\geq\delta\}\) is at least \(1-[2(2s_{n,1-\alpha}-\delta)^{-2}]\) if \(\delta>2s_{n,1-\alpha}\).

(iii) Use (ii) to show that, if \(F_{n}\in{\bf F_{0}}\) is any sequence of distributions satisfying \(n^{1/2}|\mu(F_{n})|\to\infty\), then \(E_{F_{n}}(\phi_{n})\to 1\).

**Problem 13.37**: Prove the second equality in (13.44). In the proof of Lemma 13.4.2, show that \(\kappa_{n}(n)\to 0\).

**Problem 13.38**: Let \(Y_{n,1}\),..., \(Y_{n,n}\) be i.i.d. bernoulli variables with success probability \(p_{n}\), where \(np_{n}=\lambda\) and \(\lambda^{1/2}=\delta\). Let \(U_{n,1}\),..., \(U_{n,n}\) be i.i.d. uniform variables on \((-\tau_{n},\tau_{n})\), where \(\tau_{n}^{2}=3p_{n}^{2}\). Then, let \(X_{n,i}=Y_{n,i}+U_{i}\), so that \(F_{n}\) is the distribution of \(X_{n,i}\). (Note that \(n^{1/2}\mu(F_{n})/\sigma(F_{n})=\delta\).)

(i) If \(t_{n}\) is the \(t\)-statistic, show that, under \(F_{n}\), \(t_{n}\stackrel{{ d}}{{\to}}V^{1/2}\), where \(V\) is Poisson with mean \(\delta^{2}\), and so if \(z_{1-\alpha}\) is not an integer,

\[P_{F_{n}}\{t_{n}>t_{n-1,1-\alpha}\}\to\,P\{V^{1/2}>z_{1-\alpha}\}\.\]

(ii) Show, for \(\alpha<1/2\), the limiting power of the \(t\)-test against \(F_{n}\) satisfies

\[P\{V^{1/2}>z_{1-\alpha}\}\leq 1-P\{V=0\}=\exp(-\delta^{2})\.\]

This is strictly smaller than \(1-\Phi(z_{1-\alpha}-\delta)\) if and only if

\[\Phi(z_{1-\alpha}-\delta)<\exp(-\delta^{2})\.\]

Certainly, for small \(\delta\), this inequality holds, since the left-hand side tends to \(1-\alpha\) as \(\delta\to 0\) while the right-hand side tends to \(1\).

### _Section 13.5_

**Problem 13.39**: For the Chi-squared test discussed in Section 13.5.1, assume that \(\delta_{n}^{2}/\sqrt{2n}\to\infty\). Show that the limiting power of the Chi-squared test against such an alternative sequence tends to one.

**Problem 13.40**: (i) If \(\phi(\cdot)\) denotes the standard normal density and \(Z\sim N(0,1)\), then for any \(t>0\),

\[(\frac{1}{t}-\frac{1}{t^{3}})\phi(t)<P\{Z\geq t\}\leq\frac{\phi(t)}{t}. \tag{13.64}\]Prove the right-hand inequality.

(ii) Prove the left inequality in (13.64). _Hint_: Feller (1968) p.179 notes the negative of the derivative of the left side \((\frac{1}{t}-\frac{1}{t^{3}})\phi(t)\) is equal to \((1-3t^{-4})\phi(t)\), which is certainly less than \(\phi(t)\).

(iii) Use (13.64) to show that, for any fixed \(\alpha\), any \(\delta>0\), and all large enough \(n\):

\[\sqrt{(1-\delta)2\log n}\leq z_{1-\frac{\alpha}{n}}\leq\sqrt{2\log n}. \tag{13.65}\]

**Problem 13.41**: Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(N(0,1)\). Let \(M_{n}=\max(X_{1},\ldots,X_{n})\).

(i) Show that \(P\{M_{n}\geq\sqrt{2\log n}\}\to 0\).

(ii) Compute the limit of \(P\{M_{n}\geq z_{1-\frac{\alpha}{n}}\}\).

**Problem 13.42**: Under the setting of Lemma 13.5.1 calculate \(Var(L_{n})\) and determine which values of \(r\) it tends to \(0\).

**Problem 13.43**: Prove Lemma 13.5.1 as follows. Let \(\eta=1-\sqrt{r}\). Let

\[\tilde{L}_{n}=\frac{1}{n}\sum_{i=1}^{n}\exp(\delta_{n}X_{i}-\frac{\delta_{n}^{ 2}}{2})I\{X_{i}\leq\sqrt{2\log n}\}\.\]

First, show \(L_{n}-\tilde{L}_{n}\stackrel{{ P}}{{\to}}0\) (using Problem 13.41). Then, show

\[E(\tilde{L}_{n})=\Phi(\eta\sqrt{2\log(n)})\to 1\.\]

The proof then follows by showing \(Var(\tilde{L}_{n})\to 0\). To this end, show

\[Var(\tilde{L}_{n})\leq\frac{1}{n}E[X_{i}^{2}I\{X_{i}\leq\sqrt{2\log n}\}]= \frac{1}{n}\exp(\delta_{n}^{2})\Phi((2\eta-1)\sqrt{2\log n})\]

\[\leq\frac{1}{n}\exp(\delta_{n}^{2})\phi((1-2\eta)\sqrt{2\log n})=\frac{1}{ \sqrt{2\pi}}\exp[-\eta^{2}\log n]\to 0\.\]

**Problem 13.44**: Prove Lemma 13.5.1 by using Problem 13.32. That is, if \(1<\beta_{n}=o(n)\) and

\[Y_{n,i}=\exp(\delta_{n}X_{i}-\frac{\delta_{n}^{2}}{2})\,\]

show that

\[E[|Y_{n,i}-1|I\{|Y_{n,i}-1|>\beta_{n}\}]\to 0. \tag{13.66}\]

Since \(Y_{n,i}>0\) and \(\beta_{n}>1\), this is equivalent to showing

\[E[(Y_{n,i}-1)I\{Y_{n,i}>\beta_{n}+1\}]\to 0. \tag{13.67}\]

The event \(\{Y_{n,i}>\lambda+1\}\) is equivalent to \(\{X_{i}>b_{n}(\beta_{n})\}\), where \[b_{n}(\beta_{n})=\frac{\log(\beta_{n}+1)}{\delta_{n}}+\frac{\delta_{n}}{2}\.\]

Show the left side of (13.67) is equal to

\[\int_{b_{n}(\beta_{n})}^{\infty}[\exp(\delta_{n}x-\frac{\delta_{n}^{2}}{2})-1] \phi(x)dx=\Phi(b_{n}(\beta_{n}))-\Phi(b_{n}(\beta_{n})-\delta_{n})\,\]

and show this last expression tends to zero by appropriate choice of \(\beta_{n}\).

**Problem 13.45**: Prove (13.61).

**Problem 13.46**: In the setting of Section 13.5.3, show that the Bonferroni test that rejects \(H_{0}\) when \(M_{n}\geq z_{1-\frac{n}{n}}\) is equivalent to the test that rejects \(H_{i}\) if min \(\hat{p}_{i}\leq\alpha/n\), where \(\hat{p}_{i}=1-\Phi(X_{i})\).

### Notes

Concern about the robustness of classical normal theory tests began to be voiced in the 1920s (Neyman and Pearson, 1928; Shewhart and Winters, 1928; Sophister, 1928; Pearson, 1929) and has been an important topic ever since. Particularly influential were Box (1953), where the term _robustness_ was introduced; also see Scheffe (1959, 10), Tukey (1960) and Hotelling (1961). The robustness of regression tests studied in Section 13.2.3 is based on Huber (1973).

As remarked in Example 13.2.4, the \(F\)-test for testing equality of means is not robust if the underlying variances differ, even if the sample sizes are equal and \(s>2\); see Scheffe (1959). More appropriate tests for this generalized Behrens-Fisher problem have been proposed by Welch (1951), James (1951), and Brown and Forsythe (1974b), and are further discussed by Clinch and Kesselman (1982). The corresponding robustness problem for more general linear hypotheses is treated by James (1954a, 1954b) and Johansen (1980); see also Rothenberg (1984).

The linear model \(F\)-test--as was seen to be the case for the \(t\)-test--is highly nonrobust against dependence of the observations. Tests of the hypothesis that the covariance matrix is proportional to the identity against various specified forms of dependence are considered in King and Hillier (1985). For recent work on robust testing in linear models, see Muller (1998) and the references cited there.

The usual test for equality of variances is Bartlett's test, which is discussed in Cyr and Monoukian (1982) and Glaser (1982). Bartlett's test is highly sensitive to the assumption of normality, and therefore is rarely appropriate. More robust tests for this latter hypothesis are reviewed in Conover et al. (1981). For testing homogeneity of covariance matrices, see Beran and Srivastava (1985) and Zhang and Boos (1992).

Robustness properties of the \(t\)-test are studied in Efron (1969), Lehmann and Loh (1990), Basu and DasGupta (1995), Basu (1999) and Romano (2004). The nonexistence results of Bahadur and Savage (1956), and also Hoeffding (1956),have been generalized to other problems; see Donoho (1988) and Romano (2004) and the references there.

The idea of expanding the distribution of the sample mean in order to study the error in normal approximation can be traced to Chebyshev (1890) and Edgeworth (1905). But it was not until later that Cramer (1928, 1937) provided some rigorous results. The fundamental theory of Edgeworth expansions is developed in Bhattacharya and Rao (1978); also see Bickel (1974), Bhattacharya and Ghosh (1978), Hall (1992) and Hall and Jing (1995).

Section 13.5 was inspired by class notes of Emmanuel Candes. Much more general results are available in Arias-Castro et al. (2011). The "needles" in "haystack" problem is attributed to Johnstone and Silverman (2004). Much further discussion of the Higher Criticism can be found in Donoho and Jin (2004). Extensions to the sparse regression setting appear in Ingster and Tsybakov (2010).

## Chapter 14 Quadratic Mean Differentiable Families

### 14.1 Introduction

As mentioned at the beginning of Chapter 11, the finite-sample theory of optimality for hypothesis testing is applied only to rather special parametric families, primarily exponential families and group families. On the other hand, asymptotic optimality will apply more generally to parametric families satisfying smoothness conditions. In particular, we shall assume a certain type of differentiability condition, called _quadratic mean differentiability_. Such families will be considered in Section 14.2. In Section 14.3, the notion of _contiguity_ will be developed, primarily as a technique for calculating the limiting distribution or power of a test statistic under an alternative sequence, especially when the limiting distribution under the null hypothesis is easy to obtain. In Section 14.4, these techniques will then be applied to classes of tests based on the likelihood function, namely the Wald, Rao, and likelihood ratio tests. The asymptotic optimality of these tests will be established in Chapter 15.

### 14.2 Quadratic Mean Differentiability (q.m.d.)

Consider a parametric model \(\{P_{\theta},\,\theta\in\Omega\}\), where, throughout this section, \(\Omega\) is assumed to be an open subset of \(\mathbf{R}^{k}\). The probability measures \(P_{\theta}\) are defined on some measurable space \((\mathcal{X},\mathcal{C})\). Assume each \(P_{\theta}\) is absolutely continuous with respect to a \(\sigma\)-finite measure \(\mu\), and set \(p_{\theta}(x)=d\,P_{\theta}(x)/d\mu(x)\). In this section, smooth parametric models will be considered. To motivate the smoothness condition given in Definition 14.2.1 below, consider the case of \(n\) i.i.d. random variables \(X_{1},\,\ldots,\,X_{n}\) and the problem of testing a simple null hypothesis \(\theta=\theta_{0}\) against a simple alternative \(\theta_{1}\) (possibly depending on \(n\)). The most powerful test rejects when the loglikelihood ratio statistic

\[\log[L_{n}(\theta_{1})/L_{n}(\theta_{0})]\]is sufficiently large, where

\[L_{n}(\theta)=\prod_{i=1}^{n}p_{\theta}(X_{i}) \tag{14.1}\]

denotes the likelihood function. We would like to obtain certain expansions of the loglikelihood ratio, and the smoothness condition we impose will ensure the existence of such an expansion.

**Example 14.2.1** (**Normal Location Model**): Suppose \(P_{\theta}\) is \(N(\theta,\sigma^{2})\), where \(\sigma^{2}\) is known. It is easily checked that

\[\log[L_{n}(\theta_{1})/L_{n}(\theta_{0})]=\frac{n}{\sigma^{2}}[(\theta_{1}- \theta_{0})\bar{X}_{n}-\frac{1}{2}(\theta_{1}^{2}-\theta_{0}^{2})]\, \tag{14.2}\]

where \(\bar{X}_{n}=\sum_{i=1}^{n}X_{i}/n\). By the Weak Law of Large Numbers, under \(\theta_{0}\),

\[(\theta_{1}-\theta_{0})\bar{X}_{n}-\frac{1}{2}(\theta_{1}^{2}-\theta_{0}^{2}) \stackrel{{ P}}{{\rightarrow}}(\theta_{1}-\theta_{0})\theta_{0}- \frac{1}{2}(\theta_{1}^{2}-\theta_{0}^{2})=-\frac{1}{2}(\theta_{1}-\theta_{0} )^{2}\,\]

and so \(\log[L_{n}(\theta_{1})/L_{n}(\theta_{0})]\stackrel{{ P}}{{ \rightarrow}}-\infty\). Therefore, \(\log[L_{n}(\theta_{1})/L_{n}(\theta_{0})]\) is asymptotically unbounded in probability under \(\theta_{0}\). As in Example 11.3.2, a more useful result is obtained if \(\theta_{1}\) in (14.2) is replaced by \(\theta_{0}+hn^{-1/2}\). We then find

\[\log[L_{n}(\theta_{0}+hn^{-1/2})/L_{n}(\theta_{0})]=\frac{hn^{1/2}(\bar{X}_{n} -\theta_{0})}{\sigma^{2}}-\frac{h^{2}}{2\sigma^{2}}=h\,Z_{n}-\frac{h^{2}}{2 \sigma^{2}}\, \tag{14.3}\]

where \(Z_{n}=n^{1/2}(\bar{X}_{n}-\theta_{0})/\sigma^{2}\) is \(N(0,1/\sigma^{2})\). Notice that the expansion (14.3) is a linear function of \(Z_{n}\) and a simple quadratic function of \(h\), with the coefficient of \(h^{2}\) nonrandom. Furthermore, \(\log[L_{n}(\theta_{0}+hn^{-1/2})/L_{n}(\theta_{0})]\) is distributed as \(N(-h^{2}/2\sigma^{2},\,h^{2}/\sigma^{2})\) under \(\theta_{0}\) for every \(n\). (The relationship that the mean is the negative of half the variance will play a key role in the next section.)

The following more general family permits an asymptotic version of (14.3).

**Example 14.2.2** (**One-parameter Exponential Family**): Let \(X_{1},\ldots,X_{n}\) be i.i.d. having density

\[p_{\theta}(x)=\exp[\theta T(x)-A(\theta)]\]

with respect to a \(\sigma\)-finite measure \(\mu\). Assume \(\theta_{0}\) lies in the interior of the natural parameter space. Then,

\[\log[L_{n}(\theta_{0}+hn^{-1/2})/L_{n}(\theta_{0})]=hn^{-1/2}\sum_{i=1}^{n}T(X _{i})-n[A(\theta_{0}+hn^{-1/2})-A(\theta_{0})]\.\]

Recall (Problem 2.16) that \(E_{\theta_{0}}[T(X_{i})]=A^{\prime}(\theta_{0})\) and \(Var_{\theta_{0}}[T(X_{i})]=A^{\prime\prime}(\theta_{0})\). By a Taylor expansion,\[n[A(\theta_{0}+hn^{-1/2})-A(\theta_{0})]=hn^{1/2}A^{\prime}(\theta_{0})+\frac{1}{2} h^{2}A^{\prime\prime}(\theta_{0})+o(1)\]

as \(n\to\infty\), so that

\[\log[L_{n}(\theta_{0}+hn^{-1/2})/L_{n}(\theta_{0})]=hZ_{n}-\frac{1}{2}h^{2}A^{ \prime\prime}(\theta_{0})+o(1)\, \tag{14.4}\]

where, under \(\theta_{0}\),

\[Z_{n}=n^{-1/2}\sum_{i=1}^{n}\{T(X_{i})-E_{\theta_{0}}[T(X_{i})]\}\stackrel{{ d}}{{\to}}N(0,A^{\prime\prime}(\theta_{0}))\.\]

Thus, the loglikelihood ratio (14.4) behaves asymptotically like the loglikelihood ratio (14.3) from a normal location model. As we will see, such approximations allow one to deduce asymptotic optimality properties for the exponential model (or any model whose likelihood ratios satisfy an appropriate generalization of (14.4)) from optimality properties of the simple normal location model.

We would like to obtain an approximate result like (14.4) for more general families. Classical smoothness conditions usually assume that, for fixed \(x\), the function \(p_{\theta}(x)\) is differentiable in \(\theta\) at \(\theta_{0}\); that is, for some function \(\dot{p}_{\theta}(x)\),

\[p_{\theta_{0}+h}(x)-p_{\theta_{0}}(x)-\langle\dot{p}_{\theta_{0}}(x),h\rangle =o(|h|)\]

as \(|h|\to 0\). In addition, higher order differentiability is typically assumed with further assumptions on the remainder terms. In order to avoid such strong assumptions, it turns out to be useful to work with square roots of densities. For fixed \(x\), differentiability of \(p_{\theta}^{1/2}(x)\) at \(\theta=\theta_{0}\) requires the existence of a function \(\eta(x,\theta_{0})\) such that

\[R(x,\theta_{0},h)\equiv p_{\theta_{0}+h}^{1/2}(x)-p_{\theta_{0}}^{1/2}(x)- \langle\eta(x,\theta_{0}),h\rangle=o(|h|)\.\]

To obtain a weaker, more generally applicable condition, we will not require \(R^{2}(x,\theta_{0},h)=o(|h|^{2})\) for every \(x\), but we will impose the condition that \(R^{2}(X,\theta_{0},h)\) averaged with respect to \(\mu\) is \(o(|h|^{2})\). Let \(L^{2}(\mu)\) denote the space of functions \(g\) such that \(\int g^{2}(x)\,d\mu(x)<\infty\). The convenience of working with square roots of densities is due in large part to the fact that \(p_{\theta}^{1/2}(\cdot)\in L^{2}(\mu)\), a fact first exploited by Le Cam; see Pollard (1997) for an explanation. The desired smoothness condition is now given by the following definition.

**Definition 14.2.1** The family \(\{P_{\theta},\theta\in\Omega\}\) is _quadratic mean differentiable_ (abbreviated q.m.d.) at \(\theta_{0}\) if there exists a vector of real-valued functions \(\eta(\cdot,\theta_{0})=(\eta_{1}(\cdot,\theta_{0}),\,\ldots,\,\eta_{k}(\cdot, \theta_{0}))^{\top}\) such that

\[\int_{\mathcal{X}}\Big{[}\sqrt{p_{\theta_{0}+h}(x)}-\sqrt{p_{\theta_{0}}(x)}- <\eta(x,\theta_{0}),h>\Big{]}^{2}\,d\mu(x)=o(|h|^{2}) \tag{14.5}\]as \(|h|\to 0\).1

Footnote 1: The definition of q.m.d. is a special case of Fréchet differentiability of the map \(\theta\to p_{\theta}^{1/2}(\cdot)\) from \(\Omega\) to \(L^{2}(\mu)\).

The vector-valued function \(\eta(\cdot,\theta_{0})\) will be called the quadratic mean derivative of \(P_{\theta}\) at \(\theta_{0}\). Clearly, \(\eta(x,\,\theta_{0})\) is not unique since it can be changed on a set of \(x\) values having \(\mu\)-measure zero. If q.m.d. holds at all \(\theta_{0}\), then we say the family is q.m.d.

The following are useful facts about q.m.d. families.

**Lemma 14.2.1**: _Assume \(\{P_{\theta},\theta\in\Omega\}\) is q.m.d. at \(\theta_{0}\). Let \(h\in{\rm I\kern-1.8ptR}^{k}\)._

_(i) Under_ \(P_{\theta_{0}}\)_,_ \(\frac{\eta(X,\theta_{0})}{p_{\theta_{0}}^{1/2}(X)}\)_,_ \(h\) _is a random variable with mean_ \(0\)_; i.e., satisfying_

\[\int p_{\theta_{0}}^{1/2}(x)\langle\eta(x,\,\theta_{0}),\,h\rangle d\mu(x)=0\.\]

_(ii) The components of_ \(\eta(\cdot,\,\theta_{0})\) _are in_ \(L^{2}(\mu)\)_; that is, for_ \(i=1,\,\ldots,k\)_,_

\[\int\eta_{i}^{2}(x,\,\theta_{0})\,d\mu(x)<\infty\.\]

Proof. In the definition of q.m.d., replace \(h\) by \(hn^{-1/2}\) to deduce that

\[\int\left\{n^{1/2}\left[p_{\theta_{0}+hn^{-1/2}}^{1/2}(x)-p_{\theta_{0}}^{1/2} (x)\right]-\langle\eta(x,\,\theta_{0}),\,h\rangle\right\}^{2}d\mu(x)\to 0\]

as \(n\to\infty\). But, if \(\int(g_{n}-g)^{2}\,d\mu\to 0\) and \(\int g_{n}^{2}d\mu<\infty\), then \(\int g^{2}d\mu<\infty\) (Problem 14.3). Hence, for any \(h\in{\rm I\kern-1.8ptR}^{k}\), \(\langle\eta(x,\,\theta_{0}),\,h\rangle\in L^{2}(\mu)\). Taking \(h\) equal to the vector of zeros except for a \(1\) in the \(i\)th component yields \((ii)\). Also, if \(\int(g_{n}-g)^{2}d\mu\to 0\) and \(\int p^{2}d\mu<\infty\) then \(\int pg_{n}d\mu\to\int pg\,d\mu\) (Problem 14.4). Taking \(p=p_{\theta_{0}}^{1/2}\) and \(g_{n}=n^{1/2}\left[p_{\theta_{0}+hn^{-1/2}}^{1/2}(x)-p_{\theta_{0}}^{1/2}(x)\right]\) yields

\[\int p_{\theta_{0}}^{1/2}(x)\langle\eta(x,\,\theta_{0}),\,h\rangle d\mu(x)\]

\[=\lim_{n\to\infty}n^{1/2}\int p_{\theta_{0}}^{1/2}(x)[p_{\theta_{0}+hn^{-1/2} }^{1/2}(x)-p_{\theta_{0}}^{1/2}(x)]\,d\mu(x)\]

\[=\lim_{n\to\infty}n^{1/2}\left[\int p_{\theta_{0}}^{1/2}(x)\,p_{\theta_{0}+hn ^{-1/2}}^{1/2}(x)\,d\mu(x)-1\right]\]

\[=-\tfrac{1}{2}\lim n^{-1/2}\,n\int[p_{\theta_{0}}^{1/2}(x)-p_{\theta_{0}+hn^{ -1/2}}^{1/2}(x)]^{2}d\mu(x)\.\]

But,\[n\int\Big{[}p_{\theta_{0}}^{1/2}(x)-p_{\theta_{0}+hn^{-1/2}}^{1/2}(x) \Big{]}^{2}d\mu(x)\] \[\to\int|\langle\eta(x,\,\theta_{0}),\,h\rangle|^{2}d\mu(x)<\infty\, \tag{14.6}\]

and (\(i\)) follows.

Note that Lemma 14.2.1 (\(i\)) asserts that the finite-dimensional set of vectors \(\{\langle\eta(\cdot,\,\theta_{0}),\,h\rangle,\,\,h\in{\rm I\kern-1.567ptR}^{k}\}\) in \(L^{2}(\mu)\) is orthogonal to \(p_{\theta_{0}}^{1/2}(\cdot)\).

It turns out that, when q.m.d. holds, the integrals of products of the components of \(\eta(\cdot,\,\theta)\) play a vital role in the theory of asymptotic efficiency. Such values (multiplied by 4 for convenience) are gathered into a matrix, which we call the _Fisher Information matrix_. The use of the term _information_ is justified by Problem 14.5.

**Definition 14.2.2**: For a q.m.d. family with derivative \(\eta(\cdot,\,\theta)\), define the _Fisher Information matrix_ to be the matrix \(I(\theta)\) with (\(i,\,j\)) entry

\[I_{i,\,j}(\theta)=4\int\eta_{i}(x,\,\theta)\eta_{j}(x,\,\theta)\,d\mu(x)\.\]

The existence of \(I(\theta)\) follows from Lemma 14.2.1 (\(ii\)) and the Cauchy-Schwarz inequality. Furthermore, \(I(\theta)\) does not depend on the choice of dominating measure \(\mu\) (Problem 14.8).

**Lemma 14.2.2**: _For any \(h\in{\rm I\kern-1.567ptR}^{k}\),_

\[\int|\langle h,\eta(x,\,\theta_{0})\rangle|^{2}\,d\mu(x)={{{1}\over{4}}} \langle h,\,I(\theta_{0})h\rangle\.\]

Proof. Of course

\[\langle h,\,\eta(x,\,\theta_{0})\rangle\ =\Sigma h_{i}\eta_{i}(x,\,\theta_{0})\.\]

Square it and integrate.

Next, we would like to determine simple sufficient conditions for q.m.d. to hold. Assuming that the pointwise derivative of \(p_{\theta}(x)\) with respect to \(\theta\) exists, one would expect that the quadratic mean derivative \(\eta(\cdot,\,\theta_{0})\) is given by

\[\eta_{i}(\cdot,\,\theta)={\partial\over\partial\theta_{i}}\ p_{\theta}^{1/2}(x )={{{1}\over{2}}}{{{\partial\over{\partial\theta_{i}}}\ p_{\theta}(x)} \over p_{\theta}^{1/2}(x)}. \tag{14.7}\]

In fact, Hajek (1972) gave sufficient conditions where this is the case, and the following result for the case \(k=1\) is based on his argument.

**Theorem 14.2.1**: _Suppose \(\Omega\) is an open subset of \(\mathbb{R}\) and fix \(\theta_{0}\in\Omega\). Assume \(p_{\theta}^{1/2}(x)\) is an absolutely continuous function of \(\theta\) in some neighborhood of \(\theta_{0}\), for \(\mu\)-almost all \(x\).2 Also, assume for \(\mu\)-almost all \(x\), the derivative \(p_{\theta}^{\prime}(x)\) of \(p_{\theta}(x)\) with respect to \(\theta\) exists at \(\theta=\theta_{0}\). Define_

Footnote 2: A real-valued function \(g\) defined on an interval \([a,b]\) is absolutely continuous if \(g(\theta)=g(a)+\int_{a}^{\theta}h(x)dx\) for some integrable function \(h\) and all \(\theta\in[a,b]\); Problem 2 on p. 182 of Dudley (1989) clarifies the relationship between this notion of absolute continuity of a function and the general notion of a measure being absolute continuous with respect to another measure, as defined in Section 2.2.

\[\eta(x,\,\theta)=\frac{p_{\theta}^{\prime}(x)}{2p_{\theta}^{1/2}(x)} \tag{14.8}\]

_if \(p_{\theta}(x)>0\) and \(p_{\theta}^{\prime}(x)\) exists and define \(\eta(x,\,\theta)=0\) otherwise. Also, define_

\[I(\theta)=4\int\eta^{2}(x,\,\theta)\mu(x)\,\]

_and assume that \(I(\theta)\) is finite and continuous in \(\theta\) at \(\theta_{0}\). Then, \(\{P_{\theta}\}\) is q.m.d. at \(\theta_{0}\) with quadratic mean derivative \(\eta(\cdot,\theta_{0})\) and so \(I(\theta)\) is the Fisher Information._

Proof. If \(p_{\theta}(x)>0\) and \(p_{\theta}^{\prime}(x)\) exists, then from standard calculus it follows that

\[\frac{d}{d\theta}p_{\theta}^{1/2}(x)=\eta(x,\,\theta)\.\]

Also, if \(p_{\theta}(x)=0\) and \(p_{\theta}^{\prime}(x)\) exists, then \(p_{\theta}^{\prime}(x)=0\) (since \(p_{\theta}(\cdot)\) is nonnegative). Now, if \(x\) is such that \(p_{\theta}^{1/2}(x)\) is absolutely continuous in \([\theta_{0},\,\theta_{0}+\delta]\), then

\[\left\{\frac{1}{\delta}[p_{\theta_{0}+\delta}^{1/2}(x)-p_{\theta_{0}}^{1/2}(x )]\right\}^{2}=\frac{1}{\delta^{2}}\left[\int_{0}^{\delta}\eta(x,\,\theta_{0}+ \lambda)d\lambda\right]^{2}\leq\frac{1}{\delta}\int_{0}^{\delta}\eta^{2}(x,\, \theta_{0}+\lambda)d\lambda\,.\]

Integrating over all \(x\) with respect to \(\mu\) yields

\[\int\left\{\frac{1}{\delta}[p_{\theta_{0}+\delta}^{1/2}(x)-p_{\theta_{0}}^{1/2 }(x)]\right\}^{2}d\mu(x)\leq\frac{1}{4\delta}\int_{0}^{\delta}I(\theta_{0}+ \lambda)d\lambda\.\]

By continuity of \(I(\theta)\) at \(\theta_{0}\), the right-hand side tends to

\[\frac{1}{4}I(\theta_{0})=\int\eta^{2}(x,\,\theta_{0})d\mu(x)\]

as \(\delta\to 0\). But, for \(\mu\)-almost all \(x\),

\[\frac{1}{\delta}[p_{\theta_{0}+\delta}^{1/2}(x)-p_{\theta_{0}}^{1/2}(x)] \to\,\eta(x,\,\theta_{0})\.\]The result now follows by Vitali's Theorem (Corollary 2.2.1).

**Corollary 14.2.1**: _Suppose \(\mu\) is Lebesgue measure on \(\mathbb{R}\) and that \(p_{\theta}(x)=f(x-\theta)\) is a location model, where \(f^{1/2}(\cdot)\) is absolutely continuous. Let_

\[\eta(x,\theta)=\frac{-f^{\prime}(x-\theta)}{2f^{1/2}(x-\theta)}\]

_if \(f(x-\theta)>0\) and \(f^{\prime}(x-\theta)\) exists; otherwise, define \(\eta(x,\theta)=0\). Also, let_

\[I=4\int_{-\infty}^{\infty}\eta^{2}(x,0)dx\,\]

_and assume \(I<\infty\). Then, the family is q.m.d. at \(\theta_{0}\) with quadratic mean derivative \(\eta(x,\theta_{0})\) and constant Fisher Information 1._

The assumption that \(f^{1/2}\) is absolutely continuous can be replaced by the assumption that \(f\) is absolutely continuous; see Hajek (1972), Lemma A.1. For other conditions, see Le Cam and Yang (2000), Section 7.3.

**Example 14.2.3**: (**Cauchy Location Model**) The previous corollary applies to the Cauchy location model, where \(p_{\theta}(x)=f(x-\theta)\) and \(f(x)=\frac{1}{\pi}\frac{1}{1+x^{2}}\), and \(I(\theta)=1/2\) (Problem 14.9).

**Example 14.2.4**: (**Double Exponential Location Model**) Consider the location model \(p_{\theta}(x)=f(x-\theta)\) where \(f(x)=\frac{1}{2}\exp(-|x|)\). Although \(f(\cdot)\) is not differentiable at 0, the corollary shows the family is q.m.d. Also, \(I(\theta)=1\) (Problem 14.9).

**Example 14.2.5**: Consider the location model \(p_{\theta}(x)=f(x-\theta)\), where

\[f(x)=C(\beta)\exp\{-|x|^{\beta}\},\]

where \(\beta\) is a fixed positive constant and \(C(\beta)\) is a normalizing constant. By the previous corollary, this family is q.m.d. if \(\beta>\frac{1}{2}\). In fact, one can check that

\[\int_{-\infty}^{\infty}\frac{[f^{\prime}(x)]^{2}}{f(x)}dx<\infty\]

if and only if \(\beta>\frac{1}{2}\) (Problem 14.10). This suggests that q.m.d. fails if \(\beta\leq\frac{1}{2}\), which is the case; see Rao (1968) or Le Cam and Yang (2000), pp. 188-190.

In the \(k\)-dimensional case, sufficient conditions for a family to be q.m.d. in terms of "ordinary" differentiation can be obtained by an argument similar to the proof of Theorem 14.2.1. As an example, we state the following (Problem 14.11, or Bickel et al. (1993), Proposition 2.1).

**Theorem 14.2.2**: _Suppose \(\Omega\) is an open subset of \({\bf I}\!{\bf R}^{k}\), and \(P_{\theta}\) has density \(p_{\theta}(\cdot)\) with respect to a measure \(\mu\). Assume \(p_{\theta}(x)\) is continuously differentiable in \(\theta\) for \(\mu\)-almost all \(x\), with gradient vector \(\dot{p}_{\theta}(x)\) (of dimension \(1\times k\)). Let_

\[\eta(x,\theta)=\frac{\dot{p}_{\theta}(x)}{2p_{\theta}^{1/2}(x)} \tag{14.9}\]

_if \(p_{\theta}(x)>0\) and \(\dot{p}_{\theta}(x)\) exists, and set \(\eta(x,\theta)=0\) otherwise. Assume the Fisher Information matrix \(I(\theta)\) exists and is continuous in \(\theta\). Then, the family is q.m.d. with derivative \(\eta(x,\theta)\)._

**Example 14.2.6**: (Exponential Families in Natural Form) Suppose

\[\frac{dP_{\theta}}{d\mu}(x)=p_{\theta}(x)=C(\theta)\exp[\langle\theta,T(x) \rangle],\]

where

\[\Omega=\mbox{int}\{\theta\in{\bf I}\!{\bf R}^{k}\colon\int\exp[\langle\theta,T (x)\rangle]\,d\mu(x)<\infty\}\]

and \(T(x)=(T_{1}(x),\ldots,\,T_{k}(x))^{\top}\) is a Borel vector-valued function on the space \({\cal X}\) where \(\mu\) is defined. This family is q.m.d.

**Example 14.2.7**: (Three-Parameter Lognormal Family) Suppose \(P_{\theta}\) is the distribution of \(\gamma+\exp(X)\), where \(X\sim N(\mu,\,\sigma^{2})\). Here, \(\theta=(\gamma,\,\mu,\,\sigma)\), where \(\gamma\) and \(\mu\) may take on any real-value and \(\sigma\) any positive value. Note the support of the distribution varies with \(\theta\). Theorem 14.2.2 yields that this family is q.m.d., even though the likelihood function is unbounded.

**Example 14.2.8**: (Uniform Family) Suppose \(P_{\theta}\) is the uniform distribution on \([0,\,\theta]\). This family is _not_ q.m.d., which can be seen by the fact that the convergence (14.6) fails for any choice of \(\eta\). Indeed, for \(h>0\),

\[n\int[p_{\theta_{0}}^{1/2}(x)-p_{\theta_{0}+hn^{-1/2}}^{1/2}(x)]^{2}dx\,\geq n \int_{\theta_{0}}^{\theta_{0}+hn^{-1/2}}\frac{1}{\theta_{0}+hn^{-1/2}}dx\to \infty\;.\]

In fact, it is quite typical that families whose support depends on unknown parameters will not be q.m.d., though Example 14.2.7 is an exception.

We are now in a position to obtain an asymptotic expansion of the loglikelihood ratio whose asymptotic form corresponds to that of the normal location model in Example 14.2.1. First, define the _score function_ (or _score vector_) \(\tilde{\eta}(x,\,\theta)\) by

\[\tilde{\eta}(x,\,\theta)=\frac{2\eta(x,\,\theta)}{p_{\theta}^{1/2}(x)} \tag{14.10}\]if \(p_{\theta}(x)>0\) and \(\tilde{\eta}(x,\,\theta)=0\) otherwise. Under the conditions of Theorem 14.2.2, \(\tilde{\eta}(x,\,\theta)\) can often be computed as the gradient vector of \(\log p_{\theta}(x)\). Also, define the normalized score vector \(Z_{n}\) by

\[Z_{n}=Z_{n,\theta_{0}}=n^{-1/2}\sum_{i=1}^{n}\tilde{\eta}(X_{i},\theta_{0}). \tag{14.11}\]

The following theorem, due to Le Cam, is the main result of this section.

**Theorem 14.2.3**: _Suppose \(\{P_{\theta},\theta\in\Omega\}\) is q.m.d. at \(\theta_{0}\) with derivative \(\eta(\cdot,\,\theta_{0})\) and \(\Omega\) is an open subset of \({\rm I\kern-1.8ptR}^{k}\). Suppose \(I(\theta_{0})\) is nonsingular. Fix \(\theta_{0}\) and consider the likelihood ratio \(L_{n,h}\) defined by_

\[L_{n,h}=\frac{L_{n}(\theta_{0}+hn^{-1/2})}{L_{n}(\theta_{0})}=\prod_{i=1}^{n} \frac{p_{\theta_{0}+hn^{-1/2}}(X_{i})}{p_{\theta_{0}}(X_{i})}\, \tag{14.12}\]

_where the likelihood function \(L_{n}(\cdot)\) is defined in (14.1)._

* _Then, as_ \(n\to\infty\)_,_ \[\log(L_{n,h})-\left[\langle h,\,Z_{n}\rangle-\frac{1}{2}\langle h,\,I(\theta_ {0})h\rangle\right]=o_{P_{\theta_{0}}^{n}}(1).\] (14.13)
* _Under_ \(P_{\theta_{0}}^{n}\)_,_ \(Z_{n}\stackrel{{ d}}{{\to}}N(0,\,I(\theta_{0}))\) _and so_ \[\log(L_{n,h})\stackrel{{ d}}{{\to}}N\left(-\tfrac{1}{2}\langle h, \,I(\theta_{0})h\rangle,\,\langle h,\,I(\theta_{0})h\rangle\right).\] (14.14)

Proof. Consider the triangular array \(Y_{n,1},\,\ldots,\,Y_{n,n}\), where

\[Y_{n,i}=\frac{p_{\theta_{0}+hn^{-1/2}}(X_{i})}{p_{\theta_{0}}^{1/2}(X_{i})}-1.\]

Note that \(E_{\theta_{0}}(Y_{n,i}^{2})\leq 2<\infty\) and

\[\log(L_{n,h})=2\sum_{i=1}^{n}\log(1+Y_{n,i}). \tag{14.15}\]

But,

\[\log(1+y)=y-\tfrac{1}{2}y^{2}+y^{2}r(y)\,\]

where \(r(y)\to 0\) as \(y\to 0\), so that \[\log(L_{n,h})=2\sum_{i=1}^{n}Y_{n,i}-\sum_{i=1}^{n}Y_{n,i}^{2}+2\sum_{i=1}^{n}Y_{n, i}^{2}\,r(Y_{n,i})\.\]

The idea of expanding the likelihood ratio in terms of variables involving square roots of densities is known as Le Cam's square root trick; see Le Cam (1969). The proof of (_i_) will follow from the following four convergence results:

\[\sum_{i=1}^{n}E_{\theta_{0}}(Y_{n,i})\to-\tfrac{1}{8}\langle h,\,I(\theta_{0})h\rangle \tag{14.16}\]

\[\sum_{i=1}^{n}[Y_{n,i}-E_{\theta_{0}}(Y_{n,i})]-\frac{1}{2}\langle h,\,Z_{n} \rangle\overset{P_{\theta_{0}}^{n}}{\to}0 \tag{14.17}\]

\[\sum_{i=1}^{n}Y_{n,i}^{2}\overset{P_{\theta_{0}}^{n}}{\to}\frac{1}{4}\langle h,\,I(\theta_{0})h\rangle \tag{14.18}\]

\[\sum Y_{n,i}^{2}\,r(Y_{n,i})\overset{P_{\theta_{0}}^{n}}{\to}0. \tag{14.19}\]

Once these four convergences have been established, part (_ii_) of the theorem follows by the Central Limit Theorem and the facts that

\[E_{\theta_{0}}[\langle\tilde{\eta}(X_{1},\theta_{0}),\,h\rangle]=0\qquad\text {by Lemma \ref{lem:L1}}(i)\]

and

\[\text{Var}_{\theta_{0}}[\langle\tilde{\eta}(X_{1},\theta_{0}),\,h\rangle]= \langle h,\,I(\theta_{0})h\rangle\qquad\text{by Lemma \ref{lem:L2}}.\]

_(a)_ To show (14.16),

\[\sum_{i=1}^{n}E_{\theta_{0}}(Y_{n,i}) = n\int\left[\frac{p_{\theta_{0}+hn^{-1/2}}^{1/2}(x)}{p_{\theta_{ 0}}^{1/2}(x)}-1\right]p_{\theta_{0}}(x)\,d\mu(x)\] \[= -\tfrac{n}{2}\int\left[p_{\theta_{0}+hn^{-1/2}}^{1/2}(x)-p_{\theta _{0}}^{1/2}(x)\right]^{2}d\mu(x)\] \[\to -\tfrac{1}{2}\int|\langle\eta(x,\theta_{0}),\,h\rangle|^{2}d\mu(x)\]

by (14.6). This last expression is equal to \(-\tfrac{1}{8}\langle h,\,I(\theta_{0})h\rangle\) by Lemma 14.2.2, and (14.16) follows.

_(b)_ To show (14.17), write \[Y_{n,i}=\frac{1}{2}n^{-1/2}\langle h,\,\tilde{\eta}(X_{i},\,\theta_{0})\rangle+\,n^ {-1/2}\frac{R_{n}(X_{i})}{p_{\theta_{0}}^{1/2}(X_{i})}\, \tag{14.20}\]

where \(\int\,R_{n}^{2}(x)\,d\mu(x)\to 0\) (by q.m.d.). Hence,

\[\sum_{i=1}^{n}[Y_{n,i}-E_{\theta_{0}}(Y_{n,i})]=\frac{1}{2}\langle h,\,Z_{n} \rangle+hn^{-1/2}\sum_{i=1}^{n}\left[\frac{R_{n}(X_{i})}{p_{\theta_{0}}^{1/2}( X_{i})}-E_{\theta_{0}}\left(\frac{R_{n}(X_{i})}{p_{\theta_{0}}^{1/2}(X_{i})} \right)\right].\]

The last term, under \(P_{\theta_{0}}^{n}\), has mean 0 and variance bounded by

\[h^{2}E_{\theta_{0}}\left[\frac{R_{n}^{2}(X_{i})}{p_{\theta_{0}}(X_{i})}\right] =h^{2}\int\,R_{n}^{2}(x)\,d\mu(x)\to 0\.\]

So, (14.17) follows.

_(c)_ To prove (14.18), by the Weak Law of Large Numbers, under \(\theta_{0}\),

\[\tfrac{1}{n}\sum_{i=1}^{n}[\langle h,\,\tilde{\eta}(X_{i},\,\theta_{0})\rangle ]^{2}\stackrel{{ P}}{{\to}}E_{\theta_{0}}[\{\langle h,\,\tilde{ \eta}(X_{1},\,\theta_{0})\rangle\}]^{2}=\langle h,\,I(\theta_{0})h\rangle. \tag{14.21}\]

Now using Equation (14.20), we get

\[\sum_{i=1}^{n}Y_{n,i}^{2}=\tfrac{1}{4n}\sum_{i=1}^{n}[\langle h,\,\tilde{\eta }(X_{i},\,\theta_{0})\rangle]^{2}+\tfrac{1}{n}\sum_{i=1}^{n}\frac{R_{n}^{2}(X _{i})}{p_{\theta_{0}}(X_{i})}\]

\[+\tfrac{1}{n}\sum_{i=1}^{n}[\langle h,\,\tilde{\eta}(X_{i},\,\theta_{0}) \rangle]\sum_{j=1}^{n}\frac{R_{n}(X_{j})}{p_{\theta_{0}}^{1/2}(X_{j})}. \tag{14.22}\]

By (14.21), the first term converges in probability under \(\theta_{0}\) to \(\tfrac{1}{4}\langle h,\,I(\theta_{0})h\rangle\). The second term is nonnegative and has expectation under \(\theta_{0}\) equal to

\[\int\,R_{n}^{2}(x)\mu(dx)\to 0\ ;\]

hence, the second term goes to 0 in probability under \(P_{\theta_{0}}^{n}\) by Markov's inequality. The last term goes to 0 in probability under \(P_{\theta_{0}}^{n}\) by the Cauchy-Schwarz inequality and the convergences of the first two terms. Thus, (14.18) follows. By taking expectations in (14.22), a similar argument shows

\[n\,E_{\theta_{0}}(Y_{n,i}^{2})=\frac{1}{4}\langle h,\,I(\theta_{0})h\rangle+o(1) \tag{14.23}\]

as \(n\to\infty\), which also implies \(E_{\theta_{0}}(Y_{n,i})\to 0\).

_(d)_ Finally, to prove (14.19), note that \[\left|\sum_{i=1}^{n}Y_{n,i}^{2}\,r(Y_{n,i})\right|\leq\max_{1\leq i\leq n}|r(Y_{n,i })|\sum_{i=1}^{n}Y_{n,i}^{2}.\]

So, it suffices to show \(\max_{i}|r(Y_{n,i})|\to 0\) in probability under \(\theta_{0}\), which follows if we can show

\[\max_{1\leq i\leq n}|Y_{n,i}|\stackrel{{ P_{\theta_{0}}^{n}}}{{ \to}}0. \tag{14.24}\]

But, \(\sum_{i=1}^{n}[Y_{n,i}-E_{\theta_{0}}(Y_{n,i})]\) is asymptotically normal by (14.17) and the Central Limit Theorem. Hence, Corollary 11.2.2 is applicable with \(s_{n}^{2}=\,O(1)\), which yields the Lindeberg Condition

\[n\,E_{\theta_{0}}[|Y_{n,i}-E_{\theta_{0}}(Y_{n,i})|^{2}I\{|Y_{n,i}-E_{\theta_{ 0}}(Y_{n,i})|\geq\epsilon\}]\to\,0 \tag{14.25}\]

for any \(\epsilon>0\). But then,

\[P_{\theta_{0}}\{\max_{1\leq i\leq n}|Y_{n,i}-E_{\theta_{0}}(Y_{n,i})|>\epsilon \}\leq n\,P_{\theta_{0}}\{|Y_{n,i}-E_{\theta_{0}}(Y_{n,i})|^{2}>\epsilon^{2}\}\,\]

which can be bounded by the expression on the left side of (14.25) divided by \(\epsilon^{2}\), and so \(\max_{1\leq i\leq n}|Y_{n,i}-E_{\theta_{0}}(Y_{n,i})|\to 0\) in probability under \(\theta_{0}\). The result (14.24) follows, since \(E_{\theta_{0}}(Y_{n,i})\to 0\).

**Remark 14.2.1**: Since the theorem concerns the local behavior of the likelihood ratio near \(\theta_{0}\), it is not entirely necessary to assume \(\Omega\) is open. However, it is important to assume \(\theta_{0}\) is an interior point; see Problem 14.14.

**Remark 14.2.2**: The theorem holds if \(h\) is replaced by \(h_{n}\) on the left side of each part of the theorem where \(h_{n}\to h\). Under further assumptions, it is plausible that the left side of (14.13) tends to \(0\) in probability uniformly in \(h\) as long as \(h\) varies in a compact set; that is, for any \(c>0\), the supremum over \(h\) such that \(|h|\leq c\) of the absolute value of the left side of (14.13) tends to \(0\) in probability under \(\theta_{0}\); see Problem 15.12.

### Contiguity

Contiguity is an asymptotic form of a probability measure \(Q\) being absolutely continuous with respect to another probability measure \(P\). In order to motivate the concept, suppose \(P\) and \(Q\) are two probability measures on some measurable space \((\mathcal{X},\mathcal{F})\). Assume that \(Q\) is absolutely continuous with respect to \(P\). This means that \(E\in\mathcal{F}\) and \(P(E)=0\) implies \(Q(E)=0\).

Suppose \(T=T(X)\) is a random vector from \(\mathcal{X}\) to \(\hbox{\rm I$\kern-2.0pt\rm R$}^{k}\), such as an estimator, test statistic, or test function. How can one compute the distribution of \(T\) under \(Q\) if you know how to compute probabilities or expectations under \(P\)? Specifically, suppose it is required to compute \(E_{Q}[f(T)]\), where \(f\) is some measurable function from \(\hbox{\rm I$\kern-2.0pt\rm R$}^{k}\) to \(\hbox{\rm I$\kern-2.0pt\rm R$}\). Let \(p\) and \(q\) denote the densities of \(P\) and \(Q\) with respect to a common measure \(\mu\). Then, assuming \(Q\) is absolutely continuous with respect to \(P\),

\[E_{Q}[f(T(X))]=\int_{\mathcal{X}}f(T(x))d\,Q(x) \tag{14.26}\]

\[=\int_{\mathcal{X}}f(T(x))\frac{q(x)}{p(x)}p(x)d\mu(x)=E_{P}[f(T(X))L(X)]\, \tag{14.27}\]

where \(L(X)\) is the usual likelihood ratio statistic:

\[L(X)=\frac{q(X)}{p(X)}\ . \tag{14.28}\]

Hence, the distribution of \(T(X)\) under \(Q\) can be computed if the joint distribution of \((T(X),L(X))\) under \(P\) is known. Let \(F^{T,L}\) denote the joint distribution of \((T(X),L(X))\) under \(P\). Then, by taking \(f\) to be the indicator function \(f(T(X))=I_{B}[T(X)]\) defined to be equal to one if \(T(X)\) falls in \(B\) and equal to zero otherwise, we obtain:

\[Q\{T(X)\in B\}=\int_{\mathcal{X}}I(T(x)\in B)L(x)\,p(x)\mu(dx) \tag{14.29}\]

\[=E_{P}[I(T(X)\in B)L(X)]=\int_{B\times\hbox{\rm I$\kern-2.0pt\rm R$}}rdF^{T,L} (t,r). \tag{14.30}\]

Thus, under absolute continuity of \(Q\) with respect to \(P\), the problem of finding the distribution of \(T(X)\) under \(Q\) can in principle be obtained from the joint distribution of \(T(X)\) and \(L(X)\) under \(P\).

More generally, if \(f=f(t,r)\) is a function from \(\hbox{\rm I$\kern-2.0pt\rm R$}^{k}\times\hbox{\rm I$\kern-2.0pt\rm R$}\) to \(\hbox{\rm I$\kern-2.0pt\rm R$}\),

\[E_{Q}[f(T(X),L(X))]=\int_{\hbox{\rm I$\kern-2.0pt\rm R$}^{k}\times\hbox{\rm I $\kern-2.0pt\rm R$}}f(t,r)rdF^{T,L}(t,r) \tag{14.31}\]

(Problem 14.18).

Contiguity is an asymptotic version of absolute continuity that permits an analogous asymptotic statement. Consider sequences of pairs of probabilities \(\{P_{n},\,Q_{n}\}\), where \(P_{n}\) and \(Q_{n}\) are probabilities on some measurable space \((\mathcal{X}_{n},\,\mathcal{F}_{n})\). Let \(T_{n}\) be some random vector from \(\mathcal{X}_{n}\) to \(\hbox{\rm I$\kern-2.0pt\rm R$}^{k}\). Suppose the asymptotic distribution of \(T_{n}\) under \(P_{n}\) is easily obtained, but the behavior of \(T_{n}\) under \(Q_{n}\) is also required. For example, if \(T_{n}\) represents a test function for testing \(P_{n}\) versus \(Q_{n}\), the power of \(T_{n}\) is the expectation of \(T_{n}\) under \(Q_{n}\). Contiguity provides a means of performing the required calculation. An example may help fix ideas.

**Example 14.3.1**: (**The Wilcoxon Signed-Rank Statistic**) Let \(X_{1}\),..., \(X_{n}\) be i.i.d. real-valued random variables with common density \(f(\cdot)\). Assume that \(f(\cdot)\) is symmetric about \(\theta\). The problem is to test the null hypothesis that \(\theta=0\) against the alternative hypothesis that \(\theta>0\). Consider the Wilcoxon signed-rank statistic defined by:

\[W_{n}=W_{n}(X_{1},\ldots,X_{n})=n^{-3/2}\sum_{i=1}^{n}R_{i,n}^{+}\text{sign}(X _{i})\, \tag{14.32}\]

where \(\text{sign}(X_{i})\) is 1 if \(X_{i}\geq 0\) and is \(-1\) otherwise, and \(R_{i,n}^{+}\) is the rank of \(|X_{i}|\) among \(|X_{1}|\),..., \(|X_{n}|\). Note that \(W_{n}=2n^{-3/2}[V_{n}-n(n+1)]\), where \(V_{n}\) was previously studied in Example 12.3.6. Under the null hypothesis, the behavior of \(W_{n}\) is fairly easy to obtain. (Alternatively, one can use Example 12.3.6.) If \(\theta=0\), the variables \(\text{sign}(X_{i})\) are i.i.d., each 1 or \(-1\) with probability 1/2, and are independent of the variables \(R_{i,n}^{+}\). Hence, \(E_{\theta=0}(W_{n})=0\). Define \(\tilde{I}_{k}\) to be 1 if the \(k\)th largest \(|X_{i}|\) corresponds to a positive observation and \(-1\) otherwise. Then, we have

\[Var_{\theta=0}(W_{n})=n^{-3}Var(\sum_{k=1}^{n}k\tilde{I}_{k}) \tag{14.33}\]

\[=n^{-3}\sum_{k=1}^{n}k^{2}=n^{-3}\frac{n(n+1)(2n+1)}{6}\rightarrow\frac{1}{3} \tag{14.34}\]

as \(n\rightarrow\infty\). Not surprisingly, \(W_{n}\stackrel{{ d}}{{\rightarrow}}N(0,\frac{1}{3})\). To see why, note that (Problem 14.19)

\[W_{n}-n^{-1/2}\sum_{i=1}^{n}U_{i}\text{sign}(X_{i})=o_{P}(1)\, \tag{14.35}\]

where \(U_{i}=G(|X_{i}|)\) and \(G\) is the c.d.f. of \(|X_{i}|\). But, under the null hypothesis, \(U_{i}\) and \(\text{sign}(X_{i})\) are independent. Moreover, the random variables \(U_{i}\text{sign}(X_{i})\) are i.i.d., and so the Central Limit Theorem is applicable. Thus, \(W_{n}\) is asymptotically normal with mean 0 and variance 1/3, and this is true whenever the underlying distribution has a symmetric density about 0. Indeed, the exact distribution of \(W_{n}\) is the same for all distributions symmetric about 0. Hence, the test that rejects the null hypothesis if \(W_{n}\) exceeds \(3^{-1/2}z_{1-\alpha}\) has limiting level \(1-\alpha\). Of course, for finite \(n\), critical values for \(W_{n}\) can be obtained exactly. Suppose now that we want to approximate the power of this test. The above argument does not generalize to even close alternatives since it heavily uses the fact that the variables are symmetric about zero. Contiguity provides a fairly simple means of attacking this problem, and we will reconsider this example later.

We now return to the general setup.

**Definition 14.3.1**: Let \(P_{n}\) and \(Q_{n}\) be probability distributions on \(({\cal X}_{n},{\cal F}_{n})\). The sequence \(\{Q_{n}\}\) is _contiguous_ to the sequence \(\{P_{n}\}\) if \(P_{n}(E_{n})\to 0\) implies \(Q_{n}(E_{n})\to 0\) for every sequence \(\{E_{n}\}\) with \(E_{n}\in{\cal F}_{n}\).

The following equivalent definition is sometimes useful. The sequence \(\{Q_{n}\}\) is contiguous to \(\{P_{n}\}\) if for every sequence of real-valued random variables \(T_{n}\) such that \(T_{n}\to 0\) in \(P_{n}\)-probability we also have \(T_{n}\to 0\) in \(Q_{n}\)-probability.

If \(\{Q_{n}\}\) is contiguous to \(\{P_{n}\}\) and \(\{P_{n}\}\) is contiguous to \(\{Q_{n}\}\), then we say the sequences \(\{P_{n}\}\) and \(\{Q_{n}\}\) are _mutually contiguous_, or just contiguous.

**Example 14.3.2**: Suppose \(P_{n}\) is the standard normal distribution \(N(0,1)\) and \(Q_{n}\) is \(N(\xi_{n},1)\). Unless \(\xi_{n}\) is bounded, \(P_{n}\) and \(Q_{n}\) cannot be contiguous. Indeed, suppose \(\xi_{n}\to\infty\) and consider \(E_{n}=\{x:\ |x-\xi_{n}|<1\}\). Then, \(Q_{n}(E_{n})\approx 0.68\) for all \(n\), but \(P_{n}(E_{n})\to 0\). Note that, regardless of the values of \(\xi_{n}\), \(P_{n}\) and \(Q_{n}\) are mutually absolutely continuous for every \(n\).

**Example 14.3.3**: Suppose \(P_{n}\) is the joint distribution of \(n\) i.i.d. observations \(X_{1},\,\ldots,\,X_{n}\) from \(N(0,1)\) and \(Q_{n}\) is the joint distribution of \(n\) i.i.d. observations from \(N(\xi_{n},1)\). Unless \(\xi_{n}\to 0\), \(P_{n}\) and \(Q_{n}\) cannot be contiguous. For example, suppose \(\xi_{n}>\epsilon>0\) for all large \(n\). Let \(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\) and consider \(E_{n}=\{\bar{X}_{n}>\epsilon/2\}\). By the law of large numbers, \(P_{n}(E_{n})\to 0\) but \(Q_{n}(E_{n})\to 1\). As will be seen shortly, in order for \(P_{n}\) and \(Q_{n}\) to be contiguous, it will be necessary and sufficient for \(\xi_{n}\to 0\) in such a way so that \(n^{1/2}\xi_{n}\) remains bounded.

We now would like a useful means of determining whether or not \(Q_{n}\) is contiguous to \(P_{n}\). Suppose \(P_{n}\) and \(Q_{n}\) have densities \(p_{n}\) and \(q_{n}\) with respect to \(\mu_{n}\). For \(x\in{\cal X}_{n}\), define the _likelihood ratio_ of \(Q_{n}\) with respect to \(P_{n}\) by

\[L_{n}(x)=\begin{cases}\frac{q_{n}(x)}{p_{n}(x)}&\mbox{if}\,p_{n}(x)>0\\ \infty&\mbox{if}\,\,p_{n}(x)=0<q_{n}(x)\\ 1&\mbox{if}\,\,p_{n}(x)=q_{n}(x)=0.\end{cases} \tag{14.36}\]

Under \(P_{n}\) or \(Q_{n}\), the event \(\{p_{n}=q_{n}=0\}\) has probability 0, so it really doesn't matter how \(L_{n}\) is defined in this case (as long as it is measurable). Note that \(L_{n}\) is regarded as an extended random variable, which means it is allowed to take on the value \(\infty\), at least under \(Q_{n}\). Of course, under \(P_{n}\), \(L_{n}\) is finite with probability one.

Observe that

\[E_{P_{n}}(L_{n})=\int_{{\cal X}_{n}}L_{n}(x)\,p_{n}(x)\mu_{n}(dx )=\int_{\{x:\ p_{n}(x)>0\}}q_{n}(x)\mu_{n}(dx)\] \[=Q_{n}\{x:\ p_{n}(x)>0\}=1-

**Example 14.3.4**: **(Contiguous but not absolutely continuous sequence)** Suppose \(P_{n}\) is uniformly distributed on \([0,1]\) and \(Q_{n}\) is uniformly distributed on \([0,\,\theta_{n}]\), where \(\theta_{n}>1\). Then, \(Q_{n}\) is not absolutely continuous with respect to \(P_{n}\). Note that the likelihood ratio \(L_{n}\) is equal to \(1/\theta_{n}\) with probability one under \(P_{n}\), and so

\[E_{P_{n}}(L_{n})=\frac{1}{\theta_{n}}<1\.\]

It will follow from Theorem 14.3.1 that \(Q_{n}\) is contiguous to \(P_{n}\) if \(\theta_{n}\to 1\).

The notation \(\mathcal{L}(T|P)\) refers to the distribution of a random variable (or possibly an extended random variable) \(T=T(X)\) when \(X\) is governed by \(P\). Let \(G_{n}=\mathcal{L}(L_{n}|P_{n})\), the distribution of the likelihood ratio under \(P_{n}\). Note that \(G_{n}\) is a tight sequence, because by Markov's inequality,

\[P_{n}\{L_{n}>c\}\leq\frac{E_{P_{n}}(L_{n})}{c}\leq\frac{1}{c}\, \tag{14.38}\]

where the last inequality follows from (14.37).

The statement that \(E_{P_{n}}(L_{n})=1\) implies that \(Q_{n}\) is absolutely continuous with respect to \(P_{n}\), by (14.37). The following result, known as Le Cam's First Lemma, may be regarded as an asymptotic version of this statement.

**Theorem 14.3.1**: _Given \(P_{n}\) and \(Q_{n}\), consider the likelihood ratio \(L_{n}\) defined in (14.36). Let \(G_{n}\) denote the distribution of \(L_{n}\) under \(P_{n}\). Suppose \(G_{n}\) converges weakly to a distribution \(G\). If \(G\) has mean 1, then \(Q_{n}\) is contiguous to \(P_{n}\)._

Proof. Suppose \(P_{n}(E_{n})=\alpha_{n}\to 0\). Let \(\phi_{n}\) be a most powerful level \(\alpha_{n}\) test of \(P_{n}\) versus \(Q_{n}\). By the Neyman-Pearson Lemma, the test is of the form

\[\phi_{n}=\begin{cases}1&\text{if }L_{n}>k_{n}\\ 0&\text{if }L_{n}<k_{n},\end{cases} \tag{14.39}\]

for some \(k_{n}\) chosen so the test is level \(\alpha_{n}\). Since \(\phi_{n}\) is at least as powerful as the test that has rejection region \(E_{n}\),

\[Q_{n}\{E_{n}\}\leq\int\phi_{n}d\,Q_{n}\,\]

so it suffices to show the right side tends to zero. Now, for any \(y<\infty\),

\[\int\phi_{n}d\,Q_{n}=\int_{L_{n}\leq y}\phi_{n}d\,Q_{n}+\int_{L_{n}>y}\phi_{n} d\,Q_{n}\]

\[\leq y\int\phi_{n}dP_{n}+\int_{L_{n}>y}d\,Q_{n}\leq y\int\phi_{n}dP_{n}+1-\int _{L_{n}\leq y}d\,Q_{n}\]\[=y\alpha_{n}+1-\int_{L_{n}\leq y}L_{n}d\,P_{n}=y\alpha_{n}+1-\int_{0}^{y}xd\,G_{n}( x)\.\]

Fix any \(\epsilon>0\) and take \(y\) to be a continuity point of \(G\) with

\[\int_{0}^{y}xd\,G(x)>1-\frac{\epsilon}{2}\,\]

which is possible since \(G\) has mean \(1\). But \(G_{n}\) converges weakly to \(G\) implies

\[\int_{0}^{y}xd\,G_{n}(x)\to\int_{0}^{y}xd\,G(x)\, \tag{14.40}\]

by an argument like that in Example 11.4.4 (Problem 14.27). Thus, for sufficiently large \(n\),

\[1-\int_{0}^{y}xd\,G_{n}(x)<\frac{\epsilon}{2}\]

and \(y\alpha_{n}<\epsilon/2\). It follows that, for sufficiently large \(n\),

\[\int\phi_{n}d\,Q_{n}<\epsilon\,\]

as was to be proved.

The following result summarizes some equivalent characterizations of contiguity. The notation \({\cal L}(T|P)\) refers to the distribution (or law) of a random variable \(T\) under \(P\).

**Theorem 14.3.2**: _The following are equivalent characterizations of \(\{Q_{n}\}\) being contiguous to \(\{P_{n}\}\)._

1. _For every sequence of real-valued random variables_ \(T_{n}\) _such that_ \(T_{n}\to 0\) _in_ \(P_{n}\)_-probability, it also follows that_ \(T_{n}\to 0\) _in_ \(Q_{n}\)_-probability._
2. _For every sequence_ \(T_{n}\) _such that_ \({\cal L}(T_{n}|P_{n})\) _is tight, it also follows that_ \({\cal L}(T_{n}|Q_{n})\) _is tight._
3. _If_ \(G\) _is any limit point_ 3 _of_ \({\cal L}(L_{n}|P_{n})\)_, then_ \(G\) _has mean 1._ Footnote 3: \(G\) is a limit point of a sequence \(G_{n}\) of distributions if \(G_{n_{j}}\) converges in distribution to \(G\) for some subsequence \(n_{j}\).

Proof. First, we show that (ii) implies (i). Suppose \(T_{n}\to 0\) in \(P_{n}\)-probability; that is, \(P_{n}\{|T_{n}|>\delta\}\to 0\) for every \(\delta>0\). Then, there exists \(\epsilon_{n}\downarrow 0\) such that \(P_{n}\{|T_{n}|>\epsilon_{n}\}\to 0\). So, \(|T_{n}|/\epsilon_{n}\) is tight under \(\{P_{n}\}\). By hypothesis, \(|T_{n}|/\epsilon_{n}\) is also tight under \(\{Q_{n}\}\). Assume the conclusion that \(T_{n}\to 0\) in \(Q_{n}\)-probability fails; then, one could find \(\epsilon>0\) such that \(Q_{n}\{|T_{n}|>\epsilon\}>\epsilon\) for infinitely many \(n\). Then, of course, \(Q_{n}\{|T_{n}|>\sqrt{\epsilon_{n}}\}>\epsilon\) for infinitely many \(n\). Since \(1/\sqrt{\epsilon_{n}}\uparrow\infty\), it follows that \(|T_{n}|/\epsilon_{n}\) cannot be tight under \(\{Q_{n}\}\), which is a contradiction.

Conversely, to show that (i) implies (ii), assume that \({\cal L}(T_{n}|P_{n})\) is tight. Then, given \(\epsilon>0\), there exists \(k\) such that \(P_{n}\{|T_{n}|>k\}<\epsilon/2\) for all \(n\). If \({\cal L}(T_{n}|Q_{n})\) is not tight, then for every \(j\), \(Q_{n}\{|T_{n}|>j\}>\epsilon\) for some \(n\). That is, there exists a subsequence \(n_{j}\) such that \(Q_{n_{j}}\{|T_{n_{j}}|>j\}>\epsilon\) for every \(j\). As soon as \(j>k\),

\[P_{n_{j}}\{|T_{n_{j}}|>j\}\leq P_{n_{j}}\{|T_{n_{j}}|>k\}<\frac{\epsilon}{2}\,\]

a contradiction.

To show (iii) implies (i), first recall (14.38), which implies \(G_{n}\) is tight. Assuming \(P_{n}\{A_{n}\}\to 0\), we must show \(Q_{n}\{A_{n}\}\to 0\). Assume that this is not the case. Then, there exists a subsequence \(n_{j}\) and \(\epsilon>0\) such that \(Q_{n_{j}}\{A_{n_{j}}\}\geq\epsilon\) for all \(n_{j}\). But, there exists a further subsequence \(n_{j_{k}}\) such that \(G_{n_{j_{k}}}\) converges to some \(G\). Assuming (iii), \(G\) has mean 1. By Theorem 14.3.1, \(P_{n_{j_{k}}}\) and \(Q_{n_{j_{k}}}\) are contiguous. Since \(Q_{n_{j_{k}}}\{A_{n_{j_{k}}}\}\to 0\), this is a contradiction.

Conversely, suppose (i) and that \(G_{n}\) converges weakly to \(G\) (or apply the following argument to any convergent subsequence). By Example 11.4.4, it follows that

\[\int xdG(x)\leq\liminf_{n}E_{P_{n}}(L_{n})\leq 1\,\]

so it suffices to show \(\int xdG(x)\geq 1\). Let \(t\) be a continuity point of \(G\). Then, also by Example 11.4.4 (specifically (11.39)),

\[\int xdG(x)\geq\int_{\{x\leq t\}}xdG(x)=\lim_{n}E_{P_{n}}(L_{n}1\{L_{n}\leq t\} )=\lim_{n}Q_{n}\{L_{n}\leq t\}\.\]

So, it suffices to show that, given any \(\epsilon>0\), there exists a \(t\) such that \(Q_{n}\{L_{n}>t\}<\epsilon\) for all large \(n\). If this fails, then for every \(j\), there exists \(n_{j}\) such that \(Q_{n_{j}}\{L_{n_{j}}>j\}>\epsilon\). But, by (14.38),

\[P_{n_{j}}\{L_{n_{j}}>j\}\leq\frac{1}{j}\to 0\]

as \(j\to\infty\), which would contradict (i).

As will be seen in many important examples, loglikelihood ratios are typically asymptotically normally distributed, and the following corollary is useful.

**Corollary 14.3.1**: _Consider a sequence \(\{P_{n},\,Q_{n}\}\) with likelihood ratio \(L_{n}\) defined in (14.36). Assume_

\[{\cal L}(L_{n}|P_{n})\stackrel{{ d}}{{\to}}{\cal L}(e^{Z})\, \tag{14.41}\]

_where \(Z\) is distributed as \(N(\mu,\,\sigma^{2})\). Then, \(Q_{n}\) and \(P_{n}\) are mutually contiguous if and only if \(\mu=-\sigma^{2}/2\)._Proof. To show \(Q_{n}\) is contiguous to \(P_{n}\), apply part (iii) of Theorem 14.3.2 by showing \(E(e^{Z})=1\). But, recalling the characteristic function of \(Z\) from equation (11.10), it follows that

\[E(e^{Z})=\exp(\mu+\frac{1}{2}\sigma^{2})\,\]

which equals 1 if and only if \(\mu=-\sigma^{2}/2\). That \(P_{n}\) is contiguous to \(Q_{n}\) follows by Problem 14.23.

We may write (14.41) equivalently as

\[\mathcal{L}(\log(L_{n})|\,P_{n})\stackrel{{ d}}{{\to}}\mathcal{L }(Z)\.\]

However, since \(P_{n}\{L_{n}=0\}\) may be positive, we may have \(\log(L_{n})=-\infty\) with positive probability, in which case \(\log(L_{n})\) is regarded as an extended real-valued random variable taking values in \(\mbox{\rm I$\!$R}\,\bigcup\{\pm\infty\}\). If \(X_{n}\) is an extended real-valued random variable and \(X\) is a real-valued random variable with c.d.f. \(F\), we say (as in Definition 11.2.1) \(X_{n}\) converges in distribution to \(X\) if

\[P_{n}\{X_{n}\in(-\infty,t]\}\to\,F(t)\]

whenever \(t\) is a continuity point of \(F\). It follows that if \(X_{n}\) converges in distribution to a random variable that is finite (with probability one), then the probability that \(X_{n}\) is finite must tend to 1.

**Example 14.3.5**: **(Example 14.3.2, continued)**. Again, suppose that \(P_{n}=N(0,\,1)\) and \(Q_{n}=N(\xi_{n},\,1)\). In this case,

\[L_{n}=L_{n}(X)=\exp(\xi_{n}X-\frac{1}{2}\xi_{n}^{2})\.\]

Thus,

\[\mathcal{L}(\log(L_{n})|\,P_{n})=N\left(-\frac{\xi_{n}^{2}}{2},\xi_{n}^{2} \right)\.\]

Such a sequence of distributions will converge weakly along a subsequence \(n_{j}\) if and only if \(\xi_{n_{j}}\to\xi\) (for some \(|\xi|<\infty\)), in which case, the limiting distribution is \(N(\frac{-\xi^{2}}{2},\xi^{2})\) and the relationship between the mean and the variance (\(\mu=-\sigma^{2}/2\)) is satisfied. Hence, \(Q_{n}\) is contiguous to \(P_{n}\) if and only if \(\xi_{n}\) is bounded. In fact, \(Q_{n}\) and \(P_{n}\) are mutually contiguous under the same condition.

**Example 14.3.6**: **(Example 14.3.3, continued)**. Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. with common distribution \(N(\xi,\,1)\). Let \(P_{n}\) represent the joint distribution when \(\xi=0\) and let \(Q_{n}\) represent the joint distribution when \(\xi=\xi_{n}\). Then,

[MISSING_PAGE_EMPTY:2281]

a limit law \(F(\cdot,\cdot)\); that is, for any bounded continuous function \(f\) on \((-\infty,\infty)\times[0,\infty)\),_

\[E_{P_{n}}[f(T_{n},L_{n})]\to\int\int f(t,r)dF(t,r). \tag{14.44}\]

_Then, the limiting distribution of \((T_{n},L_{n})\) under \(Q_{n}\) has density \(rd\,F(t,r)\); that is,_

\[E_{Q_{n}}[f(T_{n},L_{n})]\to\int\int f(t,r)rd\,F(t,r) \tag{14.45}\]

_for any bounded continuous \(f\). Equivalently, if under \(P_{n}\)\((T_{n},\log(L_{n}))\) converges weakly to a limit law \(\bar{F}(\cdot,\cdot)\), then_

\[E_{Q_{n}}[f(T_{n},\log(L_{n}))]\to\int\int f(t,r)e^{\prime}\,d\bar{F}(t,r) \tag{14.46}\]

_for any bounded continuous \(f\)._

Note that equation (14.45) is simply an asymptotic version of (14.31).

**Remark 14.3.1**: The result is also true if \(T_{n}\) is vector-valued, and the proof is the same.

Proof. Let \(F_{n}=\mathcal{L}((T_{n},L_{n})|P_{n})\) and \(G_{n}=\mathcal{L}((T_{n},L_{n})|Q_{n})\). Since \(L_{n}\) converges in distribution under \(P_{n}\), contiguity and Theorem 14.3.2 (iii) imply that

\[\int rd\,F(t,r)=1\.\]

Thus, \(rd\,F(t,r)\) defines a probability distribution on \((-\infty,\infty)\times[0,\infty)\).

Let \(f\) be a nonnegative, continuous function on \((-\infty,\infty)\times[0,\infty]\). By the Portmanteau Theorem (11.2.1 (vi)), it suffices to show that

\[\liminf_{n}\int f(t,r)dG_{n}(t,r)\geq\int f(t,r)rd\,F(t,r)\.\]

Note that

\[\int f(t,r)dG_{n}(t,r)=E_{Q_{n}}[f(T_{n},L_{n})]=\int f(T_{n},L_{n})d\,Q_{n}\]

\[\geq\int_{\{p_{n}>0\}}f(T_{n},L_{n})d\,Q_{n}=\int f(T_{n},L_{n})L_{n}d\,P_{n}= \int f(t,r)rd\,F_{n}(t,r)\.\]

So, it suffices to show

\[\liminf_{n}\int f(t,r)rd\,F_{n}(t,r)\geq\int rf(t,r)dF(t,r)\.\]But, \(rf(t,r)\) is a nonnegative, continuous function, and so the result follows again by the Portmanteau Theorem.

The following special case is often referred to as Le Cam's Third Lemma.

**Corollary 14.3.2**: _Assume that, under \(P_{n}\), \((T_{n},\log(L_{n}))\stackrel{{ d}}{{\to}}(T,Z)\), where \((T,Z)\) is bivariate normal with \(E(T)=\mu_{1}\), \(Var(T)=\sigma_{1}^{2}\), \(E(Z)=\mu_{2}\), \(Var(Z)=\sigma_{2}^{2}\), and \(Cov(T,Z)=\sigma_{1,2}\). Assume \(\mu_{2}=-\sigma_{2}^{2}/2\), so that \(Q_{n}\) is contiguous to \(P_{n}\). Then, under \(Q_{n}\), \(T_{n}\) is asymptotically normal:_

\[{\cal L}(T_{n}|Q_{n})\stackrel{{ d}}{{\to}}N(\mu_{1}+\sigma_{1,2},\sigma_{1}^{2})\.\]

Proof. Let \(\bar{F}(\cdot,\cdot)\) denote the bivariate normal distribution of \((T,Z)\). By Theorem 14.3.3, the limiting distribution of \({\cal L}((T_{n},\log(L_{n}))|Q_{n})\) has density \(e^{\prime}d\bar{F}(x,r)\); let \((\bar{T},\bar{Z})\) denote a random variable having this distribution. The characteristic function of \(\bar{T}\) is given by:

\[E(e^{i\lambda\bar{T}})=\int e^{i\lambda x}e^{\prime}d\bar{F}(x,r)=E(e^{i \lambda T+Z})\, \tag{14.47}\]

which is the characteristic function of \((T,Z)\) evaluated at \(t=(t_{1},t_{2})^{\top}=(\lambda,-i)^{\top}\). By Example 11.2.1, this is given by

\[\exp(i\langle\mu,t\rangle-\frac{1}{2}\langle\Sigma t,t\rangle)=\exp(i\mu_{1} \lambda+\mu_{2}-\frac{1}{2}\langle\Sigma(\lambda,-i)^{\top},(\lambda,-i)^{ \top}))\]

\[=\exp(i\mu_{1}\lambda+\mu_{2}-\frac{1}{2}\lambda^{2}\sigma_{1}^{2}+\lambda i \sigma_{1,2}+\frac{\sigma_{2}^{2}}{2})=\exp[i\langle\mu_{1}+\sigma_{1,2} \rangle\lambda-\frac{1}{2}\lambda^{2}\sigma_{1}^{2}]\,\]

the last equality following from the fact that \(\mu_{2}=-\sigma_{2}^{2}/2\) (by contiguity). But, this last expression is indeed the characteristic function of the normal distribution with mean \(\mu_{1}+\sigma_{1,2}\) and variance \(\sigma_{1}^{2}\).

**Example 14.3.8**: **(Asymptotically Linear Statistic)** Let \(\{P_{\theta},\ \theta\in\Omega\}\) with \(\Omega\) an open subset of \(\mathbb{R}^{k}\) be q.m.d., with corresponding densities \(p_{\theta}(\cdot)\). Recall Example 14.3.7, which shows that \(P_{\theta_{0}+hn^{-1/2}}^{n}\) and \(P_{\theta_{0}}^{n}\) are mutually contiguous. The expansion (14.43) shows a lot more. For example, suppose an estimator (sequence) \(\hat{\theta}_{n}\) is asymptotically linear in the following sense: under \(\theta_{0}\),

\[n^{1/2}(\hat{\theta}_{n}-\theta_{0})=n^{-1/2}\sum_{i=1}^{n}\psi_{\theta_{0}}( X_{i})+o_{P_{\theta_{0}}^{n}}(1)\, \tag{14.48}\]

where \(E_{\theta_{0}}[\psi_{\theta_{0}}(X_{1})]=0\) and \(\tau^{2}\equiv Var_{\theta_{0}}[\psi_{\Then, the joint behavior of \(\hat{\theta}_{n}\) with the loglikelihood ratio satisfies

\[(n^{1/2}(\hat{\theta}_{n}-\theta_{0}),\,\log(\frac{dP^{n}_{\theta_{0}+hn^{-1/2}}} {dP^{n}_{\theta_{0}}})) \tag{14.49}\]

\[=[n^{-1/2}\sum_{i=1}^{n}(\psi_{\theta_{0}}(X_{i}),\,\langle h,\,\tilde{\eta}(X _{i},\theta_{0})\rangle)]+(0,-\frac{1}{2}\langle h,\,I(\theta_{0})h\rangle)+o_ {P^{n}_{\theta_{0}}}(1)\.\]

By the bivariate Central Limit Theorem, this converges under \(\theta_{0}\) to a bivariate normal distribution with covariance

\[\sigma_{1,2}\equiv Cov_{\theta_{0}}(\psi_{\theta_{0}}(X_{1}),\,\langle h,\, \tilde{\eta}(X_{i},\theta_{0})\rangle). \tag{14.50}\]

Hence, under \(P^{n}_{\theta_{0}+hn^{-1/2}},\,n^{1/2}(\hat{\theta}_{n}-\theta_{0})\) converges in distribution to \(N(\sigma_{1,2},\tau^{2}),\) by Corollary 14.3.2. It follows that, under \(P^{n}_{\theta_{0}+hn^{-1/2}},\)

\[n^{1/2}(\hat{\theta}_{n}-(\theta_{0}+hn^{-1/2}))\stackrel{{ d}}{{ \rightarrow}}N(\sigma_{1,2}-h,\,\tau^{2})\.\ \blacksquare\]

**Example 14.3.9**: (**t-statistic**) Consider a location model \(f(x-\theta)\) for which \(f(x)\) has mean 0 and variance \(\sigma^{2},\) and which satisfies the assumptions of Corollary 14.2.1, which imply this family is q.m.d. For testing \(\theta=\theta_{0}=0,\) consider the behavior of the usual \(t\)-statistic

\[t_{n}=\frac{n^{1/2}\tilde{X}_{n}}{S_{n}}=\frac{n^{1/2}\tilde{X}_{n}}{\sigma}+o_ {P_{\theta_{0}}}(1)\.\]

Then, (14.48) holds with \(\psi_{\theta_{0}}(X_{i})=X_{i}/\sigma.\) We seek the behavior of \(t_{n}\) under \(\theta_{n}=h/n^{1/2}.\) Although this can be obtained by direct means, let us obtain the results by contiguity. Note that (14.43) holds with

\[\tilde{\eta}(X_{i},\theta_{0})=-\frac{f^{\prime}(x)}{f(x)}\.\]

Thus, \(\sigma_{1,2}\) in (14.50) reduces to

\[\sigma_{1,2}=-\frac{h}{\sigma}Cov_{\theta_{0}=0}\left(X_{i},\,\frac{f^{\prime }(X_{i})}{f(X_{i})}\right)=-\frac{h}{\sigma}\int_{-\infty}^{\infty}xf^{\prime} (x)dx=\frac{h}{\sigma}\.\]

Hence, under \(\theta_{n}=h/n^{1/2},\)

\[t_{n}\stackrel{{ d}}{{\rightarrow}}N(\frac{h}{\sigma},1)\.\ \blacksquare\]

**Example 14.3.10**: **(Sign Test)** As in the previous example, consider a location model \(f(x-\theta)\), where \(f\) is a density with respect to Lebesgue measure. Assume the conditions in Corollary 14.2.1, so that the family is q.m.d. Further suppose that \(f(x)\) is continuous at \(x=0\) and \(P_{\theta=0}\{X_{i}>0\}=1/2\). For testing \(\theta=\theta_{0}=0\), consider the (normalized) sign statistic

\[S_{n}=n^{-1/2}\sum_{i=1}^{n}[I\{X_{i}>0\}-\frac{1}{2}]\,\]

where \(I\{X_{i}>0\}\) is one if \(X_{i}>0\) and is \(0\) otherwise. Then, (14.48) holds with \(\psi_{0}(X_{i})=I\{X_{i}>0\}-\frac{1}{2}\) and so

\[S_{n}\stackrel{{ d}}{{\to}}N(0,\frac{1}{4})\.\]

Under \(\theta_{n}=h/n^{1/2}\), \(S_{n}\stackrel{{ d}}{{\to}}N(\sigma_{1,2},\,1/4)\), where \(\sigma_{1,2}\) is given by (14.50) and equals

\[\sigma_{1,2}=-hCov_{0}\left[I\{X_{i}>0\},\,\frac{f^{\prime}(X_{i})}{f(X_{i})} \right]=-h\int_{0}^{\infty}f^{\prime}(x)dx=hf(0)\.\]

Hence, under \(\theta_{n}=h/n^{1/2}\),

\[S_{n}\stackrel{{ d}}{{\to}}N(hf(0),\frac{1}{4})\.\ \blacksquare\]

**Example 14.3.11**: **(Example 14.3.1, continued)**. Recall the Wilcoxon signed-rank statistic \(W_{n}\) given by (14.32). For illustration, suppose the underlying density \(f(\cdot)\) of the observations is normal with mean \(\theta\) and variance \(1\). Under the null hypothesis \(\theta=0\), \(W_{n}\) is asymptotically normal \(N(0,\frac{1}{3})\). The problem now is to compute the asymptotic power against the sequence of alternatives \(\theta_{n}=h/n^{1/2}\) for some \(h>0\). Under the null hypothesis, by (14.35) and (14.42),

\[(W_{n},\,\log(L_{n}))=(n^{-1/2}\sum_{i=1}^{n}U_{i}\mbox{sign}(X_{i}),\,hn^{-1/ 2}\sum_{i=1}^{n}X_{i}-\frac{h^{2}}{2})+o_{P_{0}^{*}}(1)\, \tag{14.51}\]

where \(U_{i}=G(|X_{i}|)\) and \(G\) is the c.d.f. of \(|X_{i}|\). This last expression is asymptotically bivariate normal with covariance under \(\theta=0\) equal to

\[\sigma_{1,2}=hCov_{0}[G(|X_{1}|)\mbox{sign}(X_{1}),\,X_{1}]=hE_{0}[G(|X_{1}|) |X_{1}|]\, \tag{14.52}\]

and thus \(\sigma_{1,2}\) is equal to \(h/\sqrt{\pi}\) (Problem 14.28). Hence, under \(\theta_{n}=h/n^{1/2}\), \(W_{n}\) is asymptotically normal with mean \(h/\sqrt{\pi}\) and variance \(1/3\). Thus, the asymptotic power of the test that rejects when \(W_{n}>3^{-1/2}z_{1-\alpha}\) is\[\lim_{n\to\infty}P_{\theta_{n}}\{W_{n}-\frac{h}{\sqrt{\pi}}\succ 3^{-1/2}z_{1- \alpha}-\frac{h}{\sqrt{\pi}}\}=1-\Phi(z_{1-\alpha}-(3/\pi)^{1/2}h)\,\]

where \(\Phi(\cdot)\) is the standard normal c.d.f.

More generally, assume the underlying model is a location model \(f(x-\theta)\), where \(f(x)\) is assumed symmetric about zero. Assume \(f^{\prime}(x)\) exists for Lebesgue almost all \(x\) and

\[0<I\equiv\int\frac{[f^{\prime}(x)]^{2}}{f(x)}dx<\infty\.\]

Then, by Corollary 14.2.1, this model is q.m.d. and (14.43) holds with

\[\tilde{\eta}(x,0)=-\frac{f^{\prime}(x)}{f(x)}\.\]

Under the null hypothesis \(\theta=0\), \(W_{n}\stackrel{{ d}}{{\to}}N(0,1/3)\), as in the normal case. Under the sequence of alternatives \(\theta_{n}=h/n^{1/2}\),

\[W_{n}\stackrel{{ d}}{{\to}}N(\sigma_{1,2},\frac{1}{3})\,\]

where \(\sigma_{1,2}\) is given by (14.50). In this case,

\[\sigma_{1,2}=Cov_{\theta=0}[U\,{\rm sign}(X),-h\frac{f^{\prime}(X)}{f(X)}]\,\]

where \(U=G(|X|)\) and \(G\) is the c.d.f. of \(|X|\) when \(X\) has density \(f(\cdot)\). So, \(G(x)=2F(x)-1\), where \(F\) is the c.d.f. of \(X\). By an integration by parts (see Problem 14.29),

\[\sigma_{1,2}=-h\,E_{\theta=0}[G(|X|){\rm sign}(X)\frac{f^{\prime}(X)}{f(X)}]=2 h\int_{-\infty}^{\infty}f^{2}(x)dx. \tag{14.53}\]

Thus, under \(\theta_{n}=h/n^{1/2}\),

\[W_{n}\stackrel{{ d}}{{\to}}N(2h\int_{-\infty}^{\infty}f^{2}(x)dx, \frac{1}{3})\.\]

An alternative approach that uses the projection of \(U\)-statistics is given in Problem 14.30.

**Example 14.3.12**: (**Neyman-Pearson Statistic**) Assume \(\{P_{\theta},\ \theta\in\Omega\}\) is q.m.d. at \(\theta_{0}\), where \(\Omega\) is an open subset of \({\rm I\kern-1.8ptR}^{k}\) and \(I(\theta_{0})\) is nonsingular, so that the assumptions behind Theorem 14.2.3 are in force. Let \(p_{\theta}(\cdot)\) be the corresponding density of \(P_{\theta}\). Consider the likelihood ratio statistic based on \(n\) i.i.d. observations \(X_{1}\),..., \(X_{n}\) given by \[L_{n,h}=\frac{d\,P_{\theta_{0}+hn^{-1/2}}}{d\,P_{\theta_{0}}^{n}}=\prod_{i=1}^{n} \frac{P_{\theta_{0}+hn^{-1/2}}(X_{i})}{p_{\theta_{0}}(X_{i})}. \tag{14.54}\]

By Theorem 14.2.3, under \(P_{\theta_{0}}\),

\[\log(L_{n,h})\xrightarrow{d}N(-\frac{\sigma_{h}^{2}}{2},\sigma_{h}^{2})\, \tag{14.55}\]

where \(\sigma_{h}^{2}=\langle h,\,I(\theta_{0})h\rangle\). Apply Corollary 14.3.2 with \(T_{n}\equiv\log(L_{n,h})\), so that \(T=Z\) and \(\sigma_{1,2}=\sigma_{h}^{2}\). Then, under \(P_{\theta_{0}+hn^{-1/2}}^{n}\), \(\log(L_{n,h})\) is asymptotically \(N(\frac{\sigma_{h}^{2}}{2},\sigma_{h}^{2})\). Hence, the test that rejects when \(\log(L_{n,h})\) exceeds \(-\frac{1}{2}\sigma_{h}^{2}+z_{1-\alpha}\sigma_{h}\) is asymptotically level \(\alpha\) for testing \(\theta=\theta_{0}\) versus \(\theta=\theta_{0}+hn^{-1/2}\), where \(z_{1-\alpha}\) denotes the \(1-\alpha\) quantile of \(N(0,\,1)\). Then, the limiting power of this test sequence for testing \(\theta=\theta_{0}\) versus \(\theta=\theta_{0}+hn^{-1/2}\) is \(1-\Phi(z_{1-\alpha}-\sigma_{h})\) (Problem 14.31).

### Likelihood Methods in Parametric Models

The goal of this section is to study some classical large-sample methods based on the likelihood function. The classical likelihood ratio test, as well as the tests of Wald and Rao will be introduced, but optimality of these tests will be deferred until the next chapter. Throughout this section, we will assume that \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. with common distribution \(P_{\theta}\), where \(\theta\in\Omega\) and \(\Omega\) is an open subset of \(\hbox{\rm I$\kern-1.8pt$\rm R$}^{k}\). We will also assume each \(P_{\theta}\) is absolutely continuous with respect to a common \(\sigma\)-finite measure \(\mu\), so that \(p_{\theta}\) denotes the density of \(P_{\theta}\) with respect to \(\mu\). The _likelihood function_ is defined by

\[L_{n}(\theta)=\prod_{i=1}^{n}p_{\theta}(X_{i}). \tag{14.56}\]

It is thus the (joint) probability density of the observations at fixed values of \(X_{1},\,\ldots,\,X_{n}\), viewed as a function of \(\theta\). Note that, for the sake of simplicity, the dependence of \(L_{n}(\theta)\) on \(X_{1},\,\ldots,\,X_{n}\) has been suppressed. (In the case that \(X_{1},\,\ldots,\,X_{n}\) are not i.i.d., \(L_{n}(\theta)\) is modified so that the joint density of the \(X_{i}\)'s is used rather than the product of the marginal densities.)

#### Efficient Likelihood Estimation

In preparation for the construction of reasonable large-sample tests and confidence regions, we begin by studying some efficient point estimators of \(\theta\) which will serve as a basis for such tests. If the likelihood \(L_{n}(\theta)\) has a unique maximum \(\hat{\theta}_{n}\), then \(\hat{\theta}_{n}\) is called the _maximum likelihood estimator_ (MLE) of \(\theta\). If, in addition, \(L_{n}(\theta)\) is differentiable in \(\theta\), \(\hat{\theta}_{n}\) will be a solution of the _likelihood equations_

\[\frac{\partial}{\partial\theta_{j}}\log L_{n}(\theta)=0\quad j=1,\ldots,k\.\]

**Example 14.4.1** (**Normal Family**): Suppose \(X_{1},\ldots,X_{n}\) is an i.i.d. sample from \(N(\mu,\sigma^{2})\), with both parameters unknown, so \(\theta=(\mu,\sigma^{2})^{\top}\). In this case, the loglikelihood function is

\[\log L_{n}(\mu,\sigma^{2})=-\frac{n}{2}\log(2\pi)-n\log(\sigma)-\frac{1}{2 \sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu)^{2}\,\]

and the likelihood equations reduce to

\[\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu)=0\]

and

\[-\frac{n}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum_{i=1}^{n}(X_{i}-\mu)^{2}=0\.\]

These equations have a unique solution, given by the maximum likelihood estimator (\(\hat{\mu}_{n}\), \(\hat{\sigma}_{n}^{2}\)), where \(\hat{\mu}_{n}=\bar{X}_{n}\) is the usual sample mean and \(\hat{\sigma}_{n}^{2}\) is the biased version of the sample variance given by

\[\hat{\sigma}_{n}^{2}=n^{-1}\sum(X_{i}-\bar{X}_{n})^{2}\]

(Problem 14.36). By the weak law of large numbers, \(\bar{X}_{n}\rightarrow\mu\) in probability; by Example 11.3.3, \(\hat{\sigma}_{n}^{2}\rightarrow\sigma^{2}\) in probability as well. A direct argument easily establishes the joint limiting distribution of the MLE. First note that

\[n^{1/2}[\hat{\sigma}_{n}^{2}-n^{-1}\sum_{i=1}^{n}(X_{i}-\mu)^{2}]=n^{1/2}(\bar {X}_{n}-\mu)^{2}\stackrel{{ P}}{{\rightarrow}}0\]

since \(n^{1/2}(\bar{X}_{n}-\mu)\) is \(N(0,\sigma^{2})\) and \(\bar{X}_{n}-\mu\stackrel{{ P}}{{\rightarrow}}0\). Hence, by Slutsky's Theorem, \(n^{1/2}((\bar{X}_{n},\hat{\sigma}_{n}^{2})^{\top}-(\mu,\sigma^{2})^{\top})\) has the same limiting distribution as

\[n^{1/2}[(\bar{X}_{n},n^{-1}\sum_{i=1}^{n}(X_{i}-\mu)^{2})^{\top}-(\mu,\sigma^{ 2})^{\top}]\,\]which by the multivariate CLT tends in distribution to \(N(0,\,\Sigma)\), where \(\Sigma\) is the \(2\times 2\) diagonal matrix with \((i,\,j)\) entry \(\sigma_{i,\,j}\) given by \(\sigma_{1,1}=\sigma^{2}\) and \(\sigma_{2,2}=Var[(X_{1}-\mu)^{2}]=2\sigma^{4}\). In fact, \(\Sigma=I^{-1}(\theta)\) in this case.

**Example 14.4.2** (**MLE for a one-parameter exponential family**): Suppose \(X_{1},\,\ldots,\,X_{n}\) is an i.i.d. sample from a one-parameter exponential family with common density with respect to a \(\sigma\)-finite measure \(\mu\) given by

\[p_{\theta}(x)=\exp[\theta T(x)-A(\theta)]\.\]

Here, \(\theta\) is assumed to be an interior point of the natural parameter space. From Problem 2.16, recall that \(E_{\theta}[T(X_{i})]=A^{\prime}(\theta)\) and \(Var_{\theta}[T(X_{i})]=A^{\prime\prime}(\theta)\). To show the maximum likelihood estimator is well-defined and to find an expression for it, we examine the derivative of the log of \(L_{n}(\theta)\), which is equal to

\[\frac{\partial\log L_{n}(\theta)}{\partial\theta}=\sum_{i=1}^{n}[T(X_{i})-A^{ \prime}(\theta)]\.\]

The likelihood equation sets this equal to zero, which reduces to the equation \(\tilde{T}_{n}=A^{\prime}(\theta)\), where \(\tilde{T}_{n}=n^{-1}\sum_{i=1}^{n}T(X_{i})\). Hence, the MLE is found by equating the sample mean of the \(T(X_{i})\) values to its expected value. Assuming the equation \(\tilde{T}_{n}=A^{\prime}(\theta)\) can be solved for \(\theta\), it must be the maximum likelihood estimator. Indeed, the second derivative of the loglikelihood is \(-nA^{\prime\prime}(\theta)<0\), which also shows there can be at most one solution to the likelihood equation. Furthermore, by the law of large numbers, \(\tilde{T}_{n}\stackrel{{ P}}{{\rightarrow}}A^{\prime}(\theta)\), which combined with the fact that \(A^{\prime\prime}(\theta)>0\) yields that, with probability tending to one, there exists exactly one solution to the likelihood equation. Thus, \(\hat{\theta}_{n}\) is well-defined with probability tending to one. To determine its limiting distribution, first note that

\[n^{1/2}[\tilde{T}_{n}-A^{\prime}(\theta)]\stackrel{{ d}}{{ \rightarrow}}N(0,\,A^{\prime\prime}(\theta))\,\]

by the Central Limit Theorem. Since \(A^{\prime}\) is strictly increasing, we can define the inverse function \(B\) of \(A^{\prime}\), so that \(B(A^{\prime}(\theta))=\theta\). Then, \(\hat{\theta}_{n}=B(A^{\prime}(\hat{\theta}_{n}))=B(\tilde{T}_{n})\). By the delta method,

\[n^{1/2}(\hat{\theta}_{n}-\theta)\stackrel{{ d}}{{\rightarrow}}N( 0,\,\tau^{2})\,\]

where

\[\tau^{2}=A^{\prime\prime}(\theta)[B^{\prime}(A^{\prime}(\theta))]^{2}\.\]

But using the chain rule to differentiate both sides of the identity \(B(A^{\prime}(\theta))=\theta\) yields \(B^{\prime}(A^{\prime}(\theta))A^{\prime\prime}(\theta)=1\), so

\[n^{1/2}(\hat{\theta}_{n}-\theta)\stackrel{{ d}}{{\rightarrow}}N \left(0,\,\frac{1}{A^{\prime\prime}(\theta)}\right)\.\]In fact, the asymptotic variance \([A^{\prime\prime}(\theta)]^{-1}\) is \(I^{-1}(\theta)\), where \(I(\theta)\) is the Fisher Information.

Problem 14.38 generalizes the previous example to multiparameter exponential families.

The general theory of asymptotic normality of the MLE is much more difficult and we shall here only give a heuristic treatment. For precise conditions and rigorous proofs, see Lehmann and Casella (1998), Chapter 6 and Ibragimov and Has'minskii (1981), Section 3.3. Let \(X_{1},\ldots,X_{n}\) be i.i.d. according to a family \(\{P_{\theta}\}\) which is q.m.d. at \(\theta_{0}\) with nonsingular Fisher Information matrix \(I(\theta_{0})\) and quadratic mean derivative \(\eta(\cdot,\,\theta_{0})\). Define

\[L_{n,h}=\frac{L_{n}(\theta_{0}+hn^{-1/2})}{L_{n}(\theta_{0})}. \tag{14.57}\]

By Theorem 14.2.3,

\[\log(L_{n,h})=\langle h,\,Z_{n}\rangle-\frac{1}{2}\langle h,\,I(\theta_{0})h \rangle+o_{P_{\theta_{0}}^{n}}(1)\, \tag{14.58}\]

where \(Z_{n}\) is the normalized score vector

\[Z_{n}=Z_{n}(\theta_{0})=2n^{-1/2}\sum_{i=1}^{n}[\eta(X_{i},\,\theta_{0})/p_{ \theta_{0}}^{1/2}(X_{i})] \tag{14.59}\]

and satisfies, under \(\theta_{0}\),

\[Z_{n}\stackrel{{ d}}{{\to}}N(0,\,I(\theta_{0}))\.\]

Note that \(Z_{n}=Z_{n}(\theta_{0})\) depends on \(\theta_{0}\), but we will usually omit this dependence in the notation.

If the MLE \(\hat{\theta}_{n}\) is well-defined, then \(\hat{\theta}_{n}=\theta_{0}+\hat{h}_{n}n^{-1/2}\), where \(\hat{h}_{n}\) is the value of \(h\) maximizing \(L_{n,h}\). The result (14.58) suggests that, if \(\theta_{0}\) is the true value, \(\hat{h}_{n}\) is approximately equal to \(\tilde{h}_{n}\) which maximizes

\[\log(\tilde{L}_{n,h})\equiv\langle h,\,Z_{n}\rangle-\frac{1}{2}\langle h,\,I( \theta_{0})h\rangle. \tag{14.60}\]

Since \(\log(\tilde{L}_{n,h})\) is a simple (quadratic) function of \(h\), it is easily checked (Problem 14.46) that

\[\tilde{h}_{n}=I^{-1}(\theta_{0})Z_{n}. \tag{14.61}\]

It then follows that

\[n^{1/2}(\hat{\theta}_{n}-\theta_{0})=\hat{h}_{n}\approx\tilde{h}_{n}=I^{-1}( \theta_{0})Z_{n}\stackrel{{ d}}{{\to}}N(0,\,I^{-1}(\theta_{0}))\.\]The symbol \(\approx\) is used to indicate an approximation based on heuristic considerations. Unfortunately, the above approximation is not rigorous without further conditions. In fact, without further conditions, the maximum likelihood estimator may not even be consistent. Indeed, an example of Le Cam (presented in Example 4.1 of Chapter 6 in Lehmann and Casella (1998)) shows that the maximum likelihood estimator \(\hat{\theta}_{n}\) may exist and be unique but does not converge to the true value \(\theta\) in probability (i.e., it is inconsistent). Moreover, the example shows this can happen even in very smooth families in which good estimators do exist. Rigorous conditions for the MLE to be consistent were given by Wald (1949), and have since then been weakened (for a survey, see Perlman (1972)). Cramer (1943) derived good asymptotic behavior of the maximum likelihood estimator under just certain smoothness conditions, often known as _Cramer type conditions_. Furthermore, he gave conditions under which there exists a consistent sequence of roots \(\hat{\theta}_{n}\) of the likelihood equations (not necessarily the MLE) satisfying

\[n^{1/2}(\hat{\theta}_{n}-\theta_{0})=I^{-1}(\theta_{0})Z_{n}+o_{P_{\theta_{0}} ^{*}}(1)\, \tag{14.62}\]

from which asymptotic normality follows. Cramer's conditions required that the underlying family of densities was three times differentiable with respect to \(\theta\), as well as further technical assumptions on differentiability inside the integral signs; see Chapter 6 of Lehmann and Casella (1998). Estimators satisfying (14.62) are called _efficient_. In the case where \(\hat{\theta}_{n}\) is a solution to the likelihood equations, it is called an _efficient likelihood estimator_ (ELE) sequence.

Determination of an efficient sequence of roots of the likelihood equations tends to be difficult when the equations have multiple roots. Asymptotically equivalent estimators can be constructed by starting with any estimator \(\hat{\theta}_{n}\) that is \(n^{1/2}\)-consistent, i.e., for which \(n^{1/2}(\tilde{\theta}_{n}-\theta)\) is bounded in probability. The resulting estimator can be taken to be the root closest to \(\tilde{\theta}_{n}\), or an approximation to it based on a Newton-Raphson linearization method; for more details, see Section 6.4 of Lehmann and Casella (1998), Gan and Jiang (1999) and Small, Wang and, Yang (2000). A similar, but distinct, approach based on discretization of an initial estimator, leads to Le Cam's (1956, 1969) _one-step maximum likelihood estimator_, which satisfies (14.62) under fairly weak conditions.

If \(\hat{\theta}_{n}\) is any estimator sequence (not necessarily the MLE or an ELE) which satisfies (14.62), it follows that, under \(\theta_{0}\),

\[n^{1/2}(\hat{\theta}_{n}-\theta_{0})\stackrel{{ d}}{{\to}}N(0,\,I^ {-1}(\theta_{0}))\.\]

For the remainder of this section, we will assume such an estimator sequence \(\hat{\theta}_{n}\) is available, by means of verification of Cramer type assumptions presented in Lehmann and Casella (1998), or by direct verification as in the case of exponential families of Example 14.4.2 and Problem 14.38. For testing applications, it is also important to study the behavior of the estimator under contiguous alternatives. The following theorem assumes the expansion (14.62) (which is only assumed to hold under \(\theta_{0}\)) in order to derive the limiting behavior of \(\hat{\theta}_{n}\) under contiguous sequences \(\theta_{n}\).

**Theorem 14.4.1**: _Assume \(X_{1},\ldots,X_{n}\) are i.i.d. according to a q.m.d. model \(\{P_{\theta},\ \theta\in\Omega\}\) with nonsingular Information matrix \(I(\theta)\), \(\theta\in\Omega\), an open subset of \({\rm I\kern-1.8ptR}^{k}\). Suppose an estimator \(\hat{\theta}_{n}\) has the expansion (14.62) when \(\theta=\theta_{0}\). Let \(\theta_{n}=\theta_{0}+h_{n}n^{-1/2}\), where \(h_{n}\to h\in{\rm I\kern-1.8ptR}^{k}\). Then, under \(P_{\theta_{n}}^{n}\),_

\[n^{1/2}(\hat{\theta}_{n}-\theta_{n})\stackrel{{ d}}{{\to}}N(0,\,I ^{-1}(\theta_{0}))\ ; \tag{14.63}\]

_equivalently, under \(P_{\theta_{n}}^{n}\),_

\[n^{1/2}(\hat{\theta}_{n}-\theta_{0})\stackrel{{ d}}{{\to}}N(h,\,I ^{-1}(\theta_{0})). \tag{14.64}\]

_Furthermore, if \(g(\theta)\) is a differentiable map from \(\Omega\) to \({\rm I\kern-1.8ptR}\) with nonzero gradient \(\dot{g}(\theta)\) of dimension \(1\times k\), then under \(P_{\theta_{n}}^{n}\),_

\[n^{1/2}(g(\hat{\theta}_{n})-g(\theta_{n}))\stackrel{{ d}}{{\to}}N( 0,\sigma_{\theta_{0}}^{2})\, \tag{14.65}\]

_where_

\[\sigma_{\theta_{0}}^{2}=\dot{g}(\theta_{0})I^{-1}(\theta_{0})\dot{g}(\theta_{0 })^{\top}. \tag{14.66}\]

Proof. We prove the result in the case \(h_{n}=h\), the more general case deferred to Problem 15.13. We will first show (14.64). By the Cramer-Wold Device, it is enough to show that, for any \(t\in{\rm I\kern-1.8ptR}^{k}\), under \(P_{\theta_{n}}^{n}\),

\[\langle n^{1/2}(\hat{\theta}_{n}-\theta_{0}),\,t\rangle\stackrel{{ d}}{{\to}}N(\langle h,t\rangle,\,\langle t,\,I^{-1}(\theta_{0})t\rangle)\.\]

By the assumption (14.62), we only need to show that, under \(P_{\theta_{n}}^{n}\),

\[\langle I^{-1}(\theta_{0})Z_{n},\,t\rangle\stackrel{{ d}}{{\to}}N(\langle h,t\rangle,\,\langle t,\,I^{-1}(\theta_{0})t\rangle)\.\]

By Example 14.3.7, \(P_{\theta_{n}}^{n}\) is contiguous to \(P_{\theta_{0}}^{n}\), so we can apply Corollary 14.3.2 with \(T_{n}=\langle I^{-1}(\theta_{0})Z_{n},\,t\rangle\). Then,

\[(T_{n},\log(L_{n,h}))=(\langle I^{-1}(\theta_{0})Z_{n},\,t\rangle,\,\langle h,\,Z_{n}\rangle-\frac{1}{2}\langle h,\,I(\theta_{0})h\rangle)+o_{P_{\theta_{0}}^{n}}(1)\.\]

But, under \(\theta_{0}\), \(Z_{n}\) converges in law to \(Z\), where \(Z\) is distributed as \(N(0,\,I(\theta_{0}))\). By Slutsky's Theorem and the Continuous Mapping Theorem (or the bivariate Central Limit Theorem), under \(\theta_{0}\),

\[(T_{n},\,\log(L_{n,h}))\stackrel{{ d}}{{\to}}(\langle I^{-1}( \theta_{0})Z,\,t\rangle,\,\langle h,\,Z\rangle-\frac{1}{2}\langle h,\,I( \theta_{0})h\rangle)\.\]This limiting distribution is bivariate normal with covariance

\[\sigma_{1,2} =Cov(\langle I^{-1}(\theta_{0})Z,t\rangle,\,\langle h,\,Z\rangle)=E[( h^{\top}Z)(I^{-1}(\theta_{0})Z)^{\top}t]\] \[=h^{\top}E(Z_{1}Z_{1}^{\top})I^{-1}(\theta_{0})t=h^{\top}I(\theta _{0})I^{-1}(\theta_{0})t=\langle h,t\rangle\.\]

The result (14.64) follows from Corollary 14.3.2. The assertion (14.65) follows from (14.63) and the delta method.

Under the conditions of the previous theorem, the estimator sequence \(g(\hat{\theta}_{n})\) possesses a weak robustness property in the sense that its limiting distribution is unchanged by small perturbations of the parameter values. In the literature, such estimator sequences are sometimes called _regular_.

**Corollary 14.4.1**: _Assume \(X_{1},\ldots,X_{n}\) are i.i.d. according to a q.m.d. model \(\{P_{\theta},\ \theta\in\Omega\}\) with normalized score vector \(Z_{n}\) given by (14.59) and nonsingular Information matrix \(I(\theta_{0})\). Let \(\theta_{n}=\theta_{0}+h_{n}n^{-1/2}\), where \(h_{n}\to h\in\mathbb{R}^{k}\). Then, under \(P_{\theta_{n}}^{n}\),_

\[Z_{n}\stackrel{{ d}}{{\to}}N(I(\theta_{0})h,\,I(\theta_{0})). \tag{14.67}\]

The proof is left as an exercise (Problem 14.39).

#### Wald Tests and Confidence Regions

Wald proposed tests and confidence regions based on the asymptotic distribution of the maximum likelihood estimator. In this section, we introduce these methods and study their large-sample behavior; some optimality properties will be discussed in Sections 15.3 and 15.4. We assume \(\hat{\theta}_{n}\) is any estimator satisfying (14.62). Let \(g(\theta)\) be a mapping from \(\Omega\) to the real line, assumed differentiable with nonzero gradient vector \(\dot{g}(\theta)\) of dimension \(1\times k\). Suppose the problem is to test the null hypothesis \(g(\theta)=0\) versus the alternative \(g(\theta)>0\). Let \(\theta_{0}\) denote the true value of \(\theta\). Under the assumptions of Theorem 14.4.1, under \(\theta_{0}\),

\[n^{1/2}[g(\hat{\theta}_{n})-g(\theta_{0})]\stackrel{{ d}}{{\to}}N (0,\sigma_{\theta_{0}}^{2})\,\]

where

\[\sigma_{\theta_{0}}^{2}=\dot{g}(\theta_{0})I^{-1}(\theta_{0})\dot{g}(\theta_{0 })^{\top}\.\]

Assuming that \(\dot{g}(\cdot)\) and \(I(\cdot)\) are continuous, the asymptotic variance can be consistently estimated by

\[\hat{\sigma}_{n}^{2}\equiv\dot{g}(\hat{\theta}_{n})I^{-1}(\hat{\theta}_{n}) \dot{g}(\hat{\theta}_{n})^{\top}\.\]

Hence, the test that rejects when \[n^{1/2}g(\hat{\theta}_{n})>\hat{\sigma}_{n}z_{1-\alpha}\]

is pointwise asymptotically level \(\alpha\).

We can also calculate the limiting power against a sequence of alternatives \(\theta_{n}=\theta_{0}+hn^{-1/2}\). Assume \(g(\theta_{0})=0\). Then,

\[P_{\theta_{n}}\{n^{1/2}g(\hat{\theta}_{n})>\hat{\sigma}_{n}z_{1-\alpha}\}=P_{ \theta_{n}}\{n^{1/2}[g(\hat{\theta}_{n})-g(\theta_{n})]>\hat{\sigma}_{n}z_{1- \alpha}-n^{1/2}g(\theta_{n})\}\.\]

By Theorem 14.4.1, \(n^{1/2}[g(\hat{\theta}_{n})-g(\theta_{n})]\) is asymptotically \(N(0,\sigma_{\theta_{0}}^{2})\), under \(\theta_{n}\). Also, \(\hat{\sigma}_{n}\to\sigma_{\theta_{0}}\) in probability under \(\theta_{n}\) (since this convergence holds under \(\theta_{0}\) and therefore under \(\theta_{n}\) by contiguity). Finally, \(n^{1/2}g(\theta_{n})\to\dot{g}(\theta_{0})h\). Hence, the limiting power is

\[\lim_{n\to\infty}P_{\theta_{n}}\{n^{1/2}g(\hat{\theta}_{n})>\hat{\sigma}_{n}z_ {1-\alpha}\}=1-\Phi(z_{1-\alpha}-\sigma_{\theta_{0}}^{-1}\dot{g}(\theta_{0})h ). \tag{14.68}\]

Similarly, a pointwise asymptotically level \(1-\alpha\) confidence interval for \(g(\theta)\) is given by

\[g(\hat{\theta}_{n})\pm z_{1-\frac{\alpha}{2}}n^{-1/2}\hat{\sigma}_{n}\.\]

**Example 14.4.3** (**Normal Coefficient of Variation**) Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(N(\mu,\sigma^{2})\) with both parameters unknown, as in Example 14.4.1. Consider inferences for \(g((\mu,\sigma^{2})^{\top})=\mu/\sigma\), the coefficient of variation. Recall that a uniformly most accurate invariant one-sided confidence bound exists for \(\mu/\sigma\); however, it is quite complicated to compute since it involves the noncentral \(t\)-distribution and no explicit formula is available. However, a normal approximation leads to an interval that is asymptotically valid. Note that

\[\dot{g}((\mu,\sigma^{2})^{\top})=(\frac{1}{\sigma},-\frac{\mu}{2\sigma^{3}})\.\]

By Example 14.4.1, \(n^{1/2}[(\tilde{X}_{n},S_{n}^{2})^{\top}-(\mu,\sigma^{2})^{\top}]\) is asymptotically bivariate normal with asymptotic covariance matrix \(\Sigma\), where \(\Sigma\) is the diagonal matrix with \((1,\,1)\) entry \(\sigma^{2}\) and \((2,\,2)\) entry \(2\sigma^{4}\). Then, the delta method implies that

\[n^{1/2}(\frac{\tilde{X}_{n}}{S_{n}}-\frac{\mu}{\sigma})\stackrel{{ d}}{{\to}}N(0,1+\frac{\mu^{2}}{2\sigma^{2}})\.\]

Thus, the interval

\[\frac{\tilde{X}_{n}}{S_{n}}\pm n^{-1/2}(1+\frac{\tilde{X}_{n}^{2}}{2S_{n}^{2} })z_{1-\frac{\alpha}{2}}\]

is asymptotically pointwise level \(1-\alpha\).

Consider now the general problem of constructing a confidence region for \(\theta\), under the assumptions of Theorem 14.4.1. The convergence \[n^{1/2}(\hat{\theta}_{n}-\theta)\stackrel{{ d}}{{\to}}N(0,I^{-1}( \theta)) \tag{14.69}\]

implies that

\[I^{1/2}(\theta)n^{1/2}(\hat{\theta}_{n}-\theta)\stackrel{{ d}}{{\to }}N(0,I_{k})\,\]

the multivariate normal distribution in \(\mathbb{R}^{k}\) with mean \(0\) and identity covariance matrix \(I_{k}\). Hence, by the Continuous Mapping Theorem 11.2.10 and Example 11.2.5,

\[n(\hat{\theta}_{n}-\theta)^{\top}I(\theta)(\hat{\theta}_{n}-\theta)\stackrel{{ d}}{{\to}}\chi_{k}^{2}\,\]

the Chi-squared distribution with \(k\) degrees of freedom. Thus, a pointwise asymptotic level \(1-\alpha\) confidence region for \(\theta\) is

\[\{\theta:n(\hat{\theta}_{n}-\theta)^{\top}I(\theta)(\hat{\theta}_{n}-\theta) \leq c_{k,1-\alpha}\}\, \tag{14.70}\]

where \(c_{k,1-\alpha}\) is the \(1-\alpha\) quantile of \(\chi_{k}^{2}\). In (14.70), \(I(\theta)\) is often replaced by a consistent estimator, such as \(I(\hat{\theta}_{n})\) (assuming \(I(\cdot)\) is continuous), and the resulting confidence region is known as Wald's confidence ellipsoid.

By the duality between confidence regions and tests, this leads to an asymptotic level \(\alpha\) test of \(\theta=\theta_{0}\) versus \(\theta\neq\theta_{0}\), known as Wald tests. Specifically, for testing \(\theta=\theta_{0}\) versus \(\theta\neq\theta_{0}\), Wald's test rejects if

\[n(\hat{\theta}_{n}-\theta_{0})I(\hat{\theta}_{n})(\hat{\theta}_{n}-\theta_{0}) >c_{k,1-\alpha}. \tag{14.71}\]

Alternatively, \(I(\hat{\theta}_{n})\) may be replaced by \(I(\theta_{0})\) or any consistent estimator of \(I(\theta_{0})\). Under \(\theta_{n}=\theta_{0}+hn^{-1/2}\), the limiting distribution of the Wald statistic given by the left side of (14.71) is \(\chi_{k}^{2}(|I^{1/2}(\theta_{0})h|^{2})\), the noncentral Chi-squared distribution with \(k\) degrees of freedom and noncentrality parameter \(|I^{1/2}(\theta_{0})h|^{2}\) (Problem 14.50).

More generally, consider inference for \(g(\theta)\), where \(g=(g_{1},\ldots,g_{p})^{\top}\) is a mapping from \(\mathbb{R}^{k}\) to \(\mathbb{R}^{p}\). Assume \(g_{i}\) is differentiable and let \(D=D(\theta)\) denote the \(p\times k\) matrix with \((i,\,j)\) entry given by

\[D_{i,\,j}(\theta)=\partial g_{i}(\theta_{1},\ldots,\theta_{k})/\partial\theta _{j}. \tag{14.72}\]

Then, the Delta Method and (14.69) imply that

\[n^{1/2}[g(\hat{\theta}_{n})-g(\theta)]\stackrel{{ d}}{{\to}}N(0, V(\theta))\, \tag{14.73}\]

where \(V(\theta)=D(\theta)I^{-1}(\theta)D^{\top}(\theta)\). Assume \(V(\theta)\) is positive definite and continuous in \(\theta\). By the Continuous Mapping Theorem,

\[n[g(\hat{\theta}_{n})-g(\theta)]^{\top}V^{-1}(\theta)[g(\hat{\theta}_{n})-g( \theta)]\stackrel{{ d}}{{\to}}\chi_{p}^{2}\.\]Hence, a pointwise asymptotically level \(1-\alpha\) confidence region for \(g(\theta)\) is

\[\{\theta:\ n[g(\hat{\theta}_{n})-g(\theta)]^{\top}V^{-1}(\hat{\theta}_{n})[g(\hat {\theta}_{n})-g(\theta)]\leq\chi_{p}^{2}(1-\alpha)\}\.\]

Next, suppose it is desired to test \(g(\theta)=0\). The Wald test rejects when

\[W_{n}=ng(\hat{\theta}_{n})V^{-1}(\hat{\theta}_{n})g^{\top}(\hat{\theta}_{n})\]

exceeds \(\chi_{p}^{2}(1-\alpha)\), and it is pointwise asymptotically level \(\alpha\).

#### Rao Score Tests

Instead of the Wald tests, it is possible to construct tests based directly on \(Z_{n}\) in (14.59), which have the advantage of not requiring computation of a maximum likelihood estimator. Assume q.m.d. holds at \(\theta_{0}\), with derivative \(\eta(\cdot,\theta_{0})\) and, as usual, set

\[\tilde{\eta}(x,\theta_{0})=2\eta(x,\theta_{0})/p_{\theta_{0}}^{1/2}(x)\.\]

Under the assumptions of Theorem 14.2.2, the quadratic mean derivative \(\eta(\cdot,\theta_{0})\) is given by (14.9) and \(n^{1/2}Z_{n}\) can then be computed by

\[n^{1/2}Z_{n}=\sum_{i=1}^{n}\tilde{\eta}(X_{i},\theta_{0})=\]

\[\sum_{i=1}^{n}\frac{\dot{p}_{\theta_{0}}(X_{i})}{p_{\theta_{0}}(X_{i})}=( \frac{\partial}{\partial\theta_{1}}\log L_{n}(\theta),\ldots,\left.\frac{ \partial}{\partial\theta_{k}}\log L_{n}(\theta)\right)\bigg{|}_{\theta=\theta _{0}}. \tag{14.74}\]

As mentioned earlier, the statistic \(Z_{n}\) is known as the normalized _score_ vector. Its use stems from the fact that inference can be based on \(Z_{n}\), which involves differentiating the loglikelihood at a single point \(\theta_{0}\), avoiding the problem of maximizing the likelihood. Even if the ordinary differentiability conditions assumed in Theorem 14.2.2 fail, inference can be based on \(Z_{n}\), as we will now see.

Suppose for the moment that \(\theta\) is real-valued and consider testing \(\theta=\theta_{0}\) versus \(\theta>\theta_{0}\). For a given test \(\phi=\phi(X_{1},\ldots,X_{n})\), let

\[\beta_{\phi}(\theta)=E_{\theta}[\phi(X_{1},\ldots,X_{n})]\]

denote its power function. By Problem 14.17, assuming q.m.d., \(\beta_{\phi}(\theta)\) is differentiable at \(\theta_{0}\) with \[\beta^{\prime}_{\phi}(\theta_{0})=\int\cdots\int\phi(x_{1},\,\ldots,\,x_{n})\sum_{i =1}^{n}\tilde{\eta}(x_{i},\,\theta_{0})\prod_{i=1}^{n}p_{\theta_{0}}(x_{i})\mu( dx_{1})\cdots\mu(dx_{n})\.\]

Consider the problem of finding the level \(\alpha\) test \(\phi\) that maximizes \(\beta^{\prime}_{\phi}(\theta_{0})\). By the general form of the Neyman-Pearson Lemma, the optimal test rejects for large values of \(\sum_{i}\tilde{\eta}(X_{i},\,\theta_{0})\), or equivalently, large values of \(Z_{n}\). By Problem 8.4, if this is the unique test maximizing the slope of the power function at \(\theta_{0}\), then it is also locally most powerful. Thus, tests based on \(Z_{n}\) are appealing from this point of view.

We turn now to the asymptotic behavior of tests based on \(Z_{n}\). Assume the assumptions of quadratic mean differentiability hold for general \(k\), so that under \(\theta_{0}\),

\[Z_{n}\stackrel{{ d}}{{\to}}N(0,\,I(\theta_{0}))\.\]

By Corollary 14.4.1, under \(\theta_{n}=\theta_{0}+hn^{-1/2}\),

\[Z_{n}\stackrel{{ d}}{{\to}}N(I(\theta_{0})h,\,I(\theta_{0}))\.\]

It follows that, under \(\theta_{n}=\theta_{0}+hn^{-1/2}\),

\[I^{-1/2}(\theta_{0})Z_{n}\stackrel{{ d}}{{\to}}N(I^{1/2}(\theta_{0 })h,\,I_{k}). \tag{14.75}\]

Now, suppose \(k=1\) and the problem is to test \(\theta=\theta_{0}\) versus \(\theta>\theta_{0}\). Rao's score test rejects when the one-sided _score statistic_\(I^{-1/2}(\theta_{0})Z_{n}\) exceeds \(z_{1-\alpha}\) and is asymptotically level \(\alpha\). In this case, the Wald test that rejects when \(I^{1/2}(\theta_{0})n^{1/2}(\hat{\theta}_{n}-\theta_{0})\) exceeds \(z_{1-\alpha}\) and the score test are asymptotically equivalent, in the sense that the probability that the two tests yield the same decision tends to one, both under the null hypothesis \(\theta=\theta_{0}\) and under a sequence of alternatives \(\theta_{0}+hn^{-1/2}\). The equivalence follows from contiguity, the expansion (14.62), and the fact that \(I(\hat{\theta}_{n})\to I(\theta_{0})\) in probability under \(\theta_{0}\) and under \(\theta_{0}+hn^{-1/2}\). Note that the two tests may differ greatly for alternatives far from \(\theta_{0}\); see Example 15.3.3.

**Example 14.4.4** (**Bivariate Normal Correlation**) Assume \(X_{i}=(U_{i},\,V_{i})\) are i.i.d. according to the bivariate normal distribution with means zero and variances one, so that the only unknown parameter is \(\rho\), the correlation. In this case,

\[\log L_{n}(\rho)=-n\log(2\pi)-\frac{n}{2}\log(1-\rho^{2})-\sum_{i=1}^{n}[\frac {1}{2(1-\rho^{2})}(U_{i}^{2}-2\rho U_{i}\,V_{i}+V_{i}^{2})]\]

and so

\[\frac{\partial}{\partial\rho}\log L_{n}(\rho)=\frac{n\rho}{1-\rho^{2}}+\frac{ 1}{1-\rho^{2}}\sum_{i=1}^{n}U_{i}\,V_{i}-\frac{\rho}{(1-\rho^{2})^{2}}\sum_{i =1}^{n}(U_{i}^{2}-2\rho U_{i}\,V_{i}+V_{i}^{2})\.\]

In the special case \(\theta_{0}=\rho_{0}=0\),\[Z_{n}=n^{-1/2}\sum_{i=1}^{n}U_{i}\,V_{i}\,\stackrel{{ d}}{{\to}}\,N(0, 1)\.\]

For other values of \(\rho_{0}\), the statistic is more complicated; however, we have bypassed maximizing the likelihood function which may have multiple roots in this example.

For general \(k\), consider testing a simple null hypothesis \(\theta=\theta_{0}\) versus a multi-sided alternative \(\theta\neq\theta_{0}\). Then, assuming the expansion (14.62), we can replace \(n^{1/2}(\hat{\theta}_{n}-\theta_{0})\) in the Wald statistic (14.70) by \(I^{-1}(\theta_{0})Z_{n}\). In this case, the _score test_ rejects the null hypothesis when the multi-sided _score statistic_\(Z_{n}^{\top}I^{-1}(\theta_{0})Z_{n}\) exceeds \(c_{k,1-\alpha}\), and is asymptotically level \(\alpha\). Again, the Wald test and Rao's score test are asymptotically equivalent in the sense described above.

Next, we consider a composite null hypothesis. Interest focuses on the first \(p\) components of \(\theta\), \(\theta_{1}\),..., \(\theta_{p}\), with the remaining \(k-p\) components viewed as nuisance parameters. Let \(\theta_{1,0}\),..., \(\theta_{p,0}\) be fixed and consider testing the null hypothesis \(\theta_{i}=\theta_{i,0}\) for \(i=1,\)..., \(p\). The Wald test is based on the limit

\[n^{1/2}(\hat{\theta}_{n,1}-\theta_{1},\ldots,\hat{\theta}_{n,p}-\theta_{p}) \stackrel{{ d}}{{\to}}\,N\left(0,\,\Sigma^{(p)}(\theta)\right)\,\]

where \(\Sigma(\theta)=I^{-1}(\theta)\) and \(\Sigma^{(p)}(\theta)\) is the \(p\times p\) matrix formed by the intersection of the first \(p\) rows and columns of \(\Sigma(\theta)\). Similarly, define \(I^{(p)}(\theta)\) as the \(p\times p\) matrix formed by the intersection of the first \(p\) rows and columns of \(I(\theta)\). Partition \(I(\theta)\) as

\[I(\theta)=\left(\begin{array}{cc}I^{(p)}(\theta)&I_{12}(\theta)\\ I_{21}(\theta)&I_{22}(\theta)\end{array}\right). \tag{14.76}\]

Note that (Problem 14.51)

\[[\Sigma^{(p)}(\theta)]^{-1}=[I^{(p)}(\theta)]\ -I_{12}(\theta)I_{22}^{-1}( \theta)I_{21}(\theta). \tag{14.77}\]

The score test is based on \(Z_{n}^{(p)}(\theta)\), the \(p\)-vector obtained as the first \(p\) components of \(Z_{n}(\theta)\), where \(Z_{n}(\theta)\) is defined in (14.59). Under q.m.d. at \(\theta\),

\[Z_{n}^{(p)}(\theta)\stackrel{{ d}}{{\to}}\,N\left(0,\,I^{(p)}( \theta)\right)\,\]

and so

\[T_{n}(\theta)=[Z_{n}^{(p)}(\theta)]^{\top}[I^{(p)}(\theta)]^{-1}[Z_{n}^{(p)}( \theta)]\stackrel{{ d}}{{\to}}\chi_{p}^{2}\.\]

However, when the null hypothesis is not completely specified, the Rao score test statistic is \(T_{n}(\hat{\theta}_{n,0})\), where

\[\hat{\theta}_{n,0}=(\theta_{1,0},\ldots,\theta_{p,0},\,\hat{\theta}_{p+1,0}, \ldots,\hat{\theta}_{k,0})\]is the constrained maximum likelihood estimator of \(\theta\), that is, the maximum likelihood estimator under the restricted parameter space satisfying the constraints of the null hypothesis. In fact, as argued by Hall and Mathiason (1990), any \(\sqrt{n}\)-consistent estimator can be used in the score statistic.

In order to determine the constrained maximum likelihood estimator \(\tilde{\theta}_{n}\), one typically introduces the Lagrangian function and maximizes

\[\log L_{n}(\theta)-\sum_{i=1}^{p}\lambda_{i}(\theta_{i}-\theta_{i,0})\]

over \(\theta\) and the so-called Lagrange multipliers \(\lambda_{1}\),..., \(\lambda_{p}\). Assuming differentiability, the first-order conditions require that

\[\frac{\partial}{\partial\theta_{i}}\log L_{n}(\theta)|_{\theta=\tilde{\theta}_ {n}}=\tilde{\lambda}_{i}\quad i=1,\ldots,p\.\]

Since the left-hand side represents the components of \(\sqrt{n}Z_{n}^{(p)}(\tilde{\theta}_{n})\), tests based on \(Z_{n}^{(p)}(\tilde{\theta}_{n})\) are equivalent to tests based on \(\tilde{\lambda}=(\tilde{\lambda}_{1}\),..., \(\tilde{\lambda}_{p})^{\top}\). For this reason, score tests are sometimes referred to as Lagrange multiplier tests. Such terminology is particularly popular in econometrics.

More generally, suppose the null hypothesis \(H_{0}\) specifies \(g_{i}(\theta)=c_{i}\), for \(i=1,\ldots,p\), where \(p\leq k\) and the \(c_{i}\) are fixed. Let \(\tilde{\theta}_{n}\) be the restricted maximum likelihood estimator for the null hypothesis parameter space, assuming it exists. The score test is based on

\[Z_{n}(\tilde{\theta}_{n})^{\top}I^{-1}(\tilde{\theta}_{n})Z_{n}(\tilde{\theta} _{n})\,\]

which, under smoothness assumptions on the \(g_{j}\), is asymptotically Chi-squared with \(p\) degrees of freedom under \(H_{0}\). Note \(Z_{n}(\tilde{\theta}_{n})\) can also be represented in terms of Lagrange multipliers. To see how, let \(S_{n}(\theta)\) be the \(k\times 1\) score vector with \(i\)th component

\[S_{n,i}(\theta)=\frac{\partial}{\partial\theta_{i}}\log L_{n}(\theta)\.\]

(So, \(Z_{n}(\theta)=n^{-1/2}S_{n}(\theta)\) under usual differentiability; see Theorem 14.2.2 and (14.74)). The Lagrangian function is

\[\log L_{n}(\theta)-\sum_{i=1}^{p}\lambda_{i}[g_{i}(\theta)-c_{i}]\.\]

Let \(D_{n}(\theta)\) be the \(p\times k\) matrix with \((i,j)\) entry given in (14.72). Then, \(\tilde{\theta}_{n}\) satisfies the first-order conditions if

\[g_{i}(\tilde{\theta}_{n})=c_{i}\quad\text{for $i=1,\ldots,p$}\]and

\[S_{n}(\tilde{\theta}_{n})-D(\tilde{\theta}_{n})^{\top}\tilde{\lambda}=0\,\]

or \(Z_{n}(\tilde{\theta}_{n})=\sqrt{n}\,D(\tilde{\theta}_{n})^{\top}\tilde{\lambda}\).

#### Likelihood Ratio Tests

In addition to that Wald and Rao scores tests of Sections 14.4.2 and 14.4.3, let us now consider a third test of \(\theta\in\Omega_{0}\) versus \(\theta\notin\Omega_{0}\), based on the _likelihood ratio statistic_\(2\log(R_{n})\), where

\[R_{n}=\frac{\sup_{\theta\in\Omega}L_{n}(\theta)}{\sup_{\theta\in\Omega_{0}}L_{n }(\theta)}. \tag{14.78}\]

The _likelihood ratio test_ rejects for large values of \(2\log(R_{n})\). If \(\hat{\theta}_{n}\) and \(\hat{\theta}_{n,0}\) are MLEs for \(\theta\) as \(\theta\) varies in \(\Omega\) and \(\Omega_{0}\), respectively, then

\[R_{n}=L_{n}(\hat{\theta}_{n})/L_{n}(\hat{\theta}_{n,0}). \tag{14.79}\]

In the real-valued case when testing the simple null hypothesis \(\theta=\theta_{0}\), Figure 14.1 plots the logarithm of the likelihood function \(L_{n}(\theta)\) as a function of \(\theta\). The difference between the MLE \(\hat{\theta}_{n}\) and \(\theta_{0}\) can easily be seen on the horizontal \(\theta\)-axis, and serves as a basis for a Wald test. The difference between the loglikelihood function at \(\hat{\theta}_{n}\) and \(\theta_{0}\) is depicted as \(\log(R_{n})\). Rao's score test derives from the tangent (in green) to the loglikelihood when \(\theta=\theta_{0}\).

**Example 14.4.5** (Multivariate Normal Mean): Suppose \(X=(X_{1},\ldots,X_{k})^{\top}\) is multivariate normal with unknown mean vector \(\theta\) and known positive definite covariance matrix \(\Sigma\). The likelihood function is given by

\[\frac{|\Sigma|^{-1/2}}{(2\pi)^{k/2}}\exp\left[-\frac{1}{2}(X-\theta)^{\top} \Sigma^{-1}(X-\theta)\right]\,.\]

Assume \(\theta\in\mathbb{R}^{k}\) and that the null hypothesis asserts \(\theta_{i}=0\) for \(i=1,\ldots,k\). Then,

\[2\log(R_{1})=-\inf_{\theta}(X-\theta)^{\top}\Sigma^{-1}(X-\theta)+X^{\top} \Sigma^{-1}X=X^{\top}\Sigma^{-1}X=|\Sigma^{-1/2}X|^{2}\.\]

Under the null hypothesis, \(\Sigma^{-1/2}X\) is exactly standard multivariate normal, and so the null distribution of \(2\log(R_{1})\) is exactly \(\chi_{k}^{2}\) in this case.

Now, consider testing the composite hypothesis \(\theta_{i}=0\) for \(i=1,\ldots,p\), with the remaining parameters \(\theta_{p+1},\ldots,\theta_{k}\) regarded as nuisance parameters. More generally, suppose

\[\Omega_{0}=\{\theta=(\theta_{1},\ldots,\theta_{k}):\ A(\theta-a)=0\}\, \tag{14.80}\]where \(A\) is a \(p\times k\) matrix of rank \(p\) and \(a\) is some fixed \(k\times 1\) vector. Then,

\[2\log(R_{1})=-\inf_{\theta\in\mathbf{R}^{k}}(X-\theta)^{\top}\Sigma^{-1}(X-\theta )+\inf_{\theta\in\Omega_{0}}(X-\theta)^{\top}\Sigma^{-1}(X-\theta)\]

\[=\inf_{\theta\in\Omega_{0}}(X-\theta)^{\top}\Sigma^{-1}(X-\theta). \tag{14.81}\]

The null distribution of (14.81) is \(\chi_{p}^{2}\) (Problem 14.52).

Let us now consider the large-sample behavior of the likelihood ratio test in greater generality. First, suppose \(\Omega_{0}=\{\theta_{0}\}\) is simple. Then,

\[\log(R_{n})=\sup_{h}[\log(L_{n,h})]\,\]

where \(L_{n,h}\) is defined in (14.57). If the family is q.m.d. at \(\theta_{0}\), then

\[\log(R_{n})=\sup_{h}[\langle h,\,Z_{n}\rangle-\frac{1}{2}\langle h,\,I(\theta_ {0})h\rangle+o_{P_{\theta_{0}}^{n}}(1)]\.\]

It is plausible that \(\log(R_{n})\) should behave like

\[\log\tilde{R}_{n}\equiv\sup_{h}[\log(\tilde{L}_{n,h})]\,\]

where \(\tilde{L}_{n,h}\) is defined by (14.60). But \(\tilde{L}_{n,h}\) is maximized at \(\tilde{h}_{n}=I^{-1}(\theta_{0})Z_{n}\) and so

Figure 14.1: Loglikelihood Function

\[\log(R_{n})\approx\log(\tilde{R}_{n})=\log(\tilde{L}_{n,\tilde{h}_{n}})=\frac{1}{2}Z _{n}^{\top}I^{-1}(\theta_{0})Z_{n}\.\]

Since, \(2\log(\tilde{R}_{n})\stackrel{{ d}}{{\to}}\chi_{k}^{2}\), the heuristics suggest that \(2\log(R_{n})\stackrel{{ d}}{{\to}}\chi_{k}^{2}\) as well. In fact, \(2\log(\tilde{R}_{n})\) is Rao's score test statistic, and so these heuristics also suggest that Rao's score test, the likelihood ratio test, and Wald's test are all asymptotically equivalent in the sense described earlier in comparing the Wald test and the score test. Note, however, that the tests are not always asymptotically equivalent; some striking differences will be presented in Section 15.3.

These heuristics can be made rigorous under stronger assumptions, such as Cramer type differentiability conditions used in proving asymptotic normality of the MLE or an ELE; see Theorem 7.7.2 in Lehmann (1999). Alternatively, once the general heuristics point toward the limiting behavior, the approximations may be made rigorous by direct calculation in a particular situation. A general theorem based on the existence of efficient likelihood estimators will be presented following the next example.

**Example 14.4.6**: **(Multinomial Goodness of Fit)** Consider a sequence of \(n\) independent trials, each resulting in one of \(k+1\) outcomes \(1\),..., \(k+1\). Outcome \(j\) occurs with probability \(p_{j}\) on any given trial. Let \(Y_{j}\) be the number of trials resulting in outcome \(j\). Consider testing the simple null hypothesis \(p_{j}=\pi_{j}\) for \(j=1,\ldots,k+1\). The parameter space \(\Omega\) is

\[\Omega=\{(p_{1},\ldots,p_{k})\in\mathbb{R}^{k}:\ p_{i}\geq 0,\ \ \sum_{j=1}^{k}p_{j}\leq 1\} \tag{14.82}\]

since \(p_{k+1}\) is determined as \(1-\sum_{j=1}^{k}p_{j}\). In this case, the likelihood can be written as

\[L_{n}(p_{1},\ldots,p_{k})=\frac{n!}{Y_{1}!\cdots Y_{k+1}!}p_{1}^{Y_{1}}\cdots p _{k+1}^{Y_{k+1}}\.\]

By solving the likelihood equations, it is easily checked that the unique MLE is given by \(\hat{p}_{j}=Y_{j}/n\) (Problem 14.57 (i)). Hence, the likelihood ratio statistic is

\[R_{n}=\frac{L_{n}(Y_{1}/n,\ldots,Y_{k}/n)}{L_{n}(\pi_{1},\ldots,\pi_{k})}\,\]

and so (Problem 14.57 (ii))

\[\log(R_{n})=n\sum_{j=1}^{k+1}\hat{p}_{j}\log(\frac{\hat{p}_{j}}{\pi_{j}}). \tag{14.83}\]

The previous heuristics suggest that \(2\log(R_{n})\) converges in distribution to \(\chi_{k}^{2}\), which will be proved in Theorem 14.4.2 below. Note that the Taylor expansion \[f(x)=x\log(x/x_{0})=(x-x_{0})+\frac{1}{2x_{0}}(x-x_{0})^{2}+o[(x-x_{0})^{2}]\]

as \(x\to x_{0}\) implies \(2\log(R_{n})\approx Q_{n}\), where \(Q_{n}\) is Pearson's Chi-squared statistic given by

\[Q_{n}=\sum_{j=1}^{k+1}\frac{(Y_{j}-n\pi_{j})^{2}}{n\pi_{j}}. \tag{14.84}\]

Indeed, \(2\log(R_{n})-Q_{n}\stackrel{{ P}}{{\to}}0\), under the null hypothesis (Problem 14.59) and so they have the same limiting distribution. Moreover, it can be checked (Problem 14.58) that Rao's Score test statistic is exactly \(Q_{n}\). The Chi-squared test will be treated more fully in Section 16.3.

Next, we present a fairly general result on the asymptotic distribution of the likelihood ratio statistic. Actually, we consider a generalization of the likelihood ratio statistic. Rather than having to compute the maximum likelihood estimators \(\hat{\theta}_{n}\) and \(\hat{\theta}_{n,0}\) in (14.79), we assume these estimators satisfy (14.62) under the models with parameter spaces \(\Omega\) and \(\Omega_{0}\), respectively.

**Theorem 14.4.2**: _Assume \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. according to q.m.d. family \(\{P_{\theta},\ \theta\in\Omega\}\), where \(\Omega\) is an open subset of \(\mathbb{R}^{k}\) and \(I(\theta)\) is positive definite. Further assume, for \(\theta\) in a neighborhood of \(\theta_{0}\) and a (measurable) function \(M(x)\) satisfying \(E_{\theta_{0}}[M(X_{i})]<\infty\),_

\[|\log p_{\theta}(x)-\log p_{\theta_{0}}(x)-(\theta-\theta_{0})\tilde{\eta}_{ \theta_{0}}(x)|\leq M(x)|\theta-\theta_{0}|^{2}. \tag{14.85}\]

_(i) Consider testing the simple null hypothesis \(\theta=\theta_{0}\). Suppose \(\hat{\theta}_{n}\) is an efficient estimator for \(\theta\) assuming \(\theta\in\Omega\) in the sense that it satisfies (14.62) when \(\theta=\theta_{0}\). Then, the likelihood ratio \(R_{n}=L_{n}(\hat{\theta}_{n})/L_{n}(\theta_{0})\) satisfies, under \(\theta_{0}\),_

\[2\log(R_{n})\stackrel{{ d}}{{\to}}\chi_{k}^{2}\.\]

_(ii) Consider testing the composite null hypothesis \(\theta\in\Omega_{0}\), where_

\[\Omega_{0}=\{\theta=(\theta_{1},\,\ldots,\,\theta_{k}):\ A(\theta-a)=0\}\,\]

_and \(A\) is a \(p\times k\) matrix of rank \(p\) and \(a\) is a fixed \(k\times 1\) vector. Let \(\hat{\theta}_{n,0}\) denote an efficient estimator of \(\theta\) assuming \(\theta\in\Omega_{0}\); that is, assume the expansion (14.62) holds based on the model \(\{P_{\theta},\ \theta\in\Omega_{0}\}\) and any \(\theta\in\Omega_{0}\). Then, the likelihood ratio \(R_{n}=L_{n}(\hat{\theta}_{n})/L_{n}(\hat{\theta}_{n,0})\) satisfies, under any \(\theta_{0}\in\Omega_{0}\),_

\[2\log(R_{n})\stackrel{{ d}}{{\to}}\chi_{p}^{2}\.\]

[MISSING_PAGE_EMPTY:2304]

\[\liminf_{n}\,P\{2\log(L_{n,\hat{h}_{n}})\geq x\}\geq P\{\chi_{k}^{2}\geq x\}\, \tag{14.87}\]

and (i) is proved.

The proof of (ii) is based on a similar argument, combined with the results of Example 14.4.5 for testing a composite null hypothesis about a multivariate normal mean vector. The proof of (iii) is left as an exercise (Problem 14.62).

**Example 14.4.7** (**One-Sample Normal Mean**): Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\) with both parameters unknown. Consider testing \(\mu=0\) versus \(\mu\neq 0\). Then (Problem 14.47),

\[2\log(R_{n})=n\log(1+\frac{t_{n}^{2}}{n-1})\, \tag{14.88}\]

where \(t_{n}^{2}=n\bar{X}_{n}^{2}/S_{n}^{2}\) is the one-sample \(t\)-statistic. By Problem 13.28, one can deduce the following Edgeworth expansion for \(2\log(R_{n})\) (Problem 14.48):

\[P\{2\log(R_{n})\leq r\}=1-2[\Phi(-z)+\frac{3}{4n}z\phi(z)]+\mathit{O}(n^{-2})\, \tag{14.89}\]

where \(z=\sqrt{r}\), \(\Phi\) is the standard normal c.d.f. and \(\Phi^{\prime}=\phi\). This implies that the test that rejects when \(2\log(R_{n})>z_{1-\frac{n}{2}}\) has rejection probability equal to \(\alpha+\mathit{O}(n^{-1})\). But, a simple correction, known as a _Bartlett correction_, can improve the \(\chi_{1}^{2}\) approximation. Indeed, (14.89) and a Taylor expansion implies

\[P\{2\log(R_{n})(1+\frac{b}{n})>z_{1-\frac{n}{2}}\}=\alpha+\mathit{O}(n^{-2})\, \tag{14.90}\]

if we take \(b=3/2\). Thus, the error in rejection probability of the Bartlett-corrected test is \(\mathit{O}(n^{-2})\). Of course, in this example, the exact two-sided \(t\)-test is available.

It is worth knowing that, quite generally, a simple multiplicative correction to the likelihood ratio statistic greatly improves the quality of the approximation. Specifically, for an appropriate choice of \(b\), comparing \(2\log(R_{n})(1+\frac{b}{n})\) to the usual limiting \(\chi_{p}^{2}\) reduces the error in rejection probability from \(\mathit{O}(n^{-1})\) to \(\mathit{O}(n^{-2})\). In practice, \(b\) can be derived by analytical means or estimated. The idea for such a Bartlett correction originated in Bartlett (1937). For appropriate regularity conditions that imply a Bartlett correction works, see Barndorff-Nielsen and Hall (1988), Bickel and Ghosh (1990), Jensen (1993) and DiCiccio and Stern (1994).

For hypotheses of the form assumed in Theorem 14.4.2, the degrees of freedom can be remembered as the dimension of \(\Omega\) minus the dimension of \(\Omega_{0}\). In the special case where the null hypothesis is specified by \(\theta_{i}=\theta_{i,0}\) for \(i=1,\ldots,p\) and \(\theta_{j}\) regarded as a nuisance parameter for \(j=p+1,\ldots,k\), then the dimension of \(\Omega\) is \(k\) and the dimension of \(\Omega_{0}\) is \(k-p\); the degrees of freedom reduces to the number of parameters \(p\) with values specified under the null hypothesis.

However, even for very smooth models, the limiting distribution of the likelihood ratio test need not be Chi-squared under the null hypothesis when the null parameter space takes a different form than assumed in Theorem 14.4.2. The problem of testing moment inequalities is an important example, as illustrated next.

**Example 14.4.8**: (**Moment Inequalities**) The problem of testing moment inequalities was first considered in Example 8.7.3. Assume \(X_{1}\),..., \(X_{n}\) are i.i.d. multivariate normal with unknown mean vector \(\theta\in\Omega=\mbox{\rm I$\!$R}^{k}\) and known invertible covariance matrix \(\Sigma\). The problem is to test the null hypothesis \(\theta\in\Omega_{0}\), where

\[\Omega_{0}=\{\theta:\theta_{i}\leq 0\ \ \mbox{ for all }i=1,\ldots k\}\.\]

Of course, one can reduce by sufficiency to the sample mean vector \(\bar{X}_{n}\). Then, the argument leading to (14.81) shows that the likelihood ratio statistic in this case becomes

\[2\log(R_{n})=n\inf_{\theta\in\Omega_{0}}(\bar{X}_{n}-\theta)^{\top}\Sigma^{-1} (\bar{X}_{n}-\theta)\.\]

Assume \(\Sigma\) is the identity matrix. Then, \(R_{n}\) reduces to

\[2\log(R_{n})=n\sum_{i=1}^{k}\max^{2}(\bar{X}_{n,i},0)\.\]

If \(\theta=0\), then the distribution of \(2\log(R_{n})\) can be represented (exactly for any \(n\)) as that of

\[\sum_{i=1}^{k}\max^{2}(Z_{i},0)\,\]

where the \(Z_{i}\) are i.i.d. standard normal. Let \(c_{k,1-\alpha}\) be the \(1-\alpha\) quantile of this distribution. Then, by monotonicity (as in Example 8.7.3), the test that rejects when \(2\log(R_{n})\) exceeds \(c_{k,1-\alpha}\) has size \(\alpha\). On the other hand, if \(\theta\) is such that exactly \(p\) components are \(0\) and the remaining \(k-p\) components are negative, then

\[2\log(R_{n})\stackrel{{ d}}{{\rightarrow}}\sum_{i=1}^{p}\max(Z_{i }^{2},0)\.\]

Note the family of distributions on the right-hand side is stochastically increasing in \(p\). Finally, if \(\theta\) lies in the interior of \(\Omega_{0}\), \(2\log(R_{n})\stackrel{{ P}}{{\rightarrow}}0\).

Suppose that \(\theta_{n}=\theta_{0}+hn^{-1/2}\), where \(\theta_{0}=(\theta_{0,1},\)..., \(\theta_{0,k})\) lies on the boundary of \(\Omega\). Let \(I\) be the set of indices \(i\) for which \(\theta_{0,i}=0\). Then, the limiting power of the likelihood ratio test can be represented as (Problem 14.67)

\[P\{\sum_{i\in I}\max^{2}(Z_{i}+h_{i},0)>c_{k,1-\alpha}\}. \tag{14.91}\]If \(p=|I|\) is the number of indices of \(\theta_{0}\) that are zero and \(p<k\), then since \(c_{p,1-\alpha}<c_{k,1-\alpha}\), (14.92) is bounded above by

\[P\{\sum_{i\in I}\max^{2}(Z_{i}+h_{i},0)>c_{p,1-\alpha}\}\.\]

Moment selection methods allow one to achieve power (14.92); see Problem 14.68.

### Problems

#### Section 14.2

**Problem 14.1**: Generalize Example 14.2.1 to the case where \(X\) is multivariate normal with mean vector \(\theta\) and nonsingular covariance matrix \(\Sigma\).

**Problem 14.2**: Generalize Example 14.2.2 to the case of a multiparameter exponential family. Compare with the result of Problem 14.1.

**Problem 14.3**: Suppose \(g_{n}\) is a sequence of functions in \(L^{2}(\mu)\); that is, \(\int g_{n}^{2}d\mu<\infty\). Assume, for some function \(g\), \(\int(g_{n}-g)^{2}d\mu\to 0\). Prove that \(\int g^{2}d\mu<\infty\).

**Problem 14.4**: Suppose \(g_{n}\) is a sequence of functions in \(L^{2}(\mu)\) and, for some function \(g\), \(\int(g_{n}-g)^{2}d\mu\to 0\). If \(\int h^{2}d\mu<\infty\), show that \(\int hg_{n}d\mu\to\int hgd\mu\).

**Problem 14.5**: Suppose \(X\) and \(Y\) are independent, with \(X\) distributed as \(P_{\theta}\) and \(Y\) as \(\tilde{P}_{\theta}\), as \(\theta\) varies in a common index set \(\Omega\). Assume the families \(\{P_{\theta}\}\) and \(\{\tilde{P}_{\theta}\}\) are q.m.d. with Fisher Information matrices \(I_{X}(\theta)\) and \(I_{Y}(\theta)\), respectively. Show that the model based on the joint data \((X,Y)\) is q.m.d. and its Fisher Information matrix is given by \(I_{X}(\theta)+I_{Y}(\theta)\).

**Problem 14.6**: Fix a probability \(P\). Let \(u(x)\) satisfy

\[\int u(x)d\,P(x)=0\.\]

(i) Assume \(\sup_{x}|u(x)|<\infty\), so that

\[p_{\theta}(x)=[1+\theta u(x)]\]

defines a family of densities (with respect to \(P\)) for all small \(|\theta|\). Show this family is q.m.d. at \(\theta=0\). Calculate the quadratic mean derivative, score function, and \(I(0)\).

(ii) Alternatively, if \(u\) is unbounded, define \(p_{\theta}(x)=C(\theta)\exp(\theta u(x))\), assuming \(\int\exp(\theta u(x))d\,P(x)\) exists for all small \(|\theta|\). For this family, argue the family is q.m.d. at \(\theta=0\), and calculate the score function and \(I(0)\).

(iii) Suppose \(\int u^{2}(x)dP(x)<\infty\). Define

\[p_{\theta}(x)=C(\theta)2[1+\exp(-2\theta u(x))]^{-1}\.\]

Show this family is q.m.d. at \(\theta=0\), and calculate the score function and \(I(0)\). [The constructions in this problem are important for nonparametric applications, used later in Chapters 15 and 16. The last construction is given in van der Vaart (1998).]

**Problem 14.7**: Fix a probability \(P\) on \(S\) and functions \(u_{i}(x)\) such that \(\int u_{i}(x)dP(x)=0\) and \(\int u_{i}^{2}(x)dP(x)<\infty\), for \(i=1,2\). Adapt Problem 14.6 to construct a family of distributions \(P_{\theta}\) with \(\theta\in{\rm I\kern-1.8ptR}^{2}\), defined for all small \(|\theta|\), such that \(P_{0,0}=P\), the family is q.m.d. at \(\theta=(0,0)\) with score vector at \(\theta=(0,0)\) given by \((u_{1}(x),u_{2}(x))\). If \(S\) is the real line, construct the \(P_{\theta}\) that works even if \(P_{\theta}\) is required to be smooth if \(P\) and the \(u_{i}\) are smooth (i.e., having differentiable densities) or subject to moment constraints (i.e., having finite \(p\)th moments).

**Problem 14.8**: Show that the definition of \(I(\theta)\) in Definition 14.2.2 does not depend on the choice of dominating measure \(\mu\).

**Problem 14.9**: In Examples 14.2.3 and 14.2.4, find the quadratic mean derivative and \(I(\theta)\).

**Problem 14.10**: In Example 14.2.5, show that \(\int[[f^{\prime}(x)]^{2}/f(x)]dx\) is finite iff \(\beta>1/2\).

**Problem 14.11**: Prove Theorem 14.2.2 using an argument similar to the proof of Theorem 14.2.1.

**Problem 14.12**: Suppose \(\{P_{\theta}\}\) is q.m.d. at \(\theta_{0}\) with derivative \(\eta(\cdot,\,\theta_{0})\). Show that, on \(\{x:\ p_{\theta_{0}}(x)=0\}\), we must have \(\eta(x,\,\theta_{0})=0\), except possibly on a \(\mu\)-null set. _Hint_: On \(\{p_{\theta_{0}}(x)=0\}\), write

\[0\leq n^{1/2}P_{\theta_{0}+hn^{-1/2}}^{1/2}(x)=\langle h,\,\eta(x,\,\theta_{0} )\rangle+r_{n,h}(x)\,\]

where \(\int r_{n,h}^{2}(x)\mu(dx)\to 0\). This implies, with \(h\) fixed, that \(r_{n,h}(x)\to 0\) except for \(x\) in \(\mu\)-null set, at least along some subsequence.

**Problem 14.13**: Suppose \(\{P_{\theta}\}\) is q.m.d. at \(\theta_{0}\). Show

\[P_{\theta_{0}+h}\{x:\ p_{\theta_{0}}(x)=0\}=o(|h|^{2})\]

as \(|h|\to 0\). Hence, if \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. with likelihood ratio \(L_{n,h}\) defined by (14.12), show that

\[P_{\theta_{0}+hn^{-1/2}}^{n}\{L_{n,h}=\infty\}\to 0\.\]

**Problem 14.14**: To see what might happen when the parameter space is not open, let \[f_{0}(x)=xI\{0\leq x\leq 1\}+(2-x)I\{1<x\leq 2\}\.\]

Consider the family of densities indexed by \(\theta\in[0,\,1)\) defined by

\[p_{\theta}(x)=(1-\theta^{2})f_{0}(x)+\theta^{2}f_{0}(x-2)\.\]

Show that the condition (14.5) holds when \(\theta_{0}=0\), if it is only required that \(h\) tends to \(0\) through positive values. Investigate the behavior of the likelihood ratio (14.12) for such a family. (For a more general treatment, consult Pollard (1997)).

**Problem 14.15**: Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. and uniformly distributed on \((0,\,\theta)\). Let \(p_{\theta}(x)=\theta^{-1}I\{0<x<\theta\}\) and \(L_{n}(\theta)=\prod_{i}\,p_{\theta}(X_{i})\). Fix \(p\) and \(\theta_{0}\). Determine the limiting behavior of \(L_{n}(\theta_{0}+hn^{-p})/L_{n}(\theta_{0})\) under \(\theta_{0}\). For what \(p\) and \(h\) is the limiting distribution nondegenerate?

**Problem 14.16**: Suppose \(\{P_{\theta},\,\theta\in\Omega\}\) is a model with \(\Omega\) an open subset of \({\rm I\!R}^{k}\), and having densities \(p_{\theta}(x)\) with respect to \(\mu\). Define the model to be \(L_{1}\)-differentiable at \(\theta_{0}\) if there exists a vector of real-valued functions \(\zeta(\cdot,\,\theta_{0})\) such that

\[\int|p_{\theta_{0}+h}(x)-\,p_{\theta_{0}}(x)-\langle\zeta(x,\,\theta_{0}),\,h \rangle|d\mu(x)=o(|h|) \tag{14.93}\]

as \(|h|\,\to\,0\). Show that, if the family is q.m.d. at \(\theta_{0}\) with q.m. derivative \(\eta(\cdot,\,\theta_{0})\), then it is \(L_{1}\)-differentiable with

\[\zeta(x,\,\theta_{0})=2\eta(x,\,\theta_{0})p_{\theta_{0}}^{1/2}(x)\,\]

but the converse is false.

**Problem 14.17**: Assume \(\{P_{\theta},\,\theta\in\Omega\}\) is \(L_{1}\)-differentiable, so that (14.93) holds. For simplicity, assume \(k=1\) (but the problem generalizes). Let \(\phi(\cdot)\) be uniformly bounded and set \(\beta(\theta)=E_{\theta}[\phi(X)]\). Show \(\beta^{\prime}(\theta)\) exists at \(\theta_{0}\) and

\[\beta^{\prime}(\theta_{0})=\int\phi(x)\zeta(x,\,\theta_{0})\mu(dx). \tag{14.94}\]

Hence, if \(\{P_{\theta}\}\) is q.m.d. at \(\theta_{0}\) with derivative \(\eta(\cdot,\,\theta_{0})\), then

\[\beta^{\prime}(\theta_{0})=\int\phi(x)\widetilde{\eta}(x,\,\theta_{0})\,p_{ \theta_{0}}(x)\mu(dx)\, \tag{14.95}\]

where \(\widetilde{\eta}(x,\,\theta_{0})=2\eta(x,\,\theta_{0})/p_{\theta_{0}}^{1/2}(x)\). More generally, if \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. \(P_{\theta}\) and \(\phi(X_{1},\,\ldots,\,X_{n})\) is uniformly bounded, then \(\beta(\theta)=E_{\theta}[\phi(X_{1},\,\ldots,\,X_{n})]\) is differentiable at \(\theta_{0}\) with \[\beta^{\prime}(\theta_{0})=\int\dots\int\phi(x_{1},\dots,x_{n})\sum_{i=1}^{n}\tilde {\eta}(x_{i},\theta_{0})\prod_{i=1}^{n}p_{\theta_{0}}(x_{i})\mu(dx_{1})\cdots \mu(dx_{n}). \tag{14.96}\]

_Section 14.3_

**Problem 14.18**: Prove (14.31).

**Problem 14.19**: Show the convergence (14.35).

**Problem 14.20**: Fix two probabilities \(P\) and \(Q\) and let \(P_{n}=P\) and \(Q_{n}=Q\). Show that \(\{P_{n}\}\) and \(\{Q_{n}\}\) are contiguous iff \(P\) and \(Q\) are absolutely continuous.

**Problem 14.21**: Fix two probabilities \(P\) and \(Q\) and let \(P_{n}=P^{n}\) and \(Q_{n}=Q^{n}\). Show that \(\{P_{n}\}\) and \(\{Q_{n}\}\) are contiguous iff \(P=Q\).

**Problem 14.22**: Suppose \(Q_{n}\) is contiguous to \(P_{n}\) and let \(L_{n}\) be the likelihood ratio defined by (14.36). Show that \(E_{P_{n}}(L_{n})\to 1\). Is the converse true?

**Problem 14.23**: Consider a sequence \(\{P_{n},\,Q_{n}\}\) with likelihood ratio \(L_{n}\) defined in (14.36). Assume \(\mathcal{L}(L_{n}|P_{n})\stackrel{{ d}}{{\to}}W\), where \(P\{W=0\}=0\); show \(P_{n}\) is contiguous to \(Q_{n}\). Also, under (14.41), deduce that \(P_{n}\) is contiguous to \(Q_{n}\) and hence \(P_{n}\) and \(Q_{n}\) are mutually contiguous if and only if \(\mu=-\sigma^{2}/2\).

**Problem 14.24**: Suppose, under \(P_{n}\), \(X_{n}=Y_{n}+o_{P_{n}}(1)\); that is, \(X_{n}-Y_{n}\to 0\) in \(P_{n}\)-probability. Suppose \(Q_{n}\) is contiguous to \(P_{n}\). Show that \(X_{n}=Y_{n}+o_{Q_{n}}(1)\).

**Problem 14.25**: Suppose \(X_{n}\) has distribution \(P_{n}\) or \(Q_{n}\) and \(T_{n}=T_{n}(X_{n})\) is sufficient. Let \(P_{n}^{T}\) and \(Q_{n}^{T}\) denote the distribution of \(T_{n}\) under \(P_{n}\) and \(Q_{n}\), respectively. Prove or disprove: \(Q_{n}\) is contiguous to \(P_{n}\) if and only if \(Q_{n}^{T}\) is contiguous to \(P_{n}^{T}\).

**Problem 14.26**: Suppose \(Q\) is absolutely continuous with respect to \(P\). If \(P\{E_{n}\}\to 0\), then \(Q\{E_{n}\}\to 0\).

**Problem 14.27**: Prove the convergence (14.40).

**Problem 14.28**: Show that \(\sigma_{1,2}\) in (14.52) reduces to \(h/\sqrt{\pi}\).

**Problem 14.29**: Verify (14.53) and evaluate it in the case where \(f(x)=\exp(-|x|)/2\) is the double exponential density.

**Problem 14.30**: Reconsider Example 14.3.11. Rather than finding the limiting distribution of \(W_{n}\) under contiguous alternatives, find the limiting distribution of \(U_{n}\) (properly normalized) under the same set of alternatives, where \(U_{n}\) is the \(U\)-statistic introduced in Example 12.3.6. First, find the projection of \(U_{n}\) under the null hypothesis, which represents \(U_{n}\) as an asymptotically linear statistic. Then, relating \(U_{n}\) and \(W_{n}\), check that your solution agrees with the solution in Example 14.3.11.

**Problem 14.31**: Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. according to a model which is q.m.d. at \(\theta_{0}\). For testing \(\theta=\theta_{0}\) versus \(\theta=\theta_{0}+hn^{-1/2}\), consider the test \(\psi_{n}\) that rejects \(H\) if \(\log(L_{n,h})\) exceeds \(z_{1-\alpha}\sigma_{h}-\frac{1}{2}\sigma_{h}^{2}\), where \(L_{n,h}\) is defined by (14.54) and \(\sigma_{h}^{2}=\langle h,I(\theta_{0})h\rangle\). Find the limiting value of \(E_{\theta_{0}+hn^{-1/2}}(\psi_{n})\).

**Problem 14.32**: Suppose \(P_{\theta}\) is the uniform distribution on \((0,\,\theta)\). Fix \(h\) and determine whether or not \(P_{1}^{n}\) and \(P_{1+h/n}^{n}\) are mutually contiguous. Consider both \(h>0\) and \(h<0\).

**Problem 14.33**: Generalize Corollary 14.3.2 in the following way. Suppose \(T_{n}=(T_{n,1},\,\ldots,\,T_{n,k})\in{\rm I\!R}^{k}\). Assume that, under \(P_{n}\),

\[(T_{n,1},\,\ldots,\,T_{n,k},\,\log(L_{n}))\stackrel{{ d}}{{ \rightarrow}}(T_{1},\,\ldots,\,T_{k},\,Z)\,\]

where \((T_{1},\,\ldots,\,T_{k},\,Z)\) is multivariate normal with \(Cov(T_{i},\,Z)=c_{i}\). Then, under \(Q_{n}\),

\[(T_{n,1},\,\ldots,\,T_{n,k})\stackrel{{ d}}{{\rightarrow}}(T_{1}+ c_{1},\,\ldots,\,T_{k}+c_{k})\.\]

**Problem 14.34**: Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. according to a model \(\{P_{\theta}:\theta\in\Omega\}\), where \(\Omega\) is an open subset of \({\rm I\!R}^{k}\). Assume that the model is q.m.d. Show that there cannot exist an estimator sequence \(T_{n}\) satisfying

\[\lim_{n\rightarrow\infty}\sup_{|\theta-\theta_{0}|\leq n^{-1/2}}P_{\theta}^{n }(n^{1/2}|T_{n}-\theta|>\epsilon)=0 \tag{14.97}\]

for every \(\epsilon>0\) and any \(\theta_{0}\). (Here \(P_{\theta}^{n}\) means the joint probability distribution of \((X_{1},\,\ldots,\,X_{n})\) under \(\theta\).) Suppose the above condition (14.97) only holds for some \(\epsilon>0\). Does the same conclusion hold?

**Problem 14.35**: Assume \(X_{i}\) are independent, normally distributed with \(E(X_{i})=\mu_{i}\). Let \(P_{n}\) be the distribution of \((X_{1},\,\ldots,\,X_{n})\) when \(\mu_{i}=0\) for all \(i\). Let \(Q_{n}\) be the distribution of \((X_{1},\,\ldots,\,X_{n})\) when the \(\mu_{i}\) are arbitrary constants. Find a necessary and sufficient condition on \(\mu_{1}\), \(\mu_{2}\),... so that \(P_{n}\) and \(Q_{n}\) are mutually contiguous.

### _Section 14.4_

**Problem 14.36**: In Example 14.4.1, show that the likelihood equations have a unique solution which corresponds to a global maximum of the likelihood function.

**Problem 14.37**: Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. \(P_{\theta}\) according to the lognormal model of Example 14.2.7. Write down the likelihood function and show that it is unbounded.

**Problem 14.38**: Generalize Example 14.4.2 to multiparameter exponential families.

**Problem 14.39**: Prove Corollary 14.4.1. _Hint_: Simply define \(\hat{\theta}_{n}=\theta_{0}+n^{-1/2}I^{-1}(\theta_{0})Z_{n}\) and apply Theorem 14.4.1.

**Problem 14.40**: Let \((X_{i},Y_{i}),i=1\ldots n\) be i.i.d. such that \(X_{i}\) and \(Y_{i}\) are independent and normally distributed, \(X_{i}\) has variance \(\sigma^{2}\), \(Y_{i}\) has variance \(\tau^{2}\) and both have common mean \(\mu\).

(i) If \(\sigma\) and \(\tau\) are known, determine an efficient likelihood estimator (ELE) \(\hat{\mu}\) of \(\mu\) and find the limit distribution of \(n^{1/2}(\hat{\mu}-\mu)\).

(ii) If \(\sigma\) and \(\tau\) are unknown, provide an estimator \(\tilde{\mu}\) for which \(n^{1/2}(\tilde{\mu}-\mu)\) has the same limit distribution as \(n^{1/2}(\hat{\mu}-\mu)\).

(iii) What can you infer from your results (i) and (ii) regarding the Information matrix \(I(\theta),\ \theta=(\mu,\sigma,\tau)\)?

**Problem 14.41**: Let \(X_{1},\ldots,X_{n}\) be a sample from a Cauchy location model with density \(f(x-\theta)\), where

\[f(z)=\frac{1}{\pi(1+z^{2})}.\]

Compare the limiting distribution of the sample median with that of an efficient likelihood estimator.

**Problem 14.42**: Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(N(\theta,\,\theta^{2})\). Compare the asymptotic distribution of \(\tilde{X}_{n}^{2}\) with that of an efficient likelihood estimator sequence.

**Problem 14.43**: Let \(X_{1},\cdots,X_{n}\) be i.i.d. with density

\[f(x,\theta)=[1+\theta\cos(x)]/2\pi,\]

where the parameter \(\theta\) satisfies \(|\theta|<1\) and \(x\) ranges between 0 and \(2\pi\). (The observations \(X_{i}\) may be interpreted as directional data. The case \(\theta=0\) corresponds to the uniform distribution on the circle.) Construct an efficient likelihood estimator of \(\theta\), as explicitly as possible.

**Problem 14.44**: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. with common density function

\[p_{\theta}(x)=\frac{\theta c^{\theta}}{x^{\theta+1}}\,\ 0<c<x,\ \theta>0\.\]

Here, \(c\) is fixed and known and \(\theta\) is unknown.

(i) Show that the maximum likelihood estimator \(\hat{\theta}_{n}\) is well-defined and determine the limiting distribution of \(\sqrt{n}(\hat{\theta}_{n}-\theta)\) under \(\theta\).

(ii) What is the score test for testing the null hypothesis \(\theta=\theta_{0}\) vs. \(\theta\neq\theta_{0}\)?

**Problem 14.45**: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d., uniformly distributed on [0, \(\theta\)]. Find the maximum likelihood estimator \(\hat{\theta}_{n}\) of \(\theta\). Determine a sequence \(\tau_{n}\) such that \(\tau_{n}(\hat{\theta}_{n}-\theta)\) has a limiting distribution, and determine the limit law.

**Problem 14.46**: Verify that \(\tilde{h}_{n}\) in (14.61) maximizes \(\tilde{L}_{n,h}\).

**Problem 14.47**: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\) with both parameters unknown. Consider testing \(\mu=0\) versus \(\mu\neq 0\). Find the likelihood ratio test statistic, and determine its limiting distribution under the null hypothesis. Calculate the limiting power of the test against the sequence of alternatives \((\mu,\sigma^{2})=(h_{1}n^{-1/2},\sigma^{2}+h_{2}n^{-1/2})\).

**Problem 14.48**: In Example 14.4.7, verify (14.89) and (14.90).

**Problem 14.49**: Suppose a time series \(X_{0},\,X_{1},\,X_{2},\,\ldots\) evolves in the following way. The process starts at 0, so \(X_{0}=0\). For any \(i\geq 1\), conditional on \(X_{0}\),..., \(X_{i-1},\,X_{i}=\rho X_{i-1}+\epsilon_{i}\), where the \(\epsilon_{i}\) are i.i.d. standard normal. You observe \(X_{0},\,X_{1},\,X_{2},\,\ldots,\,X_{n}\). For testing the null hypothesis \(\rho=0\) versus \(\rho>0\), determine both Wald and Rao score tests as well as appropriate critical values. (Compare with Problems 3.35 and 14.49).

**Problem 14.50**: Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. \(P_{\theta}\), with \(\theta\in\Omega\), an open subset of \(\mathbb{R}^{k}\). Assume the family is q.m.d. at \(\theta_{0}\) and consider testing the simple null hypothesis \(\theta=\theta_{0}\). Suppose \(\hat{\theta}_{n}\) is an estimator sequence satisfying (14.62), and consider the Wald test statistic \(n(\hat{\theta}_{n}-\theta_{0})^{\top}I(\theta_{0})(\hat{\theta}_{n}-\theta_{0})\). Find its limiting distribution against the sequence of alternatives \(\theta_{0}+hn^{-1/2}\), as well as an expression for its limiting power against such a sequence of alternatives.

**Problem 14.51**: Prove (14.77). Then, show that

\[[\Sigma^{(p)}(\theta)]^{-1}\leq[I^{(p)}(\theta)]\.\]

What is the statistical interpretation of this inequality?

**Problem 14.52**: In Example 14.4.5, consider the case of a composite null hypothesis with \(\Omega_{0}\) given by (14.80). Show that the null distribution of the likelihood ratio statistic given by (14.81) is \(\chi^{2}_{p}\). _Hint_: First consider the case \(a=0\), so that \(\Omega_{0}\) is a linear subspace of dimension \(k-p\). Let \(Z=\Sigma^{-1/2}X\), so

\[2\log(R_{n})=\inf_{\theta\in\Omega_{0}}|Z-\Sigma^{-1/2}\theta|^{2}\.\]

As \(\theta\) varies in \(\Omega_{0}\), \(\Sigma^{-1/2}\theta\) varies in a subspace \(L\) of dimension \(k-p\). If \(P\) is the projection matrix onto \(L\) and \(I\) is the identity matrix, then \(2\log(R_{n})=|(I-P)Z|^{2}\).

**Problem 14.53**: In Example 14.4.5, determine the distribution of the likelihood ratio statistic against an alternative, both for the simple and composite null hypotheses.

**Problem 14.54**: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\) with both parameters unknown. Consider testing the simple null hypothesis \((\mu,\sigma^{2})=(0,1)\). Find and compare the Wald test, Rao's Score test, and the likelihood ratio test.

**Problem 14.55**: Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. with the gamma \(\Gamma(g,b)\) density

\[f(x)=\frac{1}{\Gamma(g)b^{g}}x^{g-1}e^{-x/b}\ \ \ x>0\,\]

with both parameters unknown (and positive). Consider testing the null hypothesis that \(g=1\), i.e., under the null hypothesis the underlying density is exponential. Determine the likelihood ratio test statistic and find its limiting distribution.

**Problem 14.56**: Suppose \((X_{1},\,Y_{1})\),..., \((X_{n},\,Y_{n})\) are i.i.d., with \(X_{i}\) also independent of \(Y_{i}\). Further suppose \(X_{i}\) is normal with mean \(\mu_{1}\) and variance 1, and \(Y_{i}\) is normal with mean \(\mu_{2}\) and variance 1. It is known that \(\mu_{i}\geq 0\) for \(i=1,2\). The problem is to test the null hypothesis that at most one \(\mu_{i}\) is positive versus the alternative that both \(\mu_{1}\) and \(\mu_{2}\) are positive.

(i) Determine the likelihood ratio statistic for this problem.

(ii) In order to carry out the test, how would you choose the critical value (sequence) so that the size of the test is \(\alpha\)?

**Problem 14.57**: (i) In Example 14.4.6, check that the MLE is given by \(\hat{p}_{j}=Y_{j}/n\).

(ii) Show (14.83).

**Problem 14.58**: In Example 14.4.6, show that Rao's Score test is exactly Pearson's Chi-squared test.

**Problem 14.59**: In Example 14.4.6, show that \(2\log(R_{n})-Q_{n}\stackrel{{ p}}{{\to}}0\) under the null hypothesis.

**Problem 14.60**: Prove (14.87).

**Problem 14.61**: Provide the details of the proof to part (ii) of Theorem 14.4.2.

**Problem 14.62**: Prove (iii) of Theorem 14.4.2. _Hint_: If \(\theta_{0}\) satisfies the null hypothesis \(g(\theta_{0})=0\), then testing \(\Omega_{0}\) behaves asymptotically like testing the null hypothesis \(D(\theta_{0})(\theta-\theta_{0})=0\), which is a hypothesis of the form considered in part (ii) of the theorem.

**Problem 14.63**: The problem is to test independence in a contingency table. Specifically, suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d., where each \(X_{i}\) is cross-classified, so that \(X_{i}=(r,s)\) with probability \(p_{r,s}\), \(r=1,\,\ldots,\,R,\,s=1,\,\ldots,\,S\). Under the full model, the \(p_{r,s}\) vary freely, except they are nonnegative and sum to 1. Let \(p_{r}\). = \(\sum_{s}\,p_{r,s}\) and \(p_{\cdot s}=\sum_{r}\,p_{r,s}\). The null hypothesis asserts \(p_{r,s}=p_{r}\,p_{\cdot s}\) for all \(r\) and \(s\). Determine the likelihood ratio test and its limiting null distribution.

**Problem 14.64**: Consider the following model which therefore generalizes model (iii) of Section 4.7. A sample of \(n_{i}\) subjects is obtained from class \(A_{i}(i=1,\,\ldots,\,a)\), the samples from different classes being independent. If \(Y_{i,j}\) is the number of subjects from the \(i\)th sample belonging to \(B_{j}(j=1,\ldots,\,b)\), the joint distribution of \((Y_{i,1},\,\ldots,\,Y_{i,b})\) is multinomial, say,\[M(n_{i};\,p_{1|i},\ldots,\,p_{b|i})\.\]

Determine the likelihood ratio statistic for testing the hypothesis of _homogeneity_ that the vector \((p_{1|i},\ldots,\,p_{b|i})\) is independent of \(i\), and specify its asymptotic distribution.

**Problem 14.65**: The _hypothesis of symmetry_ in a square two-way contingency table arises when one of the responses \(A_{1},\ldots,\,A_{a}\) is observed for each of \(n\) subjects on two occasions (e.g., before and after some intervention). If \(Y_{i,j}\) is the number of subjects whose responses on the two occasions are \((A_{i},\,A_{j})\), the joint distribution of the \(Y_{i,j}\) is multinomial, with the probability of a subject response of \((A_{i},\,A_{j})\) denoted by \(p_{i,\,j}\). The hypothesis \(H\) of _symmetry_ states that \(p_{i,j}=p_{j,i}\) for all \(i\) and \(j\); that is, that the intervention has not changed the probabilities. Determine the likelihood ratio statistic for testing \(H\), and specify its asymptotic distribution. [Bowker (1948)].

**Problem 14.66**: In the situation of Problem 14.65, consider the _hypothesis of marginal homogeneity_\(H^{\prime}:p_{i+}=p_{+i}\) for all \(i\), where \(p_{i+}=\sum_{j=1}^{a}p_{iij},\ p_{i+}=\sum_{j=1}^{a}p_{jii}\).

1. The maximum-likelihood estimates of the \(p_{iij}\) under \(H^{\prime}\) are given by \(\hat{\bar{h}}_{j}=Y_{ij}/(1+\lambda_{i}-\lambda_{j})\), where the \(\lambda\)'s are the solutions of the equations \(\sum_{j}Y_{ij}/(1+\lambda_{i}-\lambda_{j})=\sum_{j}Y_{ij}/(1+\lambda_{j}- \lambda_{i})\). (These equations have no explicit solutions.)
2. Determine the number of degrees of freedom for the limiting \(\chi^{2}\)-distribution of the likelihood ratio criterion.

**Problem 14.67**: In Example 14.4.8, show (14.91).

**Problem 14.68**: Consider testing moment inequalities under the setting of Example 14.4.8. Rather than the likelihood ratio procedure discussed there, consider the following moment selection procedure. Let \(J=\{j:\,\sqrt{n}\bar{X}_{n,j}>-\log(n)\}\). Then, reject if the likelihood ratio statistic \(2\log(R_{n})>c_{|J|,1-\alpha}\), where \(|J|\) is the cardinality of \(J\).

(i) For any fixed \(\theta\in\Omega_{0}\), show that the probability of a Type 1 error under \(\theta\) is asymptotically no bigger than \(\alpha\). Does the size of the test tend to \(\alpha\)?

(ii) Show that this procedure asymptotically achieves the power in (14.92). Can you think of any criticism of the procedure?

(iii) Consider the test that rejects \(H_{0}\) when

\[M_{n}=\sqrt{n}\max_{1\leq i\leq k}(\bar{X}_{n,i})>d_{k,1-\alpha}\,\]

where \(d_{k,1-\alpha}\) is the distribution of \(\max_{1\leq i\leq k}Z_{i}\) when the \(Z_{i}\) are i.i.d. standard normal. Compute the limiting rejection probability under \(\theta=\theta_{0}+hn^{-1/2}\) for any \(\theta_{0}\) on the boundary of \(\Omega_{0}\) and any \(h\in\mathbb{R}^{k}\).

(iv) As in (ii) above, apply a moment selection procedure based on the test statistic \(M_{n}\), and repeat (iii) for the procedure. [Moment selection methods for testing moment inequalities are discussed in Andrews and Barwick (2012) and Romano, Shaikh and Wolf (2014). Special emphasis is placed on error control that is uniform in the underlying distribution.]

**Problem 14.69** Consider the third of the three sampling schemes for a \(2\times 2\times K\) table discussed in Section 4.8, and the two hypotheses

\[H_{1}:\Delta_{1}=\cdots=\Delta_{K}=1\ \ \ \mbox{and}\ \ \ H_{2}:\Delta_{1}=\cdots=\Delta_{K}.\]

(i) Obtain the likelihood ratio test statistic for testing \(H_{1}\).

(ii) Obtain equations that determine the maximum likelihood estimates of the parameters under \(H_{2}\). (These equations cannot be solved explicitly.)

(iii) Determine the number of degrees of freedom of the limiting \(\chi^{2}\)-distribution of the likelihood ratio test for testing (a) \(H_{1}\), (b) \(H_{2}\).

[For a discussion of these and related hypotheses, see for example Shaffer (1973), Plackett (1981), or Bishop, Fienberg, and Holland (1975), and the recent study by Liang and Self (1985).]

**Problem 14.70** Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\theta,1)\). Consider Hodges' superefficient estimator of \(\theta\) (unpublished, but cited in Le Cam (1953)), defined as follows. Let \(\hat{\theta}_{n}\) be \(0\) if \(|\tilde{X}_{n}|\leq n^{-1/4}\); otherwise, let \(\hat{\theta}_{n}=\tilde{X}_{n}\). For any fixed \(\theta\), determine the limiting distribution of \(n^{1/2}(\hat{\theta}_{n}-\theta)\). Next, determine the limiting distribution of \(n^{1/2}(\hat{\theta}_{n}-\theta_{n})\) under \(\theta_{n}=hn^{-1/2}\). Is \(\hat{\theta}_{n}\) regular?

**Problem 14.71** Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. random vectors in \({\bf R}^{k}\) having the multivariate normal distribution with unknown mean vector \(\mu\) and identity covariance matrix. Fix a constant \(c>0\) and consider the shrinkage estimator of \(\mu\) defined by

\[\hat{\mu}_{n}=\left(1-\frac{c}{n\|\tilde{X}_{n}\|^{2}}\right)\tilde{X}_{n}\,\]

where \(\tilde{X}_{n}\) is the sample mean vector and \(\|\cdot\|\) is Euclidean norm. Determine whether or not \(\hat{\mu}_{n}\) is regular at \(\mu=0\) by deriving the limiting distribution of \(\sqrt{n}(\hat{\mu}_{n}-\mu)\) under \(\mu_{n}=h/\sqrt{n}\).

**Problem 14.72** Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. according to a quadratic mean differentiable model \(\{P_{\theta},\,\theta\in\Omega\}\), where \(\Omega\) is an open subset of the real line. Suppose an estimator sequence \(\hat{\theta}_{n}\) is asymptotically linear in the sense that,

\[n^{1/2}(\hat{\theta}_{n}-\theta_{0})=n^{-1/2}\sum_{i=1}^{n}\psi_{\theta_{0}}(X _{i})+o_{P_{\theta_{0}}^{n}}(1)\]

where \(E_{\theta_{0}}[\psi_{\theta_{0}}(X_{i})]=0\) and \(\tau^{2}=Var_{\theta_{0}}[\psi_{\theta_{0}}(X_{i})]<\infty\).

(i) Find the joint limiting behavior of (\(n^{1/2}(\hat{\theta}_{n}-\theta_{0})\), \(Z_{n}\)) under \(\theta_{0}+hn^{-1/2}\), where \(Z_{n}\) is the normalized score statistic given by \[Z_{n}=n^{-1/2}\sum_{i=1}^{n}\tilde{\eta}(X_{i},\,\theta_{0})\]

and \(\tilde{\eta}(\cdot,\,\theta_{0})\) is the usual score function.

(ii) Find a simple if and only if condition for \(\hat{\theta}_{n}\) to be regular (at \(\theta_{0}\)). (Your answer should depend on something about the functions \(\psi_{\theta_{0}}(\cdot)\) and \(\tilde{\eta}(\cdot,\,\theta_{0})\).)

(iii) Find a simple if and only if condition for the statistic

\[D_{n}\equiv n^{1/2}(\hat{\theta}_{n}-\theta_{0})-I^{-1}(\theta_{0})Z_{n}\]

to be asymptotically ancillary at \(\theta_{0}\), in the sense that its limiting distribution under \(\theta_{0}+hn^{-1/2}\) does not depend on \(h\).

(iv) Find a simple if and only if condition for \(D_{n}\) and \(Z_{n}\) to be asymptotically independent.

(v) Under the additional assumption of regularity, find a simple if and only if condition for the statistic sequence \(D_{n}\) defined above to tend in probability under \(\theta_{0}\) to 0 (and hence \(\hat{\theta}_{n}\) is efficient under this condition).

**Problem 14.73**: Let \((X_{j,1},\,X_{j,2}),\,\,j=1,\,\ldots,\,n\) be independent pairs of independent exponentially distributed random variables with \(E(X_{j,1})=\theta\lambda_{j}\) and \(E(X_{j,2})=\lambda_{j}\). Here, \(\theta\) and the \(\lambda_{j}\) are all unknown. The problem is to test \(\theta=1\) against \(\theta>1\). Compare the Rao, Wald, and likelihood ratio tests for this problem. Without appealing to any general results, find the limiting distribution of your statistics, as well as the limiting power against suitable local alternatives. (Note: the number of parameters is increasing with \(n\) so you can't directly appeal to our previous large-sample results.)

### Notes

According to Le Cam and Yang (2000), the notion of quadratic mean differentiability was initiated in conversations between Hajek and Le Cam in 1962. Hajek (1962) appears to be the first publication making use of this notion. The importance of q.m.d. was prominent in the fundamental works of Le Cam (1969, 1970) and Hajek (1972), and has been used extensively ever since.

The notion of (mutual) contiguity is due to Le Cam (1960). Its usefulness was soon recognized by Hajek (1962), who first considered the one-sided version. Three of Le Cam's fundamental lemmas concerning contiguity became known as Le Cam's three lemmas, largely due to their prominence in Hajek and Sidak (1967). Further results can be found in Roussas (1972), Le Cam (1990), Chapter 6, Hajek et al. (1999), and Le Cam and Yang (2000), Chapter 3.

The methods studied in Section 14.4 are based on the notion of _likelihood_, whose general importance was recognized in Fisher (1922, 1925a, 1925b). Rigorous approaches were developed by Wald (1939, 1943) and Cramer (1943). Cramer defined the asymptotic efficiency of an asymptotically normal estimator to be the ratio of its asymptotic variance to the Fisher Information; that such a definition is flawed even for asymptotically normal estimators was made clear by Hodges superefficient estimator (Problem 14.70). Le Cam (1956) introduced the _one-step maximum likelihood estimator_, which is based on a discretization trick coupled with a Newton-Raphson approximation. Such estimators satisfy (14.62) under weak assumptions and enjoy other optimality properties; for example, see Section 7.3 of Millar (1983). The notion of a _regular_ estimator sequence introduced at the end of Section 14.4.1 plays an important role in the theory of efficient estimation and the Hajek-Inagaki Convolution Theorem; see Hajek (1970), Le Cam (1979), Beran (1999), Millar (1985), and van der Vaart (1988).

The asymptotic behavior of the likelihood ratio statistic was studied in Wilks (1938) and Chernoff (1954). Pearson's Chi-squared statistic was introduced in Pearson (1900) and the Rao score tests by Rao (1947). In fact, the Rao score test was actually introduced in the univariate case by Wald (1941b). One-sided score tests are studied in Silvapulle and Silvapulle (1995). In econometrics, score tests are more commonly known as Lagrange multiplier tests; see Silvey (1959) and Bera and Bilais (2001). The asymptotic equivalence of many of the classical tests is explored in Hall and Mathiason (1990). Methods based on integrated likelihoods are reviewed in Berger et al. (1999). Caveats about the finite-sample behavior of Rao and Wald tests are given in Le Cam (1990); also see Fears et al. (1996) and Pawitan (2000). The behavior of likelihood ratio tests under nonstandard conditions is studied in Vu and Zhou (1997). Extensions of likelihood methods to semiparametric and nonparametric models are developed in Murphy and van der Vaart (1997), Owen (1988, 2001), and Fan et al. (2001). Robust version of the Wald, likelihood, and score tests are given in Heritier and Ronchetti (1994).

## Chapter Large-Sample Optimality

### 15.1 Testing Sequences, Metrics, and Inequalities

In this chapter, some asymptotic optimality theory of hypothesis testing is developed. We consider testing one sequence of distributions against another (the asymptotic version of testing a simple hypothesis against a simple alternative). It turns out that this problem degenerates if the two sequences are too close together or too far apart. The non-degenerate situation can be characterized in terms of a suitable distance or metric between the distributions of the two sequences. Two such metrics, the total variation and the Hellinger metric, will be introduced below.

We begin by considering some of the basic metrics for probability distributions that are useful in statistics. Fundamental inequalities relating these metrics are developed, from which some large-sample implications can be derived. We now recall the definition of a metric space; also see Section A.2 in the appendix.

**Definition 15.1.1**: A set \(\mathcal{P}\) is a _metric space_ if there exists a real-valued function \(d\) defined on \(\mathcal{P}\times\mathcal{P}\) such that, for all points \(p\), \(q\), and \(r\) in \(\mathcal{P}\), \(d(p,q)\geq 0\), \(d(p,q)=d(q,p)\) and \(d(p,q)\leq d(p,r)+d(r,q)\). A function \(d\) satisfying these conditions is called a _metric_.

In the present context, \(\mathcal{P}\) will be a collection of probabilities on a (measurable) space \(\mathcal{X}\) (endowed with a \(\sigma\)-field). We have already encountered two metrics on the collection of probability distributions on \(\mathbbm{R}\). One is the Levy distance \(\rho_{L}(F,G)\), defined in Definition 11.2.3. The other, used in Example 11.4.2, is the Kolmogorov-Smirnov distance between distribution functions \(F\) and \(G\) on the real line, defined as

\[d_{K}(F,G)=\sup_{t}|F(t)-G(t)|. \tag{15.1}\]

It is easy to see that \(d_{K}\) is indeed a metric (Problem 11.22). In the context of hypothesis testing, two additional distances arise naturally, the total variation distance and the Hellinger distance.

[MISSING_PAGE_FAIL:718]

By symmetry, both expressions are the same, so summing the last two equations and then dividing by two yields

\[S_{P_{0},P_{1}}(\phi^{*}) = 1+\frac{1}{2}[\int_{D_{-}}[p_{0}(x)-p_{1}(x)]\mu(x)+\int_{D_{+}}[p_ {1}(x)-p_{0}(x)]\mu(dx)] \tag{15.8}\] \[=1-\frac{1}{2}\int_{\mathcal{X}}|p_{1}(x)-p_{0}(x)|\mu(dx)\.\qed \tag{15.9}\]

The integral appearing in the last expression leads us to the so-called _total variation_ distance between \(P_{0}\) and \(P_{1}\).

**Definition 15.1.2**: The _total variation distance_ between \(P_{0}\) and \(P_{1}\), denoted \(\|P_{1}-P_{0}\|_{1}\), is given by

\[\|P_{1}-P_{0}\|_{1}=\int|p_{1}-p_{0}|d\mu\, \tag{15.10}\]

where \(p_{i}\) is the density of \(P_{i}\) with respect to any measure \(\mu\) dominating both \(P_{0}\) and \(P_{1}\).

It is easy to see that this distance defines a metric (Problem 15.1) and that this distance is independent of the choice of dominating measure \(\mu\). For alternative characterizations of the total variation distance, see Problem 15.2. Equation (15.9) can be restated as

\[S_{P_{0},P_{1}}(\phi^{*})=1-\frac{1}{2}\|P_{1}-P_{0}\|_{1}. \tag{15.11}\]

If \(X_{1},\ldots,X_{n}\) are i.i.d. \(P\), let \(P^{n}\) denote their joint distribution. We will next consider a sequence of tests \(\phi_{n}\) for testing \(P_{n}^{n}\) against \(Q_{n}^{n}\). The minimum sum of error probabilities is then \(S(P_{n}^{n},Q_{n}^{n})\). The test (sequence) that minimizes the sum of error probabilities is connected with the more usual test in which probability of false rejection of \(P_{n}^{n}\) is fixed at \(\alpha\) by the following lemma. The proof is left as an exercise (Problem 15.5).

**Lemma 15.1.1**: _(i) If there exists a sequence of tests \(\phi_{n}\) for which the sum of error probabilities tends to 0, then given any fixed \(\alpha\) (\(0<\alpha<1\)) and \(n\) sufficiently large, the level of \(\phi_{n}\) will be less than \(\alpha\), and its power will tend to 1 as \(n\to\infty\). (ii) If for every sequence \(\{\phi_{n}\}\), the sum of the error probabilities tends to 1, then for any sequence whose rejection probability under \(P_{n}^{n}\) tends to \(\alpha\), the limiting power is \(\alpha\), and hence is no better than that of a test that rejects \(P_{n}^{n}\) with probability \(\alpha\) independent of the data._

We would like to determine conditions for which the limiting sum of error probabilities is zero or one, as well as for the more important intermediate situation. In order to determine the limiting behavior of \(S(P_{n}^{n},Q_{n}^{n})\), we need to study the behavior of \(\|P_{n}^{n}-Q_{n}^{n}\|_{1}\). Unfortunately, this quantity is often difficult to compute, but it is related to another distance which is easier to manage. This is the following Hellinger distance.

**Definition 15.1.3** Let \(P_{0}\) and \(P_{1}\) be probabilities on \((\mathcal{X},\,\mathcal{F})\). The _Hellinger distance_\(H(P_{0},\,P_{1})\) between \(P_{0}\) and \(P_{1}\) is given by

\[H^{2}(P_{0},\,P_{1})=\frac{1}{2}\int_{\mathcal{X}}[\sqrt{p_{1}(x)}-\sqrt{p_{0} (x)}]^{2}d\mu(x)\, \tag{15.12}\]

where \(p_{i}\) is the density of \(P_{i}\) with respect to any measure \(\mu\) dominating \(P_{0}\) and \(P_{1}\).

The value of \(H(P_{0},\,P_{1})\) is independent of the choice of \(\mu\) (Problem 15.1) and one can, for example, always use \(\mu=P_{0}+P_{1}\). It is also easy to see that this distance defines a metric.1 By squaring the integrand and using the fact that the densities \(p_{i}\) must integrate to one, it follows that

\[H^{2}(P_{0},\,P_{1})=1-\rho(P_{0},\,P_{1})\, \tag{15.13}\]

where \(\rho(P_{0},\,P_{1})\) is known as the _affinity_ between \(P_{0}\) and \(P_{1}\) and is given by

\[\rho(P_{0},\,P_{1})=\int_{\mathcal{X}}\sqrt{p_{0}(x)\,p_{1}(x)}d\mu(x). \tag{15.14}\]

Note that, by Cauchy-Schwarz, \(0\leq\rho(P_{0},\,P_{1})\leq 1\) and \(\rho(P_{0},\,P_{1})=1\) if and only if \(P_{0}=P_{1}\). Furthermore, \(\rho(P_{0},\,P_{1})=0\) if and only if \(P_{0}\) and \(P_{1}\) are _mutually singular_, i.e., there exists a (measurable) set \(E\) with \(P_{0}(E)=1\) and \(P_{1}(E)=0\). It follows, for example, that \(H(P_{0},\,P_{1})=0\) if and only if \(P_{0}=P_{1}\).

From Equation (15.14), it immediately follows that

\[\rho(P_{0}^{n},\,P_{1}^{n})=\rho^{n}(P_{0},\,P_{1}) \tag{15.15}\]

and hence

\[H^{2}(P_{0}^{n},\,P_{1}^{n})=1-\rho^{n}(P_{0},\,P_{1})=1-[1-H^{2}(P_{0},\,P_{1 })]^{n}. \tag{15.16}\]

Therefore, the behavior of \(H^{2}(P_{0}^{n},\,P_{1}^{n})\) with increasing \(n\) can be obtained from \(n\) and \(H(P_{0},\,P_{1})\) in a simple way.

Next, we will relate \(H(P_{0},\,P_{1})\) to \(\|P_{0}-P_{1}\|_{1}\), which was already seen to have a clear statistical interpretation.

**Theorem 15.1.2**: _The following relationships hold between Hellinger distance and total variation distance:_\[H^{2}(P_{0},\,P_{1})\leq\frac{1}{2}\|P_{0}-P_{1}\|_{1}\]

\[\leq H(P_{0},\,P_{1})[2-H^{2}(P_{0},\,P_{1})]^{1/2}=[1-\rho^{2}(P_{0},\,P_{1})]^ {1/2}\.\]

Proof. To prove the first inequality, note that

\[H^{2}(P_{0},\,P_{1})=\frac{1}{2}\int[\sqrt{p_{1}}-\sqrt{p_{0}}]^{2}d\mu\leq\frac {1}{2}\int|\sqrt{p_{1}}-\sqrt{p_{0}}|\cdot|\sqrt{p_{1}}+\sqrt{p_{0}}|d\mu\]

\[=\frac{1}{2}\int|p_{1}-p_{0}|d\mu=\frac{1}{2}\|P_{0}-P_{1}\|_{1}\.\]

To prove the second inequality, apply the Cauchy-Schwarz inequality to get

\[\frac{1}{2}\|P_{0}-P_{1}\|_{1}=\frac{1}{2}\int|\sqrt{p_{1}}-\sqrt{p_{0}}|\cdot| \sqrt{p_{1}}+\sqrt{p_{0}}|d\mu\]

\[\leq[\frac{1}{2}\int(\sqrt{p_{1}}-\sqrt{p_{0}})^{2}d\mu]^{1/2}[\frac{1}{2}\int (\sqrt{p_{1}}+\sqrt{p_{0}})^{2}d\mu]^{1/2}\]

\[=H(P_{0},\,P_{1})[\frac{1}{2}\int(\sqrt{p_{1}}+\sqrt{p_{0}})^{2}d\mu]^{1/2}\]

\[=H(P_{0},\,P_{1})[1+\rho(P_{0},\,P_{1})]^{1/2}=H(P_{0},\,P_{1})[2-H^{2}(P_{0},\, P_{1})]^{1/2}\,\]

with the last equality following from the definition \(H^{2}(P_{0},\,P_{1})=1-\rho(P_{0},\,P_{1});\) the last equality in the statement of the theorem follows immediately from this definition as well.

Consider now the problem of deciding between \(P_{0}^{n}\) and \(P_{1}^{n}\) based on \(n\) i.i.d. observations from \(P_{0}\) or \(P_{1}.\) Theorems 15.1.1 and 15.1.2 immediately yield the following result.

**Corollary 15.1.1**: _Fix any \(P_{0}\) and \(P_{1}\) with \(P_{0}\neq P_{1}.\) Then, \(S(P_{0}^{n},\,P_{1}^{n})\) tends to 0 exponentially fast; more specifically,_

\[S(P_{0}^{n},\,P_{1}^{n})\leq\rho^{n}(P_{0},\,P_{1})\to 0\ \ \ \ \ as\ \ n\to\infty. \tag{15.18}\]

Proof. By Theorem 15.1.2 and Equation (15.16),

\[\frac{1}{2}\|P_{0}^{n}-P_{1}^{n}\|_{1}\geq H^{2}(P_{0}^{n},\,P_{1}^{n})=1-\rho ^{n}(P_{0},\,P_{1}). \tag{15.19}\]

Hence, by Theorem 15.1.1 and (15.19),

[MISSING_PAGE_FAIL:722]

By Theorem 15.1.1 and then Theorem 15.1.2,

\[S(P^{n}_{\theta_{0}},\,P^{n}_{\theta_{n}})=1-\frac{1}{2}\|P^{n}_{\theta_{n}}-P^{n }_{\theta_{0}}\|_{1}\geq 1-[1-\rho^{2}(P^{n}_{\theta_{0}},\,P^{n}_{\theta_{n}})]^{1/2}\.\]

By (15.15), this becomes

\[1-[1-\rho^{2n}(P_{\theta_{0}},\,P_{\theta_{n}})]^{1/2}=1-\{1-[1-H^{2}(P_{\theta _{0}},\,P_{\theta_{n}})]^{2n}\}^{1/2}\to 1-[1-\exp(-2c)]^{1/2}\,\]

and the result follows.

Thus, from an asymptotic point of view, it is reasonable to consider alternatives \(\theta_{n}\) to \(\theta_{0}\) such that \(n\,H^{2}(\,P_{\theta_{0}},\,P_{\theta_{n}})\) is bounded away from \(0\) and \(\infty\). Otherwise, the problem is asymptotically degenerate in the sense that, either there exists a test sequence \(\phi_{n}\) for testing \(\theta_{0}\) versus \(\theta_{n}\) such that the probability of a Type 1 error tends to zero and the power at \(\theta_{n}\) tends to one, or no sequence of level-\(\alpha\) tests will have asymptotic power greater than \(\alpha\). We next consider what the condition on \(n\,H^{2}(\,P_{\theta_{0}},\,P_{\theta_{n}})\) becomes in some classical examples.

**Example 15.1.1** (**Quadratic Mean Differentiable Families**): Assume that \(\{P_{\theta},\,\theta\in\Omega\}\) is q.m.d. with derivative \(\eta(\cdot,\theta_{0})\) at \(\theta_{0}\) and positive definite \(I(\theta_{0})\). Suppose \(n^{1/2}(\theta_{n}-\theta_{0})\to h\). By Equation (14.6) and Lemma 14.2.2,

\[2n\,H^{2}(\,P_{\theta_{0}},\,P_{\theta_{n}})=n\int[\sqrt{p_{\theta_{n}}}-\sqrt {p_{\theta_{0}}}]^{2}d\mu\]

\[\rightarrow\int|(\eta(x,\theta_{0}),\,h)|^{2}d\mu(x)=\frac{1}{4}\langle h,\,I( \theta_{0})h\rangle\,<\infty. \tag{15.23}\]

Thus, the nondegenerate situation occurs when \(|\theta_{n}-\theta_{0}|=O(n^{-1/2})\). Note that the limiting value (15.23) is never \(0\) unless \(h=0\) (Problem 15.8).

**Example 15.1.2** (**Uniform Family; Example 14.2.8**, **continued**): Let \(P_{\theta}\) be the uniform distribution on \((0,\,\theta)\). Then, \(n\,H^{2}(\,P_{\theta_{0}},\,P_{\theta_{n}})\) tends to a finite, positive limit if and only if \(n(\theta_{n}-\theta_{0})\to h<\infty\) (Problem 15.4). Hence, alternatives \(\theta_{n}\) such that \(\theta_{n}-\theta_{0}\asymp n^{-1}\) cannot be perfectly discriminated, yet tests can be constructed that have reasonable power against these alternatives.

To clarify the difference between the previous two examples, note that in Example 15.1.1 we have

\[H^{2}(\,P_{\theta_{0}},\,P_{\theta_{n}})\asymp(\theta_{n}-\theta_{0})^{2}\]

while in Example 15.1.2 we have

\[H^{2}(\,P_{\theta_{0}},\,P_{\theta_{n}})\asymp|\theta_{n}-\theta_{0}|\.\]

[MISSING_PAGE_FAIL:724]

\(nH^{2}(P_{\theta_{0}+hn^{-1/2}},\,P_{\theta_{0}+h_{n}n^{-1/2}})\to 0\) (Problem 15.11). Therefore, by Theorem 15.1.4(i), \(P_{\theta_{0}+h_{n}n^{-1/2}}^{n}\) is contiguous to \(P_{\theta_{0}}^{n}\). This result forms the basis for generalizing results such as Theorem 14.2.3, Theorem 14.4.1, and Corollary 14.4.1, which have been shown to be true when \(h_{n}=h\), to the more general case when \(h_{n}\to h\); see Problems 15.12 and 15.13.

In the intermediate situation \(nH^{2}(P_{n},\,Q_{n})\asymp 1\), \(P_{n}^{n}\) and \(Q_{n}^{n}\) may or may not be contiguous. Example 15.1.1 provides an example where contiguity holds. However reconsider Example 15.1.2, where \(P_{n}\) is uniform on \([0,\,1]\) and \(Q_{n}\) is uniform on \([0,\,1+hn^{-1}]\), where \(h\,>0\). Then, \(nH^{2}(P_{n},\,Q_{n})\asymp 1\), but \(Q_{n}^{n}\) is not contiguous with respect to \(P_{n}^{n}\). To see why, let \(A_{n}\) be the event that the maximum of \(n\) i.i.d. observations exceeds \(1\). Then, \(P_{n}^{n}(A_{n})=0\), while \(Q_{n}^{n}(A_{n})\to 1-e^{-h}\). For a sharp result on the relationship between contiguity and Hellinger distance, see Oosterhoff and van Zwet (1979).

### Asymptotic Relative Efficiency

Consider the problem of testing \(H:\theta\in\Omega_{0}\) against \(\theta\notin\Omega_{0}\) when \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. according to a model \(\{P_{\theta},\,\,\theta\in\Omega\}\). Our main goal is to derive tests that are asymptotically optimal. However, other considerations (such as robustness) may suggest using non-optimal tests. It is then important to know how much is lost by the use of such sub-optimal tests. In this section, we shall therefore compare the performance of two test procedures \(\phi_{n}\) and \(\tilde{\phi}_{n}\). In this context, performance is measured in terms of power. Roughly speaking, the relative efficiency of \(\tilde{\phi}_{n}\) with respect to \(\phi_{n}\) is defined to be \(n/\bar{n}\), where \(n\) and \(\bar{n}\) are the sample sizes required for \(\phi_{n}\) and \(\tilde{\phi}_{\bar{n}}\) to have the same power at the same level against the same alternative. For instance, a ratio of \(2\) would indicate that \(\tilde{\phi}_{n}\) is twice as efficient as \(\phi_{n}\) because twice as many observations are required for \(\phi_{n}\) to have the same power at a given alternative as \(\tilde{\phi}_{n}\). Such a comparison can be based on the following result.

**Theorem 15.2.1**: _Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. according to a q.m.d. family indexed by a real parameter \(\theta\), and consider testing \(\theta=\theta_{0}\) versus \(\theta>\theta_{0}\). Assume the sequence \(\phi=\{\phi_{n}\}\) is based on test statistics \(T_{n}\) satisfying the following: there exists a function \(\mu(\cdot)\) and a number \(\sigma^{2}>0\) such that, under any sequence \(\theta_{n}\) satisfying \(n^{1/2}(\theta_{n}-\theta_{0})=O(1)\),_

\[n^{1/2}[T_{n}-\mu(\theta_{n})]\stackrel{{ d}}{{\to}}N(0,\sigma^{ 2})\,\,\,\,; \tag{15.24}\]

_moreover, \(\mu(\cdot)\) is assumed to have a right-hand derivative \(\mu^{\prime}(\theta_{0})>0\) at \(\theta_{0}\). Suppose \(\phi_{n}\) rejects when \(n^{1/2}[T_{n}-\mu(\theta_{0})]>\hat{c}_{n}\), where_

\[\hat{c}_{n}\to z_{1-\alpha}\sigma \tag{15.25}\]

_in probability under \(\theta_{0}\). Then, the following is true.__(i) \(E_{\theta_{n}}(\phi_{n})\to\alpha\) as \(n\to\infty\). (ii) The limiting power of \(\phi_{n}\) against \(\theta_{n}\) satisfying \(n^{1/2}(\theta_{n}-\theta_{0})\to h\) is_

\[\lim_{n}E_{\theta_{n}}(\phi_{n})=1-\Phi\left[z_{1-\alpha}-h\,\frac{\mu^{\prime }(\theta_{0})}{\sigma}\right]\,. \tag{15.26}\]

_(iii) Fix \(0<\alpha<\beta<1\). Let \(\theta_{k}\) be any sequence satisfying \(\theta_{k}>\theta_{0}\) and \(\theta_{k}\to\theta_{0}\) as \(k\to\infty\), and let \(n_{k}\) be any sequence for which \(E_{\theta_{k}}(\phi_{n_{k}})\to\beta\). Then,2_

Footnote 2: The notation \(a_{k}\sim b_{k}\) means \(a_{k}/b_{k}\to 1\).

\[n_{k}\sim\frac{(z_{1-\alpha}-z_{1-\beta})^{2}\sigma^{2}}{[(\theta_{k}-\theta_{ 0})\mu^{\prime}(\theta_{0})]^{2}}\,\,. \tag{15.27}\]

Proof. Part (i) follows by Slutsky's Theorem. To prove (ii), let \(\theta_{n}\) satisfy \(n^{1/2}(\theta_{n}-\theta_{0})\to h\). By contiguity (Example 15.1.4), it follows that \(\hat{c}_{n}\to z_{1-\alpha}\sigma\) in probability under \(\theta_{n}\). Also,

\[n^{1/2}[\mu(\theta_{n})-\mu(\theta_{0})]\to h\mu^{\prime}(\theta_{0})\,\,.\]

Letting \(Z\) denote a standard normal variable, by Slutsky's Theorem,

\[E_{\theta_{n}}(\phi_{n})=P_{\theta_{n}} \{n^{1/2}[T_{n}-\mu(\theta_{n})]>\hat{c}_{n}-n^{1/2}[\mu(\theta_{n })-\mu(\theta_{0})]\}\] \[\to P\{\sigma\,Z>z_{1-\alpha}\sigma-h\mu^{\prime}(\theta_{0})\}\,\,,\]

implying (ii).

To prove (iii), choose \(h=h_{\beta}\) so that the right side of (15.26) is \(\beta\), and hence

\[h_{\beta}=(z_{1-\alpha}-z_{1-\beta})\cdot\frac{\sigma}{\mu^{\prime}(\theta_{0} )}\,\,.\]

By (ii), if \(\theta_{n}\) satisfies \(n^{1/2}(\theta_{n}-\theta_{0})\to h_{\beta}\), then the limiting power of \(\phi_{n}\) against \(\theta_{n}\) is \(\beta\). It follows that the limiting power of \(\phi_{n}\) against \(\theta_{n}\) is \(\beta\) if and only if \(\theta_{n}\) satisfies

\[n\sim\frac{(z_{1-\alpha}-z_{1-\beta})^{2}\sigma^{2}}{[(\theta_{n}-\theta_{0}) \mu^{\prime}(\theta_{0})]^{2}}\,\,.\]

For an arbitrary sequence \(\theta_{k}\to\theta_{0}\), let \(m_{k}\) satisfy \(m_{k}^{1/2}(\theta_{k}-\theta_{0})\to h_{\beta}\). Then, since \(m_{k}^{1/2}(\theta_{k}-\theta_{0})=O(1)\), the asymptotic normality assumption for \(T_{m_{k}}\) holds, and the above argument shows the limiting power of \(\phi_{m_{k}}\) against \(\theta_{k}\) is \(\beta\) iff

\[m_{k}\sim\frac{(z_{1-\alpha}-z_{1-\beta})^{2}\sigma^{2}}{[(\theta_{k}-\theta_{ 0})\mu^{\prime}(\theta_{0})]^{2}}\,\,.\]To show \(n_{k}\sim m_{k}\), we first show that \(\lim\sup(n_{k}/m_{k})\geq 1\). But, the q.m.d. assumption precludes \(n_{k}\) being bounded (Problem 15.18), while the above argument shows the limiting power against \(n_{k}\) would be bounded above by \(\beta\) if \(n_{k}\to\infty\). So, it suffices to show \(\lim\inf(n_{k}/m_{k})\leq 1\). Fix \(\epsilon>0\) and let \(s_{k}\) satisfy

\[s_{k}^{1/2}(\theta_{k}-\theta_{0})\to(z_{1-\alpha}-z_{1-\beta})\cdot\frac{ \sigma}{\mu^{\prime}(\theta_{0})}+\epsilon\.\]

Note that \(s_{k}/m_{k}<1+C\epsilon\) for some \(C\). Then, the limiting power of \(\phi_{s_{k}}\) against \(\theta_{k}\) is, by the above argument, strictly greater than \(\beta\). Hence, for large enough \(n\), \(n_{k}\leq s_{k}\), and so

\[\lim\inf\frac{n_{k}}{m_{k}}\leq\lim\inf\frac{s_{k}}{m_{k}}\leq 1+C\epsilon\.\]

Since \(\epsilon\) was arbitrary, the result follows.

Inspection of (15.26) shows that, the larger the value \(\mu^{\prime}(\theta_{0})/\sigma\), the smaller is the sample size required to achieve a given power \(\beta\). A test sequence generated by \(T_{n}\) will therefore be more efficient the larger its value of \([\mu^{\prime}(\theta_{0})/\sigma]\). This value is called the _efficacy_ of the test sequence. Under some regularity conditions, Rao (1963) proved that

\[[\mu^{\prime}(\theta_{0})/\sigma(\theta_{0})]^{2}\leq I(\theta_{0})\,\]

where \(I(\theta_{0})\) is the usual Fisher Information. Such a result will follow from the results in Section 15.3 under the assumption of quadratic mean differentiability.

**Example 15.2.1**: (**Wald and Rao Tests**) Under the assumptions of Theorem 15.2.1, suppose \(\hat{\theta}_{n}\) satisfies (14.62), and consider the Wald test that rejects for large values of \(\hat{\theta}_{n}-\theta_{0}\). By Theorem 14.4.1, the assumptions of Theorem 15.2.1 hold with \(\mu(\theta)=\theta\) and \(\sigma^{2}=I^{-1}(\theta_{0})\). (The theorem establishes asymptotic normality under sequences \(\theta_{n}\) of the form \(\theta_{0}+hn^{-1/2}\), but it holds more generally for sequences \(\theta_{n}\) satisfying \(n^{1/2}(\theta_{n}-\theta_{0})=O(1)\), by Problem 15.14.) Hence, the squared efficacy of the Wald test is \(I(\theta_{0})\). The same is true for Rao's score test (Problem 15.19).

**Corollary 15.2.1**: _Assume the conditions of Theorem 15.2.1 hold for \(\phi=\{\phi_{n}\}\) and consider a competing test sequence \(\tilde{\phi}=\{\tilde{\phi}_{n}\}\) based on a test statistic \(\tilde{T}_{n}\) satisfying (15.24) with \(\mu\) and \(\sigma\) replaced by \(\tilde{\mu}\) and \(\tilde{\sigma}\). Fix \(0<\alpha<\beta<1\) and for \(\theta>\theta_{0}\), let \(N(\theta)\) and \(\tilde{N}(\theta)\) be the smallest sample sizes necessary for \(\phi\) and \(\tilde{\phi}\) to have power at least \(\beta\) against \(\theta\). Then,_

\[\lim_{\theta\downarrow\theta_{0}}\frac{N(\theta)}{\tilde{N}(\theta)}=\left[ \frac{\tilde{\mu}^{\prime}(\theta_{0})/\tilde{\sigma}}{\mu^{\prime}(\theta_{0 })/\sigma}\right]^{2}\, \tag{15.28}\]

_and the right-hand side is called the (Pitman) asymptotic relative efficiency (ARE) of \(\tilde{\phi}\) with respect to \(\phi\)._

Proof. Apply (iii) of Theorem 15.2.1.

Notice that the ARE is independent of \(\alpha\) and \(\beta\). Also, the tests are only required to be asymptotically level \(\alpha\), and the critical values may be random. Thus, we can, for example, compare tests based on an exact critical value, such as one obtained from the exact sampling distribution of \(T_{n}\) under \(\theta_{0}\), with tests based on asymptotic normality, possibly combined with an estimate of the asymptotic variance. Another possibility is to use a critical value obtained from a permutation distribution, such as the tests studied in Section 5.12. Nevertheless, under the assumptions stated, the resulting efficacy of a test is unchanged whether a test is based on an exact critical value or an approximate one. This implies the ARE is one when comparing two tests based on the same test statistic but with different critical values, as long as (15.25) is satisfied.

The ARE provides a single number for comparing two tests, independent of \(\alpha\) and \(\beta\). However, for finite samples, the relative efficiency depends on both \(\alpha\) and \(\beta\). Thus, the asymptotic measure may not give a very good picture of the actual finite-sample situation.

The following lemma facilitates the computation of the efficacy of a test sequence.

**Lemma 15.2.1**: _Assume \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. according to a family which is q.m.d. at \(\theta_{0}\) and that the unknown parameter \(\theta\) varies in an open subset of \(\rm I\!R\). Suppose, under \(\theta_{n}=\theta_{0}+h/n^{1/2}\), we have_

\[n^{1/2}T_{n}\stackrel{{ d}}{{\to}}\,N(hm,\sigma^{2})\.\]

_Then, the assumptions in Theorem 15.2.1 hold for \(T_{n}\) and the efficacy of \(T_{n}\) is \(m/\sigma\)._

Proof. Let \(\mu(\theta)=m(\theta-\theta_{0})\). The assumptions imply

\[n^{1/2}(T_{n}-\mu(\theta_{n}))\stackrel{{ d}}{{\to}}\,N(0,\sigma^ {2})\]

under \(\theta_{n}\) whenever \(\theta_{n}\) is of the form \(\theta_{n}=\theta_{0}+hn^{-1/2}\). By Problem 15.14, the same result holds whenever \(n^{1/2}(\theta_{n}-\theta_{0})=O(1)\), so that the asymptotic normality assumption holds for \(T_{n}\) with \(\mu^{\prime}(\theta)=m\). Thus, the efficacy of \(T_{n}\) is \(m/\sigma\).

**Example 15.2.2**: (**One-sample Tests of Location**) Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. according to a location model with density \(f(x-\theta)\), where \(f\) is assumed to be symmetric about 0. Assume \(f^{\prime}(x)\) exists for almost all \(x\), and the Fisher Information is positive and finite, so that the family is q.m.d. We would like to compare competing tests for testing \(\theta=0\) versus \(\theta>0\). Consider the three tests that reject for large values of \(t_{n}\), \(S_{n}\), and \(W_{n}\), the classical \(t\)-statistic \(t_{n}\), the sign test statistic \(S_{n}\), and the Wilcoxon signed-rank statistic \(W_{n}\) studied in Examples 14.3.9, 14.3.10, and 14.3.11, respectively. Regardless of whether or not \(f\) is known, all three tests can be used to yield tests that are pointwise consistent in level as long as \(f\) is symmetric and has finite variance. Let \(\sigma_{f}^{2}\) denote the variance of \(f\). Under \(\theta_{n}=h/n^{1/2}\), we have \[n^{1/2}t_{n}\stackrel{{ d}}{{\to}}N(\frac{h}{\sigma_{f}},1)\,\]

\[n^{1/2}S_{n}\stackrel{{ d}}{{\to}}N(hf(0),\,\frac{1}{4})\,\]

and

\[n^{1/2}W_{n}\stackrel{{ d}}{{\to}}N(2h\int_{-\infty}^{\infty}f^{2 }(x)dx,\,\frac{1}{3})\.\]

Thus, the efficacies of \(t\), \(S\), and \(W\) are \(1/\sigma\), \(2\,f(0)\), and \((12)^{1/2}\int f^{2}\), respectively. Therefore (with an obvious change of notation that shows the dependence on \(f\)),

\[e_{S,t}(f)=[2\,f(0)\sigma_{f}]^{2}\]

and

\[e_{W,t}(f)=12\sigma_{f}^{2}[\int f^{2}]^{2}\.\]

In particular, when \(f\) is the normal density \(\varphi\), \(e_{S,t}(\varphi)=2/\pi\,\approx 0.637\) and \(e_{W,t}(\varphi)=3/\pi\,\approx 0.955\). Thus, under normality, the sign test requires a sample size that is about 57% greater than the \(t\)-test to achieve the same power.

On the other hand, the efficiency loss for the Wilcoxon test is less than 5%. When \(f\) is not normal, the efficiency of both the sign test and the Wilcoxon test with respect to the \(t\)-test can be arbitrary large. To see this, modify \(\varphi\) by moving small masses out in the tails of the distribution so that \(\sigma_{f}\) becomes quite large but \(f(0)\) and \(\int f^{2}\) remain about the same. Moreover, the Wilcoxon test can never be much less efficient than the \(t\)-test, regardless of \(f\); in fact (Problem 15.22),

\[e_{W,t}(f)\geq 0.864\ \ \ \mbox{for all}\ \ f\.\]

Interestingly, when \(f\) is the double exponential density, the sign test is the most efficient of the three. In fact, it will be seen later in Section 15.3 that the sign test is asymptotically uniformly most powerful for testing the location parameter in a double exponential location model.

**Example 15.2.3**: **(Two-Sample Tests of Shift)** Suppose \(X_{1}\),..., \(X_{m}\) are i.i.d. with c.d.f. \(F\) and, independently, \(Y_{1}\),..., \(Y_{n}\) are i.i.d. with c.d.f. \(G\). Assume

\[G(x)=F(x-\theta)\]

for some \(\theta\). If \(F\) is unknown, such a nonparametric two-sample shift model was studied in Section 5.8, where the class of permutation tests was introduced. Consider the problem of testing \(\theta=0\) versus \(\theta>0\). We would like to compare the normal scores test and the Wilcoxon test \(W\) introduced in Section 6.9, as well as the two-sample \(t\)-test and the permutation \(t\)-test. It turns out that, even when \(F\) and \(G\) are normal with a common variance, the normal scores test and the Wilcoxon test are nearly as powerful as the \(t\)-test. To obtain a numerical comparison, suppose \(m=n\). Then, the notion of relative efficiency applies with no changes (by viewing the observations as pairs (\(X_{i}\), \(Y_{i}\))), and so the (Pitman) asymptotic relative efficiencies can be computed for test statistics satisfying the assumptions of Theorem 15.2.1.

In the particular case of the Wilcoxon test, \(e_{W,t}=3/\pi\) when \(F\) and \(G\) are normal with equal variance. Some numerical evidence supports the fact that the relative efficiency is nearly independent of \(\alpha\) and \(\beta\) in this context; see Lehmann (1998), p. 79. As in the one-sample case, the (Pitman) asymptotic relative efficiency is always \(\geq\)0.864, but may exceed 1 and can be infinite. The situation is even more favorable for the normal-scores test. Its asymptotic relative efficiency, relative to the \(t\)-test, is always \(\geq\)1 under the model (15.31); moreover, it is 1 only when \(F\) is normal. Thus, while the \(t\)-test is performance robust in the sense that its level and power is asymptotically independent of \(F\) as discussed in Section 13.2, the present results show that the efficiency and optimality properties of the \(t\)-test are quite nonrobust. The same comments apply to the permutation \(t\)-test whose asymptotic properties will be discussed in Section 17.2.

The above results do not depend on the assumption of equal sample sizes; they are also valid if \(m/n\asymp 1\). At least in the case that \(F\) is normal, the asymptotic results given by the (Pitman) efficiencies agree well with those found for small samples. The results also extend to testing the equality of \(s\) means, and the asymptotic relative efficiency of the Kruskal-Wallis test to the normal theory \(F\)-test is the same as the Wilcoxon to the \(t\)-test in the case \(s=2\). For a more detailed discussion of these and related efficiency results, see for example, Lehmann (1998), Randles and Wolfe (1979), Blair and Higgins (1980), and Groeneboom (1980).

The most ambitious goal in the nonparametric two-sample shift model would be to find a test which does not depend on \(F\), yet would have asymptotic efficiency at least 1 with respect to any other test, for all \(F\) (or at least all \(F\) in a nonparametric family). Such _adaptive_ tests (which achieve simultaneous optimality by adapting themselves to the unknown \(F\)) do in fact exist if \(F\) is sufficiently smooth. Their possibility was first suggested by Stein (1956b), and has been carried out for point estimation problems by Beran (1974), Stone (1975), and Bickel (1982).

We now briefly mention some other notions of asymptotic relative efficiency. Consider two test sequences \(\phi=\{\phi_{n}\}\) and \(\tilde{\phi}=\{\tilde{\phi}_{n}\}\), each indexed by the sample size \(n\). For simplicity, suppose \(\phi\) is determined by a test statistic \(T=\{T_{n}\}\) which rejects for large values. Then, \(\phi_{n}\) is really a family of tests indexed by \(n\) and \(\alpha\), where the value \(\alpha\) determines the size of the test. Define \(N(\alpha\), \(\beta\), \(\theta)\) to be the sample size necessary for the test \(\phi_{n}\) to have power \(\geq\beta\) against the fixed alternative \(\theta\), subject to the constraint that the size of \(\phi_{n}\) is \(\alpha\). Thus, \(N\) is the smallest sample size \(n\) such that, for some critical value \(c=c(n,\alpha)\), we have

\[\sup_{\phi_{0}\in\Omega_{0}}P_{\theta_{0}}\{T_{n}\asymp c\}\leq\alpha \tag{15.32}\]and

\[P_{\theta}\{T_{n}>c\}\geq\beta\ \.\]

Similarly, define \(\tilde{N}(\alpha,\,\beta,\theta)\) corresponding to a test \(\tilde{\phi}_{n}\) based on a test statistic \(\tilde{T}_{n}\). Then, the _relative efficiency_ of \(\tilde{\phi}\) with respect to \(\phi\) is defined to be

\[e_{\tilde{T},T}(\alpha,\,\beta,\theta)=N(\alpha,\,\beta,\theta)/\tilde{N}( \alpha,\,\beta,\theta)\.\]

While this measure has a useful statistical interpretation, its value depends on three arguments \(\alpha\), \(\beta\), and \(\theta\); moreover, it is typically quite difficult to compute \(N(\alpha,\,\beta,\theta)\) for a given test \(\phi\). However, it is often possible to calculate the limiting values of \(e_{\tilde{T},T}(\alpha,\,\beta,\theta)\) as \(\alpha\to 0\), \(\beta\to 1\), or \(\theta\to\theta_{0}\in\Omega_{0}\), with the remaining two arguments kept fixed. The case \(\alpha\to 0\) is known as the Bahadur efficiency, the case \(\beta\to 1\) as the Hodges-Lehmann efficiency, and the case \(\theta\to\theta_{0}\) coincides with the (Pitman) ARE already introduced. These various types of efficiency are reviewed in Serfling (1980, Chapter 10) and Nikitin (1995, Chapter 1). While each of these notions of asymptotic relative efficiency have some merit, we argue that the Pitman ARE has the most practical significance. In practice, \(\alpha\), though small, is regarded as fixed, and so comparisons based on the Bahadur efficiency with \(\alpha\to 0\) may be questionable. On the other hand, with \(\alpha\) fixed, comparing procedures with power tending to 1 seems inappropriate, since then the probability of an error of the second kind now becomes smaller than the probability of an error of the first kind. Typically, for values of the parameter at a fixed distance from \(\Omega_{0}\), any reasonable test will have power tending to one. It then becomes more important to choose a test that is better equipped to deal with the more difficult situation when \(\theta\) is near \(\Omega_{0}\), and the Pitman asymptotic relative efficiency provides a useful measure in this situation. Numerical evidence for the superiority of Pitman over Bahadur efficiency is provided in Groeneboom and Oosterhoff (1981).

### 15.3 AUMP Tests in Univariate Models

Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. \(P_{\theta}\), with \(\theta\) real-valued, and consider testing the hypothesis \(\theta=\theta_{0}\) against \(\theta>\theta_{0}\). As was discussed in Section 3.4, even in this one-parameter model, UMP tests rarely exist. In the present section we shall show that under weak smoothness assumptions, asymptotically optimal tests do exist.

As we saw in Section 15.1, when the q.m.d. assumption holds, informative power calculations for large samples are obtained not against fixed alternatives (for which the power tends to 1) but against sequences of alternatives of the form

\[\theta_{n,h}=\theta_{0}+hn^{-1/2}\ \ \ \ \ h>0\, \tag{15.33}\]

for which the power tends to a value strictly between \(\alpha\) and 1. Asymptotic optimality is most naturally studied in terms of these alternatives.

Let \(\{\alpha_{n}\}\) be a sequence of levels tending to \(\alpha\). By the Neyman-Pearson Lemma, the most powerful test \(\phi_{n,h}\) for testing \(\theta=\theta_{0}\) against \(\theta_{n,h}\) at level-\(\alpha_{n}\) rejects when

\[L_{n,h}=\prod_{i=1}^{n}[p_{\theta_{0}+hn^{-1/2}}(X_{i})/p_{\theta_{0}}(X_{i})]\]

is sufficiently large; more specifically, it is given by

\[\phi_{n,h}=\begin{cases}1&\text{if }\log(L_{n,h})>c_{n,h}\\ \gamma_{n,h}&\text{if }\log(L_{n,h})=c_{n,h}\\ 0&\text{if }\log(L_{n,h})<c_{n,h},\end{cases} \tag{15.34}\]

where the constants \(c_{n,h}\) and \(\gamma_{n,h}\) are determined so that \(E_{\theta_{0}}(\phi_{n,h})=\alpha_{n}\).

The limits of the critical values \(c_{n,h}\) and the power of the tests (15.34) against the alternatives (15.33) are given in the following lemma, under the assumption of quadratic mean differentiability.

**Lemma 15.3.1**: _Assume \(\{P_{\theta},\ \theta\in\Omega\}\) is q.m.d. at \(\theta_{0}\) with \(\Omega\) an open subset of \(\mathbbm{R}\). Consider testing \(\theta=\theta_{0}\) against \(\theta_{n,h}=\theta_{0}+hn^{-1/2}\) at level-\(\alpha_{n}\to\alpha\in(0,1)\). (i) As \(n\to\infty\), the critical values \(c_{n,h}\) of the most powerful test sequence \(\phi_{n,h}\) defined in (15.34) satisfy_

\[c_{n,h}\to\frac{-h^{2}I(\theta_{0})}{2}+hI^{1/2}(\theta_{0})z_{1-\alpha}\, \tag{15.35}\]

_where \(I(\theta_{0})\) is the Fisher Information at \(\theta_{0}\) and \(z_{1-\alpha}=\Phi^{-1}(1-\alpha)\) is the \(1-\alpha\) quantile of \(N(0,1)\). Moreover,_

\[P_{\theta_{0}}[\log(L_{n,h})>c_{n,h}\}\to\alpha \tag{15.36}\]

_and_

\[P_{\theta_{0}}[\log(L_{n,h})=c_{n,h}\}\to 0. \tag{15.37}\]

_(ii) The power of \(\phi_{n,h}\) satisfies_

\[E_{\theta_{0}+hn^{-1/2}}(\phi_{n,h})\to 1-\Phi[z_{1-\alpha}-hI^{1/2}(\theta_{0})]. \tag{15.38}\]

_(iii) More generally, consider testing \(\theta=\theta_{0}\) against \(\theta_{n,h_{n}}\) where \(h_{n}\to h\), with \(|h|<\infty\). Then, the power of \(\phi_{n,h_{n}}\) against \(\theta_{n,h_{n}}\) converges to the right side of (15.38), i.e., it has the same limiting power as \(\phi_{n,h}\)._

Proof. By Theorem 14.2.3, under \(\theta_{0},\log(L_{n,h})\) converges weakly to \(N(-\sigma_{h}^{2}/2,\sigma_{h}^{2})\), where \(\sigma_{h}^{2}=h^{2}I(\theta_{0})\). Then, (15.37) follows by Problem 11.47(i). Hence,

\[\alpha_{n}=E_{\theta_{0}}(\phi_{n,h})=P_{\theta_{0}}[\log(L_{n,h})>c_{n,h}]+o( 1)\,\]and so (15.36) follows. By Problem 11.47(ii), it follows that \(c_{n,h}\) tends to the \(1-\alpha\) quantile of \(N(-\sigma_{h}^{2}/2,\sigma_{h}^{2})\), and so (15.35) follows.

To prove (ii), under \(\theta_{n,h}\), \(\log(L_{n,h})\) converges in distribution to a variable \(Y_{h}\) distributed as \(N(\sigma_{h}^{2}/2,\sigma_{h}^{2})\), as shown in Example 14.3.12 by a contiguity argument. Hence, under \(\theta_{0}+hn^{-1/2}\), the probability that \(\log(L_{n,h})=c_{n,h}\) tends to 0, again by Problem 11.47(i). Letting \(Z\) denote a standard normal variable,

\[E_{\theta_{n,h}}(\phi_{n,h})=P_{\theta_{n,h}}[\log(L_{n,h})>c_{n,h}]+o(1)\]

\[\to P\{Y_{h}>\frac{-\sigma_{h}^{2}}{2}+\sigma_{h}z_{1-\alpha}\}=P\{Z>-\sigma_ {h}+z_{1-\alpha}\}=1-\Phi(z_{1-\alpha}-hI^{1/2}(\theta_{0})),\]

and (ii) follows.

The proof of (iii) is left to Problem 15.28.

Next, we consider the notion of an asymptotically most powerful test sequence for testing a simple hypothesis \(\theta=\theta_{0}\) against a simple alternative sequence \(\theta_{n}\).

**Definition 15.3.1**: For testing \(\theta=\theta_{0}\) against \(\theta=\theta_{n}\), \(\{\phi_{n}\}\) is _asymptotically most powerful_ (AMP) at (asymptotic) level-\(\alpha\) if \(\limsup_{n}E_{\theta_{0}}(\phi_{n})\leq\alpha\) and if for any other sequence of test functions \(\{\psi_{n}\}\) satisfying \(\limsup_{n}E_{\theta_{0}}(\psi_{n})\leq\alpha\),

\[\limsup_{n}E_{\theta_{n}}(\psi_{n})-E_{\theta_{n}}(\phi_{n})\leq 0. \tag{15.39}\]

For q.m.d. families, Lemma 15.3.1 implies the following result (Problem 15.29).

**Theorem 15.3.1**: _Assume \(\{P_{\theta},\ \theta\in\Omega\}\) is q.m.d. at \(\theta_{0}\) with \(\Omega\) an open subset of \({\rm I\kern-1.8ptR}\) and Fisher Information \(I(\theta_{0})\). Given \(X_{1},\ldots,X_{n}\) i.i.d. \(P_{\theta}\), consider testing \(\theta=\theta_{0}\) against \(\theta_{n}=\theta_{0}+h_{n}n^{-1/2}\), where \(h_{n}\to h>0\). Then, \(\phi_{n}=\phi_{n}(X_{1},\ldots,X_{n})\) is AMP level \(\alpha\) if and only if \(E_{\theta_{0}}(\phi_{n})\to\alpha\) and_

\[\limsup_{n}E_{\theta_{0}+h_{n}n^{-1/2}}(\phi_{n})=[1-\Phi(z_{1-\alpha}-hI^{1/2 }(\theta_{0}))]. \tag{15.40}\]

Of course, for testing a simple null hypothesis against a simple alternative, one always has available the optimal finite-sample Neyman-Pearson test sequence \(\phi_{n,h}\) given by (15.34). However, the tests \(\phi_{n,h}\) will typically depend on \(h\) and therefore will not be uniformly best against all alternatives. However, at this point, there is a profound difference between the finite sample and the asymptotic theory. Most powerful tests typically are unique while this is not true for asymptotically most powerful tests, since they can be changed on sets whose probability tends to zero without changing the asymptotic power. This difference opens up the possibility that among the set of AMP tests there may be one that is AMP simultaneously for all values of \(h\). This possibility will be explored in the remainder of this section.

[MISSING_PAGE_EMPTY:2336]

\[\sup_{0\leq h\leq c}\left|E_{\theta_{0}+hn^{-1/2}}(\tilde{\phi}_{n})-\{1-\Phi[z_{1- \alpha}-hI^{1/2}(\theta_{0})]\}\right|\to 0 \tag{15.46}\]

as \(n\to\infty\). For if not, there would exist a sequence \(h_{n}\in[0,c]\) for which

\[E_{\theta_{0}+h_{n}n^{-1/2}}(\tilde{\phi}_{n})-\{1-\Phi[z_{1-\alpha}-hI^{1/2}( \theta_{0})]\} \tag{15.47}\]

does not converge to 0. Then, there exists a subsequence \(h_{n_{j}}\) for which (15.47) converges along this subsequence to \(\delta\neq 0\). Take a further subsequence \(h_{n_{j_{k}}}\) which converges to a limit, say \(h\). But by (15.45), along every subsequence \(h_{n_{j_{k}}}\) which converges to \(h\), we have

\[E_{\theta_{0}+h_{n_{j_{k}}}n_{j_{k}}^{-1/2}}(\tilde{\phi}_{n_{j_{k}}})\to 1- \Phi[z_{1-\alpha}-hI^{1/2}(\theta_{0})]\,\]

which renders a contradiction. In summary, we have proved the following.

Lemma 15.3.2: _Under the assumptions of Lemma 15.3.1, let \(\tilde{\phi}_{n}\) be the test (15.43). Then, \(\tilde{\phi}_{n}\) is asymptotically level-\(\alpha\) and its limiting power against \(\theta_{0}+hn^{-1/2}\) converges to the optimal limiting power uniformly in \(h\in[0,c]\) for any \(c>0\); specifically, (15.46) holds._

Lemma 15.3.2 asserts an optimality property for \(\tilde{\phi}_{n}\). This notion of optimality is appropriate for q.m.d. families since the optimal limiting power against sequences of the form \(\theta_{0}+hn^{-1/2}\) is nondegenerate, i.e., strictly between \(\alpha\) and 1. Even for q.m.d. families, the conclusion of Lemma 15.3.2 does not imply uniform optimality against all alternative sequences with \(h\) unrestricted to all of \({\rm I\kern-1.8ptR}\). We would now like to define a general notion of asymptotically uniformly most powerful of a test sequence \(\phi_{n}\) satisfying \(\limsup E_{\theta_{0}}(\phi_{n})\leq\alpha\). A natural definition might be to require that, for any other test sequence \(\psi_{n}\) satisfying \(\limsup E_{\theta_{0}}(\psi_{n})\leq\alpha\), we have

\[\limsup_{n}[E_{\theta}(\psi_{n})-E_{\theta}(\phi_{n})]\leq 0\]

for all \(\theta\). This definition does not work because most tests are consistent, i.e., for any fixed \(\theta\), both \(E_{\theta}(\phi_{n})\) and \(E_{\theta}(\psi_{n})\) tend to one, and hence the difference will tend to zero. To avoid this difficulty, we will require \(\phi_{n}\) to behave well uniformly across \(\theta\), which implies that \(\phi_{n}\) must behave well against local alternatives \(\theta_{n}\) converging to \(\theta_{0}\) at an appropriate rate. Of course, under the q.m.d. assumption, it was seen in Section 15.1 and in Lemma 15.3.1 that the nondegenerate rate corresponds to \(\theta_{n}-\theta_{0}\asymp n^{-1/2}\).

Following Wald (1941a, 1941b) and Roussas (1972), we therefore define an asymptotically uniformly most powerful (AUMP) test sequence.

Definition 15.3.2: For testing \(\theta=\theta_{0}\) against \(\theta>\theta_{0}\), a sequence of tests \(\{\phi_{n}\}\) is called _asymptotically uniformly most powerful_ (AUMP) at (asymptotic) level-\(\alpha\) if \(\limsup_{n}E_{\theta_{0}}(\phi_{n})\leq\alpha\) and if for any other sequence of test functions \(\{\psi_{n}\}\) satisfying \(\limsup_{n}E_{\theta_{0}}(\psi_{n})\leq\alpha\),\[\limsup_{n}\ \ \sup\{E_{\theta}(\psi_{n})-E_{\theta}(\phi_{n}):\ \theta>\theta_{0}\}\leq 0\;. \tag{15.48}\]

Equivalently, \(\phi_{n}\) is AUMP level-\(\alpha\) if \(\limsup_{n}\ E_{\theta_{0}}(\phi_{n})\leq\alpha\) and \(\phi_{n}\) is AMP against _any_ sequence of alternatives \(\{\theta_{n}\}\) with \(\theta_{n}>0\) (Problem 15.30). Note that this definition is not restricted to q.m.d. families; it also easily generalizes further to problems with nuisance parameters; see (15.71). Also, note that the definition differs slightly from those of Wald and Roussas in that we allow tests that are not exactly level-\(\alpha\) for finite \(n\), as long as the \(\limsup\) of the size is bounded above by \(\alpha\). Of course, we will typically consider tests meeting the stronger requirement \(E_{\theta_{0}}(\phi_{n})\to\alpha\), but we prefer not to rule out a priori tests that do not satisfy this convergence.

A slightly weaker notion than Definition 15.3.2 is the following.

**Definition 15.3.3**: For testing \(\theta=\theta_{0}\) against \(\theta>\theta_{0}\), a sequence of tests \(\{\phi_{n}\}\) is called _locally asymptotically uniformly most powerful_ (LAUMP) at level-\(\alpha\) if \(\limsup_{n}\ E_{\theta_{0}}(\phi_{n})\leq\alpha\) and for any other sequence of test functions \(\{\psi_{n}\}\) satisfying \(\limsup_{n}\ E_{\theta_{0}}(\psi_{n})\leq\alpha\),

\[\limsup_{n}\ \ \sup\{E_{\theta}(\psi_{n})-E_{\theta}(\phi_{n}):\ 0<n^{1/2}( \theta-\theta_{0})\leq c\}\leq 0 \tag{15.49}\]

for any \(c>0\).

In (15.48), the sup over \(\{\theta:\theta>\theta_{0}\}\) can be reparametrized as the sup over \(\{h:\theta_{0}+hn^{-1/2}>0\}\). Hence, condition (15.48) can be rewritten as

\[\limsup_{n}\ \ \sup\{E_{\theta_{0}+hn^{-1/2}}(\psi_{n})-E_{\theta_{0}+hn^{-1/2} }(\phi_{n}):\ h>0\}\leq 0\]

and (15.49) can be rewritten as this same expression with the sup over \(h>0\) replaced by the sup over \(\{0<h\leq c\}\). In view of Lemma 15.3.1, under q.m.d., we can express the conditions for a test sequence \(\phi_{n}\) to be AUMP or LAUMP in terms of the limiting values of its power against local alternatives.

**Theorem 15.3.2**: _Consider testing \(\theta=\theta_{0}\) against \(\theta>\theta_{0}\) in a q.m.d. family with nonzero Fisher Information \(I(\theta_{0})\). If \(\phi_{n}=\phi_{n}(X_{1},\ldots,X_{n})\) is any sequence of tests based on \(n\) i.i.d. observations such that \(E_{\theta_{0}}(\phi_{n})\to\alpha\), then_

\[\limsup_{n}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq[1-\Phi(z_{1-\alpha}-hI^{1/2} (\theta_{0}))]. \tag{15.50}\]

_Moreover, \(\phi_{n}\) is AUMP at level-\(\alpha\) if and only if_

\[\sup_{h>0}|E_{\theta_{0}+hn^{-1/2}}(\phi_{n})-[1-\Phi(z_{1-\alpha}-hI^{1/2}( \theta_{0}))]|\to 0 \tag{15.51}\]

_and \(\phi_{n}\) is LAUMP if and only if, for every \(c>0\),_

\[\sup_{c\geq h>0}|E_{\theta_{0}+hn^{-1/2}}(\phi_{n})-[1-\Phi(z_{1-\alpha}-hI^{1 /2}(\theta_{0}))]|\to 0\;. \tag{15.52}\]Lemma 15.3.2 asserts that \(\tilde{\phi}_{n}\) defined by (15.43) is not only AMP, but LAUMP. We now obtain necessary and sufficient conditions for a test to be LAUMP, as well as a sufficient condition for a test to be AUMP. The results are summarized as follows.

**Theorem 15.3.3**: _Consider testing \(\theta=\theta_{0}\) against \(\theta>\theta_{0}\) in a q.m.d. family with nonzero Fisher Information I \((\theta_{0})\). Let \(\tilde{\phi}_{n}\) be the test defined by (15.43). (i). Then, \(\tilde{\phi}_{n}\) satisfies (15.52) and so is LAUMP at level-\(\alpha\). (ii). Any test sequence \(\phi_{n}\) satisfying, under \(\theta_{0}\),_

\[\phi_{n}-\tilde{\phi}_{n}\stackrel{{ p}}{{\to}}0 \tag{15.53}\]

_is also LAUMP at level-\(\alpha\). (iii). For \(\phi_{n}\) to be LAUMP at level-\(\alpha\), the condition (15.53) is also necessary. (iv). If, in addition, \(Z_{n}\to\infty\) in \(P^{n}_{\theta_{n}}\)-probability whenever \(n^{1/2}(\theta_{n}-\theta_{0})\to\infty\), then \(\tilde{\phi}_{n}\) is also AUMP at level-\(\alpha\)._

Proof. The proof of (i) follows from Lemma 15.3.2 and Theorem 15.3.2. To prove (ii), the condition (15.53) ensures the limiting size requirement. By contiguity, under \(\theta_{n,h_{n}}\), \(\phi_{n}-\tilde{\phi}_{n}\to 0\) in probability whenever \(h_{n}\leq c\). It follows that

\[E_{\theta_{0}+h_{n}n^{-1/2}}(\phi_{n})-E_{\theta_{0}+h_{n}n^{-1/2}}(\tilde{\phi }_{n})\to 0\]

whenever \(h_{n}\leq c\), which implies

\[\sup_{0\leq h\leq c}\left|E_{\theta_{0}+hn^{-1/2}}(\phi_{n})-E_{\theta_{0}+hn^ {-1/2}}(\tilde{\phi}_{n})\right|\to 0\,\]

and (ii) follows.

To prove (iii), fix \(h>0\) and consider the sequence of alternatives \(\theta_{n,h}\). Let \(\tilde{\phi}_{n}\) be the indicator of the event

\[L_{n,h}>k\equiv\exp(\frac{-\sigma_{h}^{2}}{2}+\sigma_{h}z_{1-\alpha})\,\]

where \(\sigma_{h}^{2}=h^{2}I(\theta_{0})\). Then, \(\tilde{\phi}_{n}\) is LAUMP level-\(\alpha\) by (ii) (from the asymptotic normality of \(\log(L_{n})\)). Suppose \(\phi_{n}^{*}\) is also LAUMP level-\(\alpha\). By Problem 15.31, \(E_{\theta_{0}}(\phi_{n}^{*})\to\alpha\). Then, letting \(p_{\theta}^{n}\) denote the joint density under \(\theta\) and letting \(\mu_{n}\) denote a measure dominating \(p_{\theta_{0}}^{n}\) and \(p_{\theta_{n,h}}^{n}\),

\[\int(\tilde{\phi}_{n}-\phi_{n}^{*})(p_{\theta_{n,h}}^{n}-kp_{\theta_{0}}^{n})d \mu_{n}\to 0\.\]

But the integrand in the above equation is always nonnegative. Hence, the integral over the set where \(\{p_{\theta_{0}}^{n}>0\}\) also tends to \(0\), so that

\[\int(\tilde{\phi}_{n}-\phi_{n}^{*})(L_{n,h}-k)p_{\theta_{0}}^{n}d\mu_{n}\to 0\.\]Since the integrand is nonnegative, it follows (by Markov's inequality) that for every \(\eta>0\), under \(\theta_{0}\),

\[P_{\theta_{0}}\{|\vec{\phi}_{n}-\phi_{n}^{*}|\cdot|L_{n,h}-k|>\eta\}\to 0. \tag{15.54}\]

We want to conclude that, for any \(\epsilon>0\),

\[P_{\theta_{0}}\{|\vec{\phi}_{n}-\phi_{n}^{*}|>\epsilon\}\to 0\.\]

But, for any \(\delta>0\),

\[P_{\theta_{0}}\{|\vec{\phi}_{n}-\phi_{n}^{*}|>\epsilon\}=P_{\theta_{0}}\{|\vec {\phi}_{n}-\phi_{n}^{*}|>\epsilon,\ |L_{n,h}-k|>\delta\}\]

\[+\ P_{\theta_{0}}\{|\vec{\phi}_{n}-\phi_{n}^{*}|>\epsilon,\ |L_{n,h}-k| \leq\delta\}. \tag{15.55}\]

As \(n\to\infty\), the last term tends to a limit \(c(\delta)\); moreover, \(c(\delta)\to 0\) as \(\delta\to 0\) since \(L_{n,h}\) has a continuous limiting distribution under \(\theta_{0}\). Thus, the last term in (15.55) can be made arbitrarily small if \(\delta\) is chosen small enough, whereas the first term is bounded above by

\[P_{\theta_{0}}\{|\vec{\phi}_{n}-\phi_{n}^{*}|\cdot|L_{n,h}-k|>\epsilon\delta\} \to 0\]

by (15.54) with \(\eta=\epsilon\delta\), and the result follows.

To prove (iv), if the result were false, there would exist a sequence \(\theta_{n}\) such that \(n^{1/2}(\theta_{n}-\theta_{0})\to\infty\) and \(E_{\theta_{n}}(\vec{\phi}_{n})\) does not converge to one. But,

\[E_{\theta_{n}}(\vec{\phi}_{n})=P_{\theta_{n}}\{Z_{n}>I^{1/2}(\theta_{0})z_{1- \alpha}\}\to 1\]

by the added assumption.

**Example 15.3.1** (**Location Models**): Suppose \(P_{\theta}\) has density with respect to Lebesgue measure on the real line given by \(f(x-\theta)\), for some fixed \(f\). Assume the conditions of Corollary 14.2.1 to ensure the family is q.m.d. so that \(f^{\prime}\) exists almost everywhere (with respect to Lebesgue measure),

\[I=I(\theta)=\int_{-\infty}^{\infty}\frac{[f^{\prime}(x)]^{2}}{f(x)}dx\]

is finite and positive, and the quadratic mean derivative is

\[\eta(x,\theta)=-\frac{1}{2}\frac{f^{\prime}(x-\theta)}{f^{1/2}(x-\theta)}\.\]

Then, the score statistic reduces to

\[Z_{n}=-n^{-1/2}\sum_{i=1}^{n}\frac{f^{\prime}(X_{i}-\theta_{0})}{f(X_{i}- \theta_{0})}\.\]The test (15.43) is LAUMP level-\(\alpha\). It is also AUMP level-\(\alpha\) if \(f\) is strongly unimodal (Problem 15.37); in this case, Example 1 of Section 8.2 shows that the test is also UMP if \(n=1\).

**Example 15.3.2**: (**Double Exponential Location Family**) As a special case of the previous example, let \(f(x)=\frac{1}{2}\exp(-|x|)\). Then, \(I(\theta)=1\). Without loss of generality, consider \(\theta_{0}=0\). Then,

\[Z_{n}=n^{-1/2}\sum_{i=1}^{n}\operatorname{sign}(X_{i})\,\]

where we take \(\operatorname{sign}(x)=1\) if \(x\geq 0\) and \(\operatorname{sign}(x)=-1\) otherwise. The resulting test which rejects when \(Z_{n}>z_{1-\alpha}\) is LAUMP at level-\(\alpha\). Moreover, this test is AUMP at level-\(\alpha\) as well. Although this follows from the previous example (since \(f\) is strongly unimodal), we give a direct proof. Note that

\[Var_{\theta}(Z_{n})=Var_{\theta}[\operatorname{sign}(X_{1})]\leq E_{\theta}[ \operatorname{sign}(X_{i})]^{2}\}=1\.\]

Hence, to show \(Z_{n}\to\infty\) in \(P_{\theta_{n}}^{n}\)-probability if \(n^{1/2}\theta_{n}\to\infty\), it is enough to show that \(E_{\theta_{n}}(Z_{n})\to\infty\) (by Chebyshev's inequality and the previous bound for \(Var_{\theta}(Z_{n})\); see Problem 15.32). Letting \(F\) denote the c.d.f. with density \(f\), we have

\[E_{\theta_{n}}(Z_{n})=2n^{1/2}[F(\theta_{n})-F(0)]=n^{1/2}[1-\exp(-\theta_{n})] \to\infty\,\]

and the result follows.

In the double exponential location model, a MLE is a sample median \(\hat{\theta}_{n}\); the test that rejects the null hypothesis if \(n^{1/2}\hat{\theta}_{n}>z_{1-\alpha}\) is also AUMP and is asymptotically equivalent to the test based on \(Z_{n}\) in the sense that the probability that both tests lead to the same conclusion tends to 1, both under the null hypothesis and against a sequence of contiguous alternatives (Problem 15.33).

The following example shows that, without strong unimodality, a LAUMP test need not be AUMP in the location model of Example 15.3.1.

**Example 15.3.3**: (**Cauchy Location Model**) Here, \(f(x)=[\pi(1+x^{2})]^{-1}\) and \(f^{\prime}(x)\)\(=-2x\pi^{-1}(1+x^{2})^{-2}\). Let \(\theta_{0}=0\). Then,

\[Z_{n}=2n^{-1/2}\sum_{i=1}^{n}\frac{X_{i}}{1+X_{i}^{2}}\.\]

By Theorem 15.3.3, since \(I(\theta)=1/2\), the Rao score test that rejects when \(Z_{n}\) exceeds \(z_{1-\alpha}/\sqrt{2}\) is LAUMP at level-\(\alpha\). However, this test is _not_ AUMP at level-\(\alpha\). To see why, first note that, for any large \(B>0\), \(P_{\theta}\{X_{i}>B\}\to 1\) as \(\theta\to\infty\), and so, with \(n\) fixed,

\[P_{\theta}\{\min(X_{1},\ldots,X_{n})>B\}\to 1\]as \(\theta\to\infty\). Since, \(x/(1+x^{2})\) is decreasing in \(x\) on the set \(\{x\geq 1\}\), this implies that, for any \(z>0\),

\[P_{\theta}\{Z_{n}>z\}\to 0\ \ \ as\ \ \theta\to\infty \tag{15.56}\]

and thus, there exists a sequence \(\theta_{n}\) such that

\[P_{\theta_{n}}\{Z_{n}>z\}\to 0\ \ \ \ as\ \ n\to\infty. \tag{15.57}\]

But, even the worst case power cannot be below \(\alpha\) for an AUMP test.

Thus, the score test based on \(Z_{n}\) cannot be AUMP. Next, compare the test based on \(Z_{n}\) with the test that rejects for large values of \(\tilde{X}_{n}\), the sample median. By Theorem 11.2.8, under \(P_{\theta}^{n}\),

\[n^{1/2}(\tilde{X}_{n}-\theta)\stackrel{{ d}}{{\to}}N\left(0,\, \frac{\pi^{2}}{4}\right)\.\]

Furthermore, since \(\tilde{X}_{n}\) is location equivariant, the distribution of \(n^{1/2}(\tilde{X}_{n}-\theta)\) under \(\theta\) does not depend on \(\theta\). Consider the asymptotically level-\(\alpha\) test that rejects when \(n^{1/2}\tilde{X}_{n}>\frac{\pi}{2}z_{1-\alpha}\). We have

\[\inf_{n^{1/2}\theta\geq c}P_{\theta}\left\{n^{1/2}\tilde{X}_{n}>\frac{\pi}{2}z _{1-\alpha}\right\}=\inf_{n^{1/2}\theta\geq c}P_{\theta}\left\{n^{1/2}(\tilde{ X}_{n}-\theta)>\frac{\pi}{2}z_{1-\alpha}-n^{1/2}\theta\right\}\]

\[=\inf_{n^{1/2}\theta\geq c}P_{0}\left\{n^{1/2}\tilde{X}_{n}>\frac{\pi}{2}z_{1- \alpha}-n^{1/2}\theta\right\}=P_{0}\left\{n^{1/2}\tilde{X}_{n}>\frac{\pi}{2}z_ {1-\alpha}-c\right\}\,\]

which, as \(n\to\infty\), tends to

\[1-\Phi\left(z_{1-\alpha}-\frac{2c}{\pi}\right)>\alpha>0\.\]

Note, however, the test based on \(\tilde{X}_{n}\) is neither LAUMP nor AUMP, though its power tends to one uniformly over \(\{\theta:\theta>\delta\}\) for any \(\delta>0\).

However, AUMP tests do exist in the present situation. One such test is the Wald test based on an efficient likelihood estimator. Actually, all that is required is a location equivariant estimator \(\hat{\theta}_{n}\) which satisfies

\[n^{1/2}(\hat{\theta}_{n}-\theta)\stackrel{{ d}}{{\to}}N(0,\,I^{- 1}(\theta))\, \tag{15.58}\]

where in this case \(I^{-1}(\theta)=2\). Indeed, the above argument with \(\hat{\theta}_{n}\) replacing \(\tilde{X}_{n}\) applies with the asymptotic variance of \(\tilde{X}_{n}\) of \(\pi^{2}/4\) replaced by \(2\).

As mentioned in Section 14.4.1, a difficulty in constructing an efficient likelihood estimator is due to the fact that the likelihood equation may have multiple roots. In order to deal with this situation, let \(\ell_{n}(\theta)=\log(L_{n}(\theta))\). Define\[\hat{\theta}_{n}=\tilde{X}_{n}+\frac{\ell^{\prime}_{n}(\tilde{X}_{n})}{nI(\tilde{ X}_{n})}. \tag{15.59}\]

The construction is based on the fact that the nearest root to a consistent estimator is efficient (under regularity conditions which hold for this model). Instead of determining the closest root exactly, which involves solving \(\ell^{\prime}_{n}(\theta)=0\), a linear approximation to \(\ell^{\prime}_{n}(\theta)\) (expanded about \(\tilde{X}_{n}\)) is used; see Section 6.4 of Lehmann and Casella (1998).

By Corollary 4.4 in Section 6.4 of Lehmann and Casella (1998), \(\hat{\theta}_{n}\) satisfies (15.58). The test that rejects when \(n^{1/2}\hat{\theta}_{n}>2^{1/2}z_{1-\alpha}\) therefore is AUMP (Problem 15.34).

**Example 15.3.4** (**Wald Tests**) As Example 15.3.3 shows, a AUMP test can be based on an efficient estimator, resulting in the Wald tests introduced in Section 14.4.2. Actually, this holds more generally. Assume the conditions of Theorem 15.3.3. Suppose \(\hat{\theta}_{n}\) satisfies (14.62). For testing \(\theta=\theta_{0}\) versus \(\theta>\theta_{0}\), the test \(\phi_{n}\) that rejects when \(n^{1/2}(\hat{\theta}_{n}-\theta_{0})>z_{1-\alpha}I^{-1/2}(\theta_{0})\) is LAUMP level-\(\alpha\). Indeed, the expansion (14.62) implies that \(\phi_{n}-\phi_{n}\to 0\) in probability under \(\theta_{0}\), so that \(\phi_{n}\) is LAUMP by (ii) of Theorem 15.3.3. To show \(\phi_{n}\) is AUMP as well, it is enough to show \(n^{1/2}(\hat{\theta}_{n}-\theta_{0})\to\infty\) under \(\theta_{n}\) whenever \(n^{1/2}(\theta_{n}-\theta_{0})\to\infty\); the argument is similar to (iv) of Theorem 15.3.3. This last condition holds in any location model if \(\hat{\theta}_{n}\) is location equivariant (Problem 15.35).

**Example 15.3.5** (**Correlation Coefficient**) Let \(X_{i}=(U_{i},\,V_{i})\) be i.i.d. bivariate normal with zero means, unit variances, and unknown correlation \(\rho\). For testing \(\rho=0\) versus \(\rho>0\), we saw in Example 14.4.4 that Rao's score test rejects for large values of

\[Z_{n}=n^{-1/2}\sum_{i=1}^{n}U_{i}V_{i}\.\]

By Theorem 15.3.3, this test is LAUMP. To show it is also AUMP, we must show \(Z_{n}\to\infty\) in probability under \(\rho_{n}\) whenever \(n^{1/2}\rho_{n}\to\infty\). Now,

\[E_{\rho_{n}}(Z_{n})=n^{1/2}\rho_{n}\to\infty\]

and

\[Var_{\rho_{n}}(Z_{n})=Var_{\rho_{n}}(U_{1}V_{1})\leq E_{\rho_{n}}(U_{1}^{2}V_ {1}^{2})=E_{\rho_{n}}[V_{1}^{2}E_{\rho_{n}}(U_{1}^{2}|V_{1})]\.\]

But, the conditional distribution of \(U_{1}\) given \(V_{1}\) is \(N(\rho_{n}V_{1},\,1-\rho_{n}^{2})\) and so

\[E_{\rho_{n}}(U_{1}^{2}|V_{1})=\rho_{n}^{2}V_{1}^{2}+(1-\rho_{n}^{2})\leq V_{1 }^{2}+1\.\]

Hence,

\[Var_{\rho_{n}}(Z_{n})\leq E_{\rho_{n}}(V_{1}^{4}+V_{1}^{2})\leq 4\.\]

The result now follows by Chebyshev's inequality; see Problem 15.32.

It is important to recognize that no asymptotic method, efficient or not, can perform well in all situations. Some anomalies with the Wald test are discussed in Vaeth (1985), Mantel (1987), Le Cam (1990), Benichou et al. (1996), and Pawitan (2000). We also remark that for two-sided hypotheses, AUMP tests, or even LAUMP tests, typically do not exist (Problem 15.40), but an asymptotic approach based on asymptotic unbiasedness is fruitful (Problem 15.57).

When \(\theta=(\theta_{1},\ldots,\theta_{k})\), it is natural to next consider one-sided tests of \(\theta_{1}\) in the presence of nuisance parameters \(\theta_{2}\),..., \(\theta_{k}\). One approach to finding an upper bound for the limiting power of a test sequence is to fix the nuisance parameters and apply the results of this section. The resulting bounds need not be attainable by any method. A more general approach that leads to bounds which are attainable is discussed in Section 15.5.

### Asymptotically Normal Experiments

In the previous section, a fairly direct approach was taken to compute the best limiting power of a sequence of tests. Since the problem there was reduced to testing a simple hypothesis versus a simple alternative, an optimal test could be derived via the Neyman-Pearson Lemma for finite sample sizes, which resulted in a calculation of the optimal limiting power. Implicit in the calculation was the fact that the likelihood ratios behave approximately like those in a normal location model. More explicitly, given \(n\) i.i.d. observations from a q.m.d. family \(\{P_{\theta}\}\), when testing \(\theta=\theta_{0}\) versus \(\theta=\theta_{0}+hn^{-1/2}\), the optimal test rejects for large values of the likelihood ratio \(L_{n,h}\). By Theorem 14.2.3, \(L_{n,h}\) satisfies

\[\log(L_{n,h})-[hZ_{n}-\frac{1}{2}h^{2}I(\theta_{0})]=o_{P_{\theta_{0}}^{n}}(1)\, \tag{15.60}\]

where \(Z_{n}\) is the score vector

\[Z_{n}=2n^{-1/2}\sum_{i=1}^{n}\eta(X_{i},\theta_{0})/p_{\theta_{0}}^{1/2}(X_{i})\]

and \(\eta(\cdot,\theta_{0})\) is the quadratic mean derivative at \(\theta_{0}\). By contiguity, the left side of this expression tends to 0 in probability under \(P_{\theta_{0}+hn^{-1/2}}^{n}\) as well. The asymptotic power calculations flow from these results.

An alternative (and more general) approach is based upon a deeper connection between the expansion (15.60) and the exact likelihood ratios for a particular normal location model. Specifically, consider the normal location model where you observe an observation \(X\) from the normal location family \(\{Q_{h},\ h\in{\rm I\kern-1.8ptR}\}\), where \(Q_{h}\) is the normal distribution with unknown mean \(h\) and known variance \(I^{-1}(\theta_{0})\). Let \(L_{h}\) denote the likelihood ratio \(d\,Q_{h}/d\,Q_{0}(X)\). Then,\[\log(L_{h})=hZ-\frac{1}{2}h^{2}I(\theta_{0})\, \tag{15.61}\]

where \(Z=I(\theta_{0})X\). Hence, the loglikelihood \(\log(L_{n,h})\) given by (15.60) behaves similarly to \(\log(L_{h})\); the former is approximately quadratic in \(h\), it is linear in \(Z_{n}\), the coefficient of \(h^{2}\) is nonrandom, and \(Z_{n}\) is asymptotically normal \(N(0,I(\theta_{0}))\). These approximations are exact for the normal experiment with \(Z_{n}\) replaced by \(Z\). In a certain sense, the experiments \(\{P_{\theta_{0}+hn^{-1/2}}^{n},\ h\in\mathbb{R}\}\) and \(\{Q_{h},h\in\mathbb{R}\}\) are close to each other. Le Cam (1964) formalized the notion of experiments being close, and he showed some profound consequences.3 For our purposes, we would like to show that, corresponding to any test \(\phi_{n}\) based on \(X_{1},\ldots,X_{n}\) from \(\{P_{\theta+hn^{-1/2}}^{n}\}\), there exists a test \(\phi\) for the normal location problem such that the power functions are approximately the same, as functions of the local parameter \(h\). Then, since an optimality result is available for the normal location model (like a UMP test in the one-sided testing problem), this will directly lead to an upper bound for what is achievable asymptotically in terms of power for the testing problem based on \(n\) observations from \(\{P_{\theta}\}\).

Footnote 3: The term _experiment_ rather than _model_ was used by Le Cam, but the terms are essentially synonymous. While a model postulates a family of probability distributions from which data can be observed, an experiment additionally specifies the exact amount of data (or sample size) that is observed. Thus, if \(\{P_{\theta},\ \theta\in\mathbb{R}\}\) is the family of normal distributions \(N(\theta,1)\) which serves as a model for some data, the experiment \(\{P_{\theta},\ \theta\in\mathbb{R}\}\) implicitly means one observation is observed from \(N(\theta,1)\); if an experiment consists of \(n\) observations from \(N(\theta,1)\), then this is denoted by \(\{P_{\theta}^{n},\theta\in\mathbb{R}\}\).

Consider the approximating normal experiment consisting of observing one observation \(X\) from \(N(h,I^{-1}(\theta_{0}))\), for which \(\theta_{0}\) is viewed as fixed. If \(Z=I(\theta_{0})X\), then \(Z\) is an observation from \(\tilde{Q}_{h}\), where \(\tilde{Q}_{h}=N(hI(\theta_{0}),I(\theta_{0}))\). Clearly, the Information contained in \(X\) is the same as that of \(Z\). Thus, we could equally well view the two experiments \(\{N(h,I^{-1}(\theta_{0})),\,h\in\mathbb{R}\}\) or \(\{N(I(\theta_{0})h,I(\theta_{0}))\}\) as limiting approximations to the experiment \(\{P_{\theta_{0}+hn^{-1/2}}^{n},\,h\in\mathbb{R}\}\). The former representation consisting of observing \(X\) from \(N(h,I^{-1}(\theta_{0}))\) seems more natural since the unknown parameter \(h\) refers to the mean of \(X\). On the other hand, the experiment of observing \(Z\) from \(N(I(\theta_{0})h,I(\theta_{0}))\) directly matches \(Z_{n}\) in (15.60). The point is that either experiment applies since they are equivalent.

This approach works, not only for one-parameter problems with no nuisance parameters, but also for more general testing problems where the hypothesis concerns a real-valued parameter in the presence of nuisance parameters, and multiparameter problems. For this purpose, we first give the definition of an asymptotically normal sequence of experiments. Consider a sequence of statistical models \(\{Q_{n,h},\,h\in\mathbb{R}^{k}\}\). (This can be easily generalized to the case where \(h\) is only defined for a subset \(\Omega_{n}\) of \(\mathbb{R}^{k}\) which can vary with \(n\).) Thus, for a given \(n\), there is available data on the (measure) space \((\mathcal{X}_{n},\,\mathcal{F}_{n})\) where the probability distributions \(Q_{n,h}\) live.

**Definition 15.4.1**: For a sequence of experiments \(\{Q_{n,h},\,h\in\mathbb{R}^{k}\}\), let \(L_{n,h}\) denote the likelihood ratio of \(Q_{n,h}\) with respect to \(Q_{n,0}\), defined by (14.36). Suppose there exists a sequence of random \(k\)-vectors \(Z_{n}\) mapping \({\cal X}_{n}\) to \({\rm I\!R}^{k}\) and a \(k\times k\) positive definite symmetric matrix \(C\) such that

\[\log(L_{n,h})=\langle h,Z_{n}\rangle-\frac{1}{2}\langle h,Ch\rangle+o_{Q_{n,0}}(1) \tag{15.62}\]

and \(Z_{n}\stackrel{{ d}}{{\to}}N(0,C)\) under \(Q_{n,0}\). Then, the sequence \(\{Q_{n,h},\,h\in{\rm I\!R}^{k}\}\) is called _asymptotically normal_.

If \(\{Q_{h}\}\) denotes \(N(Ch,C)\), the \(k\)-variate normal distribution with mean vector \(Ch\) and covariance matrix \(C\), then we also say that \(\{Q_{n,h},\,h\in{\rm I\!R}^{k}\}\) converges to the limiting experiment \(\{Q_{h}\}\). The terminology may be confusing, since \(Q_{n,h}\) is not asymptotically normal (and, in fact, \(Q_{n,h}\) typically has a distribution on a space that varies with \(n\)); it is the loglikelihood ratios from the experiment that are asymptotically normal. In particular, note that if \(L(h)\) denotes the likelihood of an observation \(Z\) from \(Q_{h}\), then

\[\log(L(h)/L(0))=\langle h,Z\rangle-\frac{1}{2}\langle h,Ch\rangle\ ;\]

that is, the right side of (15.62) without the error term is exact for the (multivariate) normal location model.

**Example 15.4.1**: **(Quadratic Mean Differentiable Families)** Suppose the family \(\{P_{\theta},\,\,\,\theta\in\Omega\}\) is q.m.d. at \(\theta_{0}\). Let \(Q_{n,h}=P_{\theta_{0}+hn^{-1/2}}^{n}\) and \(C=I(\theta_{0})\). By Theorem 14.2.3, \(\{Q_{n,h}\}\) is asymptotically normal with covariance \(C\) and \(Z_{n}\) the score vector as defined in (14.59). Because we are now parametrizing by the local parameter \(h\), we sometimes speak of \(\{P_{\theta_{0}+hn^{-1/2}}^{n}\}\) as being _locally_ asymptotically normal at \(\theta_{0}\), and the terms asymptotically normal and locally asymptotically normal are used interchangeably.

The random vector (sequence) \(Z_{n}\) defined by (15.62) is called the score vector. Note, however, that any \(\bar{Z}_{n}\) for which \(Z_{n}-\bar{Z}_{n}\to 0\) in probability under \(Q_{n,0}\) also satisfies (15.62).

**Example 15.4.2**: **(Two-Sample Problems)** Suppose that \(X_{1},\ldots,X_{m}\) are i.i.d. according to \(P_{\theta}\), \(\theta\in\Omega\), where \(\Omega\) is an open subset of \({\rm I\!R}^{k}\). Independently of the \(X^{\prime}s\), suppose \(Y_{1},\ldots,Y_{n}\) are i.i.d. according to \(\bar{P}_{\theta}\), \(\theta\in\Omega\). Suppose both families are q.m.d. at \(\theta_{0}\). Thus, \(\{P_{\theta_{0}+hn^{-1/2}}^{m},\,h\in{\rm I\!R}^{k}\}\) and \(\{\bar{P}_{\theta_{0}+hn^{-1/2}}^{n},\,h\in{\rm I\!R}^{k}\}\) are each asymptotically normal with corresponding \(Z_{m}\) and \(\bar{Z}_{n}\) satisfying as \(m,n\to\infty\), \(Z_{m}\stackrel{{ d}}{{\to}}N(0,I(\theta_{0}))\) and \(\bar{Z}_{n}\stackrel{{ d}}{{\to}}N(0,\,\bar{I}(\theta_{0}))\) under \(\theta_{0}\). Let \(L_{m,h}\) be the likelihood ratio \(dP_{\theta_{0}+hn^{-1/2}}^{m}/dP_{\theta_{0}}^{m}\) based on \(X_{1},\ldots,X_{m}\), and let \(\bar{L}_{n,h}\) be the corresponding likelihood ratio based on \(Y_{1},\ldots,Y_{n}\). Then, for the combined experiment (and noting \(hn^{-1/2}=hm^{-1/2}(m/n)^{1/2}\)),

[MISSING_PAGE_EMPTY:2347]

By Prohorov's Theorem 11.3.5, given any subsequence \(\{n_{j}\}\), there exists a further subsequence \(\{n_{j_{m}}\}\) such that

\[(\phi_{n_{j_{m}}},\,Z_{n_{j_{m}}})\stackrel{{ d}}{{\to}}(\bar{\phi},\,\bar{Z})\]

under \(Q_{n_{j_{m}},0}\), where \(\bar{Z}\) denotes a random variable with distribution \(N(0,C)\) (independent of \(h\)) and \(\bar{\phi}\in[0,1]\). Let \(L_{n,h}\) denote the likelihood ratio of \(Q_{n,h}\) with respect to \(Q_{n,0}\). Then, by (15.62), under \(Q_{n_{j_{m}},0}\),

\[(\phi_{n_{j_{m}}},\,L_{n_{j_{m}},h})\stackrel{{ d}}{{\to}}{\cal L }(\bar{\phi},\,\exp(\langle h,\,\bar{Z}\rangle-\frac{1}{2}\langle h,\,Ch\rangle) )\.\]

If \(F(\cdot,\cdot)\) denotes this limit law, then under \(Q_{n_{j_{m}},h}\), we have by Theorem 14.3.3, \((\phi_{n_{j_{m}}},\,L_{n_{j_{m}},h})\) converges to a limit law with density \(rdF(t,r)\). But since \(\phi_{n}\in[0,1]\), weak convergence implies convergence of moments, so that

\[\int\phi_{n_{j_{m}}}d\,Q_{n_{j_{m}},h}\to\int\int trdF(t,r)=E[\bar{\phi}\exp( \langle h,\,\bar{Z}\rangle-\frac{1}{2}\langle h,\,Ch\rangle)]. \tag{15.64}\]

Define \(\phi(\bar{Z})=E(\bar{\phi}|\bar{Z})\), i.e., the conditional expectation under the (fixed) joint distribution of \((\bar{\phi},\,\bar{Z})\). Then, the right side of (15.64) is equal to

\[E[\phi(\bar{Z})\exp(\langle h,\,\bar{Z}\rangle-\frac{1}{2}\langle h,\,Ch \rangle)]\]

\[=\int\phi(\bar{z})\exp(\langle h,\,\bar{z}\rangle-\frac{1}{2}\langle h,\,Ch \rangle)dN(0,C)(\bar{z})\.\]

But, \(\exp(\langle h,\,\bar{z}\rangle-\frac{1}{2}\langle h,\,Ch\rangle)dN(0,C)(\bar{ z})\) is actually the density of \(N(Ch,\,C)\) (Problem 15.45). Hence, if the experiment consists of observing \(Z\sim N(Ch,\,C)\), then the last expression is

\[E_{h}[\phi(Z)]=\int\phi(z)dN(Ch,\,C)(z)\.\]

Theorem 15.4.1 suggests the following strategy for obtaining asymptotically optimal tests in a variety of situations. First, an optimal test, say a UMP test, is derived (or quoted from an earlier chapter) and its power computed from an appropriate normal experiment. Second, the actual experiment sequence is shown (or known) to converge to the normal limiting experiment; as a result, the power of the normal model serves as an upper bound for the asymptotic power of the actual sequence. Finally, a test sequence is constructed whose asymptotic power attains the upper bound and which is therefore asymptotically optimal. A similar strategy will apply to constructing tests that are asymptotically maximin. The remainder of this section will illustrate this approach.

### Applications to Parametric Models

#### One-Sided Hypotheses

We now apply Theorem 15.4.1 to the following situation. Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. \(P_{\theta}\), where \(\theta\) varies in \(\Omega\), an open subset of \(\hbox{\rm I\kern-1.8ptR}^{k}\). Assume the family is q.m.d. with positive definite Information matrix \(I(\theta)\).

First suppose \(\theta=(\theta_{1},\ldots,\theta_{k})\) and consider testing \(\theta_{1}\leq 0\) against \(\theta_{1}>0\) in the presence of nuisance parameters \(\theta_{2}\),..., \(\theta_{k}\). Fix \(\theta_{0}=(\theta_{0,1},\ldots,\theta_{0,k})\) with \(\theta_{0,1}=0\). We now derive an upper bound for the limiting power of a test \(\phi_{n}\) satisfying, for \(h_{1}\leq 0\),

\[\limsup_{n\to\infty}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq\alpha. \tag{15.65}\]

By Theorem 15.4.1, we can approximate the power of \(\phi_{n}\) by the power of \(\phi=\phi(X)\), where \(X\sim N(h,\,I^{-1}(\theta_{0}))\). But then (15.65) implies

\[E_{h}\phi(X)\leq\alpha\quad\hbox{if }h_{1}\leq 0\,\]

i.e., \(\phi(X)\) is a level-\(\alpha\) test for testing \(h_{1}\leq 0\) against \(h_{1}>0\) in the limit experiment. But, by Example 3.9.2, a UMP level-\(\alpha\) test exists for this problem and has power \(1-\Phi(z_{1-\alpha}-h_{1}[I^{-1}(\theta_{0})_{1,1}]^{-1/2})\). By Theorem 15.4.1 with \(h_{1}>0\),

\[\limsup_{n}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq 1-\Phi(z_{1-\alpha}-h_{1}[I^ {-1}(\theta_{0})_{1,1}]^{-1/2})\.\]

More generally, let \(g\) be a function from \(\Omega\) to \(\hbox{\rm I\kern-1.8ptR}\), and assume \(g\) is differentiable with gradient vector \(\dot{g}(\theta)\) of dimension \(1\,\times\,k\). The problem is to test \(g(\theta)\,\leq 0\) against \(g(\theta)>0\). Suppose \(\phi_{n}\) is a test based on \(X_{1}\),..., \(X_{n}\) whose limiting size is \(\tilde{\alpha}\leq\alpha\) (see Definition 11.1.2). Fix \(\theta_{0}\) such that \(g(\theta_{0})=0\). We will derive an upper bound for the limiting power of \(\phi_{n}\) under \(\theta_{0}+hn^{-1/2}\) and then obtain tests for which this limiting power is attained. First, note that

\[g(\theta_{0}+hn^{-1/2})=n^{-1/2}\langle\dot{g}(\theta_{0})^{\top},h\rangle+o( n^{-1/2})\.\]

If \(h\) is such that \(\langle\dot{g}(\theta_{0})^{\top},h\rangle<0\), then \(g(\theta_{0}+hn^{-1/2})<0\) for all sufficiently large \(n\). The assumption on the limiting level of \(\phi_{n}\) implies that, for such an \(h\),

\[\limsup_{n\to\infty}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq\alpha. \tag{15.66}\]

Now, according to Theorem 15.4.1, we can approximate the power of a test sequence \(\phi_{n}\) by the power of a test \(\phi=\phi(X)\) for the experiment based on \(X\) from the model \(N(h,\,I^{-1}(\theta_{0}))\). Let \(\beta_{\phi}(h)\) denote the power of \(\phi(X)\) when \(X\sim N(h,\,I^{-1}(\theta_{0}))\). Then, (15.66) implies that \(\beta_{\phi}(h)\leq\alpha\) if \(\langle\dot{g}(\theta_{0})^{\top},h\rangle<0\). Since \(\beta_{\phi}(\cdot)\) is continuous, it follows that \(\beta_{\phi}(h)\leq\alpha\) if \(\langle\dot{g}(\theta_{0})^{\top},h\rangle\leq 0\). Now, fix an alternative \(h_{1}\) with \(\langle\dot{g}(\theta_{0})^{\top},h_{1}\rangle>0\). Theorem 15.4.1 implies that

\[\limsup_{n\to\infty}E_{\theta_{0}+h_{1}n^{-1/2}}(\phi_{n})\leq\sup_{\phi\in A_{ \alpha}}\beta_{\phi}(h_{1})\, \tag{15.67}\]

where \(A_{\alpha}=\{\phi:\ \beta_{\phi}(h)\leq\alpha\) whenever \(\langle\dot{g}(\theta_{0})^{\top},h\rangle\leq 0\}\). But then, the right side of (15.67) is maximized when \(\phi\) is the most powerful level-\(\alpha\) test for testing \(\langle\dot{g}(\theta_{0})^{\top},h\rangle\leq 0\) against \(h=h_{1}\). In fact, for the problem of testing \(\langle\dot{g}(\theta_{0})^{\top},h\rangle\leq 0\) versus \(\langle\dot{g}(\theta_{0})^{\top},h\rangle>0\), there exists a uniformly most powerful test based on \(X\) which rejects for large values of \(\langle\dot{g}(\theta_{0})^{\top},X\rangle\); see Section 3.9.2. But,

\[\langle\dot{g}(\theta_{0})^{\top},X\rangle\sim N(\langle\dot{g}(\theta_{0})^{ \top},h\rangle,\sigma_{\theta_{0}}^{2})\,\]

where

\[\sigma_{\theta_{0}}^{2}=\dot{g}(\theta_{0})I^{-1}(\theta_{0})\dot{g}(\theta_{0 })^{\top}\.\]

Hence, for testing \(\langle\dot{g}(\theta_{0})^{\top},h\rangle\leq 0\) at level-\(\alpha\), the UMP test rejects when \(\langle\dot{g}(\theta_{0})^{\top},X\rangle>z_{1-\alpha}\sigma_{\theta_{0}}\). The power of this test against \(h\) is then

\[1-\Phi(z_{1-\alpha}-\sigma_{\theta_{0}}^{-1}\langle\dot{g}(\theta_{0})^{\top}, h\rangle)\.\]

Therefore, Theorem 15.4.1 implies that, for any \(h\) such that \(\langle\dot{g}(\theta_{0})^{\top},h\rangle\),

\[\limsup_{n}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq 1-\Phi(z_{1-\alpha}-\sigma_{ \theta_{0}}^{-1}\langle\dot{g}(\theta_{0})^{\top},h\rangle). \tag{15.68}\]

An asymptotically level-\(\alpha\) test sequence whose power attains the bound for any \(h\) (and hence uniformly for \(h\) in a compact set) is LAUMP. The above development is summarized in (i) of the following theorem. Part (ii) asserts that an optimal test sequence may be constructed if an efficient estimator sequence is available.

**Theorem 15.5.1**: _Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. according to \(P_{\theta}\), \(\theta\in\Omega\), where \(\Omega\) is assumed to be an open subset of \(\mathbb{R}^{k}\). Let \(\Omega_{0}\) denote the set of \(\theta\) with \(g(\theta)\leq 0\), for some function \(g\) from \(\mathbb{R}^{k}\) to \(\mathbb{R}\) which is assumed differentiable with gradient \(\dot{g}(\theta)\). Consider testing the null hypothesis \(\theta\in\Omega_{0}\) versus \(g(\theta)>0\). Assume the family \(\{P_{\theta},\theta\in\Omega\}\) is q.m.d. at every \(\theta\) for which \(g(\theta)=0\) with nonsingular Fisher Information matrix \(I(\theta)\). (i) Let \(\phi_{n}=\phi_{n}(X_{1},\ldots,X_{n})\) be a uniformly asymptotically level-\(\alpha\) sequence of tests, so that_

\[\limsup_{n\to\infty}\sup_{\Omega_{0}}E_{\theta}(\phi_{n})\leq\alpha\, \tag{15.69}\]

_and suppose that \(g(\theta_{0})=0\). Then, for any \(h\) such that \(\langle\dot{g}(\theta_{0})^{\top},h\rangle>0\), (15.68) holds. (ii) Let \(\hat{\theta}_{n}\) be any estimator satisfying (14.62) (such as an efficient likelihood estimator). Suppose \(I(\theta)\) is continuous in \(\theta\) and \(\dot{g}(\theta)\) is continuous at \(\theta_{0}\). Then, the test sequence \(\phi_{n}\) that rejects when \(n^{1/2}g(\hat{\theta}_{n})\geq z_{1-\alpha}\hat{\sigma}_{n}\), where_

\[\hat{\sigma}_{n}^{\,2}=\dot{g}(\hat{\theta}_{n})I^{-1}(\hat{\theta}_{n})\dot{g} (\hat{\theta}_{n})^{\top}\;,\]

_is pointwise asymptotically level-\(\alpha\). Moreover, the inequality (15.68) becomes an equality, and the limsup on the left side of (15.68) may be replaced by a lim._

Proof. The proof of (i) follows from the discussion preceding the theorem (applying that argument to subsequences for which limits exist). The proof of (ii) follows from Theorem 14.4.1 and the discussion in Section 14.4.2.

In fact, the properties claimed in (ii) above hold more generally for any test sequence that rejects if \(T_{n}>t_{n}\), if \(T_{n}\) satisfies

\[T_{n}=\dot{g}(\theta_{0})I^{-1}(\theta_{0})Z_{n,\theta_{0}}+o_{P_{\theta_{0}}^{ n}}(1)\]

for every \(\theta_{0}\in\Omega_{0}\), where \(Z_{n,\theta_{0}}\) is the score vector defined in (14.59), and if

\[t_{n}\stackrel{{ P}}{{\to}}z_{1-\alpha}\sigma_{\theta_{0}}\]

under \(\theta_{0}\), where \(\sigma_{\theta_{0}}\) is given by (14.66).

**Example 15.5.1**: **(One-sample Normal Model)** Let \(X_{1},\ldots,X_{n}\) be i.i.d. normal with mean \(\mu\) and variance \(\sigma^{\,2}\) so that \(\theta=(\mu,\,\sigma^{\,2})\). Consider testing \(\mu\leq 0\) versus \(\mu>0\). Of course, the usual \(t\)-test is UMPU and UMPI. Theorem 15.5.1 applies immediately to the test \(\phi_{n}\) that rejects when \(n^{1/2}\tilde{X}_{n}/S_{n}\) exceeds \(z_{1-\alpha}\). Therefore, for any \(\sigma\),

\[\lim_{n}E_{h_{1}n^{-1/2},\sigma+h_{2}n^{-1/2}}(\phi_{n})=1-\Phi(z_{1-\alpha}- h_{1}\sigma^{-1})\;, \tag{15.70}\]

and so \(\phi_{n}\) is LAUMP. Equation (15.70) also holds for the \(t\)-test, i.e., when the normal critical value \(z_{1-\alpha}\) is replaced by the corresponding critical value obtained from the \(t\)-distribution with \(n-1\) degrees of freedom, which gives an asymptotic optimality property for the \(t\)-test that does not depend on the restriction to unbiased or invariant tests. In fact, we now show \(\phi_{n}\) is AUMP. Specifically, in the case where there is a nuisance parameter \(\sigma\), it is natural to define a test \(\phi_{n}\) to be AUMP level-\(\alpha\) if \(\phi_{n}\) is uniformly asymptotically level \(\alpha\) and for any other uniformly asymptotically level \(\alpha\) test \(\psi\), we have

\[\limsup_{n}\sup\{E_{\mu,\sigma}(\psi_{n})-E_{\mu,\sigma}(\phi_{n}):\;\mu>0, \sigma>0\}\leq 0\;. \tag{15.71}\]

(Obviously, we would modify this definition if the nuisance parameter \(\sigma\) varied in a parameter space different from the positive reals.) To see that \(\phi_{n}\) possesses this property, argue as follows. If it did not, there would exist \(\mu_{n}>0\) and \(\sigma_{n}>0\) such that \[\lim\sup_{n}\{E_{\mu_{n},\sigma_{n}}(\psi_{n})-E_{\mu_{n},\sigma_{n}}(\phi_{n})\}>0\;.\]

With \(\sigma_{n}\) now fixed, let \(\tilde{\psi}_{n}\) be the UMP test for testing \(\mu\leq 0\) versus \(\mu>0\) if \(\sigma=\sigma_{n}\) is known. Since \(\tilde{\psi}_{n}\) has greater power than \(\psi_{n}\), it follows that

\[\lim\sup_{n}\{E_{\mu_{n},\sigma_{n}}(\tilde{\psi}_{n})-E_{\mu_{n},\sigma_{n}}( \phi_{n})\}>0\;.\]

But,

\[E_{\mu_{n},\sigma_{n}}(\tilde{\psi}_{n})=1-\Phi(z_{1-\alpha}-n^{1/2}\mu_{n}/ \sigma_{n})\;.\]

Since the power of the \(t\)-test and the power of the test \(\phi_{n}\) depend on \((\mu,\sigma)\) only through \(\mu/\sigma\),

\[E_{\mu_{n},\sigma_{n}}(\phi_{n})=E_{\frac{\mu_{n}}{\sigma_{n}},1}(\phi_{n})\;.\]

So, it suffices to show, uniformly in \(\mu\) and \(\sigma=1\), that

\[\sup_{\mu>0}|1-\Phi(z_{1-\alpha}-n^{1/2}\mu)-E_{\mu,1}(\phi_{n})|\to 0\;,\]

or, for any sequence \(\mu_{n}\) with \(\mu_{n}>0\),

\[E_{\mu_{n},1}(\phi_{n})-[1-\Phi(z_{1-\alpha}-n^{1/2}\mu_{n})]\to 0\;.\]

But,

\[E_{\mu_{n},1}(\phi_{n})=P_{\mu_{n},1}\{n^{1/2}(\tilde{X}_{n}-\mu_{n})/S_{n}>z_{ 1-\alpha}-n^{1/2}\mu_{n}/S_{n}\}\;.\]

Under \(\mu=\mu_{n}\) and \(\sigma=1\), the left-hand side \(n^{1/2}(\tilde{X}_{n}-\mu_{n})/S_{n}\) has the \(t\)-distribution with \(n-1\) degrees of freedom, and so tends in distribution to \(Z\) which has the standard normal distribution. Also, \(S_{n}\to 1\) in probability. By Slutsky's theorem, if \(n^{1/2}\mu_{n}\to\delta\), then

\[E_{\mu_{n},1}(\phi_{n})\to P\{Z>z_{1-\alpha}-\delta\}\]

and (15.72) holds. If \(n^{1/2}\mu_{n}\to\infty\), then \(n^{1/2}(\tilde{X}_{n}-\mu_{n})/S_{n}\) is still asymptotically standard normal, while \(z_{1-\alpha}-n^{1/2}\mu_{n}/S_{n}\to-\infty\) in probability; then, \(E_{\mu_{n},1}(\phi_{n})\to 1\) and (15.72) holds. To complete the argument, one must pass to subsequences such that \(n^{1/2}\mu_{n}\) converges (possibly to \(\infty\)) and apply the previous argument along such subsequences. The conclusion is that \(\phi_{n}\) is AUMP.

Consider the following special case of Theorem 15.5.1. Suppose \(\theta=(\theta_{1},\ldots,\theta_{k})\) and interest focuses on inference for \(\theta_{1}\) in the presence of the nuisance parameters \(\theta_{2},\ldots,\theta_{k}\). Specifically, consider testing \(\theta_{1}=\theta_{1,0}\) versus \(\theta_{1}>\theta_{1,0}\). As usual, let \(I(\theta)\) denote the Fisher Information matrix with \((i,\,j)\) entry denoted \(I_{i,j}(\theta)\); it is assumed \(I(\theta)\) is invertible with inverse \(I^{-1}(\theta)\) having \((i,\,j)\) entry \([I^{-1}(\theta)]_{i,j}\). It is interesting to compare the power of the asymptotically optimal tests when the nuisance parameters are unknown with the situation in which they are known. If\(\theta_{2}\),..., \(\theta_{k}\) are fixed and known, then the best limiting power against the sequence of alternatives \(\theta_{1,0}+h_{1}n^{-1/2}\) of an asymptotically level-\(\alpha\) test was obtained in Theorem 15.3.2, and is equal to

\[1-\Phi(z_{1-\alpha}-h_{1}I_{1,1}^{1/2}(\theta_{1,0},\theta_{2},\ldots,\theta_{k }))\.\]

If the nuisance parameters are unknown, the best limiting power was obtained in Theorem 15.5.1; simply apply the theorem with \(g(\theta)=\theta_{1}\), \(\dot{g}(\theta)=(1,0,\ldots,0)\), and \(h=(h_{1},0,\ldots,0)\). The resulting limiting power value is equal to

\[1-\Phi(z_{1-\alpha}-h_{1}\{I^{-1}(\theta_{1,0},\theta_{2},\ldots,\theta_{k})_{1,1}\}^{-1/2})\.\]

Comparing these situations, we see that

\[\frac{1}{I_{1,1}(\theta)}\leq[I^{-1}(\theta)]_{1,1}\]

since the power of the test when \((\theta_{2},\ldots,\theta_{k})\) are known exceeds that when \((\theta_{2},\ldots,\theta_{k})\) are unknown. Equality holds if \(I_{1,j}(\theta)=0\) for all \(j\neq 1\). Since the same argument applies to any of the components of \(\theta\), there is no loss in power when testing any component in the presence of the remaining parameters if and only if \(I(\theta)\) is a diagonal matrix.

**Example 15.5.2** (**Location Scale Models**) Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. with density \(\sigma^{-1}f((x-\mu)/\sigma)\), where \(f\) is absolutely continuous. Both the location parameter \(\mu\) and the scale parameter \(\sigma\) are unknown. If \(\theta=(\mu,\sigma)\), then the components of the Information matrix are given by (Problem 15.46)

\[I_{1,1}=\sigma^{-2}\int\left[\frac{f^{\prime}(x)}{f(x)}\right]^{2}f(x)dx\,\]

\[I_{2,2}=\sigma^{-2}\int\left[\frac{xf^{\prime}(x)}{f(x)}+1\right]^{2}f(x)dx,\]

and

\[I_{1,2}=\sigma^{-2}\int x\left[\frac{f^{\prime}(x)}{f(x)}\right]^{2}f(x)dx\.\]

It follows that the off-diagonal element \(I_{1,2}\) is equal to \(0\) if \(f\) is symmetric.

We specialize further and let \(f(x)=C(\beta)\exp(-|x|^{\beta})\) for some fixed \(\beta\). Recall from Example 14.2.5 that, if \(\beta>1/2\), then \(f\) generates a location model which is q.m.d.; the location scale model with \(\sigma\) unknown is also q.m.d. (Problem 15.47). For \(\beta>1\), the MLE \(\hat{\mu}_{n}\) for \(\mu\) is the unique minimizer of \(\sum_{i}|X_{i}-\mu|^{\beta}\); for \(\beta=1\), any value between the middle-order statistics is an MLE. Moreover, the unique MLE \(\hat{\sigma}_{n}\) for \(\sigma\) is given by \[\hat{\sigma}_{n}=\beta^{1/\beta}\left[\frac{\sum_{i}|X_{i}-\hat{\mu}_{n}|^{\beta}} {n}\right]^{1/\beta}. \tag{15.73}\]

For testing \(\mu\leq 0\) against \(\mu>0\), the Wald test which rejects for large values of \(\hat{\mu}_{n}/\hat{\sigma}_{n}\) is LAUMP; If \(1/2<\beta<1\), Rao's score test is more convenient to apply and is LAUMP (Problem 15.48).

**Example 15.5.3**: (**Bivariate Normal Correlation**) As in Example 15.3.5, let \(X_{i}=(U_{i},\,V_{i})\) be i.i.d. bivariate normal with unknown correlation \(\rho\). However, here we assume the means and variances of \(U_{i}\) and \(V_{i}\) are unknown as well. The MLE \(\hat{\rho}_{n}\) is given by the sample correlation (11.29). A LAUMP test rejects when \(n^{1/2}\hat{\rho}_{n}>z_{1-\alpha}\). Note that, in this case, the Information is not diagonal and the optimal limiting power is strictly smaller than the case where only \(\rho\) is unknown (Problem 15.49).

Theorem 15.5.1 can be generalized to two-sample problems, since the proof essentially only depends on Theorem 15.4.1 and the assumption that the experiment is asymptotically normal. By Example 15.4.2, asymptotic normality holds for two-sample models if each of the one-sample models is quadratic mean differentiable. Specifically, suppose \(X_{1},\,\ldots,\,X_{m}\) are i.i.d. \(P_{\theta},\,\theta\in\Omega\), \(\Omega\) an open subset of \(\mathbb{R}^{k}\). Also, suppose \(Y_{1},\,\ldots,\,Y_{n}\) are i.i.d. \(\bar{P}_{\theta},\,\theta\in\Omega\). Let \(I(\theta)\) denote the Information an \(X_{i}\) contains about \(\theta\); similarly, let \(\bar{I}(\theta)\) be the Information a \(Y_{j}\) contains about \(\theta\). Assume these Information matrices are nonsingular and continuous. Fix any \(\theta_{0}\) and assume both models are q.m.d. at \(\theta_{0}\) with corresponding score statistics \(Z_{m}\) and \(\bar{Z}_{n}\) (in the notation of Example 15.4.2). Then, the combined experiment is asymptotically normal with score statistic

\[Z_{m,n}=(m/n)^{1/2}Z_{m}+\bar{Z}_{n}\.\]

If we also assume \(m/n\to\lambda<\infty\), then the joint experiment is asymptotically normal with covariance

\[C(\theta_{0})=\lambda I(\theta_{0})+\bar{I}(\theta_{0})\.\]

Consider testing \(g(\theta)=0\) versus \(g(\theta)>0\), for some continuously differentiable \(g\) with gradient \(\dot{g}(\theta)\). A generalization of (15.68) yields for any uniformly asymptotically level-\(\alpha\) test sequence that (Problem 15.50)

\[\limsup_{n}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq 1-\Phi(z_{1-\alpha}-\sigma_{ \theta_{0}}^{-1}(\dot{g}(\theta_{0})^{\top},h))\, \tag{15.74}\]

where

\[\sigma_{\theta_{0}}^{2}=\dot{g}(\theta_{0})C^{-1}(\theta_{0})\dot{g}(\theta_{ 0})^{\top}\.\]

To find such a test, assume there exists an estimator sequence \(\hat{\theta}_{n}\) satisfying

\[n^{1/2}(\hat{\theta}_{n}-\theta_{0})=C^{-1}(\theta_{0})Z_{m,n}+o_{P_{\theta_{ 0}}^{n}\times\bar{F}_{\theta_{0}}^{n}}(1). \tag{15.75}\]Then, the test that rejects when \(n^{1/2}g(\hat{\theta}_{n})>z_{1-\alpha}\hat{\sigma}_{n}\), where

\[\hat{\sigma}_{n}^{2}=\hat{g}(\hat{\theta}_{n})C^{-1}(\hat{\theta}_{n})\hat{g}( \hat{\theta}_{n})^{\top}\]

is pointwise asymptotically level-\(\alpha\) and the inequality (15.74) is an equality (Problem 15.51).

**Example 15.5.4**: **(Behrens-Fisher Problem)** As a special case of the above, assume \(P_{\theta}\) is \(N(\xi,\,\sigma^{2})\) and \(\bar{P}_{\theta}\) is \(N(\eta,\,\tau^{2})\) so that \(\theta=(\xi,\,\eta,\,\sigma^{2},\,\tau^{2})\), and all four parameters vary freely. Consider testing \(\eta-\xi=0\) versus \(\eta-\xi>0\), so that \(g(\theta)=\eta-\xi\) and \(\dot{g}(\theta)=(-1,\,1,\,0,\,0)\). For this problem, neither invariance nor unbiasedness considerations reduce the problem sufficiently to obtain any kind of optimal test. However, a large-sample optimality result is easily obtained. Fix \(\theta_{0}=(\xi_{0},\,\xi_{0},\sigma^{2},\,\tau^{2})\). Assume \(m/n\to\lambda<\infty\). Then, it is easy to check that the covariance matrix \(C\) in Definition 15.4.1 is the diagonal matrix with diagonal elements \(\lambda/\sigma^{2},\,1/\tau^{2},\,2\lambda/\sigma^{2}\), and \(2/\tau^{2}\). Hence,

\[\sigma_{\theta_{0}}^{2}=\dot{g}(\theta_{0})C^{-1}(\theta_{0})\dot{g}(\theta_{0 })^{\top}=\frac{\sigma^{2}}{\lambda}+\tau^{2}. \tag{15.76}\]

Thus, the bound in (15.74) with \(h=(h_{1},\,h_{2},\,0,\,0)\) reduces to

\[1-\Phi\left[z_{1-\alpha}-(\frac{\sigma^{2}}{\lambda}+\tau^{2})^{-1/2}(h_{2}-h_ {1})\right]\,.\]

It is easy to construct a test sequence that achieves this bound. Consider the test that rejects the null hypothesis when

\[n^{1/2}(\bar{Y}_{n}-\bar{X}_{m})>z_{1-\alpha}\left[S_{Y}^{2}+(\frac{n}{m})S_{X }^{2}\right]^{1/2}\,\]

where \(\bar{Y}_{n}=n^{-1}\sum_{j}Y_{j}\), \(S_{Y}^{2}=(n-1)^{-1}\sum_{j}(Y_{j}-\bar{Y}_{n})^{2}\), and similarly for \(\bar{X}_{m}\) and \(S_{X}^{2}\). This test is pointwise consistent in level; the order of error in the rejection probability will be revisited in Example 18.5.4. The limiting power of this test against the sequence of parameter values \((\xi_{0}+h_{1}n^{-1/2},\,\xi_{0}+h_{2}n^{-1/2},\,\sigma^{2},\,\tau^{2})\) is given by

\[P\left\{\frac{n^{1/2}[(\bar{Y}_{n}-h_{2}n^{1/2})-(\bar{X}_{m}-h_{1}n^{-1/2})]} {(S_{Y}^{2}+\frac{n}{m}S_{X}^{2})^{1/2}}>z_{1-\alpha}-\frac{h_{2}-h_{1}}{(S_{Y }^{2}+\frac{n}{m}S_{X}^{2})^{1/2}}\right\}\.\]

But, \(S_{Y}^{2}\to\tau^{2}\) in probability, \(S_{X}^{2}\to\sigma^{2}\) in probability, and the left-hand side is asymptotically standard normal. The result follows by Slutsky's Theorem.

#### Equivalence Hypotheses

In this section, we will apply Theorem 15.4.1 to the following situation. Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. \(P_{\theta}\) where \(\theta\in\Omega\) and \(\Omega\) is an open subset of \(\hbox{\rm I$\kern-1.8pt\rm R$}^{k}\). Interest focuses on \(g(\theta)\), where \(g\) is a function from \(\Omega\) to \(\hbox{\rm I$\kern-1.8pt\rm R$}\). Assume \(g\) is differentiable with gradient vector \(\dot{g}(\theta)\) of dimension \(1\times k\). We wish to test the null hypothesis \(|g(\theta)|\geq\Delta\) against the alternative \(|g(\theta)|<\Delta\). (We are tacitly assuming there exists values of \(\theta\) satisfying \(g(\theta)\geq\Delta\) and \(g(\theta)\leq\Delta\).) This problem was studied in Theorem 3.7.1, where a UMP test was derived for a one-parameter exponential family. A UMP equivalence test for a linear combination of means of a multivariate normal distribution was obtained in Example 3.9.3.

We will formulate the asymptotic problem in two distinct ways. First, we will consider the case when the null hypothesis parameter space is the complement of a fixed interval (\(-\Delta\), \(\Delta\)). Then, we will also consider the case when this interval shrinks with \(n\).

(i). _Fixed_\(\Delta\). Suppose \(\Delta>0\) is fixed and the problem is to test \(|g(\theta)|\geq\Delta\) versus \(|g(\theta)|<\Delta\). For any fixed alternative value \(\theta\) with \(|g(\theta)|<\Delta\), the power of any reasonable test against \(\theta\) will tend to one. Therefore, just as we did for one-sided hypotheses, we compare power functions against local alternatives. Consider any fixed \(\theta_{0}\) satisfying \(|g(\theta_{0})|=\Delta\). For the sake of argument, consider the case \(g(\theta_{0})=-\Delta\). We wish to derive an (obtainable) upper bound for the limiting power of a test sequence \(\phi_{n}\) under \(\theta_{0}+hn^{-1/2}\). But a crude way to bound the power is based on the simple fact that any level-\(\alpha\) test for testing \(|g(\theta)|\geq\Delta\) versus \(|g(\theta)|<\Delta\) is also level-\(\alpha\) for testing \(g(\theta)\leq-\Delta\) versus \(g(\theta)>-\Delta\). Since upper bounds for the asymptotic power were obtained in Theorem 15.5.1, an immediate result follows. In this asymptotic setup, the statistical problem is somewhat degenerate as it becomes one of testing a one-sided hypothesis. For example, suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. \(N(\theta,1)\). Then for large \(n\), one can distinguish \(\theta\leq-\Delta\) and \(\theta>-\Delta\) with error probabilities that are uniformly small and tend to zero exponentially fast with \(n\). In essence, the statistical issue arises only if the true \(\theta\) is near the boundary of \([-\Delta\,,\,\Delta]\), in which case determining significance essentially becomes one of testing a one-sided hypothesis.

**Theorem 15.5.2**: _Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. according to \(P_{\theta}\), \(\theta\in\Omega\), where \(\Omega\) is assumed to be an open subset of \(\hbox{\rm I$\kern-1.8pt\rm R$}^{k}\). Consider testing the null hypothesis_

\[\theta\in\Omega_{0}=\{\theta\,:\,\,|g(\theta)|\geq\Delta\}\]

_versus \(|g(\theta)|<\Delta\), where the function \(g\) from \(\hbox{\rm I$\kern-1.8pt\rm R$}^{k}\) to \(\hbox{\rm I$\kern-1.8pt\rm R$}\) is assumed differentiable with gradient \(\dot{g}(\theta)\). Assume the family \(\{P_{\theta},\theta\in\Omega\}\) is q.m.d. at every \(\theta\) with \(|g(\theta)|=\Delta\) and assume the Fisher Information matrix \(I(\theta)\) is nonsingular for such \(\theta\). Let \(\phi_{n}=\phi_{n}(X_{1},\,\ldots,\,X_{n})\) be a uniformly asymptotically level \(\alpha\) sequence of tests; that is,_

\[\limsup_{n\to\infty}\sup_{\Omega_{0}}E_{\theta}(\phi_{n})\leq\alpha. \tag{15.77}\]_(i) Assume \(\theta_{0}\) satisfies \(g(\theta_{0})=-\Delta\). Then, for any \(h\) such that \((\dot{g}(\theta_{0})^{\top},h)>0\),_

\[\limsup_{n}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq 1-\Phi(z_{1-\alpha}-\sigma_{ \theta_{0}}^{-1}(\dot{g}(\theta_{0})^{\top},h))\, \tag{15.78}\]

_where_

\[\sigma_{\theta_{0}}^{2}=\dot{g}(\theta_{0})I^{-1}(\theta_{0})\dot{g}(\theta_{0 })^{\top}. \tag{15.79}\]

_(ii) Assume \(\theta_{0}\) satisfies \(g(\theta_{0})=\Delta\). Then, for any \(h\) such that \((\dot{g}(\theta_{0})^{\top},h)<0\),_

\[\limsup_{n}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq 1-\Phi(z_{1-\alpha}-\sigma_{ \theta_{0}}^{-1}|\langle\dot{g}(\theta_{0})^{\top},h\rangle|)\, \tag{15.80}\]

_(iii) Let \(\hat{\theta}_{n}\) be any estimator satisfying (14.62). Suppose \(I(\theta)\) is continuous in \(\theta\) and \(\dot{g}(\theta)\) is continuous at \(\theta_{0}\). Then, the test sequence \(\phi_{n}\) that rejects when \(|g(\hat{\theta}_{n})|<\Delta-n^{-1/2}\hat{\sigma}_{n}z_{1-\alpha}\), where_

\[\hat{\sigma}_{n}^{2}=\dot{g}(\hat{\theta}_{n})I^{-1}(\hat{\theta}_{n})\dot{g}( \hat{\theta}_{n})^{\top} \tag{15.81}\]

_is pointwise asymptotically level-\(\alpha\) and is locally asymptotically UMP in the sense that the inequality (15.78) is an equality. In fact, the same properties hold for any test sequence that rejects if \(|T_{n}|<\Delta-n^{-1/2}\hat{\sigma}z_{1-\alpha}\), if \(T_{n}\) satisfies_

\[T_{n}=\dot{g}(\theta_{0})I^{-1}(\theta_{0})Z_{n,\theta_{0}}+o_{P_{\theta_{0}}^{ n}}(1) \tag{15.82}\]

_for every \(\theta_{0}\in\Omega_{0}\), where \(Z_{n,\theta_{0}}\) is the score vector defined in (14.59)._

Proof. As remarked above, (15.78) follows because \(\phi_{n}\) is also a uniformly asymptotically level- \(\alpha\) test for testing \(g(\theta)\leq-\Delta\) versus \(g(\theta)>-\Delta\). For this one-sided testing problem, the optimal bound was obtained in Theorem 15.5.1. The same argument applies to (15.80). To prove (iii), let \(\theta_{n}=\theta_{0}+hn^{-1/2}\). Then, assumption (14.62) and contiguity arguments imply that, under \(\theta_{n}\),

\[n^{1/2}(\hat{\theta}_{n}-\theta_{n})\stackrel{{ d}}{{\to}}N(0,\,I^ {-1}(\theta_{0}))\.\]

Thus, under \(\theta_{n}\), \(\hat{\sigma}_{n}\) tends in probability to \(\sigma_{\theta_{0}}\). Moreover, the Delta Method implies, under \(\theta_{n}\),

\[n^{1/2}(g(\hat{\theta}_{n})-g(\theta_{n}))\stackrel{{ d}}{{\to}}N (0,\sigma_{\theta_{0}}^{2})\.\]

Now, if \(g(\theta_{0})=-\Delta\) and \(\langle\dot{g}(\theta_{0})^{\top},h\rangle>0\), then

\[g(\theta_{n})=-\Delta+n^{-1/2}\langle\dot{g}(\theta_{0})^{\top},h\rangle+o(n^{- 1/2})\.\]

So, under \(\theta_{n}\),

\[n^{1/2}[g(\hat{\theta}_{n})+\Delta]\stackrel{{ d}}{{\to}}N\left( \langle\dot{g}(\theta_{0})^{\top},h\rangle,\sigma_{\theta_{0}}^{2}\right). \tag{15.83}\]Therefore,

\[E_{\theta_{n}}(\phi_{n})=P_{\theta_{n}}\{|g(\hat{\theta}_{n})|<\Delta-n^{-1/2}\hat {\sigma}_{n}z_{1-\alpha}\}\,\]

which tends to the right side of (15.78) by (15.83) and Slutsky's Theorem. The same proof works for any estimator \(T_{n}\) of the form (15.82).

**Example 15.5.5** (**Normal One-Sample Problem**): Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\), with both parameters unknown. Consider testing \(|\mu|\geq\Delta\) versus \(|\mu|<\Delta\). The standard \(t\)-test for testing the one-sided hypothesis \(\mu\leq\Delta\) against \(\mu>-\Delta\) rejects if

\[n^{1/2}(\bar{X}_{n}+\Delta)/S_{n}>t_{n-1,1-\alpha}\,\]

where \(S_{n}^{2}\) is the (unbiased) sample variance and \(t_{n-1,1-\alpha}\) is the \(1-\alpha\) quantile of the \(t\)-distribution with \(n-1\) degrees of freedom. Similarly, the standard \(t\)-test of the hypothesis \(\mu\geq\Delta\) rejects if

\[n^{1/2}(\bar{X}_{n}-\Delta)/S_{n}<-t_{n-1,1-\alpha}\.\]

The intersection of these rejection regions is therefore a level-\(\alpha\) test of the null hypothesis \(|\mu|\geq\Delta\). Such a construction that intersects the rejection regions of two one-sided tests (TOST) was proposed in Westlake (1981) and Schuirmann (1981), and can be viewed as a special case of Berger's (1982) intersection-union tests. The resulting test is denoted by \(\phi_{n}^{TOST}\) that rejects when \(|\bar{X}_{n}|<\Delta-n^{-1/2}S_{n}t_{n-1,1-\alpha}\). (In fact, we see here that our general asymptotic construction in (iii) of the above theorem merely replaces the \(t_{n-1}\) quantiles by the standard normal quantiles; that is, the intersection two rejection regions, each of _asymptotic_ size \(\alpha\) yields a rejection region whose asymptotic size is bounded above by \(\alpha\).) In general, by combining two one-sided tests, the resulting TOST can be quite conservative in that its size can be quite less than \(\alpha\). However, in this example, the size of \(\phi_{n}^{TOST}\) is actually \(\alpha\), as can be seen by calculating the rejection probability under (\(\mu\), \(\sigma\)) with \(\mu=\Delta\) and \(\sigma\to 0\) (Problem 15.55). The asymptotic power of \(\phi_{n}^{TOST}\) against a sequence with mean \(-\Delta+hn^{-1/2}\) (\(h>0\)) and variance fixed at \(\sigma^{2}\) is obtained by the previous theorem or calculated directly as

\[\lim_{n\to\infty}P_{\Delta+hn^{-1/2},\sigma}\{|\bar{X}_{n}|<\Delta-n^{-1/2}S_{ n}t_{n-1,1-\alpha}\}=\Phi(z_{1-\alpha}-\frac{h}{\sigma})\,\]

which is the optimal bound when (15.78) is specialized to this situation. A similar calculation applies to sequences of the form \(\Delta-hn^{-1/2}\). Thus, the TOST is asymptotically optimal in this setup. It should be remarked that the TOST has been criticized because it is biased (in finite samples) and tests have been proposed that have greater power; some proposals are discussed in Brown et al. (1995), Berger and Hsu (1996), and Perlman and Wu (1999). Such tests cannot have greater asymptotic power against local alternatives, at least under the setup of Theorem 15.5.2. On the other hand, the TOST will be seen to be inefficient under the asymptotic formulation treated below.

_(ii) Shrinking_\(\Delta\). We now consider a second asymptotic formulation of the problem, in which the null hypothesis \(|g(\theta)|\geq\delta n^{-1/2}\) is tested against the alternative hypothesis \(|g(\theta)|<\delta n^{-1/2}\). Notice that now the parameter spaces (or hypotheses) are changing with \(n\). Of course, a given hypothesis testing situation deals with a particular \(n\), and there is flexibility in how the problem is embedded into a sequence of similar problems to get a useful approximation. In particular, if equivalence corresponds to \(|g(\theta)|<\Delta\), we can always make the identification \(\delta=\Delta n^{1/2}\). From an asymptotic point of view, it makes sense to allow the null hypothesis parameter space to change with \(n\), since otherwise the problem becomes degenerate in the sense that the values of \(\Delta\) and \(-\Delta\) for \(g(\theta)\) can be perfectly distinguished asymptotically. In testing for bioequivalence, for example, \(\Delta\) is chosen so small that a value of \(|g(\theta)|\leq\Delta\) is deemed to be essentially zero. In a particular situation such as Example 15.5.5 with \(\sigma\) not too small, if a value for \(\mu\) of \(\Delta\) cannot be perfectly tested against a value for \(\mu\) of \(0\), then \(\Delta\) and \(-\Delta\) cannot be perfectly tested as well, and the asymptotic setup should reflect this.

The main result of this subsection is the following theorem.

**Theorem 15.5.3**: _Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. according to \(P_{\theta}\), \(\theta\in\Omega\), where \(\Omega\) is assumed to be an open subset of \({\rm I\kern-1.8ptR}^{k}\). Consider testing the null hypothesis_

\[\theta\in\Omega_{0,n}=\{\theta:\ |g(\theta)|\geq\delta n^{-1/2}\}\]

_versus \(|g(\theta)|<\delta n^{-1/2}\), where the function \(g\) from \({\rm I\kern-1.8ptR}^{k}\) to \({\rm I\kern-1.8ptR}\) is assumed differentiable with gradient \(\dot{g}(\theta)\). Assume for every \(\theta\) with \(g(\theta)=0\) that the family \(\{P_{\theta},\theta\in\Omega\}\) is q.m.d. at \(\theta\) and \(I(\theta)\) is nonsingular._

_(i) Let_ \(\phi_{n}=\phi_{n}(X_{1},\ldots,X_{n})\) _be a uniformly asymptotically level-_\(\alpha\) _sequence of tests, so that_

\[\limsup_{n\to\infty}\sup_{\Omega_{0,n}}E_{\theta}(\phi_{n})\leq\alpha\.\]

_Assume_ \(\theta_{0}\) _satisfies_ \(g(\theta_{0})=0\)_. Then, for any_ \(h\) _such that_ \(|\langle\dot{g}(\theta_{0})^{\top},h\rangle|=\delta^{\prime}<\delta\)_,_

\[\limsup_{n\to\infty}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq\Phi\left(\frac{C- \delta^{\prime}}{\sigma_{\theta_{0}}}\right)-\Phi\left(\frac{-C-\delta^{ \prime}}{\sigma_{\theta_{0}}}\right)\, \tag{15.84}\]

_where_ \(\sigma_{\theta_{0}}^{2}\) _is given by_

\[\sigma_{\theta_{0}}^{2}=\dot{g}(\theta_{0})I^{-1}(\theta_{0})\dot{g}(\theta_{ 0})^{\top} \tag{15.85}\]

_and_ \(C=C(\alpha,\,\delta,\,\sigma_{\theta_{0}})\) _satisfies_

\[\Phi\left(\frac{C-\delta}{\sigma_{\theta_{0}}}\right)-\Phi\left(\frac{-C- \delta}{\sigma_{\theta_{0}}}\right)=\alpha. \tag{15.86}\]_(ii) Let \(\hat{\theta}_{n}\) be any estimator satisfying (14.62). Suppose \(I(\theta)\) is continuous in \(\theta\) and \(\dot{g}(\theta)\) is continuous at \(\theta_{0}\). Then, the test sequence \(\phi_{n}\) that rejects when \(n^{1/2}|g(\hat{\theta}_{n})|\leq C(\alpha,\delta,\hat{\sigma}_{n})\), where_

\[\hat{\sigma}_{n}^{2}=\dot{g}(\hat{\theta}_{n})I^{-1}(\hat{\theta}_{n})\dot{g}( \hat{\theta}_{n})^{\top}\,\]

_is pointwise asymptotically level \(\alpha\) and is locally asymptotically UMP in the sense that the inequality (15.84) is an equality. In fact, the same properties hold for any test sequence that rejects if \(|T_{n}|<C(\alpha,\delta,\hat{\sigma}_{n})\), if \(T_{n}\) satisfies_

\[T_{n}=\dot{g}(\theta_{0})I^{-1}(\theta_{0})Z_{n,\theta_{0}}+o_{P_{\theta_{0}}^ {n}}(1)\]

_for every \(\theta_{0}\in\Omega_{0}\), where \(Z_{n,\theta_{0}}\) is the score vector defined in (14.59)._

Proof. Fix \(\theta_{0}\) satisfying \(g(\theta_{0})=0\). We will derive an upper bound for the limiting power of a test sequence \(\phi_{n}\) under \(\theta_{0}+hn^{-1/2}\). Note that

\[g(\theta_{0}+hn^{-1/2})=n^{-1/2}\langle\dot{g}(\theta_{0})^{\top},h\rangle+o(n ^{-1/2})\.\]

So, if \(h\) is such that \(|\langle\dot{g}(\theta_{0})^{\top},h\rangle|>\delta\), then \(|g(\theta_{0}+hn^{-1/2})|>\delta n^{-1/2}\) for all sufficiently large \(n\). Hence, if \(\phi_{n}\) has limiting size \(\alpha\), then for such an \(h\),

\[\limsup_{n\to\infty}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\leq\alpha\ \.\]

By Theorem 15.4.1, we can approximate the power of a test sequence \(\phi_{n}\) by the power of a test \(\phi=\phi(X)\) for the (limit) experiment based on \(X\) from the model \(N(h,I^{-1}(\theta_{0}))\). Let \(\beta_{\phi}(h)\) denote the power function of \(\phi(X)\) when \(X\sim N(h,I^{-1}(\theta_{0}))\). Then, (15.87) implies \(\beta_{\phi}(h)\leq\alpha\) if \(|\langle\dot{g}(\theta_{0})^{\top},h\rangle|>\delta\). By continuity of \(\beta_{\phi}(h)\), \(\beta_{\phi}(h)\leq\alpha\) for any \(h\) with \(|\langle\dot{g}(\theta_{0})^{\top},h\rangle|\geq\delta\). The test \(\phi\) that maximizes \(\beta_{\phi}(h)\) for this limiting normal problem was given in Example 3.9.3 with \(\Sigma=I^{-1}(\theta_{0})\), \(\dot{g}=h\), and \(a^{\top}=\dot{g}(\theta_{0})\). Thus, if \(\phi\) is level-\(\alpha\) for testing \(|\langle\dot{g}(\theta_{0})^{\top},h\rangle|\geq\delta\) and \(h\) satisfies \(|\langle\dot{g}(\theta_{0})^{\top},h\rangle|=\delta^{\prime}<\delta\), then

\[\beta_{\phi}(h)\leq\Phi\left(\frac{C-\delta^{\prime}}{\sigma_{\theta_{0}}} \right)-\Phi\left(\frac{-C-\delta^{\prime}}{\sigma_{\theta_{0}}}\right).\]

and \(C=C(\alpha,\delta,\sigma_{\theta_{0}})\) satisfies (15.86).

To prove (ii), consider the test that rejects when \(n^{1/2}|g(\hat{\theta}_{n})|\leq C(\alpha,\delta,\hat{\sigma}_{n})\). Fix \(h\) such that \(|\langle\dot{g}(\theta_{0})^{\top},h\rangle|<\delta\) and let \(\theta_{n}=\theta_{0}+hn^{-1/2}\). Then, as in the proof of Theorem 15.5.2 (iii), under \(\theta_{n}\),

\[n^{1/2}[g(\hat{\theta}_{n})-g(\theta_{n})]\stackrel{{ d}}{{\to}}N(0, \sigma_{\theta_{0}}^{2})\.\]

But,

\[n^{1/2}g(\theta_{n})=\langle h,\dot{g}(\theta_{0})^{\top}\rangle+o(1)\.\]Therefore, under \(\theta_{n}\),

\[n^{1/2}g(\hat{\theta}_{n})\stackrel{{ d}}{{\to}}N\left(\langle h,\, \dot{g}(\theta_{0})^{\top}\rangle,\sigma_{\theta_{0}}^{2}\right)\.\]

Also, under \(\theta_{n}\), \(\hat{\sigma}_{n}\) tends in probability to \(\sigma_{\theta_{0}}\), and so \(C(\alpha,\,\delta,\,\hat{\sigma}_{n})\) tends in probability to \(C(\alpha,\,\delta,\,\sigma_{\theta_{0}})\). Hence, letting \(Z\) denote a standard normal variable,

\[P_{\theta_{n}}\{n^{1/2}|g(\hat{\theta}_{n})|\leq C(\alpha,\,\delta,\,\hat{ \sigma}_{n})\}\to P\{|\sigma_{\theta_{0}}Z+\langle h,\,\dot{g}(\theta_{0})^{ \top}\rangle|\leq C(\alpha,\,\delta,\sigma_{\theta_{0}})\}\,\]

which agrees with the right-hand side of (15.84).

**Example 15.5.6** (**Normal Problem, Example 15.5.5, continued**) Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. \(N(\mu,\,\sigma^{2})\) with both parameters unknown, so \(\theta=(\mu,\,\sigma)\). Let \(g(\theta)=\mu\) and consider testing \(|\mu|\geq\delta n^{-1/2}\) versus \(|\mu|<\delta n^{-1/2}\). By the previous theorem, for any test sequence \(\phi_{n}\) with limiting size bounded by \(\alpha\) and any \(h\) with \(|h|<\delta\),

\[E_{hn^{-1/2},\sigma}(\phi_{n})\leq\Phi\left(\frac{C-h}{\sigma}\right)-\Phi \left(\frac{-C-h}{\sigma}\right)\, \tag{15.88}\]

where \(C=C(\alpha,\,\delta,\,\sigma)\) satisfies (15.86). A test whose limiting power achieves this bound is given by the test \(\phi_{n}^{*}\) that rejects when

\[n^{1/2}|\tilde{X}_{n}|\leq C(\alpha,\,\delta,\,S_{n})\,\]

where \(S_{n}^{2}\) is the (unbiased) sample variance (or any consistent estimator of \(\sigma^{2}\)). On the other hand, the test \(\phi_{n}^{TOST}\) given in Example 15.5.5 is _no longer_ asymptotically efficient. This test (with \(\Delta=\delta n^{-1/2}\)) rejects when

\[n^{1/2}|\tilde{X}_{n}|<\delta-S_{n}t_{n-1,1-\alpha}\]

and has power against \((\mu,\,\sigma)=(hn^{-1/2},\sigma)\) given by

\[P_{hn^{-1/2},\sigma}\left\{\frac{-\delta+S_{n}t_{n-1,1-\alpha}-h}{\sigma}<Z_{ n}<\frac{\delta-S_{n}t_{n-1,1-\alpha}-h}{\sigma}\right\}\, \tag{15.89}\]

where

\[Z_{n}=n^{1/2}(\tilde{X}_{n}-hn^{-1/2})/\sigma\sim N(0,\,1)\.\]

Also, \(S_{n}\to\sigma\) in probability and \(t_{n-1,1-\alpha}\to z_{1-\alpha}\). By Slutsky's Theorem, (15.89) converges to

\[P\left\{\frac{-\delta}{\sigma}+z_{1-\alpha}-\frac{h}{\sigma}<Z<\frac{\delta}{ \sigma}-z_{1-\alpha}-\frac{h}{\sigma}\right\}\, \tag{15.90}\]

where \(Z\sim N(0,\,1)\). Observe that this last expression is positive only if \(\sigma z_{1-\alpha}<\delta\); otherwise, the limiting power is zero! On the other hand, the limiting optimal power of \(\phi_{n}^{*}\) is always positive (and greater than \(\alpha\) when \(|h|<\delta\)). Even when the limiting power of \(\phi_{n}^{TOST}\) is positive, it is always strictly less than that of \(\phi_{n}^{*}\).

Note that the limiting expression (15.90) for the power of \(\phi_{n}^{TOST}\) corresponds exactly to using a TOST test in the limiting experiment \(N(h,\sigma^{2})\) for testing \(|h|\geq\delta\) versus \(|h|<\delta\) with \(\sigma\) known based on one observation \(X\). In the limit experiment, the TOST procedure corresponds to the test that rejects if \(|X|<\delta-\sigma z_{1-\alpha}\) (which can be viewed as a TOST construction because its rejection region is the intersection of the rejection regions of the two one-sided tests of \(h<\delta\) and \(h>-\delta\)). But, for this limit experiment, the optimal UMP procedure of Section 3.7 rejects when \(|X|<C(\alpha,\delta,\sigma)\). In general,

\[C(\alpha,\delta,\sigma)>\delta-\sigma z_{1-\alpha}\]

(Problem 15.56), which shows that the test \(\phi_{n}^{*}\) of Theorem 15.5.3 is always more powerful than the asymptotic TOST construction of Theorem 15.5.2.

#### Multisided Hypotheses

We now consider the problem of testing \(\theta=\theta_{0}\) versus \(\theta\neq\theta_{0}\) as \(\theta\) varies in an open subset of \(\hbox{\rm I\kern-1.8ptR}^{k}\). Theorem 15.4.1 relates this problem to testing \(h=0\) versus \(h\neq 0\) based on an observation \(X\) from the normal model \(N(h,I^{-1}(\theta_{0}))\), where, as usual, \(I(\theta_{0})\) is the Fisher Information. For this normal model, no UMP test exists, and Theorem 15.4.1 does not lead to an asymptotically UMP test sequence for the original problem. However, we will obtain an optimality result based on the maximin approach. Indeed, for this limiting normal model, an optimal maximin test exists, which allows one to construct an asymptotically maximin test sequence.

In order to have a nondegenerate asymptotically maximin procedure, it is necessary to consider alternatives at some distance from the null hypothesis, just as in the finite-sample maximin theory. When testing based on \(n\) i.i.d. observations, this distance must shrink with \(n\), in order to avoid a degenerate asymptotic theory, since there will typically exist test sequences whose asymptotic power tends to one uniformly over alternatives whose distance from \(\theta_{0}\) is fixed. It is convenient to consider this fixed distance as given by \(|I^{1/2}(\theta_{0})(\theta-\theta_{0})|\), where \(|\cdot|\) denotes the usual Euclidean norm of a vector in \(\hbox{\rm I\kern-1.8ptR}^{k}\). For q.m.d. models, it will be seen that it is necessary to let this distance shrink at rate \(n^{-1/2}\) in order to obtain a limiting minimum power greater than \(\alpha\) and less than \(1\).

In the following theorem, \(c_{k,1-\alpha}\) denotes the upper \(1-\alpha\) quantile of the Chi-squared distribution with \(k\) degrees of freedom.

**Theorem 15.5.4**: _Assume \(X_{1},\ldots,X_{n}\) are i.i.d. \(P_{\theta}\), where \(\theta\) varies in an open subset \(\Omega\) of \(\hbox{\rm I\kern-1.8ptR}^{k}\). Assume this family is q.m.d. at \(\theta_{0}\) with positive definite Information matrix \(I(\theta_{0})\). The problem is to test the null hypothesis \(\theta=\theta_{0}\) against \(\theta\neq\theta_{0}\). Let \(\phi_{n}=\phi_{n}(X_{1},\ldots,X_{n})\) be any sequence of tests such that \(E_{\theta_{0}}(\phi_{n})\to\alpha\). Then, for any \(b>0\),_\[\limsup_{n\to\infty}\inf\{E_{\theta_{0}+hn^{-1/2}}(\phi_{n}):|I^{1/2}(\theta_{0})h| \geq b\}\leq P\{\chi_{k}^{2}(b^{2})\geq c_{k,1-\alpha}\}\, \tag{15.91}\]

_where \(\chi_{k}^{2}(b^{2})\) denotes a random variable that has the noncentral Chi-squared distribution with \(k\) degrees of freedom and noncentrality parameter \(b^{2}\)._

Proof. Denote by \(\beta_{n}(h)\) the rescaled power function of \(\phi_{n}\), i.e.,

\[\beta_{n}(h)\equiv E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\.\]

By assumption, \(\beta_{n}(0)\to\alpha\). Denote by \(R=R(\alpha,\,b)\) the right-hand side of (15.91). Now, argue by contradiction; that is, assume for the test sequence \(\phi_{n}\) and some subsequence \(\{n_{j}\}\),

\[\lim_{n_{j}\to\infty}\inf\{\beta_{n_{j}}(h):\ |I^{1/2}(\theta_{0})h|\geq b\}>R+ \epsilon\quad\mbox{for some $\epsilon>0$}\.\]

Then, by Theorem 15.4.1, there exists a further subsequence \(n_{j_{n}}\) such that

\[\beta_{n_{j_{n}}}(h)\to\,\beta(h)\]

for every \(h\), where \(\beta(h)\) corresponds to a level-\(\alpha\) test of \(h=0\) versus \(h\neq 0\) in the (limiting) experiment consisting of observing an \(X\) which is \(N(h,\,I^{-1}(\theta_{0}))\). Thus, \(\beta(h)>R+\epsilon\) for every \(h\) such that \(|I^{1/2}(\theta_{0})h|\geq b\), which implies

\[\inf\{\beta(h):\ |I^{1/2}(\theta_{0})h|\geq b\}\geq R+\epsilon\.\]

This is a contradiction, since \(R\) is the maximin power for testing \(h=0\) versus \(|I^{1/2}(\theta_{0})h|\geq b\) based on \(X\) (Problem 8.34).

We first illustrate the theorem in the case \(k=1\).

**Example 15.5.7** (Simple vs. Two-sided Alternative): Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. \(P_{\theta},\,\theta\in{\rm I\kern-1.8ptR}\). Consider testing \(\theta=\theta_{0}\) versus \(\theta\neq\theta_{0}\). Assume the family is q.m.d. at \(\theta_{0}\). Let \(\phi_{n}\) be any test sequence satisfying \(E_{\theta_{0}}(\phi_{n})\to\alpha\). By Theorem 15.5.4 with \(d=I^{-1/2}(\theta_{0})b\), an upper bound for the limiting maximin power over the complement of shrinking neighborhoods is given by

\[\limsup_{n}\inf\{E_{\theta_{0}+hn^{-1/2}}(\phi_{n}):\ |h|\geq d\}\leq P\{\chi_{1}^{2}(I(\theta_{0})d^{2})\geq c_{1,1-\alpha}\}\.\]

In the one-sided case, an AUMP level-\(\alpha\) test (15.43) rejects for large values of the score statistic \(Z_{n}\) given by (15.42). Consider the two-sided version \(\phi_{n,2}\) of this test which rejects when \(I^{-1}(\theta_{0})Z_{n}^{2}>c_{1,1-\alpha}\). Since \(I^{-1}(\theta_{0})Z_{n}^{2}\) is asymptotically Chi-squared with one degree of freedom, this test is consistent in level. Moreover, its power function satisfies, for any \(0<d<D<\infty\),\[\inf[P_{\theta_{0}+hn^{-1/2}}\{I^{-1}(\theta_{0})Z_{n}^{2}>c_{1,1-\alpha}\}:\ d\leq h \leq D]\]

\[\to\ P\{\chi_{1}^{2}(I(\theta_{0})d^{2})\geq c_{1,1-\alpha}\}\.\]

To see why, the convergence (15.44) implies that, under \(\theta_{n}=\theta_{0}+h_{n}n^{-1/2}\),

\[I^{-1}(\theta_{0})Z_{n}^{2}\stackrel{{ d}}{{\to}}\chi_{1}^{2}(I( \theta_{0})h^{2})\.\]

If (15.92) failed, there would exist \(h_{n}\) satisfying \(h_{n}\to h\in[d,D]\) such that the limiting power of \(\phi_{n,2}\) against \(\theta_{n}\) tends to

\[P\{\chi_{1}^{2}(I(\theta_{0})h^{2})>c_{1,1-\alpha}\}<P\{\chi_{1}^{2}(I(\theta_ {0})d^{2})>c_{1,1-\alpha}\}\.\]

But, this last inequality is a contradiction since \(h\geq d\) and the family of \(\chi_{1}(\psi^{2})\) with \(\psi^{2}\) varying has monotone likelihood ratio (see Problem 7.4). It is typically possible to prove the stronger result with \(D\) in (15.92) replaced by \(\infty\). This technical issue is the same as encountered in the one-sided case in Section 15.3 when determining whether or not Rao's score test is not only LAUMP but AUMP; see Theorem 15.3.3 (iv). For an alternative asymptotic optimality approach in the two-sided case, see Problem 15.57.

By a similar argument, we can prove the following optimality result for Rao's test in the general \(k\) multi-sided testing problem. Analogous results hold for both the Wald and likelihood ratio tests (Problem 15.59).

**Theorem 15.5.5**: _Assume the conditions of Theorem 15.5.4. For testing \(\theta=\theta_{0}\) versus \(\theta\neq\theta_{0}\), consider the test \(\phi_{n}^{*}\) that rejects when \(Z_{n}^{\top}I^{-1}(\theta_{0})Z_{n}>c_{k,1-\alpha}\). Then, \(E_{\theta_{0}}(\phi_{n}^{*})\to\alpha\) and for any \(b\) and \(B\) satisfying \(0<b<B<\infty\),_

\[\inf\{E_{\theta_{0}+hn^{-1/2}}(\phi_{n}^{*}):\ b\leq|I^{1/2}(\theta_{0})h|\leq B \}\to P\{\chi_{k}^{2}(b^{2})\geq c_{k,1-\alpha}\}\.\]

Proof. First suppose \(h_{n}\to h\) with \(h\) satisfying \(|I^{1/2}(\theta)h|\geq b\). By the Continuous Mapping Theorem, under \(\theta_{0}+h_{n}n^{-1/2}\), Corollary 14.4.1 implies that

\[Z_{n}^{\top}I^{-1}(\theta_{0})Z_{n}\stackrel{{ d}}{{\to}}\chi_{k} ^{2}(|I^{1/2}(\theta_{0})h|^{2})\.\]

Hence, the limiting power of \(\phi_{n}^{*}\) against such a sequence is

\[P\{\chi_{k}^{2}(|I^{1/2}(\theta_{0})h|^{2})\geq c_{k,1-\alpha}\}\geq P\{\chi_{ k}^{2}(b^{2})\geq c_{k,1-\alpha}\}\,\]

where the last inequality follows since the family of noncentral Chi-squared distributions with fixed degrees of freedom and varying noncentrality parameter has monotone likelihood ratio. Now, if the result (15.93) were false, there would exist a sequence \(h_{n}\) satisfying \(b\leq|I^{1/2}(\theta_{0})h|\leq B\) and such that the limiting power of \(\phi_{n}^{*}\) under \(h_{n}\) is less than the right-hand side of (15.94). But, \(h_{n}\) lies in a compact set, so we can extract a further subsequence \(h_{n_{j}}\) (if necessary) so that \(h_{n_{j}}\) converges. Applying the argument leading to (15.94) to such a subsequence results in a contradiction.

We will later apply these results to obtain some asymptotically maximin tests of goodness of fit in Sections 16.3 and 16.4.

Note that the construction of asymptotically optimal tests in the multi-sided case depends on the existence of an optimal test for testing the mean vector \(h=0\) when \(X\sim N(h,\,I^{-1}(\theta_{0}))\) and \(I^{-1}(\theta_{0})\) is a known nonsingular covariance matrix. For this problem, if the alternatives are specified by \(|I^{1/2}(\theta_{0})h|\geq b\), then the maximin test rejects for large values of \(X^{\top}\Sigma^{-1}(\theta_{0})X\). But, the maximin optimality of this test need not hold if the alternative parameter space is specified differently; see Problem 8.35. Moreover, if \(C\) is any closed, convex set in \({\rm I\kern-1.8ptR}^{k}\), then the test that accepts if and only if \(X\in C\) is admissible; see Problem 6.41. Thus, the optimality of the maximin test is not so compelling, particularly when \(k>1\).

### Applications to Nonparametric Models

#### Nonparametric Mean

Let \(X_{1},\ldots,\,X_{n}\) be i.i.d. with c.d.f. \(F\), mean \(\mu(F)\) and variance \(\sigma^{2}(F)\). Assume \(F\in\tilde{\bf F}\), where \(\tilde{\bf F}\) satisfies (13.40). We now would like to derive an optimality property of the \(t\)-test for the mean in a nonparametric setting. Theorem 13.4.3 implies that the power of the \(t\)-test is bounded away from \(\alpha\) for distributions \(F\) whose standardized mean \(n^{1/2}\mu(F)/\sigma(F)\) is bounded away from \(0\). It is then of interest to measure a test sequence by its maximin power over such alternatives, with the goal of finding the test that asymptotically maximizes the minimum power over such alternatives. Consider testing \(\mu(F)=0\) against the alternatives \(\mu(F)/\sigma(F)\geq\delta/n^{1/2}\). By Theorem 13.4.3, the limiting minimum power of the \(t\)-test is \(1\,-\,\Phi(z_{1-\alpha}-\delta)\). We now show that this is indeed the optimal limiting maximin power in a nonparametric setting.

If the unknown family of distributions \(\tilde{\bf F}\) contains the family \(N(\theta,\,1)\) for \(\theta\geq 0\), then an optimality result is easy to obtain. Indeed, for any sequence of test functions \(\phi_{n}=\phi_{n}(X_{1},\ldots,\,X_{n})\) which satisfies \(E_{F}(\phi_{n})\to\alpha\) for any \(F\in\tilde{\bf F}\) with mean \(0\), we have

\[\limsup_{n}\inf_{\{F\in\tilde{\bf F},\ \mu(F)/\sigma(F)\geq\delta n^{-1/2}\}}E_{ F}(\phi_{n})\]

\[\leq\limsup_{n}E_{F=N(\delta n^{-1/2},1)}(\phi_{n})=1-\Phi(z_{1-\alpha}-\delta)\,\]

since the right-hand side is the optimal limiting power for testing \(\theta=0\) versus \(\theta=\delta/n^{1/2}\) in the normal location model \(N(\theta,\,1)\). Hence, the \(t\)-test is asymptotically maximin since its limiting minimum power attains this bound.

If the family of distributions \(\tilde{\bf F}\) does not contain the normal distributions, the above argument does not work. For example, suppose we consider distributions supported

[MISSING_PAGE_EMPTY:2366]

Hence, we have shown that

\[\limsup_{n}\inf_{F\in{\bf F}_{0},\ \pi^{1/2}\mu(F)/\sigma(F)\geq\delta}E_{F}(\phi_{n})\leq 1-\Phi(z_{1-\alpha}-\delta)\.\]

But, the \(t\)-test attains the right-hand side, and so is asymptotically maximin.

Of course, one can obtain a bound using other parametric submodels. The family \(p_{\theta}\) chosen above certainly works in that it yields an optimality result for the \(t\)-test. To gain some insight into why this family works, let us consider the more general family of densities with densities

\[p_{T,\theta}(x)=\exp[\theta T(x)-C_{T}(\theta)]\]

with respect to \(F\). This assumes that the function \(T(x)\) is bounded on \([-1,1]\), or at least that \(T(X)\) has a moment generating function if \(X\) has distribution \(F\). Let

\[\mu_{T,\theta}=\int xp_{T,\theta}(x)dF(x)\]

and \(\sigma_{T,\theta}^{2}\) be the variance of \(p_{T,\theta}\). The functions \(\mu_{T,\theta}\) and \(\sigma_{T,\theta}\) are infinitely differentiable in \(\theta\). Then,

\[\frac{\partial\mu_{T,\theta}}{\partial\theta}=\int x[T(x)-C_{T}^{{}^{\prime}}(\theta)]p_{T,\theta}(x)\,\]

so that

\[\mu_{T,0}^{{}^{\prime}}=\int x[T(x)-C_{T}^{{}^{\prime}}(0)]dF(x)=Cov_{F}[X,T(X)]\.\]

Then,

\[\mu_{T,\theta}=\theta Cov_{F}[X,T(X)]+o(\theta)\]

and

\[\sigma_{T,\theta}=\sigma(F)+o(\theta)\]

as \(\theta\to 0\). Hence,

\[\frac{\mu_{T,\theta}}{\sigma_{T,\theta}}=\frac{\theta Cov_{F}[X,T(X)]}{\sigma( F)}+o(\theta)\]

as \(\theta\to 0\). Assume \(Cov_{F}[X,T(X)]\neq 0\), in which case we may assume without loss of generality that it is positive (or replace \(T\) with \(-T\)). Let \(\theta_{n}\) be any fixed sequence with \(n^{1/2}\theta_{n}>\delta\sigma(F)/Cov_{F}[X,T(X)]\) and

\[n^{1/2}\theta_{n}\to\delta\sigma(F)/Cov_{F}[X,T(X)]\.\]

Then,\[n^{1/2}\frac{\mu_{T,\theta_{n}}}{\sigma_{T,\theta_{n}}}=n^{1/2}\theta_{n}\frac{Cov_ {F}[X,T(X)]}{\sigma(F)}+o(1)\.\]

So, \(n^{1/2}\mu_{T,\theta_{n}}/\sigma_{T,\theta_{n}}>\delta\) for all sufficiently large \(n\). Thus, for any test sequence \(\phi_{n}\),

\[\limsup_{n}\inf_{F\in\mathbf{F}_{0},\ n^{1/2}\mu(F)/\sigma(F)\geq\delta}E_{F}( \phi_{n})\leq E_{T,\theta_{n}}(\phi_{n})\,\]

where \(E_{T,\theta_{n}}\) denotes expectation with respect to \(p_{T,\theta_{n}}\). Note that, for this model, the Information at \(\theta=0\) satisfies

\[I_{T}(0)=C_{T}^{\prime\prime}(0)=Var_{F}[T(X)]\.\]

The best limiting power among asymptotically level-\(\alpha\) tests of \(\theta=0\) versus \(\theta=\theta_{n}\) was obtained in Theorem 15.3.2 (with \(h=\delta\sigma(F)/Cov_{F}[X,T(X)]\)) as

\[1-\Phi(z_{1-\alpha}-h\,I_{T}^{1/2}(0))=1-\Phi(z_{1-\alpha}-\delta\sigma(F)I_{ T}^{1/2}(0)/Cov_{F}[X,T(X)])\.\]

This reduces to the previous bound in the case \(T(X)=X\). The sharpest possible result is obtained by choosing \(T\) to minimize the right-hand side, which is equivalent to maximizing

\[\frac{Cov_{F}[X,T(X)]}{\{Var_{F}(X)Var_{F}[T(X)]\}^{1/2}}\.\]

By the Cauchy-Schwarz inequality, this is bounded above by 1, and the resulting value of 1 is attained when \(X=T(X)\).

Thus, in some sense, the model with \(T(X)=X\) is least favorable in that it is the hardest parametric submodel to achieve high (limiting) power. The idea of using a parametric submodel to obtain efficiency results in nonparametric models dates back to Stein (1956b).

#### Nonparametric Testing of Functionals

Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. \(P\in\mathbf{P}\). In this section, the family \(\mathbf{P}\) is a nonparametric family. Specifically, we would like to consider problems where we do not assume much or anything about \(P\). Thus, \(\mathbf{P}\) could be the family of _all_ distributions on some sample space \(S\), but it might be restricted by moment or smoothness conditions, in which case \(\mathbf{P}\) is still quite large.

Let \(\theta(\cdot)\) be a statistical functional; that is, \(\theta(P)\) is a real-valued function of \(P\), defined for \(P\in\mathbf{P}\). For example, if \(P\) is a distribution on \(\mathbf{R}\), \(\theta(P)\) could be the mean of \(P\), or the variance of \(P\). In such cases, \(\mathbf{P}\) could be the set of all distributions with finite variance. Or, if \(P\) is a distribution on \(\mathbf{R}^{2}\), \(\theta(P)\) might be the correlation of \(P\), defined on the set \(\mathbf{P}\) of all distributions whose marginals have a finite nonzero variance.

We wish to test the null hypothesis \(\theta(P)\leq 0\) against \(\theta(P)>0\). Fix \(P\) with \(\theta(P)=0\). In order to assess the power of a test at some distribution \(Q\) near \(P\), we will consider parametric submodels that contain \(P\). The basic idea is that the power attainable in the full nonparametric model can be no greater than for any submodel.

Let \(L^{2}(P)\) denote the space of (equivalence classes of) functions \(u\) which are square integrable with respect to \(P\). The inner product is given by

\[\langle u,\,v\rangle_{P}=\int u(x)v(x)d\,P(x),\]

and \(|u|_{P}^{2}=\langle u,\,u\rangle_{P}\). Also, let \(L^{2}_{0}(P)\) denote the subset of \(u\in L^{2}(P)\) satisfying \(\int u(x)dP(x)=0\). By Problem 14.6, if \(u\in L^{2}_{0}(P)\), we can construct a one-dimensional q.m.d. family \(P_{u,t}\) indexed by \(t\) in some neighborhood of \(0\), such that \(P_{u,0}=P\) and the score function at \(t=0\) is \(u\). For example, if \(u\) is bounded and \(|t|\leq[\sup_{x}|u(x)|]^{-1}\), then we can take \(P_{u,t}\) to be the distribution with density with respect to \(P\) given by

\[\frac{dP_{u,t}}{dP}(x)=1+tu(x). \tag{15.96}\]

(Note that \(P_{u,t}\in\mathbf{P}\) if \(\mathbf{P}\) is the set of all probabilities on \(S\), but if there are restrictions on \(P\), this construction may not work.)

In order to test \(\theta(P_{u,t})\) along such a parametric submodel, we assume that \(\theta(\cdot)\) is differentiable in the sense

\[\frac{\theta(P_{u,t})-\theta(P)}{t}\to\langle u,\,\tilde{\theta}_{P}\rangle_{P }\quad\mbox{as $t\to 0$ }, \tag{15.97}\]

for some function \(\tilde{\theta}_{P}\in L^{2}(P)\). Evidently, this condition implies that, as a real-valued function of the real variable \(t\), \(\theta(P_{u,t})\) is differentiable at \(t=0\).4 Note that, if \(\tilde{\theta}_{P}\) satisfies (15.97), then so does \(\tilde{\theta}_{P}+c\) for any constant \(c\); we will henceforth assume \(\int\tilde{\theta}_{P}(x)dP(x)=0\).

Footnote 4: The condition (15.97) further asserts that, as a function of \(u\), the limiting value on the right side of (15.97) is linear in \(u\) as \(u\) varies in \(L^{2}_{0}(P)\). In fact, the Riesz representation theorem (see Theorem 6.4.1 of Dudley (1989)) asserts that any linear function of \(u\) must be of the form \(\langle u,\,\tilde{\theta}\rangle_{P}\) for some \(\tilde{\theta}\).

**Example 15.6.1** (**Linear Functionals**) A statistical functional is _linear_ if it can be represented as

\[\theta(P)=\int f(x)dP(x) \tag{15.98}\]

for some function \(f\in L^{2}(P)\). In this case, if \(P_{u,t}\) is given by (15.96), then

\[\frac{\theta(P_{u,t})-\theta(P)}{t}=\langle u,\,f\rangle_{P}\]with no error term; that is,

\[\tilde{\theta}_{P}(x)=f(x)-\int f(x)dP. \tag{15.99}\]

Even if \(P_{u,t}\) is not specifically of the form (15.96), then it can be shown that \(\theta(P)\) is differentiable in the sense of (15.97) with \(\tilde{\theta}_{P}\) given by (15.99) if

\[\sup_{P\in{\bf P}}E_{P}[f^{2}(X)]<\infty\ ;\]

see Bickel et al. (1993, pp. 457-458). In particular, if \(f\) is a bounded function on a set \(S\) and \({\bf P}\) is the set of all probabilities on \(S\), then \(\theta(\cdot)\) is differentiable in the sense of (15.97).

Next, for testing \(\theta(P)\leq 0\) against \(\theta(P)>0\), we obtain an upper bound for the limiting local power function along a one-dimensional q.m.d. submodel, among tests that are pointwise consistent in level. Note that, under (15.97),

\[\theta(P_{u,t})=\theta(P)+t\langle\tilde{\theta}_{P},u\rangle_{P}+o(t)\quad \mbox{as}\ t\to 0\, \tag{15.100}\]

which implies \(\theta(P_{u,t})>0\) for all small \(t>0\) if \(\langle\tilde{\theta}_{P},u\rangle_{P}>0\).

By Lemma 15.3.1(ii), if \(h>0\) and \(\langle\theta_{P},u\rangle_{P}>0\), then (Problem 15.62)

\[\limsup_{n}E_{P_{u,h(n,\delta)n^{-1/2}}}(\phi_{n})\leq 1-\Phi(z_{1-\alpha}-h| u|_{P}). \tag{15.101}\]

Fix \(\delta>0\) and let

\[h=h(u,\delta)=\frac{\delta}{\langle\tilde{\theta}_{P},u\rangle_{P}}\ ;\]

then, \(n^{1/2}\theta(P_{u,h(n,\delta)n^{-1/2}})\to\delta\). The bound (15.101) at \(h(u,\delta)n^{-1/2}\) becomes

\[\limsup_{n}E_{P_{u,h(n,\delta)n^{-1/2}}}(\phi_{n})\leq 1-\Phi\left(z_{1- \alpha}-\frac{\delta|u|_{P}}{\langle\tilde{\theta}_{P},u\rangle_{P}}\right). \tag{15.102}\]

As \(u\) varies, the bound is smallest when \(|u|_{P}/\langle\tilde{\theta}_{P},u\rangle_{P}\) is minimized. But, by Cauchy-Schwarz,

\[\frac{|u|_{P}}{\langle\tilde{\theta}_{P},u\rangle_{P}}\geq\frac{1}{|\tilde{ \theta}_{P}|_{P}}\,\]

and equality occurs when \(u=\tilde{\theta}_{P}\). Note that, when \(u=\tilde{\theta}_{P}\), the bound (15.102) becomes

\[1-\Phi\left(z_{1-\alpha}-\frac{\delta}{|\tilde{\theta}_{P}|_{P}}\right). \tag{15.103}\]

[MISSING_PAGE_FAIL:769]

\[Z_{1}\langle u,\tilde{\theta}_{P}\rangle_{P}+Z_{2}\langle v,\tilde{\theta}_{P} \rangle_{P}>z_{1-\alpha}\sqrt{\langle u,\tilde{\theta}_{P}\rangle_{P}^{2}+ \langle v,\tilde{\theta}_{P}\rangle_{P}^{2}|v|_{P}^{2}}=z_{1-\alpha}|\tilde{ \theta}_{P}|_{P}\,\]

which has power with \(h_{1}=h\) and \(h_{2}=0\) given by the right side of (15.104).

**Remark 15.6.1**: The tests \(\phi_{n}\) need not be exact level-\(\alpha\). All that is required is that \(\lim\sup_{n}E_{P_{u,hn^{-1/2}}}(\phi_{n})\leq\alpha\) if \(h\) has the opposite sign of \(\langle\tilde{\theta}_{P},u\rangle_{P}\). This must hold for \(u\) in the statement of (15.104) as well as any linear combination of \(u\) and \(\tilde{\theta}_{P}\).

The result and the proof applies even if \(\mathbf{P}\) is not the set of all probabilities on \(S\). What is required is that the two-dimensional model \(P_{u,v,t_{1},t_{2}}\) used in the proof also belongs to \(\mathbf{P}\). Also, it only requires that the differentiability condition need only hold for submodels \(P_{u,v,h_{1}t,h_{2}t}\). For semiparametric models, the result needs to be modified, but a similar result holds; see Theorem 25.44 of van der Vaart (1988).

Next, we consider tests whose power attains the bound (15.104).

**Example 15.6.2**: (**Linear Functionals, continued**) Let \(\hat{P}_{n}\) be the empirical measure, i.e., \(\hat{P}_{n}\{E\}\) is the proportion of observations that fall in \(E\). Then, tests of \(\theta(P)\) can be based on \(\theta(\hat{P}_{n})=n^{-1}\sum_{i}f(X_{i})\). Under \(\theta(P)=0\),

\[n^{1/2}\theta(\hat{P}_{n})\stackrel{{ d}}{{\to}}N(0,|f|_{P}^{2})\.\]

Since \(|f|_{P}\) is unknown, consider the test that rejects when \(n^{1/2}\theta(\hat{P}_{n})/S_{n}>z_{1-\alpha}\), where

\[S_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}[f(X_{i})-\theta(\hat{P}_{n})]^{2}\.\]

Under \(P\), \(S_{n}^{2}\stackrel{{ P}}{{\to}}|f|_{P}^{2}\); by contiguity, this holds under \(P_{u,hn^{-1/2}}^{n}\) as well. By Example 14.3.8, under \(P_{u,hn^{-1/2}}^{n}\),

\[n^{1/2}\theta(\hat{P}_{n})\stackrel{{ d}}{{\to}}N(h(f,u)_{P},|f| _{P}^{2})\.\]

By Slutsky's Theorem, under \(P_{u,hn^{-1/2}}\),

\[n^{1/2}\theta(\hat{P}_{n})/S_{n}\stackrel{{ d}}{{\to}}N(h\frac{ \langle f,u\rangle_{P}}{|f|_{P}},1)\.\]

Therefore, the limiting power of the above test against \(P_{u,hn^{-1/2}}^{n}\) is the upper bound (15.104). Moreover, the convergence to the limiting power is uniform in \(h\) for \(0\leq h\leq c\) and any \(c>0\) (Problem 15.64). The resulting test is locally AUMP against all such alternatives. For example, the result applies to one-sided tests of \(\theta(P)=P\{E\}\), and tests based on the empirical measure are asymptotically LAUMP.

**Example 15.6.3**: (**Variance Functional**) Suppose \(P\) is a distribution on \(\mathbb{R}\), and \(\mathbf{P}\) is the set of all distributions with a uniformly bounded fourth moment. Let \(\sigma^{2}(P)\)denote the variance of \(P\), and \(\mu(P)\) denote the mean of \(P\). The problem is to test \(\sigma^{2}(P)\leq\sigma_{0}^{2}\). Let \(\theta(P)=\sigma^{2}(P)-\sigma_{0}^{2}\). Then, the conditions of Theorem 15.6.1 hold with

\[\tilde{\theta}_{P}(x)=[x-\mu(P)]^{2}-\theta(P)\,\]

and the test that rejects when \(n^{1/2}[\theta(\hat{P}_{n})-\sigma_{0}^{2}]/S_{n}>z_{1-\alpha}\) attains the bound (15.104), where \(S_{n}^{2}\) is a consistent estimator of the variance of \([X_{i}-\mu(P)]^{2}\), such as

\[S_{n}^{2}=n^{-1}\sum_{i}(X_{i}-\bar{X}_{n})^{4}-\sigma^{4}(\hat{P}_{n})\.\]

The details are left to Problem 15.66.

In general, consider tests of \(\theta(P)\) based on \(\theta(\hat{P}_{n})\). This implicitly assumes \(\theta(\cdot)\) is defined for empirical measures. Suppose \(\theta(\hat{P}_{n})\) is an asymptotically linear statistic in the sense that

\[n^{1/2}[\theta(\hat{P}_{n})-\theta(P)]=\int\tilde{\theta}_{P}d(\hat{P}_{n}-P) +o_{P}(1). \tag{15.105}\]

This can be verified directly in examples where \(\theta(\hat{P}_{n})\) is a smooth function of sample means, such as the previous example. Otherwise, \(\theta\) must be differentiable in an appropriate sense, but such an approach is beyond the scope of the treatment here; see Serfling (1980, Chapter 10) or (1996, Section 3.9). Note that (15.105) implies that, under \(P\),

\[n^{1/2}[\theta(\hat{P}_{n})-\theta(P)]\stackrel{{ d}}{{\to}}N(0,| \tilde{\theta}_{P}|_{P}^{2})\.\]

In order to construct an optimal test, it is necessary to construct a consistent estimator of \(|\tilde{\theta}_{P}|_{P}\). Assuming \(S_{n}\) is such a consistent estimator, the test that rejects for large \(n^{1/2}\theta(\hat{P}_{n})/S_{n}\) is asymptotically LAUMP, by the same argument used in Example 15.6.2. General approaches for constructing an estimator of the asymptotic variance of \(n^{1/2}\theta(\hat{P}_{n})\), as well as a means of estimating its sampling distribution, are provided by bootstrap resampling and subsampling, which will be discussed in Chapter 18.

### Problems

#### Section 15.1

**Problem 15.1**: (i). Let \(P_{i}\) have density \(p_{i}\) with respect to a dominating measure \(\mu\). Show that \(\|P_{1}-P_{0}\|_{1}\) defined by \(\int|p_{1}-p_{0}|d\mu\) is independent of the choice of \(\mu\) and is a metric.

(ii). Show the Hellinger distance defined in (15.12) is also independent of \(\mu\) and is a metric.

**Problem 15.2**: Show that \(\|P_{1}-P_{0}\|_{1}\) can also be computed as

\[2\sup_{B}|P_{1}(B)-P_{0}(B)|\,\]

where the supremum is over all measurable sets \(B\). In addition, it may be computed as

\[\sup_{\{\phi:|\phi|\leq 1\}}\left|\int\phi(x)d\,P_{1}(x)-\int\phi(x)d\,P_{0}(x) \right|\,\]

where the supremum is over all measurable functions \(\phi\) such that \(\sup_{x}|\phi(x)|\leq 1\).

**Problem 15.3**: (i) Suppose \(X\) is a random variable taking values in a sample space \(S\) with probability law \(P\). Let \(\omega_{0}\) and \(\omega_{1}\) be disjoint families of probability laws. Assume that, for every \(Q\in\omega_{1}\) and any \(\epsilon>0\), there exists a subset \(A\) of \(S\) (which may depend on \(\epsilon\)) such that \(Q(A)\geq 1-\epsilon\) and such that, if \(X\) has distribution \(Q\), then the conditional distribution of \(X\) given \(X\in A\) is a distribution in \(\omega_{0}\); call it \(P_{\epsilon}\). Show \(\|Q-P_{\epsilon}\|_{1}\to 0\) as \(\epsilon\to 0\).

(ii) Based on data \(X\) with probability law \(P\), consider the problem of testing the null hypothesis \(P\in\omega_{0}\) versus \(P\in\omega_{1}\). Suppose that, for every \(Q\in\omega_{1}\), there exists a sequence \(\{P_{k}\}\) with \(P_{k}\in\omega_{0}\) such that \(\|Q-P_{k}\|_{1}\to 0\) as \(k\to\infty\). Show that if a test \(\phi\) is level-\(\alpha\), then \(E_{Q}[\phi(X)]\leq\alpha\) for all \(Q\in\omega_{1}\).

(iii) Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. on the real line. Let \(\omega_{0}\) be distributions with a finite mean and \(\omega_{1}\) those without a finite mean. Apply (i) and (ii) to show that no level-\(\alpha\) test of \(\omega_{0}\) versus \(\omega_{1}\) has power \(>\alpha\) against any \(Q\in\omega_{1}\).

[Such nonexistence results data back to Bahadur (1955); see Lemma 13.4.4. This example in (iii) and others are treated in Romano (2004), which also contains many references on such problems.]

**Problem 15.4**: Let \(P_{\theta}\) be uniform on \([0,\,\theta]\). Let \(\theta_{n}=\theta_{0}+h/n\). Calculate the limit of \(nH^{2}(P_{\theta_{0}},\,P_{\theta_{0}+h/n})\). If \(h>0\), let \(\phi_{n}\) be the UMP level-\(\alpha\) test which rejects when the maximum order statistic is too large. Evaluate the limit of the power of \(\phi_{n}\) against the alternative \(\theta_{n}\).

**Problem 15.5**: Prove Lemma 15.1.1.

**Problem 15.6**: Consider testing \(P_{\theta_{0}}^{n}\) versus \(P_{\theta_{n}}^{n}\) and assume \(nH^{2}(P_{\theta_{0}},\,P_{\theta_{n}})\to 0\). Let \(\phi_{n}\) be any test sequence such that \(\limsup E_{\theta_{0}}(\phi_{n})\leq\alpha\). Show that \(\limsup E_{\theta_{n}}(\phi_{n})\leq\alpha\).

**Problem 15.7**: Let \(P_{\theta}\) be \(N(\theta,\,1)\). Fix \(h\) and let \(\theta_{n}=hn^{-1/2}\). Compute \(S(P_{0}^{n},\,P_{\theta_{n}}^{n})\) and its limiting value. Compare your result with the upper bound obtained from Theorem 15.1.3.

**Problem 15.8**: If \(I(\theta_{0})\) is a positive definite Information matrix, show \(h=0\) if and only if \(\langle h,\,I(\theta_{0})h\rangle=0\).

**Problem 15.9**: Let \(X_{1},\ldots,\,X_{n}\) be i.i.d. according to a model \(\{P_{\theta},\,\theta\in\Omega\}\), where \(\theta\) is real-valued. Consider testing \(\theta=\theta_{0}\) versus \(\theta=\theta_{n}\) at level-\(\alpha\) (\(\alpha\) fixed, \(0<\alpha<1\)). Show that it is possible to have \(nH^{2}(P_{\theta_{0}},\,P_{\theta_{n}})\to c<\infty\) and still have a sequence of level-\(\alpha\) tests \(\phi_{n}=\phi_{n}(X_{1},\ldots,X_{n})\) such that \(E_{\theta_{n}}(\phi_{n})\to 1\). _Hint:_ Take \(P_{\theta}\) uniform on \([0,\,\theta]\) and \(\theta_{n}=\theta_{0}-h/n\) for \(h>0\).

**Problem 15.10**: Suppose \(\|P_{n}-Q_{n}\|_{1}\to 0\). Show that \(P_{n}\) and \(Q_{n}\) are mutually contiguous. Furthermore, show that, for any sequence of test functions \(\phi_{n}\), \(\int\phi_{n}d\,P_{n}-\int\phi_{n}d\,Q_{n}\to 0\).

**Problem 15.11**: For a q.m.d. family, show \(nH^{2}(P_{\theta_{0}+hn^{-1/2}},\,P_{\theta_{0}+hn^{-1/2}})\to 0\) whenever \(h_{n}\to h\). Then, show \(P_{\theta_{0}+h_{n}n^{-1/2}}^{n}\) is contiguous to \(P_{\theta_{0}}^{n}\) whenever \(h_{n}\to h\).

**Problem 15.12**: Use Problem 15.11 to show that Theorem 14.2.3 (i) remains valid if \(h\) is replaced by \(h_{n}\) as long as \(h_{n}\) falls in a bounded subset of \({\rm I\!R}^{k}\). Also, show part (ii) of Theorem 14.2.3 generalizes if \(h\) in the left-hand side of the convergence (14.14) is replaced by \(h_{n}\to h\) in \({\rm I\!R}^{k}\). Further assume (14.85) and show that, for any \(c>0\), the supremum over \(h\) such that \(|h|\leq c\) of the left side of (14.13) tends to \(0\) in probability under \(\theta_{0}\).

**Problem 15.13**: Use problem 15.11 to prove Theorem 14.4.1 when \(h_{n}\to h\).

**Problem 15.14**: Assume \(X_{1},\ldots,\,X_{n}\) are i.i.d. according to a family \(\{P_{\theta}\}\) which is q.m.d. at \(\theta_{0}\). Suppose, for some statistic \(T_{n}=T_{n}(X_{1},\ldots,\,X_{n})\) and some function \(\mu(\theta)\) assumed differentiable at \(\theta_{0},n^{1/2}(T_{n}-\mu(\theta_{n}))\stackrel{{ d}}{{\to}}N(0,\,\sigma^{2})\) under \(\theta_{n}\) whenever \(\theta_{n}=\theta_{0}+hn^{-1/2}\). Show the same result holds, first whenever \(h\) is replaced by \(h_{n}\to h\), and then whenever \(n^{1/2}(\theta_{n}-\theta_{0})=O(1)\).

**Problem 15.15**: Give an example where \(\|Q_{n}-P_{n}\|_{1}\to\delta>0\) but \(P_{n}\) and \(Q_{n}\) are mutually contiguous.

**Problem 15.16**: Let \(P_{n}\) and \(Q_{n}\) be two sequences of probability measures defined on \((\Omega_{n},\,{\cal F}_{n})\). Assume they are contiguous. Assume further that both of them are product measures, i.e.,

\[P_{n}=\prod_{i=1}^{n}P_{n,i}\quad\mbox{and}\quad Q_{n}=\prod_{i=1}^{n}Q_{n,i}\.\]

Let \(\|Q-P\|_{1}\) denote the total variation distance between \(P\) and \(Q\). Show that

\[\sup_{n}\sum_{i=1}^{n}\|Q_{n,i}-P_{n,i}\|_{1}^{2}<\infty\.\]

**Problem 15.17**: Let \(f(x)\) be the triangular density on \([-1,\,1]\) defined by

\[f(x)=(1-|x|)I\{x\in[-1,\,1]\}\.\]Let \(P_{\theta}\) be the distribution with density \(f(x-\theta)\). Find the asymptotic behavior of \(H(P_{\theta_{0}},\,P_{\theta_{0}+h})\) as \(h\to 0\), where \(H\) is the Hellinger distance. Compare your result with q.m.d. families.

**Problem 15.18**: Under the assumptions of Theorem 15.2.1, suppose \(\theta_{k}\to\theta_{0}\) and \(\beta>\alpha>0\). Show, for any \(N<\infty\), there does not exist a test \(\phi_{k}\) with \(k\leq N\) such that \(\lim\inf_{k}\,E_{\theta_{k}}(\phi_{k})\geq\beta\).

**Problem 15.19**: Under the assumptions of Example 15.2.1 show that the squared efficacy of the Wald test is \(I(\theta_{0})\).

**Problem 15.20**: Suppose \(\Omega_{0}=\{\theta_{0}\}\). In order to determine \(c=c(n,\alpha)\) in (15.32), define \(c(n,\alpha)\) to be

\[c(n,\alpha)=\inf\{d:\ P_{\theta_{0}}\{T_{n}>d\}\leq\alpha\}\.\]

Argue that this choice of \(c(n,\alpha)\) satisfies (15.32). What if \(T_{n}>d\) is replaced by \(T_{n}\geq d\)?

**Problem 15.21**: For a double exponential location family, calculate the Pitman AREs among pairwise comparisons of the \(t\)-test, the Wilcoxon test, and the Sign test.

**Problem 15.22**: Prove the inequality (15.30). _Hint:_ The quantity (15.29) is invariant with respect to scale. By taking \(\sigma^{2}=1\), the problem reduces to choosing \(f\) to minimize \(\int f^{2}\) subject to \(f\) being a mean \(0\) density with variance \(1\). Using the method of undetermined multipliers, it is sufficient to minimize

\[\int[f^{2}(x)+2b(x^{2}-a^{2})f(x)]dx\,\]

where \(a\) and \(b\) are chosen so that \(f\) is a mean \(0\) density with variance \(1\).

**Problem 15.23**: Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. Poisson with unknown mean \(\theta\). The problem is to test \(\theta=\theta_{0}\) versus \(\theta>\theta_{0}\). Consider the test that rejects for large \(\bar{X}_{n}\) and the test that rejects for large

\[S_{n}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}\.\]

Compute the Pitman ARE.

**Problem 15.24**: Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. \(N(0,\sigma^{2})\). Let \(T_{n,1}=\bar{Y}_{n}=n^{-1}\sum_{i=1}^{n}Y_{i}\), where \(Y_{i}=X_{i}^{2}\). Also, let \(T_{n,2}=(2n)^{-1}\sum_{i=1}^{n}(Y_{i}-\bar{Y}_{n})^{2}\). For testing \(\sigma=1\) versus \(\sigma>1\), does the Pitman asymptotic relative efficiency of \(T_{n,1}\) with respect to \(T_{n,2}\) exist? If so, find it.

#### Section 15.3

**Problem 15.25**: For testing \(\theta=\theta_{0}\) versus \(\theta>\theta_{0}\), define two test sequences \(\phi_{n}\) and \(\psi_{n}\) to be _asymptotically equivalent under the null hypothesis_ if \(\phi_{n}-\psi_{n}\to 0\) in probability under \(\theta_{0}\). Does this imply that, if \(\theta_{0}\) is the true value, the probability the tests reach the same conclusion tends to 1? Show that, under q.m.d., asymptotic equivalence under the null hypothesis also implies that, under an alternative sequence \(\theta_{n,h}=\theta_{0}+hn^{-1/2}\),

\[E_{\theta_{n,h}}(\phi_{n})-E_{\theta_{n,h}}(\psi_{n})\to 0\.\]

Furthermore, assume at least one of the two, say \(\phi_{n}\) is nonrandomized. Then, conclude the tests are _asymptotically equivalent_ in the sense that the probability the tests reach the same conclusion tends to 1, both under \(\theta_{0}\) and a sequence \(\theta_{n,h}\).

**Problem 15.26**: Under the q.m.d. assumptions of this section, show that \(\phi_{n,h}\) given by (15.34) and \(\tilde{\phi}_{n}\) given by (15.43) are asymptotically equivalent in the sense of Problem 15.25 for testing \(\theta_{0}\) against \(\theta_{0}+hn^{-1/2}\).

**Problem 15.27**: Let \(X_{1}\),..., \(X_{n}\) be i.i.d. \(N(\theta,1)\). For testing \(\theta=0\) against \(\theta>0\), let \(\phi_{n}\) be the UMP level-\(\alpha\) test. Let \(\tilde{\phi}_{n}\) be the test which rejects if \(\tilde{X}_{n}\geq b_{n}/n^{1/2}\) or \(\tilde{X}_{n}\leq-a_{n}/n^{1/2}\), where \(b_{n}=z_{1-\alpha}+n^{-1/4}\) and \(a_{n}\) is then determined to meet the level constraint. Are the tests asymptotically equivalent? Show that, for all \(\theta\geq 0\),

\[\frac{1-E_{\theta}(\phi_{n})}{1-E_{\theta}(\tilde{\phi}_{n})}\to 0\ \ \ {\rm as}\ n\to\infty\.\]

How do you interpret this result? [Lehmann (1949)]

**Problem 15.28**: Prove Lemma 15.3.1 (iii). _Hint:_ Problems 15.12-15.13.

**Problem 15.29**: Prove Theorem 15.3.1.

**Problem 15.30**: Prove the equivalence of Definition 15.3.2 and the definition in the statement immediately following Definition 15.3.2. What is an equivalent characterization for LAUMP tests?

**Problem 15.31**: For testing \(\theta_{0}\) versus \(\theta_{n}\), let \(\phi_{n}^{*}\) be a test satisfying

\[\limsup_{n}E_{\theta_{0}}(\phi_{n}^{*})=\alpha^{*}<\alpha\]

and \(E_{\theta_{n}}(\phi_{n}^{*})\to\beta^{*}\).

(i) Show there exists a test sequence \(\psi_{n}\) satisfying \(\lim\sup_{n}E_{\theta_{0}}(\psi_{n})=\alpha\) and a number \(\beta\) such that

\[\lim E_{\theta_{n}}(\psi_{n})=\beta\geq\beta^{*}\,\]

and this last inequality is strict unless \(\beta^{*}=1\).

(ii) Hence, show that, under the conditions of Theorem 15.3.3, any LAUMP level-\(\alpha\) test sequence \(\phi_{n}^{*}\) satisfies \(E_{\theta_{0}}(\phi_{n}^{*})\to\alpha\).

**Problem 15.32**: Suppose \(Z_{n}\) is any sequence of random variables such that \(Var_{\theta_{n}}(Z_{n})\leq 1\) while \(E_{\theta_{n}}(Z_{n})\to\infty\). Here, \(\theta_{n}\) merely indicates the distribution of \(Z_{n}\) at time \(n\). Show that, under \(\theta_{n}\), \(Z_{n}\to\infty\) in probability.

**Problem 15.33**: In the double exponential location model of Example 15.3.2, show that a MLE estimator is a sample median \(\hat{\theta}_{n}\). The test that rejects the null hypothesis if \(n^{1/2}\hat{\theta}_{n}>z_{1-\alpha}\) is AUMP and is asymptotically equivalent to Rao's score test in the sense of Problem 15.25.

**Problem 15.34**: For the Cauchy location model of Example 15.3.3, consider the estimator \(\hat{\theta}_{n}\) defined by (15.59). Show that the test that rejects when \(n^{1/2}\hat{\theta}_{n}>2^{1/2}z_{1-\alpha}\) is AUMP. Is the estimator location equivariant? Is the estimator \(\hat{\theta}_{n}=\hat{\theta}_{n}(X_{1},\ldots,X_{n})\) monotone in the sense it is nondecreasing as any one component \(X_{i}\) increases?

**Problem 15.35**: Let \(X_{1},\ldots,X_{n}\) be i.i.d. according to a q.m.d. location model \(f(x-\theta)\). Let \(\hat{\theta}_{n}\) be any location equivariant estimator satisfying (15.58) (such as an efficient likelihood estimator). For testing \(\theta\leq 0\) against \(\theta>0\), show that the test that rejects when \(n^{1/2}\hat{\theta}_{n}>I^{-1/2}(0)z_{1-\alpha}\) is AUMP.

**Problem 15.36**: Assume the conditions of Theorem 15.3.3. Assume \(\phi_{n}\) is LAUMP level-\(\alpha\). Suppose the power function of \(\phi_{n}\) is nondecreasing in \(\theta\), for \(\theta\geq\theta_{0}\). Show \(\phi_{n}\) is also AUMP level-\(\alpha\).

**Problem 15.37**: Assume the conditions of Example 15.3.1. Further assume \(f\) is strongly unimodal, i.e., \(-\log(f)\) is convex. Show the test \(\tilde{\phi}_{n}\) given by (15.43) is AUMP level-\(\alpha\). _Hint:_ Use Problem 15.36.

**Problem 15.38**: Suppose \(X_{1},...X_{n}\) are i.i.d. Poisson(\(\lambda\)). Consider testing the null hypothesis \(H_{0}:\lambda=\lambda_{0}\) versus the alternative, \(H_{A}:\lambda>\lambda_{0}\).

(i) Consider the test \(\phi_{n}^{1}\) with rejection region \(n^{1/2}[\tilde{X}_{n}-\lambda_{0}]>z_{1-\alpha}\lambda_{0}^{1/2}\), where \(\Phi(z_{\alpha})=\alpha\) and \(\Phi\) is the cdf of a standard normal random variable. Find the limiting power of this test against \(\lambda_{0}+hn^{-1/2}\).

(ii) Alternatively, let \(g\) be a differentiable, monotone increasing function with \(g^{\prime}(\lambda_{0})>0\), and consider the test \(\phi_{n}^{g}\) with rejection region

\[n^{1/2}[g(\tilde{X}_{n})-g(\lambda_{0})]>z_{1-\alpha}g^{\prime}(\lambda_{0}) \lambda_{0}^{1/2}\.\]

Show that \(\phi_{n}^{1}\) and \(\phi_{n}^{g}\) are equivalent in the sense that, for any \(b>0\),\[\sup_{0\leq h\leq b}E_{\lambda_{0}+hn^{-1/2}}|\phi_{n}^{1}-\phi_{n}^{g}|\to 0\.\]

(iii) Can we replace \(b\) by \(\infty\)?

**Problem 15.39**: Suppose \(X_{1}\),...\(X_{n}\) are i.i.d. \(N(\theta,1+\theta^{2})\). Consider testing \(\theta=\theta_{0}\) versus \(\theta>\theta_{0}\) and let \(\phi_{n}\) be the test that rejects when \(n^{1/2}[\tilde{X}_{n}-\theta_{0}]>z_{1-\alpha}(1+\theta_{0}^{2})^{1/2}\).

(i) Compute the limiting power of this test against \(\theta_{0}+hn^{-1/2}\).

(ii) Is this test AUMP?

**Problem 15.40**: Define appropriate extensions of the definitions of LAUMP and AUMP to two-sided testing of a real parameter. Let \(X_{1}\),..., \(X_{n}\) be i.i.d. \(N(\theta,1)\). Show that neither LAUMP nor AUMP tests exist for testing \(\theta=0\) against \(\theta\neq 0\).

**Problem 15.41**: Consider the regression model

\[Y_{i}=\beta x_{n,i}+\epsilon_{i}\,\quad i=1,\ldots,n\,\]

where \(\beta\in{\bf R}\) is unknown, the \(x_{n,i}\) are fixed and known, and the \(\epsilon_{i}\) are i.i.d. standard normal. Consider testing the null hypothesis \(\beta=0\) versus \(\beta>0\) using the UMP level-\(\alpha\) test.

(i) Let \(x_{n,i}=i/n\). For what constant \(p\) is the limiting local power function against alternatives of the form \(h/n^{p}\) not degenerate, i.e., strictly between \(0\) and \(1\). Calculate the optimal limiting local power function against such a sequence.

(ii) Repeat (i) in the case \(x_{n,i}=i\).

### _Section 15.4_

**Problem 15.42**: Suppose \(\{Q_{n,h},h\in{\rm I\kern-1.8ptR}^{k}\}\) is asymptotically normal according to Definition 15.4.1, with \(Z_{n}\) and \(C\) satisfying (15.62). Show the matrix \(C\) is uniquely determined. Moreover, if \(\tilde{Z}_{n}\) is any other sequence also satisfying (15.62), then \(Z_{n}-\tilde{Z}_{n}\to 0\) in \(Q_{n,h}\)-probability for any \(h\).

**Problem 15.43**: Suppose \(\{Q_{n,h},h\in{\rm I\kern-1.8ptR}^{k}\}\) is asymptotically normal. Show that \(Q_{n,h_{1}}\) and \(Q_{n,h_{2}}\) are mutually contiguous for any \(h_{1}\) and \(h_{2}\).

**Problem 15.44**: Assume \(\{Q_{n,h},h\in{\rm I\kern-1.8ptR}^{k}\}\) is asymptotically normal according to Definition 15.4.1, with \(Z_{n}\) and \(C\) satisfying (15.62). Show that, under \(Q_{n,h}\), \(Z_{n}\stackrel{{ d}}{{\to}}N(Ch,C)\).

**Problem 15.45**: Let \(dN(h,C)\) denote the density of the normal distribution with mean vector \(h\in{\rm I\kern-1.8ptR}^{k}\) and positive definite covariance matrix \(C\). Prove that \(\exp(\langle h,x\rangle-\frac{1}{2}\langle h,Ch\rangle)dN(0,C)(x)\) is the density of \(N(Ch,C)\) evaluated at \(x\). _Hint:_ Use characteristic functions.

### Section 15.5

**Problem 15.46**: In the location scale model of Example 15.5.2, verify the expressions for the Information matrix. Deduce that the matrix is diagonal if \(f\) is an even function.

**Problem 15.47**: For the location scale model of Example 15.5.2 with \(f(x)=C(\beta)\exp[-|x|^{\beta}]\), argue that the family is q.m.d. if \(\beta>1/2\).

**Problem 15.48**: For the location scale model in Problem 15.47, show that, for testing \(\mu\leq 0\) versus \(\mu>0\), argue that the Wald test is LAUMP if \(\beta\geq 1\). If \(\hat{\sigma}_{n}\) is replaced by any consistent estimator of \(\sigma\), does the LAUMP property continue to hold? If \(1/2<\beta<1\), argue that the Rao test is LAUMP.

**Problem 15.49**: In Example 15.5.3, for testing \(\rho\leq 0\) versus \(\rho>0\), find the optimal limiting power of the LAUMP against alternatives \(hn^{-1/2}\). Compare with the case where the means and variances are known. Generalize to the case of testing \(\rho\leq\rho_{0}\) against \(\rho>\rho_{0}\).

**Problem 15.50**: Derive the inequality (15.74) under general conditions which assume the model is asymptotically normal.

**Problem 15.51**: Assume (15.75) and the setup described there. Show that the test that rejects when \(g(\hat{\theta}_{n})>z_{1-\alpha}\hat{\sigma}_{n}\) is pointwise level-\(\alpha\) and has a power function such that there is equality in (15.74).

**Problem 15.52**: Verify (15.76) as well as the form of the matrix \(C(\theta_{0})\).

**Problem 15.53**: Assume the conditions of Theorem 15.5.1. Consider the problem of testing \(g(\theta)=0\) against \(g(\theta)\neq 0\). Restrict attention to tests \(\phi_{n}\) that are asymptotically unbiased in the sense

\[\liminf_{n}\inf_{\{\theta:\ g(\theta)\neq 0\}}E_{\theta}(\phi_{n})\geq\alpha\,\]

as well as (15.69). Prove a result analogous to Theorem 15.5.1. _Hint:_ See Problem 5.10.

**Problem 15.54**: Consider the one-sample \(N(\mu,\,1)\) problem for testing \(|\mu|\geq\Delta\) versus \(|\mu|<\Delta\). Show that the level-\(\alpha\) test based on combining the two one-sided UMP level-\(\alpha\) tests has size strictly less than \(\alpha\).

**Problem 15.55**: Show that the size of the TOST test considered in Example 15.5.5 is \(\alpha\).

**Problem 15.56**: Let \(C=C(\alpha,\delta,\sigma)\) be defined by (15.86). Show that \(C>\delta-\sigma z_{1-\alpha}\). Use this to show that, in Example 15.5.6, the limiting power of \(\phi_{n}^{*}\) always exceeds that of \(\phi_{n}^{IVT}\).

**Problem 15.57**: As in Example 15.5.7, consider testing \(\theta=\theta_{0}\) versus \(\theta\neq\theta_{0}\). Suppose \(\phi_{n}\) is asymptotically level-\(\alpha\) and asymptotically unbiased in the sense

\[\liminf_{n}E_{\theta_{0}+hn^{-1/2}}(\phi_{n})\geq\alpha\]

for any \(h\neq 0\). Argue that, among such tests \(\phi_{n}\), the two-sided Rao test \(\phi_{n,2}\) is LAUMP.

**Problem 15.58**: Generalize Example 15.5.7 to the case of testing \(\theta=\theta_{0}\) versus \(\theta\neq\theta_{0}\) in the presence of nuisance parameters.

**Problem 15.59**: Under the conditions of Theorem 15.5.5 used to prove an asymptotic maximin result for Rao's test, derive analogous optimality results for both the Wald and likelihood ratio tests.

**Problem 15.60**: Show Theorem 15.5.5 holds even with \(B=\infty\).

### _Section 15.6_

**Problem 15.61**: Show that the family of densities (15.95) satisfies (13.40) for small enough \(\theta\).

**Problem 15.62**: Verify (15.101).

**Problem 15.63**: Compare the bounds (15.101) and (15.104). For what \(u\) is each attainable? Why is (15.101) generally not attainable for all \(u\), even though there exists a test for the submodel \(\{P_{u,t}\}\) for which the bound is attainable.

**Problem 15.64**: In Example 15.6.2, argue that the given test attains the optimal limiting power uniformly in \(h\), for \(0\leq h\leq c\) and any \(c>0\).

**Problem 15.65**: In Theorem 15.6.1, compute the limiting power against \(P_{u,hn^{-1/2}}\) where \(h\) is chosen so that \(n^{1/2}\theta(P_{u,hn^{-1/2}})\to\delta\). [The solution does not depend on \(u\) but only on the value of \(\delta\), which was noted by Pfanzagl (1985).]

**Problem 15.66**: Provide the details for the optimality claimed in Example 15.6.3 for testing the variance in a nonparametric setting.

**Problem 15.67**: Let \(\mathbf{P}\) be the set of all joint distributions in \(\hbox{\rm I\kern-1.8ptR}^{2}\) on some compact set. Let \(\theta(P)\) denote the correlation functional. For testing \(\theta(P)\leq 0\), construct an asymptotically optimal test in a nonparametric setting.

**Problem 15.68**: Consider testing the difference of two population means \(\mu(P_{X})-\mu(P_{Y})\leq 0\) in a nonparametric setting. Generalize Theorem 15.6.1 to obtain locally AUMP tests.

### Notes

The Hellinger distance introduced in Section 15.1 was fundamental in Kakutani (1948) and does not seem to have been employed by Hellinger (Le Cam and Yang (2000), p. 48). The use of Hellinger distance to construct estimators and tests is developed in Beran (1977) and Simpson (1989).

The concept of Pitman asymptotic relative efficiency can be traced to an unpublished set of his lecture notes in (1949); Noether (1955) published a slightly more general result. The inequality (15.30) is due to Hodges and Lehmann (1956). Further results and references can be found in Serfling (1980) and Nikitin (1995). Some important alternative concepts of efficiency can be found in Bahadur (1960, 1965), Kallenberg (1982, 1983), and Inglot et al. (2000). Some numerical calculations are given in Groeneboom and Oosterhoff (1981). Higher-order asymptotic comparisons can be approached through the concept of _deficiency_, introduced in Hodges and Lehmann (1970). Some general results for rank and permutation tests in the one-sample problem are obtained in Albers et al. (1976); analogous results for the two-sample problem are obtained in Bickel and Van Zwet (1978). Pitman efficiencies of multivariate spatial sign and rank tests are considered in Peters and Randles (1991) and Mottonen et al. (1997). Asymptotic efficiency of rank tests is studied in Behnen and Neuhaus (1989) and Hajek et al. (1999). Higher-order efficiency is also considered in Bening (2000).

Our approach to large-sample efficiency of tests is largely due to ideas in Wald (1939, 1941a, 1941b, 1943), though his assumptions were too strong. He focused on MLEs and the tests now known as Wald tests. Wald basically argued that one could construct optimal large-sample tests based on the normal approximation to the MLE. A more formal approach was later provided by Le Cam's (1964, 1972) elegant notion of convergence of experiments, of which convergence to a normal experiment in the sense of Definition 15.4.1 is an important special case. This approach was used in Choi et al. (1996). For references of (local) asymptotically normal experiments in time series models, see Hallin et al. (1999). Generalizations to limiting Poisson experiment and locally asymptotically quadratic experiments are discussed in Le Cam and Yang (2000). Roussas (1972) formulated and developed the concept of AUMP tests. The proof of Theorem 15.4.1 is based on Lemma 3.4.4 of Rieder (1994). The results in Section 15.5.2 are obtained in Romano (2005). Nonparametric tests of equivalence are studied in Janssen (2000b); also see Wellek (2003). The reduction of a nonparametric problem to a parametric one through the use of a least favorable family is due to Stein (1956b), and is prominent in the work of Koshevnik and Levit (1976), Pfanzagl (1982, 1985), Bickel et al. (1993), and Janssen (1999), among others. The proof of Theorem 15.6.1 is based on the more general result Theorem 25.44 of van der Vaart (1988). Efficiency of nonparametric confidence intervals is discussed in Low (1997), and Romano and Wolf (2000).

Optimality in high-dimensional models is beyond the scope of the present treatment, but an entry into this literature, including many references, is provided in van de Geer et al. (2014). Some books on the subject include Buhlmann and van de Geer (2011), Giraud (2015) and Wainwright (2019).

## Chapter 16 Testing Goodness of Fit

### 16.1 Introduction

So far, the principal framework of this book has been optimality (either exact or asymptotic) in situations where both the hypothesis and the class of alternatives were specified by parametric models. In the present chapter, we shall take up the crucial problem of testing the validity of such models, the hypothesis of _goodness of fit_. For example, we would like to know whether a set of measurements \(X_{1}\),..., \(X_{n}\) is consonant with the assumption that the \(X\)'s are an i.i.d. sample from a normal distribution.

A difficulty in testing such a hypothesis is that the class of alternatives typically is enormously large and can no longer be described by a parametric model. As a result, although some asymptotic optimality results are presented, they are isolated; no general asymptotic optimality theory seems to exist for this problem. In fact, there is growing evidence, such as the results of Janssen (2000a) (see Theorem 16.6.2), that any test can achieve high asymptotic power against local or contiguous alternatives for at most a finite-dimensional parametric family.

Because of the importance of the problem of testing goodness of fit, we shall nevertheless consider this problem here. However, the focus will no longer be on optimality. Instead, we shall present some of the principal methods that have been proposed and study their relative strengths and weaknesses.

For the sake of simplifying a very complicated problem we shall consider the case where \(X_{1}\),..., \(X_{n}\) are i.i.d. according to some probability distribution \(P\), and shall mostly assume that the null hypothesis \(P=P_{0}\) completely specifies the distribution. While this assumption frequently is not fulfilled in applications, it makes it possible to cover some principal features of the problem which carry over to the more complex case of composite hypotheses.

In the case where the observations are real-valued, we index the unknown distribution by the underlying c.d.f. \(F\) and the problem is to test \(F=F_{0}\). We will typically consider the case where \(F_{0}\) is the uniform distribution on \((0,1)\). This special case can be generalized to the problem of testing the simple null hypothesisthat \(X_{1}\),..., \(X_{n}\) are i.i.d. from any fixed continuous c.d.f. \(F\) on the real line. To see how, define \(Y_{i}=F(X_{i})\), so that the \(Y_{i}\) are i.i.d. U(0,1) under \(H\) (Problem 3.22); then, test the hypothesis that \(Y_{1}\),..., \(Y_{n}\) are i.i.d. uniform on [0, 1].

Let \(\hat{F}_{n}\) be the empirical c.d.f., which uniformly tends to \(F\) with probability one, by the Glivenko-Cantelli theorem. For testing the simple null hypothesis \(F=F_{0}\), a natural starting point is to base a test statistic on some measure of discrepancy between \(\hat{F}_{n}\) and \(F_{0}\). In particular, if \(d\) is any metric on the space of distribution functions, then \(d(\hat{F}_{n},\,F_{0})\) could serve as a test statistic. A classical choice is \(d=d_{K}\), the Kolmogorov-Smirnov metric, which historically was the first test of goodness of fit that is (pointwise) consistent against any alternative. This test is studied in Section 16.2, but many other choices are possible; see Section 16.2.2. Two such choices are the Cramer-von Mises statistic and the Anderson-Darling statistic; in fact, these choices are often much more powerful than the Kolmogorov-Smirnov test.

In Section 16.3, the classical Chi-squared test is studied, and its asymptotic properties are derived. The class of Neyman's smooth tests is considered in Section 16.4; it includes the Chi-squared test as a special case, and serves to motivate the class of weighted quadratic test statistics studied in Section 16.5. The difficulty of constructing goodness of fit tests with good power against broad alternatives is studied in Section 16.6.

### The Kolmogorov-Smirnov Test

#### Simple Null Hypothesis

Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. real-valued observations with c.d.f. \(F\), and consider the problem of testing the simple null hypothesis that \(F=F_{0}\) versus \(F\neq F_{0}\). The classical Kolmogorov-Smirnov goodness of fit test statistic, introduced in Section 6.13 and Example 11.4.2, is

\[T_{n}\equiv\sup_{t\in\mathbb{R}}n^{1/2}|\hat{F}_{n}(t)-F_{0}(t)|=n^{1/2}d_{K}( \hat{F}_{n},\,F_{0})\, \tag{16.1}\]

where \(d_{K}\) is the Kolmogorov-Smirnov distance

\[d_{K}(F,\,G)=\sup_{t}|F(t)-G(t)|\.\]

Note that \(d_{K}(F,\,G)=0\) if and only if \(F=G\).

The distribution of \(T_{n}\) under \(F_{0}\) is the same for all continuous \(F_{0}\) (Problem 11.68). Let \(s_{n,1-\alpha}\) be the \(1-\alpha\) quantile of the distribution of \(T_{n}\) under any continuous \(F_{0}\). The Kolmogorov-Smirnov test rejects the null hypothesis if \(T_{n}>s_{n,1-\alpha}\). If \(F_{0}\) is not continuous, using \(s_{n,1-\alpha}\) results in a test that has level less than \(\alpha\) (Problem 11.69), but in principle, one can determine (or simulate) a critical value that yields an exact level \(\alpha\) test for this situation. Much of the remaining discussion in the section will focus on the case where the critical value \(s_{n,1-\alpha}\) is used (but the arguments apply more generally). For references to tables of critical values and finite-sample power calculations, see the references given in Example 11.4.2.

In order to study the limiting behavior of \(T_{n}\), introduce the function

\[B_{n}(t)=n^{1/2}[\hat{F}_{n}(t)-F_{0}(t)]\.\]

For each \(t\), \(B_{n}(t)\) is a real-valued random variable; in addition, \(B_{n}(\cdot)\) can be viewed as a random function (or process) on \([0,\,1]\), called the _empirical process_. By the multivariate Central Limit Theorem, if the null hypothesis is true, then for any \(t_{1}\),..., \(t_{k}\),

\[[B_{n}(t_{1}),\,\ldots,\,B_{n}(t_{k})]\stackrel{{ d}}{{\to}}[B(t_ {1}),\,\ldots,\,B(t_{k})]\,\]

where \([B(t_{1}),\,\ldots,\,B(t_{k})]\) has the multivariate normal distribution with mean \(0\) and covariance matrix \(\Sigma\), whose (\(i\), \(j\))th entry \(\sigma_{i,\,j}\) is given by

\[\sigma_{i,\,j}=\cases{F_{0}(t_{i})(1-F_{0}(t_{i}))&if $i=j$\cr F_{0}(\min(t_{i},t_{j}))-F_{0}(t_{i})F_{0}(t_{j})& otherwise.\cr}\]

By the Continuous Mapping Theorem, it follows that, for any \(t_{1}\),..., \(t_{k}\),

\[\max_{1,\ldots,k}n^{1/2}|\hat{F}_{n}(t_{i})-F_{0}(t_{i})|\stackrel{{ d}}{{\to}}\max_{1,\ldots,k}|B(t_{i})|\.\]

In fact, \(B(\cdot)\) itself can be represented as a random continuous process on \([0,\,1]\), called the Brownian Bridge process. The study of random functions and empirical processes is beyond the scope of this book, but it is developed in Pollard (1984) and van der Vaart and Wellner (1996). However, the result (16.5) provides both insight and a basis for a rigorous treatment of the limiting behavior of \(T_{n}\), which is the supremum over all \(t\), and not just a finite set, of \(|B_{n}(t)|\). It turns out that \(T_{n}\) has a limiting distribution which is continuous and strictly increasing on \((0,\infty)\). More specifically, Kolmogorov (1933) showed that if \(F_{0}\) is continuous, then for any \(d>0\),

\[P\{T_{n}>d\}\to 2\sum_{k=1}^{\infty}(-1)^{k+1}\exp(-2k^{2}d^{2})\.\]

The \(1-\alpha\) quantile of this distribution will be denoted by \(s_{1-\alpha}\).

We now discuss some power properties of the Kolmogorov-Smirnov test.

**Theorem 16.2.1**: _The Kolmogorov-Smirnov test is pointwise consistent in power against any fixed \(F\neq F_{0}\); that is,_

\[P_{F}\{T_{n}>s_{n,1-\alpha}\}\to 1\]

_as \(n\to\infty\)._Proof. By the Glivenko-Cantelli theorem, under an alternative \(F\),

\[\sup_{t}|\hat{F}_{n}(t)-F_{0}(t)|\to d_{K}(F,\,F_{0})>0\]

almost surely, and so \(T_{n}\to\infty\) almost surely. Hence, by Slutsky's Theorem,

\[P_{F}\{T_{n}>s_{n,1-\alpha}\}\to 1\,\]

since \(s_{n,1-\alpha}\to s_{1-\alpha}<\infty\).

For an alternative instructive proof of consistency (due to Massey 1950), fix any \(F\) with \(d_{K}(F,\,F_{0})>0\). Then, there exists some \(t\) with \(F(t)\neq F_{0}(t)\). First, assume \(F(t)>F_{0}(t)\). Then,

\[P_{F}\{T_{n}>s_{n,1-\alpha}\}\geq P_{F}\{\left|n^{1/2}[\hat{F}_{n}(t)-F_{0}(t)] \right|>s_{n,1-\alpha}\}\]

\[\geq P_{F}\{n^{1/2}[\hat{F}_{n}(t)-F(t)]\geq s_{n,1-\alpha}-n^{1/2}[F(t)-F_{0} (t)]\}\,\]

which tends to \(1\) as \(n\to\infty\) since the left side in the probability expression is bounded in probability while the right hand tends to \(-\infty\). Hence, the limiting power is \(1\) against any \(F\) if there exists a \(t\) with \(F(t)>F_{0}(t)\). By similar reasoning, the limiting power is \(1\) against \(F\) with \(F(t)<F_{0}(t)\) for some \(t\), and hence for any \(F\neq F_{0}\).

We now show that the Kolmogorov-Smirnov test is uniformly consistent in power against alternatives \(F\) satisfying \(n^{1/2}d_{K}(F,\,F_{0})\geq\Delta_{n}\), as long as \(\Delta_{n}\to\infty\).

**Theorem 16.2.2**: _Let \(X_{1},\,\ldots,\,X_{n}\) be i.i.d. random variables with c.d.f. \(F\). For testing \(F=F_{0}\) against \(F\neq F_{0}\), the power of the Kolmogorov-Smirnov test tends to one uniformly over all alternatives \(F\) satisfying \(n^{1/2}d_{k}(F,\,F_{0})\geq\Delta_{n}\) if \(\Delta_{n}\to\infty\) as \(n\to\infty\); that is,_

\[\inf\left\{\,P_{F}\{T_{n}>s_{n,1-\alpha}\}:\ n^{1/2}d_{K}(F,\,F_{0})\geq\Delta _{n}\right\}\to 1\]

_if \(\Delta_{n}\to\infty\)._

Proof. Let \(F_{n}\) be any sequence satisfying \(n^{1/2}d_{K}(F_{n},\,F_{0})\geq\Delta_{n}\). By the triangle inequality,

\[d_{K}(F_{n},\,F_{0})\leq d_{K}(F_{n},\,\hat{F}_{n})+d_{K}(\hat{F}_{n},\,F_{0})\,\]

which implies

\[T_{n}\geq\Delta_{n}-n^{1/2}d_{K}(\hat{F}_{n},\,F_{n})\.\]

Therefore,

\[P_{F_{n}}\{T_{n}>s_{n,1-\alpha}\}\geq P_{F_{n}}\{n^{1/2}d_{K}(\hat{F}_{n},\,F_{ n})\leq\Delta_{n}-s_{n,1-\alpha}\}. \tag{16.7}\]

[MISSING_PAGE_FAIL:785]

One can also obtain an upper bound to the power against alternatives \(F_{n}\) satisfying

\[n^{1/2}d_{K}(F_{n},\,F_{0})\to\delta<s_{1-\alpha}\.\]

By (16.9),

\[P_{F}\{T_{n}>s_{n,1-\alpha}\}\leq P_{F}\{d_{K}(\hat{F}_{n},\,F)>n^{-1/2}s_{n,1- \alpha}-d_{K}(F,\,F_{0})\}\.\]

Then, by the Dvoretzky, Kiefer and Wolfowitz Inequality (Theorem 11.4.3), the last expression is bounded above by

\[2\exp\{-2n[s_{n,1-\alpha}n^{-1/2}-d_{K}(F,\,F_{0})]^{2}\}\.\]

Therefore, if \(F_{n}\) is a sequence satisfying

\[n^{1/2}d_{K}(F_{n},\,F_{0})\to\delta<s_{1-\alpha}\,\]

then the limiting power against \(F_{n}\) is bounded above by

\[2\exp[-2(s_{1-\alpha}-\delta)^{2}]\.\]

So far, we have obtained crude upper and lower bounds to the power of the Kolmogorov-Smirnov test, and it follows from Theorems 16.2.2 and 16.2.3 that, like the parametric situations considered earlier, it is against sequences of alternatives \(F_{n}\) with

\[n^{1/2}d_{K}(F_{n},\,F_{0})\to\delta\quad(0<\delta<\infty)\]

that we expect the power of the test to tend to limits strictly between \(\alpha\) and \(1\). Let us now sketch an approach to calculating the exact limiting power against a local sequence of alternatives \(F_{n}\). Consider the normalized difference

\[d_{n}(t)=n^{1/2}[F_{n}(t)-F_{0}(t)]\,\]

and assume that for some function \(d\)

\[\sup_{t}|d_{n}(t)-d(t)|\to 0\.\]

Note the basic identity

\[n^{1/2}[\hat{F}_{n}(t)-F_{0}(t)]=n^{1/2}[\hat{F}_{n}(t)-F_{n}(t)]+d_{n}(t). \tag{16.10}\]

Under \(F_{n}\), \(n^{1/2}[\hat{F}_{n}(t)-F_{n}(t)]\) has mean \(0\) and variance

\[F_{n}(t)[1-F_{n}(t)]\to F_{0}(t)[1-F_{0}(t)]\.\]For fixed \(t\), the Lindeberg Central Limit Theorem (see Problem 11.14) implies that, under \(F_{n}\),

\[n^{1/2}[\hat{F}_{n}(t)-F_{n}(t)]\stackrel{{ d}}{{\to}}B(t)\,\]

where \(B(t)\) has the same limiting normal distribution \(N(0,\,F_{0}(t)[1-F_{0}(t)])\) that arose when studying the limiting behavior (16.3) of the empirical process \(B_{n}(t)\) (defined in 16.2) under \(F_{0}\). Hence, under \(F_{n}\), (16.10) implies that

\[n^{1/2}[\hat{F}_{n}(t)-F_{0}(t)]\stackrel{{ d}}{{\to}}B(t)+d(t) \sim N\left(d(t),\,F_{0}(t)[1-F_{0}(t)]\right)\.\]

Similarly, for any fixed \(t_{1}\),..., \(t_{k}\), under \(F_{n}\),

\[n^{1/2}[\hat{F}_{n}(t_{1})-F_{0}(t_{1}),\,\ldots,\,\hat{F}_{n}(t_{k})-F_{0}(t_ {k})]\stackrel{{ d}}{{\to}}[B(t_{1})+d(t_{1}),\ldots,\,B(t_{k})+d (t_{k})]\.\]

By the Continuous Mapping Theorem, it then follows that, under \(F_{n}\)

\[\max_{1,\ldots,k}n^{1/2}|\hat{F}_{n}(t_{i})-F_{0}(t_{i})|\stackrel{{ d}}{{\to}}\max_{1,\ldots,k}|B(t_{i})+d(t_{i})|. \tag{16.11}\]

This result suggests that, under \(F_{n}\),

\[\sup_{t}n^{1/2}|\hat{F}_{n}(t)-F_{0}(t)|\stackrel{{ d}}{{\to}}\sup_ {t}|B(t)+d(t)|\,\]

where \(B(t)\) is the Brownian Bridge process which was introduced at the beginning of this section. This suggested result does in fact hold, and so the limiting power of the Kolmogorov-Smirnov test against \(F_{n}\) can be expressed as

\[P\{\sup_{t}|B(t)+d(t)|>s_{1-\alpha}\}. \tag{16.12}\]

The evaluation of this expression involves the so-called general boundary-crossing probabilities and is beyond the present treatment; see Siegmund (1986) and the references given in Shorack and Wellner (1986), Section 4.2. Approximations to this limiting power are also obtained in Hajek et al. (1999), Section 7.4.

The results in this section show that the limiting power of the Kolmogorov-Smirnov test against alternatives \(F_{n}\) satisfying \(n^{1/2}d_{K}(F_{n},\,F_{0})\to\delta\) is 0 or 1 unless \(\delta\) is finite and positive. Moreover, the result (16.12) can be used to show that typically, the limiting power is strictly between \(\alpha\) and 1. Surprisingly, and in distinction to the typical parametric situation, the limiting power can be \(\alpha\) or 1 against a sequence of alternatives \(F_{n}\) satisfying \(n^{1/2}d_{K}(F_{n},\,F_{0})\to\delta\) even if \(0<\delta<\infty\); for a construction, see Problem 16.6.

#### Extensions of the Kolmogorov-Smirnov Test

The basis of the Kolmogorov-Smirnov test is a measure of discrepancy between the hypothesized distribution function \(F_{0}\) and the empirical (cumulative) distribution function \(\hat{F}_{n}\). Any such statistic is called an EDF statistic. In particular, if \(d\) is a metric on the space of distribution functions, any statistic of the form \(d(\hat{F}_{n},\,F_{0})\) is an EDF statistic, with the choice \(d=d_{K}\) corresponding to the Kolmogorov-Smirnov statistic.

A second class of EDF statistics is given by the Cramer-von Mises family of statistics

\[V_{n}=n\int_{-\infty}^{\infty}[\hat{F}_{n}(x)-F_{0}(x)]^{2}\psi(x)d\,F_{0}(x)\.\]

Taking \(\psi(x)=1\) yields the Cramer-von Mises statistic, while

\[\psi(x)=\{F_{0}(x)[1-F_{0}(x)]\}^{-1}\]

yields the Anderson-Darling statistic. Both choices will be studied in Section 16.5.

Tests based on EDF statistics can be used to test composite null hypotheses. For example, suppose it is desired to test whether the underlying c.d.f. is \(F_{\theta}\) for some \(\theta\) lying in a parameter space \(\Theta_{0}\), and that \(\hat{\theta}_{n}\) is some reasonable estimator of \(\theta\). Then, an EDF test statistic is defined by some measure of discrepancy between \(\hat{F}_{n}\) and \(F_{\hat{\theta}_{n}}\). For example, for testing normality with unspecified mean \(\mu\) and variance \(\sigma^{2}\), a Kolmogorov-Smirnov test statistic is given by

\[\sup_{x}|\hat{F}_{n}(x)-\Phi\left(\frac{x-\bar{X}_{n}}{\hat{\sigma}_{n}}\right) |\, \tag{16.13}\]

where \(\Phi(\cdot)\) is the standard normal c.d.f. and \((\bar{X}_{n},\hat{\sigma}_{n})\) is the MLE for \((\mu,\sigma)\) under the normal model. It is easy to see that, under the null hypothesis, the distribution of (16.13) does not depend on \((\mu,\sigma)\) (Problem 16.9), and critical values can be approximated by simulation. Many other tests have been proposed to test for normality; see D'Agostino and Stephens (1986).

Unfortunately, for testing general parametric submodels indexed by \(\theta\), the asymptotic null distribution of an EDF statistic with estimated parameters depends on \(\theta\), which limits their use. For discussion and references to the literature of this problem, see D'Agostino and Stephens (1986) and de Wet and Randles (1987). An alternative approach based on the bootstrap is given in Beran (1986) and Romano (1988); see Example 18.5.6.

EDF tests can be extended to the case where the observations are not real-valued. Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. \(P\) (on some arbitrary space). The natural extension of the empirical c.d.f. is the _empirical measure_, defined by\[\hat{P}_{n}(E)=\frac{1}{n}\sum_{i=1}^{n}I\{X_{i}\in E\}\.\]

Then, EDF test statistics can be constructed by some measure of discrepancy between \(\hat{P}_{n}\) and a hypothesized \(P_{0}\) (or \(P_{\hat{\theta}_{n}}\) in the composite null hypothesis case). See Shorack and Wellner (1986), who also discuss the two-sample problem of comparing two samples by a measure of discrepancy between the empirical c.d.f.s of the samples.

### Pearson's Chi-Squared Statistic

#### Simple Null Hypothesis

In this section, we return to the simple goodness of fit problem for categorical data that was briefly considered in Example 14.4.6. As before, we are dealing with a sequence of \(n\) independent trials, each resulting in one of \(k+1\) possible outcomes named \(1\),..., \(k+1\). The \(j\)th outcome occurs with probability \(p_{j}\) on any given trial, so that \(\sum_{j=1}^{k+1}p_{j}=1\). Let \(Y_{j}\) be the number of trials resulting in outcome \(j\). The joint distribution of (\(Y_{1}\),..., \(Y_{k+1}\)) is the multinomial distribution

\[P\{Y_{1}=y_{1},\ldots,Y_{k+1}=y_{k+1}\}=\frac{n!}{y_{1}!\cdots y_{k+1}!}p_{1}^{ y_{1}}\cdots p_{k+1}^{y_{k+1}}\, \tag{16.14}\]

with \(\sum_{j=1}^{k+1}y_{j}=n\). The parameter space \(\Omega\) is

\[\Omega=\left\{(p_{1},\ldots,p_{k})\in{\rm I\!R}^{k}:\ p_{i}\geq 0,\ \ \sum_{j=1}^{k}p_{j}\leq 1\right\} \tag{16.15}\]

since \(p_{k+1}=1-\sum_{j=1}^{k}p_{j}\).

Consider testing the simple null hypothesis \(p_{j}=\pi_{j}\) for \(j=1,\ldots,k+1\) against the alternatives \(p_{j}\neq\pi_{j}\) for some \(j\). It will be assumed that \(\pi_{1}\),..., \(\pi_{k}\) is an interior point of \(\Omega\).

A standard test, proposed by Pearson (1900), rejects for large values of Pearson's Chi-squared statistic, given by

\[Q_{n}=\sum_{j=1}^{k+1}\frac{(Y_{j}-n\pi_{j})^{2}}{n\pi_{j}}. \tag{16.16}\]

This test was already introduced in Example 14.4.6 as an approximation to the likelihood ratio test, and it was shown that the limiting null distribution of \(Q_{n}\) as \(n\to\infty\) is the Chi-squared distribution with \(k\) degrees of freedom. Below, we will give a direct argument of this result in Theorem 16.3.1. Thus, if \(c_{k,1-\alpha}\) is the \(1-\alpha\) quantile of \(\chi_{k}^{2}\), then the test that rejects when \(Q_{n}>c_{k,1-\alpha}\) is asymptotically level \(\alpha\). The accuracy of the Chi-squared approximation to the exact null distribution of the test statistic is discussed for example by Radlow and Alf (1975); for more accurate approximations in this and related problems, see McCullagh (1985, 1986) and the literature cited there.

Consider next a fixed alternative

\[(p_{1},\ldots,\,p_{k+1})\neq(\pi_{1},\ldots,\pi_{k+1})\.\]

If, for some \(j\), \(p_{j}\neq\pi_{j}\), then

\[Q_{n}\geq n\left(\frac{Y_{j}}{n}-\pi_{j}\right)^{2}\stackrel{{ p}}{{\to}}\infty\]

since \(Y_{j}/n\stackrel{{ p}}{{\to}}p_{j}\), by the law of large numbers. Hence, the power against such an alternative tends to one.

As in Example 11.3.2, a more discriminating result is obtained by considering local alternatives \(p_{j}^{(n)}\) of the form

\[p_{j}^{(n)}=\pi_{j}+n^{-1/2}h_{j}\,\]

where \(\sum_{j=1}^{k+1}h_{j}=0\). We shall now show that, against such an alternative sequence, the limiting power is nondegenerate.

**Theorem 16.3.1**: _Assume the above multinomial setup._

_(i) Under the null hypothesis_ \(H\)_:_ \(p_{j}=\pi_{j}\) _for_ \(j=1,\ldots,k+1\)_,_ \(Q_{n}\stackrel{{ d}}{{\to}}\chi_{k}^{2}\)_, the Chi-squared distribution with_ \(k\) _degrees of freedom._

_(ii) Under the alternative hypothesis (sequence)_ \(K\)_:_ \(p_{j}^{(n)}=\pi_{j}+n^{-1/2}h_{j}\) _where_ \(\sum_{j=1}^{k+1}h_{j}=0\)_,_ \(Q_{n}\stackrel{{ d}}{{\to}}\chi_{k}^{2}(\lambda)\)_, the noncentral Chi-squared distribution with_ \(k\) _degrees of freedom and noncentrality parameter_

\[\lambda=\sum_{j=1}^{k+1}\frac{h_{j}^{2}}{\pi_{j}}. \tag{16.17}\]

_(iii) The power of the_ \(\chi^{2}\) _test based on_ \(Q_{n}\) _against the alternatives in (ii) with not all the_ \(h_{j}\) _equal to 0 tends to a limit strictly greater than_ \(\alpha\) _and less than 1. This holds if the test is carried out using an exact level_ \(\alpha\) _critical value, or any critical value sequence tending to_ \(c_{k,1-\alpha}\) _in probability (such as_ \(c_{k,1-\alpha}\) _itself)._

Proof. The proof of (i) is an application of the multivariate CLT followed by the continuous mapping theorem. Let \(V_{n}\) be the \(k\times 1\) vector defined by \[V_{n}^{\top}=n^{1/2}\left(\frac{Y_{1}}{n}-\pi_{1},\ldots,\frac{Y_{k}}{n}-\pi_{k} \right). \tag{16.18}\]

By the multivariate CLT, \(V_{n}\xrightarrow{d}N(0,\,\Sigma)\), where the \(k\times k\) covariance matrix \(\Sigma\) has \((i,\,j)\) entry (Problem 16.12 (i))

\[\sigma_{i,j}=\begin{cases}\pi_{i}(1-\pi_{i})&\text{if }j=i\\ -\pi_{i}\pi_{j}&\text{otherwise}.\end{cases} \tag{16.19}\]

It can be checked that \(\Sigma\) has inverse \(\Sigma^{-1}=A\), where \(A\) has \((i,\,j)\) entry given by (Problem 16.12 (ii))

\[a_{i,j}=\begin{cases}\frac{1}{\pi_{i}}+\frac{1}{\pi_{k+1}}&\text{if }j=i\\ \frac{1}{\pi_{k+1}}&\text{otherwise}.\end{cases} \tag{16.20}\]

Hence, \(A^{1/2}V_{n}\xrightarrow{d}N(0,\,I_{k})\), where \(I_{k}\) is the \(k\times k\) identity matrix. By the Continuous Mapping Theorem 11.2.10,

\[(A^{1/2}V_{n})^{\top}(A^{1/2}V_{n})\xrightarrow{d}\chi_{k}^{2}\.\]

But, the left-hand side is \(V_{n}^{\top}A\,V_{n}\), which in turn is equal to

\[n\sum_{j=1}^{k}\frac{1}{\pi_{j}}\left(\frac{Y_{j}}{n}-\pi_{j}\right)^{2}+\frac {n}{\pi_{k+1}}\sum_{i=1}^{k}\sum_{j=1}^{k}\left(\frac{Y_{i}}{n}-\pi_{i}\right) \left(\frac{Y_{j}}{n}-\pi_{j}\right)\.\]

The last term reduces to

\[n\left[\sum_{j=1}^{k}\left(\frac{Y_{j}}{n}-\pi_{j}\right)\right]^{2}/\pi_{k+1} =n\left(\frac{Y_{k+1}}{n}-\pi_{k+1}\right)^{2}/\pi_{k+1}\,\]

where, in the last equality, we have used \(\sum_{j=1}^{k}Y_{j}=n-Y_{k+1}\) and \(\sum_{j=1}^{k}\pi_{j}=1-\pi_{k+1}\). Thus, \(V_{n}^{\top}A\,V_{n}=Q_{n}\).

The proof of (ii) is similar. First, note that

\[V_{n}^{\top}=n^{1/2}\left(\frac{Y_{1}}{n}-p_{1}^{(n)},\ldots,\frac{Y_{k}}{n}-p _{k}^{(n)}\right)+(h_{1},\ldots,h_{k})\.\]

It follows from the Cramer-Wold Device and the Berry-Esseen Theorem (Problem 16.13) that, under the alternative sequence,

\[V_{n}\xrightarrow{d}N(h,\,\Sigma). \tag{16.21}\]Therefore,

\[A^{1/2}V_{n}\stackrel{{ d}}{{\to}}N(A^{1/2}h,I_{k})\]

and so

\[(A^{1/2}V_{n})^{\top}(A^{1/2}V_{n})\stackrel{{ d}}{{\to}}\chi_{k}^ {2}(\lambda)\,\]

where

\[\lambda=(A^{1/2}h)^{\top}(A^{1/2}h)=h^{\top}Ah\ ;\]

simple algebra shows that \(h^{\top}Ah\) agrees with the expression (16.17) for \(\lambda\) and the proof of (ii) follows.

The proof of (iii) is left as an exercise (Problem 16.15).

We are now in a position to prove an optimality result for Pearson's Chi-squared test in the multinomial goodness of fit problem. The problem is to test the null hypothesis \(p=\pi\), where \(\pi\) is the vector with \(j\)th component \(\pi_{j}\). The goal is to show Pearson's Chi-squared test is asymptotically maximin over an appropriate (shrinking) set of alternatives \(p\) which tend to \(\pi\) at rate \(n^{-1/2}\). First, note that the Information matrix \(I(p)\) with (\(i\), \(j\)) entry \(a_{i,j}\) is given by

\[a_{i,j}=\begin{cases}\frac{1}{p_{i}}+\frac{1}{p_{k+1}}&\text{if $j=i$}\\ \frac{1}{p_{k+1}}&\text{otherwise.}\end{cases} \tag{16.22}\]

(Problem 16.14). Let \(h^{\top}=(h_{1},\ldots,h_{k})\) and set \(h_{k+1}=-\sum_{i=1}^{k}h_{i}\) so that \(\sum_{i=1}^{k+1}h_{i}=0\). Then,

\[|I^{1/2}(\pi)h|^{2}=\sum_{i=1}^{k+1}\frac{h_{i}^{2}}{\pi_{i}}\.\]

**Theorem 16.3.2**: _Assume the above multinomial setup. (i) For any test sequence \(\phi_{n}\) such that \(E_{\pi}(\phi_{n})\to\alpha\),_

\[\limsup_{n\to\infty}\inf\left\{E_{\pi+hn^{-1/2}}(\phi_{n}):\ \sum_{i=1}^{k+1} \frac{h_{i}^{2}}{\pi_{i}}\geq b^{2},\ \pi+hn^{-1/2}\in\Omega\right\}\]

\[\leq P\{\chi_{k}^{2}(b^{2})>c_{k,1-\alpha}\}. \tag{16.23}\]

_(ii) Pearson's Chi-squared test \(\phi_{n}^{*}\), which rejects when_

\[\sum_{i=1}^{k+1}\frac{(Y_{i}-n\pi_{i})^{2}}{n\pi_{i}}>c_{k,1-\alpha}\]_is asymptotically maximin in the sense that the inequality in (16.23) is an equality when \(\phi_{n}=\phi_{n}^{*}\). Thus, \(\phi_{n}^{*}\) maximizes_

\[\lim_{n}\inf\left\{E_{\pi+hn^{-1/2}}(\phi_{n}):\ \sum_{i=1}^{k+1}\frac{h_{i}^{2}} {\pi_{i}}\geq b^{2},\ \pi+hn^{-1/2}\in\Omega\right\}\]

_among all tests with asymptotic level \(\alpha\)._

Proof Theorem 15.5.4 immediately implies (i). To prove (ii), assume the opposite. Let \(R\) denote the right side of (16.23). Then, there exists a sequence of alternatives \(h^{(n)}\) (with \(i\)th component denoted \(h_{i}^{(n)}\)) satisfying

\[\sum_{i=1}^{k+1}\frac{[h_{i}^{(n)}]^{2}}{\pi_{i}}\geq b^{2},\ \ \sum_{i=1}^{k+1}h_{i}^{(n)}=0\]

such that

\[E_{\pi+h^{(n)}n^{-1/2}}(\phi_{n}^{*})\to\ell\,\]

and \(\ell\) is strictly less than \(R\). Since

\[\sum_{i=1}^{k+1}\frac{[h_{i}^{(n)}]^{2}}{\pi_{i}}\geq b^{2},\]

we cannot have \(h_{i}^{(n)}\to 0\) for every \(i\).

We also cannot have \([h_{i}^{(n)}]^{2}\to\infty\) for any \(i\), for then

\[E_{\pi+h^{(n)}n^{-1/2}}(\phi_{n}^{*})\to 1\,\]

which would be a contradiction since \(R<1\). To see why this expectation would tend to 1, suppose \(h_{i}^{(n)}\to\infty\) (and a similar argument holds if \(h_{i}^{(n)}\to-\infty\)). Then,

\[E_{\pi+h^{(n)}n^{-1/2}}(\phi_{n}^{*})\geq P_{\pi+h^{(n)}n^{-1/2}} \left\{\frac{(Y_{i}-n\pi_{i})^{2}}{n\pi_{i}}>c_{k,1-\alpha}\right\}\] \[>P_{\pi+h^{(n)}n^{-1/2}}\left\{n^{1/2}(\frac{Y_{i}}{n}-\pi_{i})>c _{k,1-\alpha}^{1/2}\right\}\] \[=P_{\pi+h^{(n)}n^{-1/2}}\left\{n^{1/2}\left[\frac{Y_{i}}{n}-(\pi_ {i}+h_{i}^{(n)}n^{-1/2})\right]+h_{i}^{(n)}>c_{k,1-\alpha}^{1/2}\right\}. \tag{16.24}\]

But, by Chebyshev's inequality,

\[n^{1/2}\left[\frac{Y_{i}}{n}-(\pi_{i}+h_{i}^{(n)}n^{-1/2})\right]\]is bounded in probability, since it has mean 0 and variance bounded by one. Hence, (16.24) tends to one and so

\[E_{\pi+h^{(n)}n^{-1/2}}(\phi_{n}^{*})\to 1\.\]

The same conclusion holds along any subsequence \(n_{k}\) satisfying \(h_{i}^{(n_{k})}\to\infty\).

Thus, we must have \(h_{i}^{(n)}\asymp 1\) for every \(i\). By passing to subsequences which converge, assume

\[h_{i}^{(n)}\to h_{i}^{(\infty)}<\infty\,\ \text{and}\ \ \lambda\equiv\sum_{i=1}^{k+1}\frac{[h_{i}^{(\infty)}]^{2}}{\pi_{i}}\geq b^{2}\.\]

The limiting power was obtained in Theorem (16.3.1) with \(h_{i}^{(n)}=h_{i}\) fixed, but the argument applies with obvious modifications to sequences that converge; moreover, this limiting power is

\[P\{\chi_{k}^{2}(\lambda)>c_{k,1-\alpha}\}\geq P\{\chi_{k}^{2}(b^{2})>c_{k,1- \alpha}\}\,\]

since the family of Chi-squared distributions has monotone likelihood ratio. This again yields a contradiction. The same conclusion holds for any subsequence, because we can apply the argument to further subsequences where \(h_{i}^{(n)}\) converges along the subsubsequences.

The above result states that the Chi-squared test is asymptotically maximin for the multinomial goodness of fit problem. The same result holds for the likelihood ratio test (Problem 16.16). Moreover, the above argument shows that the worst case power over alternatives \(\pi+hn^{-1/2}\) with

\[\sum_{i=1}^{k}h_{i}^{2}/\pi_{i}\geq b^{2}\]

occurs (asymptotically) when \(\sum_{i=1}^{k}h_{i}^{2}/\pi_{i}=b^{2}\).

#### Chi-Squared Test of Uniformity

So far, we have been concerned with testing the parameters of a multinomial model. Let us now return to the problem stated at the beginning of Section 16.2, where \(X_{1}\),..., \(X_{n}\) are i.i.d. real-valued observations with c.d.f. \(F\), and the problem is that of testing the null hypothesis \(H\) that \(F=F_{0}\), where \(F_{0}(t)=t\) is the uniform c.d.f. on (0, 1). To reduce this problem of goodness of fit to that of testing a multinomial hypothesis, fix a positive integer \(k\) and divide the unit interval into \(k+1\) subintervalsof length \(1/(k+1)\); for \(j=1\),..., \(k+1\), let \(Y_{j}\) be the number of \(X_{i}\) observations that fall in the interval \(I_{k,j}\) defined by

\[I_{k,j}=[(j-1)/(k+1),\,j/(k+1))\.\]

Under the null hypothesis, the joint distribution of \((Y_{1},\ldots,Y_{k+1})\) is multinomial based on \(n\) trials and equal class probabilities of \(1/(k+1)\). So, one can test \(H\) by using the Chi-squared test which rejects for large values of

\[\sum_{j=1}^{k+1}\frac{(Y_{j}-\frac{n}{k+1})^{2}}{\frac{n}{k+1}}\.\]

It follows that, for fixed \(k\), the Chi-squared test is consistent against any alternative distribution \(F\) which does not assign equal probability to all intervals \(I_{k,j}\).

Next, consider a sequence of alternative densities \(f_{n}\) of the form

\[f_{n}(x)=1+b_{n}u(x)\, \tag{16.25}\]

where \(u\) satisfies \(\int_{0}^{1}u(x)dx=0\) and \(\int u^{2}(x)dx<\infty\). Then, \(f_{n}\) assigns probability

\[\int_{I_{k,j}}[1+b_{n}u(x)]dx=\frac{1}{k+1}+b_{n}\int_{I_{k,j}}u(x)dx\]

to \(I_{k,j}\). By Theorem 16.3.1 (ii), with \(k\) fixed and \(b_{n}=hn^{-1/2}\), the limiting power of the Chi-squared test is given by

\[P\{\chi_{k}^{2}(\lambda_{k})>c_{k,1-\alpha}\}\,\]

where

\[\lambda_{k}=h^{2}(k+1)\sum_{j=1}^{k+1}\left[\int_{I_{k,j}}u(x)dx\right]^{2}\.\]

Note that, if

\[\int_{I_{k,j}}u(x)dx\]

is not zero for at least one \(j\), then the noncentrality parameter \(\lambda_{k}\) is positive. Also, if \(u\) is continuous except at most a finite number of points, then

\[\lambda_{k}\rightarrow\lambda_{\infty}\equiv h^{2}\int_{0}^{1}u^{2}(x)dx\quad \mbox{as}\ k\rightarrow\infty. \tag{16.26}\]Note that for any fixed \(k\), \(\lambda_{k}\) can be 0 even if \(\lambda_{\infty}>0\). Indeed, the Chi-squared test has power equal to the size of the test against any distribution that has mass \(1/(k+1)\) on each subintervals, and so for fixed \(k\), the Chi-squared test is not consistent against all alternatives.

Therefore, it is tempting to allow \(k=k_{n}\) to increase with \(n\) in order to obtain power against an even broader range of alternatives. On the other hand, if \(\lambda_{k}\) approaches \(\lambda_{\infty}\) quite fast, then it would be undesirable to let \(k_{n}\) increase too quickly. To illustrate this point, consider the following example. Let \(u_{0}(x)=1\) for \(x\leq 1/2\) and \(u_{0}(x)=-1\) for \(x>1/2\). Then, \(\lambda_{k}=\lambda_{\infty}=h^{2}\) for all \(k\) odd. If \(k=1\), then the limiting power of the Chi-squared test against \(f_{n}\) given by

\[f_{n}(x)=1+hn^{-1/2}u_{0}(x)\]

is

\[P\{\chi_{1}^{2}(h^{2})>c_{1,1-\alpha}\}\.\]

If instead, \(k=2j+1\) with \(j\geq 1\), the limiting power is exactly

\[P\{\chi_{k}^{2}(h^{2})>c_{k,1-\alpha}\}\.\]

Notice that the noncentrality parameter is the same for all odd \(k\). But, for fixed \(h\), this probability is decreasing in \(k\) its limiting value is \(\alpha\) as \(k\to\infty\), as shown by the following lemma.

**Lemma 16.3.1**: _Let \(M(k,h)\) be defined as_

\[M(k,h)=P\{\chi_{k}^{2}(h^{2})>c_{k,1-\alpha}\}\, \tag{16.27}\]

_where \(\chi_{k}^{2}(h^{2})\) denotes a noncentral Chi-squared variable with \(k\) degrees of freedom and noncentrality parameter \(h^{2}\). (i) For fixed \(h\), \(M(k,h)\) is nonincreasing in \(k\), and is strictly decreasing if \(h\neq 0\). (ii) If \(h_{k}\to h\) for some finite \(h\), \(M(k,h_{k})\to\alpha\) as \(k\to\infty\). In particular, \(M(k,h)\to\alpha\) as \(k\to\infty\). (iii) If \((2k)^{-1/2}h_{k}^{2}\to c\) as \(k\to\infty\), then_

\[M(k,h_{k})\to 1-\Phi(z_{1-\alpha}-c)\.\]

Proof. The proof of (i) is left as an exercise (Problem 16.17). To prove (ii), let \(Z_{1}\), \(Z_{2}\), \(\ldots\) denote i.i.d. standard normal variables. By the Central Limit Theorem,

\[(2k)^{-1/2}(\sum_{i=1}^{k}Z_{i}^{2}-k)\stackrel{{ d}}{{\to}}N(0, 1)\, \tag{16.28}\]

which implies

\[(2k)^{-1/2}(c_{k,1-\alpha}-k)\to z_{1-\alpha} \tag{16.29}\]as \(k\to\infty\). Of course, the result (16.28) holds even if the \(i=1\) term is omitted from the sum. Hence,

\[M(k,h_{k})=P\left\{(Z_{1}+h_{k})^{2}+\sum_{i=2}^{k}Z_{i}^{2}>c_{k,1 -\alpha}\right\}\] \[=P\{(2k)^{-1/2}(Z_{1}+h_{k})^{2}+(2k)^{-1/2}\left(\sum_{i=2}^{k}Z_ {i}^{2}-k\right)\] \[>(2k)^{-1/2}(c_{k,1-\alpha}-k)\}. \tag{16.30}\]

By (16.29), the right side of the last expression tends to \(z_{1-\alpha}\). Also, as \(k\to\infty\),

\[(2k)^{-1/2}(Z_{1}+h_{k})^{2}\stackrel{{ P}}{{\to}}0\.\]

By Slutsky's Theorem, the left side of (16.30) tends in distribution to \(N(0,1)\). The result (ii) follows by another application of Slutsky's Theorem. The proof of (iii) is similar. The only difference is that the term

\[(2k)^{-1/2}(Z_{1}+h_{k})^{2}\stackrel{{ P}}{{\to}}c\]

if \((2k)^{-1/2}h_{k}^{2}\to c\).

Thus, the results in (i) and (ii) of Lemma 16.3.1 show that the choice \(k=1\) is optimal for the situation with \(u=u_{0}\). The point is that increasing \(k\) too much decreases the limiting power. Furthermore, if \(k\) is quite large, the limiting power is approximately \(\alpha\). This latter conclusion applies to any alternative sequence of the form (16.25) with \(b_{n}=n^{-1/2}\); also see Problem 16.19.

Mann and Wald (1942) considered the optimal choice of \(k_{n}\). In particular, let

\[d_{K}(F,F_{0})=\sup_{t}|F(t)-t|\.\]

Mann and Wald (1942) determined an optimal rate for \(k_{n}\) which satisfies \(k_{n}=O(n^{2/5})\), and show that with such an optimal rate the limiting power is \(1/2>\alpha\) against a sequence of alternatives \(F_{n}\) satisfying \(n^{2/5}d_{K}(F_{n},F_{0})\to\infty\). This result on optimal rates is somewhat contradicted by the above analysis and other results that indicate that the best choice of \(k_{n}\) is rather small; see Stuart and Ord (1991), Chapter 30.

It is interesting to compare the results of Mann and Wald with the fact that the Kolmogorov-Smirnov goodness of fit test has limiting power one if \(n^{1/2}d_{K}(F_{n},F_{0})\to\infty\), as shown in Theorem 16.2.2. It follows that the Kolmogorov-Smirnov test (and this is also true of Cramer-von-Mises test) is asymptotically superior to the Chi-squared test in this case. However, it has been pointed out that this superiority is connected with the choice of distance with which one measures deviations from \(F_{0}\). If one replaces the Kolmogorov-Smirnov distance with an \(L_{2}\) distance based on the integral of the squared difference in densities (satisfying smoothness conditions), then the Chi-squared test can asymptotically outperform the Kolmogorov-Smirnov test; see Ingster (1993). We will later obtain further results, since Chi-squared tests can be viewed as a special case of the more general class of Neyman's smooth tests that will be studied in Section 16.4.

#### Composite Null Hypothesis

Next, we consider the application of the Chi-squared test to composite hypotheses. First, suppose data (\(Y_{1}\),..., \(Y_{k+1}\)) has the multinomial distribution (16.14), where \(Y_{j}\) is the number of trials resulting in outcome \(j\) and \(p_{j}\) is the probability of the \(j\)th outcome for any given trial. The full model allows the \(p_{j}\) to vary freely, subject to their being nonnegative and summing to one.

Consider testing the null hypothesis that the \(p_{j}\) are of the form

\[p_{j}=f_{j}(\beta_{1},\ldots,\beta_{q})\,\ \ \ \ j=1,\ldots,k+1,\]

where the \(f_{j}\) are known functions of \(\beta=(\beta_{1},\ldots,\beta_{q})\), and \(\beta\) varies in a subset of \(\mathbb{R}^{q}\) for some \(q<k\). For testing the simple null hypothesis that \(p_{j}=f_{j}(\beta)\), \(1\leq j\leq k\), for a fixed value of \(\beta\), the Chi-squared test is based on the statistic

\[Q_{n}(\beta)=\sum_{j=1}^{k+1}\frac{(Y_{j}-nf_{j}(\beta))^{2}}{nf_{j}(\beta)}. \tag{16.31}\]

If \(\beta\) is unspecified, Fisher (1925b) suggested the test statistic \(Q_{n}(\tilde{\beta}_{n})\), where \(\tilde{\beta}_{n}\) is a MLE of \(\beta\) under the null hypothesis submodel (or any efficient estimator). Following Fisher, Neyman (1949) recommends \(Q_{n}(\tilde{\beta}_{n})\), where \(\tilde{\beta}_{n}\) is chosen to minimize \(Q_{n}(\beta)\) (in which case \(\tilde{\beta}_{n}\) is called a minimum Chi-squared estimator). Not surprisingly, it is typically the case that, under the null hypothesis,

\[Q_{n}(\hat{\beta}_{n})-Q_{n}(\tilde{\beta}_{n})\stackrel{{ P}}{{ \rightarrow}}0\.\]

**Example 16.3.1** (**Fisher linkage model**) Fisher (1928b) postulated a genetics model with four possible types of offspring, whose probabilities are of the form

\[(p_{1},\,p_{2},\,p_{3},\,p_{4})=\frac{1}{4}(2+\beta,\,1-\beta,1-\beta,\beta)\]

for some \(\beta\in(0,1)\). In the above notation, \(f_{1}(\beta)=2+\beta\), \(f_{2}(\beta)=f_{3}(\beta)=1-\beta\), and \(f_{4}(\beta)=\beta\). (The parameter \(\beta\) depends on the linkage between the two genetic factors under consideration.) To test the validity of such a model, a Chi-squaredtest can be employed. To estimate \(\beta\), it is easily checked (Problem 16.23) that the likelihood equation is

\[\frac{Y_{1}}{2+\beta}-\frac{(Y_{2}+Y_{3})}{1-\beta}+\frac{Y_{4}}{\beta}=0\, \tag{16.32}\]

which reduces to a quadratic equation, and the MLE \(\hat{\beta}_{n}\) is the root of this equation that lies in [0, 1]. The resulting test statistic is then \(Q_{n}(\hat{\beta}_{n})\).

Just as in the case of simple null hypothesis, if the null hypothesis is true, then (Problem 16.20)

\[2\log(R_{n})-Q_{n}(\hat{\beta}_{n})\stackrel{{ P}}{{\to}}0. \tag{16.33}\]

Thus, under the assumptions of Theorem 14.4.2 (iii), it follows that, under the null hypothesis,

\[Q_{n}(\hat{\beta}_{n})\stackrel{{ d}}{{\to}}\chi_{k-q}^{2}. \tag{16.34}\]

As in the case of a simple null hypothesis, the problem of testing a composite hypothesis of goodness of fit can be reduced to the multinomial case. Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. according to a model \(\{P_{\theta},\ \theta\in\Omega\}\), where \(\Omega\subseteq\mathbb{R}^{k}\). The null hypothesis specifies \(\theta=f(\beta)\) for some fixed function \(f\) from \(\mathbb{R}^{q}\) to \(\mathbb{R}^{k}\). Now, partition the range of the \(X_{i}\) into \(k+1\) sets \(E_{1}\),..., \(E_{k+1}\), and let \(P_{\theta}\{E_{i}\}\) be the probability of \(E_{i}\) under \(\theta\). Let \(Y_{j}\) denote the number of \(X_{i}\) falling in \(E_{j}\) and let

\[Q_{n}(\beta)=\sum_{j=1}^{k+1}\frac{(Y_{j}-nP_{f(\beta)}\{E_{i}\})^{2}}{nP_{f( \beta)}\{E_{i}\}}\.\]

Then, a test can be based on \(Q_{n}(\hat{\beta}_{n})\), where \(\hat{\beta}_{n}\) is an estimator of \(\beta\) assuming the null hypothesis submodel.

Just as in the case of a simple null hypothesis, the choice of \(k\) (and now also of the sets \(E_{i}\)) is complex; note the references in the previous subsection.1 In addition, a further complication arises, which is the choice of estimator \(\hat{\beta}_{n}\). If the estimator is an efficient likelihood estimator based on the likelihood of the categorized data \(Y_{1}\),..., \(Y_{k+1}\), then we have returned to the setting of the multinomial case considered at the beginning of this section, and the limiting distribution of \(Q_{n}(\hat{\beta}_{n})\) is Chi-squared. On the other hand, one might also estimate \(\beta\) based on the likelihood of the original sample \(X_{1}\),..., \(X_{n}\). In this case, Chernoff and Lehmann (1954) showed that \(Q_{n}(\hat{\beta}_{n})\) need not be Chi-squared. For an example, see Problem 16.24.

### Neyman's Smooth Tests

Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. according to a probability distribution \(P\) on some sample space \(S\). Consider testing the simple null hypothesis \(P=P_{0}\), where \(P_{0}\) is some fixed probability distribution on \(S\). When \(S=\hbox{\rm I$\kern-1.8pt$\rm R$}\), one possible test is the Kolmogorov-Smirnov test, discussed in Section 16.2, which was seen to be consistent in power against any fixed alternative, and uniformly consistent against the large class of distributions \(F\) with \(d_{K}(F,F_{0})>\Delta\) for any small \(\Delta\). Even so, the Kolmogorov-Smirnov test can have poor power against local alternatives; see Problems 16.6 and 16.7. In fact, whenever the family of alternative distributions is large, it is unlikely that there will exist a single test that will perform uniformly well across against all of them, and certainly no UMP test will exist. For a q.m.d. family indexed by a real-valued parameter, one can construct AUMP tests, as discussed in Section 15.3. However, even if the family of alternatives is q.m.d. and indexed by a parameter in \(\hbox{\rm I$\kern-1.8pt$\rm R$}^{2}\), there exists no test that is asymptotically uniformly optimal (Problem 16.25). Thus, one goal might be to construct tests that perform well across a fairly broad range of alternatives. In this spirit, Neyman (1937b) considered large parametric families of alternatives and derived tests that asymptotically maximize minimum (and average) power against these alternatives. Such tests will be described in this section.

Consider the parametric model of densities \(p_{\theta}(x)\) with respect to \(P_{0}\) given by

\[p_{\theta}(x)=C_{k}(\theta)\exp\left[\,\sum_{j=1}^{k}\theta_{j}T_{j}(x)\, \right]\, \tag{16.35}\]

where \(k\) is some positive integer so that \(\theta\in\hbox{\rm I$\kern-1.8pt$\rm R$}^{k}\). Setting \(T_{0}(x)=1\), the functions \(T_{1}\),..., \(T_{k}\) are chosen so that \(T_{0}\),..., \(T_{k}\) is a set of orthonormal functions on \(L^{2}(P_{0})\), the space of functions that are square integrable with respect to \(P_{0}\); that is

\[Cov_{0}[T_{i}(X_{1}),T_{j}(X_{1})]=\int_{S}T_{i}(x)T_{j}(x)dP_{0}(x)=\delta_{i,j}\,\]

where \(\delta_{i,j}=1\) if \(i=j\) and \(\delta_{i,j}=0\) if \(i\neq j\). This implies \(E_{0}(T_{j})=0\) for \(j=1,\ldots,k\). The normalizing constant \(C_{k}(\theta)\) is given by

\[C_{k}(\theta)=\left\{\int_{S}\exp\left[\,\sum_{j=1}^{k}\theta_{j}T_{j}(x)\, \right]dP_{0}(x)\,\right\}^{-1}. \tag{16.36}\]

Let \(\Omega_{k}\) denote the set of \(\theta\) where the integral in (16.36) is finite so that \(p_{\theta}\) is a proper density. We will also assume \(0\) is an interior point of \(\Omega_{k}\), in which case the family of densities constitutes a \(k\)-parameter exponential family of full rank. The null hypothesis asserts \(\theta=0\).

**Example 16.4.1**: **(Testing uniformity using Legendre polynomials)** As a prototype, consider the goodness of fit problem of testing that \(X_{1}\),..., \(X_{n}\) are i.i.d. from the uniform distribution on [0, 1] so that \(S=[0,\,1]\) and \(P_{0}\) is the uniform distribution on [0, 1]. For this problem, Neyman (1937b) chose \(T_{j}(x)\) to be a polynomial of degree \(j\). Specifically, set \(T_{0}(x)=1\), \(T_{1}(x)=\sqrt{3}(2x-1)\), \(T_{2}(x)=\sqrt{5}(6x^{2}-6x+1)\), \(T_{3}(x)=\sqrt{7}(20x^{3}-30x^{2}+12x-1)\), and so on, so that \(T_{j}\) is constructed to be a polynomial of degree \(j\) such that it is orthogonal to \(T_{0}\),...\(T_{j-1}\), and its square integrates to one. The polynomials \(T_{j}\) are the so-called normalized Legendre polynomials. \(\blacksquare\)

Returning to the general case, we next derive Neyman's test as a special case of Rao's score test for testing \(\theta=0\) in the parametric model. The family of densities (16.35) is a \(k\)-parameter exponential family in natural form. By Example 14.2.6, this family is q.m.d. at \(\theta=\theta_{0}=0\). By Theorem 14.2.2, the score vector at \(\theta_{0}=0\) (14.74) is given by

\[Z_{n}^{\top}=n^{-1/2}\left(\frac{\partial}{\partial\theta_{1}}\log L_{n}( \theta),\,\ldots,\,\frac{\partial}{\partial\theta_{k}}\log L_{n}(\theta)\right) \bigg{|}_{\theta=0}\ \,\]

where \(L_{n}(\theta)\) is the likelihood function

\[L_{n}(\theta)=C_{k}^{n}(\theta)\exp\left[\,\sum_{i=1}^{n}\sum_{j=1}^{k}\theta_ {j}T_{j}(X_{i})\,\right]\.\]

Hence,

\[\frac{\partial}{\partial\theta_{m}}\log[L_{n}(\theta)]=n\frac{\partial}{ \partial\theta_{m}}\log[C_{k}(\theta)]+\sum_{i=1}^{n}T_{m}(X_{i})\.\]

But, by Problem 2.16,

\[-\frac{\partial}{\partial\theta_{m}}\log[C_{k}(\theta)]=E_{\theta}[T_{m}(X_{i} )]\,\]

which is 0 when \(\theta=0\) (since we are assuming \(T_{0}(x)=1\) and \(T_{m}\) is orthogonal to \(T_{0}\)). Hence, the score vector at \(\theta_{0}\) reduces to

\[Z_{n}^{\top}=n^{-1/2}\left(\sum_{i=1}^{n}T_{1}(X_{i}),\,\ldots,\,\sum_{i=1}^{n }T_{k}(X_{i})\right). \tag{16.37}\]

By the orthogonality of the \(T_{i}\), we have \(Cov[T_{i}(X_{1}),\,T_{j}(X_{1})]=\delta_{i,\,j}\). Arguing directly, the multivariate Central Limit Theorem implies that, under \(\theta=0\),

\[Z_{n}\stackrel{{ d}}{{\rightarrow}}N(0,\,I_{k})\,\]where \(I_{k}\) is the \(k\times k\) identity matrix. Moreover, the Fisher Information at \(\theta=0\) is \(I(0)=I_{k}\). Therefore, Rao's score test rejects for large values of

\[Z_{n}^{\top}I^{-1}(0)Z_{n}=Z_{n}^{\top}Z_{n}=\sum_{j=1}^{k}Z_{n,j}^{2}\,\]

where

\[Z_{n,j}=n^{-1/2}\sum_{i=1}^{n}T_{j}(X_{i})\.\]

Let \(c_{k,1-\alpha}\) be the \(1-\alpha\) quantile of the \(\chi^{2}\)-distribution with \(k\) degrees of freedom. By the Continuous Mapping Theorem,

\[Z_{n}^{\top}Z_{n}\stackrel{{ d}}{{\rightarrow}}\chi_{k}^{2}\,\]

and so the test \(\phi_{n}^{*}\) which rejects when \(Z_{n}^{\top}Z_{n}>c_{k,1-\alpha}\) is asymptotically consistent in level. The test \(\phi_{n}^{*}\) will be referred to as _Neyman's smooth test_. (Of course, one can always replace \(c_{k,1-\alpha}\) by the exact \(1-\alpha\) quantile of the finite sampling null distribution of \(Z_{n}^{\top}Z_{n}\), or the null distribution can be simulated.)

**Example 16.4.2**: (**Continuation of Example 16.4.1**) In this case,

\[Z_{n,1}^{2}=\left[n^{-1/2}\sum_{i=1}^{n}\sqrt{3}(2X_{i}-1)\right]^{2}=12\left[ n\left(\bar{X}_{n}-\frac{1}{2}\right)^{2}\right]\.\]

Thus, \(Z_{n,1}^{2}\) is large when the sample mean differs from \(1/2\), the hypothesized mean. Similarly, \(Z_{n,j}^{2}\) is large when the first \(j\) sample moments differ greatly from those of \(U(0,1)\).

**Example 16.4.3**: (**The \(\chi^{2}\) test**) As in Section 16.3, consider the goodness of fit problem for testing a multinomial distribution with \(k+1\) categories. For concreteness, suppose \(X_{1}\),..., \(X_{n}\) are i.i.d., each \(X_{i}\) taking the value \(e_{j}\) with probability \(p_{j}\), where \(e_{j}\) is the vector with \(1\) in the \(j\)th component and \(0\) in the remaining \(k\) components. Then, the Chi-squared statistic \(Q_{n}\) given by (16.16) can be viewed as a Neyman's smooth test. Recall \(V_{n}\) given by (16.18) and the matrix \(A\) given by (16.20). Now, let \(Z_{n}\) be the vector \(A^{1/2}V_{n}\), so that \(Q_{n}=Z_{n}^{\top}Z_{n}\). Furthermore, the probability mass function of \(X_{i}\) can be written in the form (16.35) with \(T_{j}\) satisfying \(n^{-1/2}\sum_{i}T_{j}(X_{i})\) equal to the \(j\)th component of \(Z_{n}\) (Problem 16.26). (Note, however, that unlike the Legendre polynomials of Example 16.4.1, the functions \(T_{j}\) depend on \(k\), so that we really have a triangular array of orthonormal functions.)

#### Fixed \(k\) Asymptotics

Assuming the model (16.35) holds, we can apply Corollary 14.4.1 to conclude that, under \(h/n^{1/2}\),

\[Z_{n}^{\top}Z_{n}\stackrel{{ d}}{{\to}}\chi_{k}^{2}(|h|^{2}). \tag{16.40}\]

We now apply Theorems 15.5.4 and 15.5.5 in order to obtain an asymptotic maximin property for \(\phi_{n}^{*}\).

**Theorem 16.4.1**: _Assume the model (16.35) and assume \(\theta=0\) is an interior point of \(\Omega_{k}\). Consider the problem of testing \(\theta=0\). (i) For any sequence of tests \(\phi_{n}\) such that \(E_{0}(\phi_{n})\to\alpha\) and for any \(b\) and \(B\) satisfying \(0<b<B\leq\infty\),_

\[\limsup_{n\to\infty}\inf\{E_{hn^{-1/2}}(\phi_{n}):\ b\leq|h|\leq B\}\leq P\{\chi _{k}^{2}(b^{2})>c_{k,1-\alpha}\}\, \tag{16.41}\]

_where \(\chi_{k}^{2}(b^{2})\) is noncentral Chi-squared with \(k\) degrees of freedom and noncentrality parameter \(b^{2}\). (ii) Neyman's smooth test \(\phi_{n}^{*}\) is asymptotically maximin in the sense that, for any \(0<b<B<\infty\),_

\[\inf\{E_{hn^{-1/2}}(\phi_{n}^{*}):\ b\leq|h|\leq B\}\to P\{\chi_{k}^{2}(b^{2}) >c_{k,1-\alpha}\}. \tag{16.42}\]

_Thus, for any \(0<b<B<\infty\), \(\phi_{n}^{*}\) maximizes_

\[\liminf_{n}\{E_{hn^{-1/2}}(\phi_{n}):\ b\leq|h|\leq B\}\]

_among all tests with asymptotic level \(\alpha\)._

Proof. Theorem 15.5.4 implies (16.41) and Theorem 15.5.5 implies (16.42).

The result (16.41) holds if \(B=\infty\) (since the inf over a larger set is bounded above by the inf over a smaller set). In many cases, one can replace \(B\) by \(\infty\) in (16.42) as well. For example, suppose \(Var_{\theta}[T_{j}(X_{1})]\) is a uniformly bounded function of \(\theta\). Then, (16.42) holds if \(B=\infty\) (Problem 16.27). This condition is satisfied, for example, if the \(T_{j}(x)\) are uniformly bounded functions of \(x\), as they are in Neyman's choice of the Legendre polynomials.

Theorem 16.4.1 states an asymptotic maximin property over alternatives \(\theta\) that are \(O(n^{-1/2})\) from \(\theta=0\). Of course, Neyman's smooth test is also consistent in power against any fixed \(\theta\neq 0\). Actually, it is consistent in power against a broad range of alternatives, not just alternatives in the parametric model (16.35).

To make this statement more precise, first consider Neyman's original construction with \(k=1\) for testing the hypothesis of uniformity, as described in Example 16.4.1. Then, the test statistic reduces to (16.39). The test statistic is designed to have power against distributions with mean not equal to \(1/2\) and it serves this purpose. For, underan alternative distribution \(P\) on \((0,1)\) with mean \(\mu(P)\neq 1/2\), the power of the test which rejects when \(Z_{n,1}^{2}>c_{1,1-\alpha}\) tends to \(1\). To see why, note that by the Weak Law of Large Numbers,

\[\left(\vec{X}_{n}-\frac{1}{2}\right)^{2}\stackrel{{ P}}{{\to}} \left(\mu(P)-\frac{1}{2}\right)^{2}>0\,\]

and so

\[12n\left(\vec{X}_{n}-\frac{1}{2}\right)^{2}\stackrel{{ P}}{{\to}} \infty\.\]

Therefore, by Slutsky's Theorem,

\[P\left\{12n\left(\vec{X}_{n}-\frac{1}{2}\right)^{2}>c_{1,1-\alpha}\right\} \to 1\.\]

The point is that the test will be consistent against any alternative \(P\) with mean \(\mu(P)\neq 1/2\), even if \(P\) is not a member of the parametric model (16.35).

Similarly, for \(k>1\), Neyman's test will be consistent against any distribution \(P\), as long as the first \(k\) moments of \(P\) are not identical to the first \(k\) moments of the uniform distribution (Problem 16.28). Thus, Neyman's test for testing \(P=P_{0}\) has good power across a broader range of distributions than just the original parametric model (16.35).

**Example 16.4.4**: **(Limiting Power Against a Contiguous Sequence)** _Consider a sequence of alternative densities of the form_

\[f_{n}(x)=1+b_{n}u(x)\, \tag{16.43}\]

_where \(b_{n}\to 0\) and \(u\) satisfies_

\[\int_{0}^{1}u(x)dx=0\.\]

_Assume \(\sup|u(x)|<\infty\), so that \(f_{n}\) is a density for \(b_{n}\) small enough. If we set \(b_{n}=hn^{-1/2}\), we can calculate the limiting power of Neyman's smooth test against \(f_{n}\) as follows. The family of densities \(1+\phi u(x)\) is q.m.d. at \(\theta=0\) (Problem 14.6) with score function \(n^{-1/2}\sum_{i}u(X_{i})\). If \(P_{n}\) denotes the probability distribution with density \(f_{n}\) with \(b_{n}=hn^{-1/2}\), then \(P_{n}^{n}\) is contiguous to \(P_{0}^{n}\). Under \(\theta=0\), \((Z_{n}^{\top},n^{-1/2}u(X_{i}))\) is asymptotically multivariate normal. By the multivariate generalization of Corollary 14.3.2 obtained in Problem 14.33, under \(f_{n}\) with \(b_{n}=hn^{-1/2}\),

\[Z_{n}^{\top}\stackrel{{ d}}{{\to}}N(c,I_{k})\,\]

where \(c\) is the vector with \(j\)th component given by \[c_{j}=Cov(Z_{n,j},hn^{-1/2}\sum_{i}u(X_{i}))=h\langle T_{j},u\rangle\,\]

and

\[\langle T_{j},u\rangle=\int_{0}^{1}T_{j}(x)u(x)dx\.\]

Hence, under \(f_{n}\),

\[Z_{n}^{\top}Z_{n}\xrightarrow{d}\chi_{k}^{2}(\delta^{2})\, \tag{16.44}\]

where

\[\delta^{2}=h^{2}\sum_{j=1}^{k}\langle T_{j},u\rangle^{2}\.\]

Thus, the limiting power is \(M(k,\delta^{2})\), with \(M(k,h)\) defined by (16.27). Note that if \(u\) is represented as \(u(x)=\sum_{j=1}^{k}\gamma_{j}T_{j}(x)\), then by Parseval's identity (see A.7),

\[\sum_{j=1}^{k}\langle T_{j},u\rangle^{2}=\int_{0}^{1}u^{2}(x)dx\.\]

Thus, Neyman's test has limiting power exceeding \(\alpha\) against alternatives of the form (16.43) with \(b_{n}\asymp n^{-1/2}\) if \(u\) is in the span of \(T_{1},\ldots,T_{k}\).

#### Neyman's Smooth Tests With Large \(k\)

In the previous section, Neyman's smooth test was shown to be an asymptotically maximin procedure for the parametric model (16.35) with \(k\) fixed. Obviously, the larger the value of \(k\), the greater the number of orthogonal directions used to construct the test statistic. For fixed \(k\), consistency of Neyman's smooth test holds for a restricted class of alternatives. For example, Neyman's construction results in a test of uniformity that is consistent in power against any distribution that does not have the same first \(k\) moments as that of the uniform distribution. This suggests the possibility that, if we let \(k\) increase with \(n\), we can obtain consistency against all distributions because on the unit interval, a distribution is uniquely determined by its moments; see Feller (1971), Section VII.3. To investigate this possibility, we now develop some basic properties of the test based on

\[S_{n,k_{n}}=\sum_{j=1}^{k_{n}}Z_{n,j}^{2}\, \tag{16.45}\]

where \(k_{n}\) is some fixed sequence satisfying \(k_{n}\to\infty\).

For fixed \(k\), we saw that, under \(H_{0}\),

\[\sum_{j=1}^{k}Z_{n,j}^{2}\stackrel{{ d}}{{\to}}\chi_{k}^{2}\.\]

If \(k\) is large, the Chi-squared distribution with \(k\) degrees of freedom is approximately \(N(k,\,2k)\), and so it is reasonable to expect that, under \(H_{0}\),

\[\frac{\sum_{j=1}^{k_{n}}Z_{n,j}^{2}-k_{n}}{(2k_{n})^{1/2}}\stackrel{{ d}}{{\to}}N(0,\,1)\.\]

In order to prove this convergence, we need the following lemma, due to Bentukus (2003), which can be viewed as a multivariate version of the Berry-Esseen Theorem. In the statement of the result, let \({\cal E}_{k}\) denote the class of Euclidean balls in \({\rm I\kern-1.8ptR}^{k}\); that is, the family of sets \(\{y\in{\rm I\kern-1.8ptR}^{k}:\ |x-y|<r\}\) as \(x\in{\rm I\kern-1.8ptR}^{k}\) and \(r>0\) vary. Also, let \({\cal C}_{k}\) denote the class of convex sets in \({\rm I\kern-1.8ptR}^{k}\).

**Lemma 16.4.1**: _Let \(Y_{1},\,Y_{2},\,\ldots,\,Y_{n}\) be i.i.d. random vectors in \({\rm I\kern-1.8ptR}^{k}\) with mean vector 0 and \(k\times k\) identity covariance matrix \(I_{k}\). Let \(\beta=E(|Y_{i}|^{3})\), and let \(Z^{(k)}\) denote a multivariate normal random vector with mean 0 and covariance matrix \(I_{k}\). Then,_

\[\sup_{B\in{\cal C}_{k}}\left|P\{n^{-1/2}\sum_{i=1}^{n}Y_{i}\in B\}-P\{Z^{(k)} \in B\}\right|\leq 400k^{1/4}\beta n^{-1/2}\.\]

_If \({\cal C}_{k}\) is replaced by \({\cal E}_{k}\), then the right side can be replaced by the upper bound \(C\beta n^{-1/2}\), where \(C\) is an absolute constant (independent of \(k\)). Hence,_

\[\sup_{t\in{\rm I\kern-1.8ptR}}\left|P\{|n^{-1/2}\sum_{i=1}^{n}Y_{i}|^{2}\leq t \}-P\{|Z^{(k)}|^{2}\leq t\}\right|\leq C\beta n^{-1/2}\.\]

We now apply the lemma with

\[Y_{i}=(T_{1}(X_{i}),\,\ldots,\,T_{k}(X_{i})) \tag{16.46}\]

so that

\[S_{n,k}=\left|n^{-1/2}\sum_{i=1}^{n}Y_{i}\right|^{2}\.\]

Note that

\[\beta=E\left([T_{1}^{2}(X_{i})+\cdots+T_{k}^{2}(X_{i})]^{3/2}\right)\.\]By Minkowski's Inequality (Problem 16.30),

\[\beta^{2/3}\leq\sum_{j=1}^{k}E[|T_{j}(X_{i})|^{3}]^{2/3}. \tag{16.47}\]

If

\[\sup_{j}E[|T_{j}(X_{i})|^{3}]\leq B<\infty\,\]

then, \(\beta\leq Bk^{3/2}\). Hence, the following is true.

**Theorem 16.4.2**: _Consider \(S_{n,k_{n}}\) given by (16.45), where_

\[Z_{n,j}=n^{-1/2}\sum_{i=1}^{n}T_{j}(X_{i})\,\]

_and let \(T_{0}=1\), and \(T_{0},\,T_{1},\,T_{2},\,\ldots\) be an infinite sequence of orthonormal functions on \(L_{2}(P_{0})\). Assume_

\[\sup_{j}E_{P_{0}}[|T_{j}(X_{i})|^{3}]=B<\infty. \tag{16.48}\]

_If \(k_{n}\to\infty\) and \(k_{n}^{3}/n\to 0\), then, under \(P=P_{0}\),_

\[\frac{S_{n,k_{n}}-k_{n}}{(2k_{n})^{1/2}}\stackrel{{ d}}{{\to}}N( 0,1)\.\]

Proof. Apply the lemma with \(Y_{i}\) given by (16.46). Then,

\[\big{|}P\{S_{n,k_{n}}\leq t\,(2k_{n})^{1/2}+k_{n}\}-P\{|Z^{(k_{n}}|^{2}\leq t\, (2k_{n})^{1/2}+k_{n}\}\big{|}\]

is bounded above by

\[(Bk_{n})^{3/2}n^{-1/2}\to 0\.\]

But, by the Central Limit Theorem,

\[P\{|Z^{(k_{n})}|^{2}\leq t\,(2k_{n})^{1/2}+k_{n}\}\to\Phi(t)\, \tag{16.49}\]

where \(\Phi\) is the standard normal c.d.f., and the result follows.

Under the assumptions of Theorem 16.4.2, the sequence of tests that rejects when

\[\frac{S_{n,k_{n}}-k_{n}}{(2k_{n})^{1/2}}>z_{1-\alpha} \tag{16.50}\]

is asymptotically level \(\alpha\).

**Example 16.4.5**: Let

\[T_{j}(x)=\sqrt{2}\cos(\pi jx)\.\]

Such a choice arises in the construction of the Cramer-von Mises test, which will be discussed further in Example 16.5.1. Under the null hypothesis \(P=P_{0}=U(0,\,1)\),

\[E_{P_{0}}[|T_{j}(X_{i})|^{3}]\leq\sqrt{2}E_{P_{0}}[T_{j}^{2}(X_{i})]=\sqrt{2}\.\]

Hence, the condition (16.48) is satisfied.

Next, we consider the power of (16.50) (with \(k_{n}\to\infty\)) against a fixed alternative. As in Theorem 16.5.1, suppose \(P\) is any probability distribution such that

\[E_{P}[T_{j}(X_{1})]\neq E_{P_{0}}[T_{j}(X_{1})]\]

for some \(j\). Then, for such a \(j\),

\[\frac{Z_{n,j}^{2}}{n}=\left[\frac{1}{n}\sum_{i=1}^{n}T_{j}(X_{i})\right]^{2} \stackrel{{ P}}{{\to}}\left\{E_{P}[T_{j}(X_{1})]\right\}^{2}>0\,\]

by the Weak Law of Large Numbers. Hence,

\[\frac{S_{n,k_{n}}-k_{n}}{(2k_{n})^{1/2}}\geq\frac{Z_{n,j}^{2}-k_{n}}{(2k_{n}) ^{1/2}}=\frac{\frac{Z_{n,j}^{2}}{n}-\frac{k_{n}}{n}}{\frac{(2k_{n})^{1/2}}{n}} \stackrel{{ P}}{{\to}}\infty\]

if \(k_{n}/n\to 0\). Hence, the test (16.50) (or the test that rejects if \(S_{n,k_{n}}>c_{k_{n},1-\alpha}\)) satisfies

\[P\left\{\frac{S_{n,k_{n}}-k_{n}}{(2k_{n})^{1/2}}>z_{1-\alpha}\right\}\geq P \left\{\frac{Z_{n,j}^{2}-k_{n}}{(2k_{n})^{1/2}}>z_{1-\alpha}\right\}\to 1\]

and is therefore pointwise consistent in power against \(P\).

Note that the condition \(k_{n}/n\to 0\) is a sufficient condition to ensure the test statistic \([S_{n,k_{n}}-k_{n}]/(2k_{n})^{1/2}\) tends to \(\infty\) in probability under an alternative \(P\). The stronger condition \(k_{n}^{3}/n\to 0\) is sufficient to show asymptotic normality under the null hypothesis. These conditions can be weakened, but the message is that one can obtain consistency against a broad family of distributions by letting \(k\) increase with \(n\).

Next, we discuss the limiting power of the test (16.50) against a local sequence of alternatives. Suppose we consider alternatives of the form (16.35) used in the construction of Neyman's smooth tests. Specifically, consider the family of densities indexed by \(\theta_{1}\in{\rm I\kern-1.8ptR}\) given by

\[p_{\theta_{1}}(x)=C_{1}(\theta_{1})\exp[\theta_{1}T_{1}(x)]\.\]

[MISSING_PAGE_FAIL:809]

Now, if \(T_{0}\), \(T_{1}\), \(T_{2}\), \(\ldots\) form a complete orthonormal system for the space of square integrable functions on (0, 1), then

\[\sum_{j=1}^{k_{n}}\langle T_{j},\,u\rangle^{2}\to\int_{0}^{1}u^{2}(x)dx\.\]

Therefore, if we take \(b_{n}=(2k_{n})^{1/4}/n^{1/2}\), we expect that (16.52) holds, where

\[\mu=\int_{0}^{1}u^{2}(x)dx\.\]

In fact, such a result is proved in Eubank and LaRiccia (1992) in the case \(T_{j}(x)=\sqrt{2}\cos(\pi\,jx)\) if \(k_{n}^{5}/n^{2}\to 0\). The conclusion is that Neyman's test with increasing order \(k_{n}\) has nonnegligible power against alternatives converging to the null at rate \(k_{n}^{1/4}/n^{1/2}\). This result suggests that \(k_{n}\) should not increase too quickly.

Further theoretical results concerning Neyman's smooth tests, especially in regard to the choice of \(k\), can be found in Eubank and LaRiccia (1992), Ledwina (1994), Kallenberg and Ledwina (1995), Fan (1996), and Inglot and Ledwina (1996). This growing literature includes simulation studies which show that Neyman's smooth tests perform well across a broad range of alternatives and are competitive with existing tests.

### Weighted Quadratic Test Statistics

In the construction of Neyman's smooth tests based on \(k\), equal weight was given to the first \(k\)_directions_ determined by the orthonormal functions \(T_{1}\), \(T_{2}\), \(\ldots\). Instead, one might consider modifying the test statistic so that different weights are given to different directions; with such a modification, it becomes possible to consider an infinite number of directions. Such weighted quadratic test statistics are considered in this section.

Under the setup and notation of Section 16.4, consider the problem of testing the simple null hypothesis \(H_{0}:\ P=P_{0}\). Let \(T_{0}=1\) and suppose \(T_{0}\), \(T_{1}\), \(T_{2}\), \(\ldots\) is an infinite sequence of orthonormal functions on \(L_{2}(P_{0})\). Let \(Z_{n,j}\) be defined by (16.38) and consider the test statistic

\[W_{n}=\sum_{j=1}^{\infty}a_{j}\,Z_{n,j}^{2}\, \tag{16.53}\]

where \(a_{j}\) is a sequence of nonnegative numbers. Typically, we would choose \(a_{j}\) to decrease with \(j\), so that less weight is given to the \(j\)th component making up \(W_{n}\).

Note that \(W_{n}\) is only computable if only finitely many \(a_{j}\) are nonzero, or--as will be exemplified later-- the infinite sum can be explicitly evaluated by an alternative computable formula.

Let \(F_{W_{n}}\) denote the c.d.f. of \(W_{n}\) under \(P_{0}\), and set

\[w_{n,1-\alpha}=\inf\{x:\ F_{W_{n}}(x)\geq 1-\alpha\}\.\]

The following result summarizes some basic properties of \(W_{n}\).

**Theorem 16.5.1**: _Assume \(a_{j}\geq 0\) and \(\sum_{j}a_{j}<\infty\). (i) Under \(H_{0}\), \(W_{n}\) is a well-defined random variable; that is, \(W_{n}<\infty\) with probability one. (ii) Under \(H_{0}\),_

\[W_{n}\stackrel{{ d}}{{\rightarrow}}W=\sum_{j=1}^{\infty}a_{j}Z_{j }^{2}\,\]

_where \(Z_{1},Z_{2},\ldots\) are i.i.d. \(N(0,1)\) random variables, and \(W\) has a continuous distribution function \(F_{W}\) which is strictly increasing on \((0,\infty)\). (iii) Let \(w_{1-\alpha}\) denote the \(1-\alpha\) quantile of the distribution of \(W\), so that_

\[F_{W}(w_{1-\alpha})=1-\alpha\.\]

_Then, \(w_{n,1-\alpha}\to w_{1-\alpha}\). (iv) Assume \(a_{j}\) is such that \(a_{j}>0\). Suppose \(P\) is any probability distribution such that_

\[E_{P}[T_{j}(X_{1})]\neq E_{P_{0}}[T_{j}(X_{1})] \tag{16.54}\]

_(where the expectation on the left side is assumed to exist). Then, the limiting power of the test that rejects when \(W_{n}>w_{n}(1-\alpha)\) against the alternative \(P\) is one. Hence, if all the \(a_{j}\) satisfy \(a_{j}>0\), then the test is consistent in level against any \(P\) which satisfies (16.54) for some \(j\)._

Proof. First, note that

\[0\leq E_{\theta_{0}}(W_{n})=\sum_{j=1}^{\infty}a_{j}\,Var_{\theta_{0}}(Z_{n,j} )\leq\sum_{j=1}^{\infty}a_{j}E_{\theta_{0}}T_{j}^{2}(X_{1})=\sum_{j=1}^{\infty }a_{j}<\infty\.\]

Part (i) follows, since a nonnegative random variable with a finite mean is finite with probability one. To prove (ii), first note that \(W\) is a well-defined random variable since

\[E(W)=\sum_{j=1}^{\infty}a_{j}<\infty\.\]Now, let

\[W^{(k)}=\sum_{j=1}^{k}a_{j}Z_{j}^{2}\.\]

Then, \(W^{(k)}\stackrel{{ d}}{{\to}}W\) as \(k\to\infty\). Indeed,

\[0\leq W-W^{(k)}=\sum_{j=k+1}^{\infty}a_{j}Z_{j}^{2}\stackrel{{ p}}{{\to}}0\]

since, by Markov's Inequality (Problem 11.27), for \(\delta>0\),

\[P\{W-W^{(k)}>\delta\}\leq\frac{E(W-W^{(k)})}{\delta}=\frac{\sum_{j=k+1}^{ \infty}a_{j}}{\delta}\to 0\]

as \(k\to\infty\). Moreover, the distribution of \(W\) is continuous and strictly increasing (Problem 16.33). To show that \(W_{n}\) converges in distribution to \(W\), write

\[W_{n}=W_{n}^{(k)}+R_{n}^{(k)}\,\]

where

\[W_{n}^{(k)}=\sum_{j=1}^{k}a_{j}Z_{n,j}^{2}\.\]

For any fixed \(k\), the multivariate Central Limit Theorem yields

\[(Z_{n,1},\ldots,Z_{n,k})\stackrel{{ d}}{{\to}}(Z_{1},\ldots,Z_{k})\.\]

By the Continuous Mapping Theorem,

\[P\{W_{n}\leq t\}\leq P\{W_{n}^{(k)}\leq t\}\to P\{W^{(k)}\leq t\}\.\]

Therefore, for any \(k\),

\[\limsup_{n}P\{W_{n}\leq t\}\leq P\{W^{(k)}\leq t\}\]

and so

\[\limsup_{n}P\{W_{n}\leq t\}\leq\lim_{k\to\infty}P\{W^{(k)}\leq t\}=P\{W\leq t\}. \tag{16.55}\]

Similarly, for any \(\delta>0\),

\[P\{W_{n}\leq t\}\geq P\{W_{n}\leq t,\ \ R_{n}^{(k)}<\delta\}\geq P\{W_{n}^{(k)} \leq t-\delta,\ \ R_{n}^{(k)}<\delta\}\.\]Using the general inequality \(P(AB)\geq P(A)-P(AB^{c})\) yields

\[P\{W_{n}\leq t\}\geq P\{W_{n}^{(k)}\leq t-\delta\}-P\{R_{n}^{(k)}\geq\delta\}\;.\]

But, by Markov's Inequality,

\[P\{R_{n}^{(k)}\geq\delta\}\leq\delta^{-1}E(R_{n}^{(k)})\leq\delta^{-1}\sum_{j=k +1}^{\infty}a_{j}\;.\]

Hence, for any \(\delta\) and \(k\),

\[P\{W_{n}\leq t\}\geq P\{W_{n}^{(k)}\leq t-\delta\}-\delta^{-1}\sum_{j=k+1}^{ \infty}a_{j}\]

and so

\[\liminf_{n}\,P\{W_{n}\leq t\}\geq P\{W^{(k)}\leq t-\delta\}-\delta^{-1}\sum_{j= k+1}^{\infty}a_{j}\;.\]

Now, let \(k\to\infty\) to conclude

\[\liminf_{n}\,P\{W_{n}\leq t\}\geq P\{W\leq t-\delta\}\;.\]

Letting \(\delta\to 0\) and using the continuity of the distribution of \(W\), we conclude

\[\liminf_{n}\,P\{W_{n}\leq t\}\geq P\{W\leq t\}\]

Combining (16.55) and (16.56) yields (ii).

Part (iii) follows from Lemma 11.2.1. To prove (iv), suppose \(j\) is such that

\[E_{P}[T_{j}(X_{1})]\neq E_{P_{0}}[T_{j}(X_{1})]\;.\]

By the Law of Large Numbers,

\[\frac{1}{n}\sum_{i=1}^{n}T_{j}(X_{i})\stackrel{{ P}}{{\to}}E_{P}[ T_{j}(X_{1})]\]

and so

\[|Z_{n,j}|=|n^{1/2}\cdot\frac{1}{n}\sum_{i=1}^{n}\{T_{j}(X_{i})-E_{P_{0}}[T_{j}( X_{i})]\}|\stackrel{{ P}}{{\to}}\infty\;.\]

[MISSING_PAGE_FAIL:814]

and \(a_{j}=1/(\pi^{2}j^{2})\). To see this, note that the functions \(\sqrt{2}\sin(\pi jx)\), \(j=1,2,\ldots\) form an orthonormal basis of the space \(L_{2}[0,1]\), the (equivalence class of) functions that are square integrable on \([0,1]\) (see Section A.3). By Parseval's formula (A.7), it follows that

\[C_{n}=n\sum_{j=1}^{\infty}\left\{\int_{0}^{1}[\hat{F}_{n}(x)-x]\sqrt{2}\sin(\pi jx )dx\right\}^{2}\.\]

By integration by parts (Billingsley (1995), Theorem 18.4),

\[\int_{0}^{1}[\hat{F}_{n}(x)-x]\sqrt{2}\sin(\pi jx)dx=\frac{-1}{\pi j}\int_{0}^{ 1}\sqrt{2}\cos(\pi jx)d(\hat{F}_{n}(x)-x)\]

\[=\frac{-1}{\pi j}\int_{0}^{1}\sqrt{2}\cos(\pi jx)d\hat{F}_{n}(x)=-\frac{1}{\pi jn} \sum_{i=1}^{n}T_{j}(X_{i})=-\frac{Z_{n,j}}{\pi jn^{1/2}}\.\]

Hence,

\[C_{n}=\sum_{j=1}^{\infty}\frac{1}{\pi^{2}j^{2}}Z_{n,j}^{2}\,\]

as required.

By Theorem 16.5.1, it follows that, under the null hypothesis,

\[C_{n}\stackrel{{ d}}{{\to}}\sum_{j=1}^{\infty}\frac{1}{\pi^{2}j^{2 }}Z_{j}^{2}\,\]

where \(Z_{1}\), \(Z_{2}\), \(\ldots\) is a sequence of i.i.d. standard normal random variables. It also follows that the test is pointwise consistent in power against any alternative c.d.f. \(F\) for which

\[E_{F}[T_{j}(X_{1})]=\int_{0}^{1}\sqrt{2}\cos(\pi jx)dF(x)\neq\int_{0}^{1}\sqrt{ 2}\cos(\pi jx)dF_{0}(x)=0\]

for some \(j\). But,

\[\int_{0}^{1}\cos(\pi jx)dF(x)=0\ \ \ \mbox{for}\ \ \mbox{all}\ j=1,2,\ldots\]

implies \(F=F_{0}\) (Problem 16.36), and so the test is pointwise consistent in power against any \(F\neq F_{0}\).

**Example 16.5.2**: **(The Anderson-Darling Test)** As in Example 16.5.1 for testing \(F(x)=F_{0}(x)=x\), consider the Anderson-Darling statistic defined by

\[A_{n}=n\int_{0}^{1}\frac{[\hat{F}_{n}(x)-x]^{2}}{x(1-x)}dx\quad. \tag{16.58}\]

It can be shown (Problem 16.37) that \(A_{n}\) has the form (16.38) of a weighted quadratic test statistic with

\[a_{j}=\frac{1}{j(j+1)}\]

and \(T_{j}(x)\) the \(j\)th normalized Legendre polynomial on \([0,1]\) (used in Neyman's original proposal of Neyman's smooth tests; see Section 16.4). Thus,

\[A_{n}=\sum_{j=1}^{\infty}\frac{1}{j(j+1)}Z_{n,j}^{2}\ \, \tag{16.59}\]

(while Neyman's test corresponds to \(\sum_{j=1}^{k}Z_{n,j}^{2}\)). It then follows that, under \(F=F_{0}\),

\[A_{n}\stackrel{{ d}}{{\rightarrow}}\sum_{j=1}^{\infty}\frac{1}{j (j+1)}Z_{j}^{2}\.\]

In fact, many test statistics defined by an integral of the form

\[\int_{0}^{1}U^{2}(x)dx\]

can be rewritten in the form of a weighted quadratic test statistic. A general treatment of such integral tests of fit can be found in Chapter 5 of Shorack and Wellner (1986); also, see van der Vaart and Wellner (1996).

Theorem 16.5.1 considered the behavior of a general weighted quadratic test under the null hypothesis \(P=P_{0}\) and under a fixed alternative. Next, we would like to consider the behavior of \(W_{n}\) under a sequence of local alternatives \(P_{n}\).

Suppose \(P_{n}\) has density \(p_{n}\) and \(P_{0}\) has density \(p_{0}\) with respect to some common measure \(\mu\). Consider the likelihood ratio based on \(n\) i.i.d. observations \(X_{1},\ldots,X_{n}\) given by

\[L_{n}=L_{n}(X_{1},\ldots,X_{n})=\frac{\prod_{i=1}^{n}p_{n}(X_{i})}{\prod_{i=1}^ {n}p_{0}(X_{i})}\.\]Assume, under \(P_{0}\),

\[\log(L_{n})=n^{-1/2}\sum_{i=1}^{n}\tilde{\eta}(X_{i})-\frac{\sigma^{2}}{2}+o_{P_{0 }^{n}}(1)\, \tag{16.60}\]

where

\[E_{P_{0}}[\tilde{\eta}(X_{i})]=0\]

and

\[0<E_{P_{0}}[\tilde{\eta}^{2}(X_{i})]=\sigma^{2}<\infty\.\]

Then, the Central Limit Theorem implies that, under \(P_{0}\),

\[\log(L_{n})\stackrel{{ d}}{{\to}}N\left(-\frac{\sigma^{2}}{2}, \sigma^{2}\right)\]

and \(\{P_{n}^{n}\}\) and \(\{P_{0}^{n}\}\) are contiguous (by Corollary 14.3.1). Furthermore, under \(P_{0}\),

\[Z_{n,j}=n^{-1/2}\sum_{i=1}^{n}T_{j}(X_{i})\stackrel{{ d}}{{\to} }N(0,1)\.\]

By the bivariate Central Limit Theorem, under \(P_{0}\), \((Z_{n,j},\,\log(L_{n}))\) is asymptotically bivariate normal with asymptotic covariance

\[c_{j}=Cov_{P_{0}}[T_{j}(X_{1}),\,\tilde{\eta}(X_{1})]. \tag{16.61}\]

It follows from Corollary 14.3.2 that, under \(P_{n}\),

\[Z_{n,j}\stackrel{{ d}}{{\to}}N(c_{j},1)\.\]

Similarly, for any fixed integer \(k\) and constants \(\alpha_{1}\),..., \(\alpha_{k}\), under \(P_{0}\),

\[\sum_{j=1}^{k}\alpha_{j}\,Z_{n,j}=n^{-1/2}\sum_{i=1}^{n}\sum_{j=1}^{k}\alpha_{j }\,T_{j}(X_{i})\stackrel{{ d}}{{\to}}N\left(0,\sum_{j=1}^{k} \alpha_{j}^{2}\right)\]

and

\[\left(\sum_{j=1}^{k}\alpha_{j}\,Z_{n,j},\,\log(L_{n})\right)\]

[MISSING_PAGE_EMPTY:2420]

\[\sum_{j=1}^{\infty}a_{j}c_{j}^{2} =\sum_{j=1}^{\infty}a_{j}Cov_{P_{0}}^{2}[T_{j}(X_{i}),\tilde{\eta}(X _{i})]\] \[\leq\sum_{j=1}^{\infty}a_{j}Var_{P_{0}}[T_{j}(X_{i})]Var_{P_{0}}[ \tilde{\eta}(X_{i})]\] \[=Var_{P_{0}}[\tilde{\eta}(X_{i})]\cdot\sum_{j=1}^{\infty}a_{j}<\infty\.\]

Hence, the condition (16.63) in (i) holds.

**Example 16.5.3**: **(Limiting Power Calculation)** As in Example 16.4.4, let \(f_{n}(x)\) be given by (16.43) with \(b_{n}=hn^{-1/2}\). As noted in Example 16.4.4, under \(f_{n}\),

\[(Z_{n,1},\ldots,Z_{n,k})^{\top}\stackrel{{ d}}{{\to}}N(c,I_{k})\,\]

where \(c\) has \(j\)th component \(c_{j}=h\langle T_{j},u\rangle\). Note that

\[\sum_{j}a_{j}c_{j}^{2}\leq h^{2}\int_{0}^{1}u^{2}(x)dx\sum_{j}a_{j}<\infty\.\]

Therefore, by Theorem 16.5.2, (16.64) holds.

Assume the hypothesis in Theorem 16.5.2 (ii). Let \(w_{1-\alpha}\) be the \(1-\alpha\) quantile of the limit distribution under the null hypothesis. Then, the limiting power against \(P_{n}\) is given by

\[P\left\{\sum_{j=1}^{\infty}a_{j}(Z_{j}+c_{j})^{2}>w_{1-\alpha}\right\}. \tag{16.65}\]

If there exists a nonzero \(c_{j}\) for which \(a_{j}>0\), then (16.65) exceeds \(\alpha\) (Problem 16.41). For example, if \(a_{j}>0\) for all \(j\), then the requirement is that there exists some \(j\) for which \(c_{j}\) is nonzero. But, this must be the case if \(1\), \(T_{1}\), \(T_{2}\), \(\ldots\) form an orthonormal basis for \(L_{2}(P_{0})\), because Parseval's identity implies

\[0<Var_{P_{0}}[\tilde{\eta}(X_{1})]=\sum_{j=1}^{\infty}c_{j}^{2}\.\]

It follows that not all \(c_{j}\) can be \(0\).

Thus, unlike Neyman's smooth test with \(k_{n}\to\infty\), the limiting power for \(W_{n}\) is nontrivial against certain contiguous alternatives, and so it appears that tests based on \(W_{n}\) are better at detecting alternatives that are close to \(H_{0}\). However, we now show that the limiting power of \(W_{n}\) can be \(\alpha\) against a contiguous sequence of alternatives.

**Example 16.5.4**: (**Another Local Power Calculation**) Let

\[T_{j}(x)=\sqrt{2}\cos(\pi jx)\.\]

Set \(p_{\theta}(x)=C(\theta)\exp[\theta T_{B}(x)]\). If \(B\) is fixed and large, the limiting distribution of \(W_{n}\) against \(\theta=hn^{-1/2}\) is given by the distribution of \(a_{B}(Z_{B}+h)^{2}\). Since \(a_{B}\to 0\) as \(B\to\infty\), it follows that

\[a_{B}(Z_{B}+h)^{2}\stackrel{{ P}}{{\to}}0\]

as \(B\to\infty\). Therefore, the limiting power against such a sequence is small. In order to obtain a limiting value of \(\alpha\), let

\[f_{n}(x)=C_{n}(\theta)\exp[\theta T_{n}(x)]. \tag{16.66}\]

Then, if \(\theta=hn^{-1/2}\), the limiting power of the test based on \(W_{n}\) against such a sequence is \(\alpha\), even though \(P_{n}^{n}\) is contiguous to \(P_{0}^{n}\), where \(P_{n}\) is the distribution with density \(f_{n}\) when \(\theta=hn^{-1/2}\) (Problem 16.39).

A difficulty in applying a weighted quadratic test statistic is the computation of critical values and power. Of course, one may resort to Monte Carlo simulation of the null distribution. Alternatively, the representation of the limiting distribution as that of

\[W=\sum_{j=1}^{\infty}a_{j}(Z_{j}+c_{j})^{2} \tag{16.67}\]

can be useful. For example, the null distribution (in the case \(c_{j}=0\)) has characteristic function

\[\zeta_{W}(t)=\prod_{j=1}^{\infty}(1-2ia_{j}t)^{-1/2}\]

(Problem 16.40). In the special case of the Cramer-von Mises test, Smirnov inverted \(\zeta_{W}\) (see Durbin (1973)) and obtained

\[P\{W>x\}=\frac{1}{\pi}\sum_{j=1}^{\infty}(-1)^{j+1}\int_{(2j-1)^{2}\pi^{2}}^{4 j^{2}\pi^{2}}\frac{1}{y}\sqrt{\frac{-\sqrt{y}}{\sin(\sqrt{y})}}\exp\left(- \frac{xy}{2}\right)dy\.\]

Alternatively, one may truncate the series (16.67) to a finite sum and use numerical methods; see Durbin and Knott (1972). Another possibility is to match moments of \(W\) to a Pearson family of distributions, as done by Stephens (1976).

Some numerical power comparisons between competing goodness of fit tests can be found in Durbin and Knott (1972) and Stephens (1974), where both the Anderson-Darling and Cramer-von Mises statistics outperform the Kolmogorov-Smirnov test. A further comparison is presented in D'Agostino and Stephens (1986), Section 8.14. However, Example 16.5.4 shows that tests based on weighted quadratic statisticscan have poor power against higher frequency alternatives, such as (16.66). In the case of the Cramer-von Mises statistic and the Anderson-Darling statistic, this can be explained by the rapid downweighting of the \(a_{j}\). Moreover, several simulation studies have demonstrated that Neyman's smooth tests can outperform tests based on \(W_{n}\) over a wide range of alternatives; see Miller and Quesenberry (1979), Rayner and Best (1989), and Eubank and LaRiccia (1992). In summary, both Neyman's smooth tests and weighted quadratic tests offer viable approaches to testing goodness of fit, but neither approach is asymptotically uniformly optimal. Unfortunately, we will see in the next section that no test can perform uniformly well against local or contiguous alternatives when the family of possible alternatives is large.

### Global Behavior of Power Functions

For testing uniformity, the Kolmogorov-Smirnov and the weighted quadratic tests such as the Cramer-von Mises test are consistent in power against any alternative. Even the Chi-squared test with a finite number of partitions and the Neyman's smooth tests with finite \(k\) are consistent in power against a broad range of alternatives. However, as we will see in this section, the power of any goodness of fit test is poor against a local sequence of (contiguous) alternatives, except possibly in a finite (bounded) number of directions, even with increasing sample size. Such a statement is not surprising for Neyman's smooth tests with \(k\) fixed, since then only a finite number of orthogonal directions are used. While a quadratic test statistic gives positive weight to infinitely many components, the weights \(a_{j}\) satisfy \(\sum_{j}a_{j}<\infty\); this condition evidently entails

\[\sum_{j=k+1}^{\infty}a_{j}<\epsilon\]

for large enough \(k\), so that the test essentially only uses a finite number of directions as well; roughly, the test behaves similar to the corresponding test obtained by summing over only the first \(k\) components. (For a rigorous statement, see Milbrodt and Strasser (1990, Remark 2.6) and Janssen (1995).) Thus, while consistency may hold against any fixed alternative as \(n\to\infty\), there remains the possibility that, for any fixed sample size \(n\), any test will perform poorly against a broad range of alternatives. Moreover, one cannot simply increase \(k\) to obtain power against a broader family of distributions. As we saw in the case of the Chi-squared test of uniformity with \(k+1\) cells, while increasing \(k\) increases the set of consistent alternatives, it will decrease the limiting power against contiguous alternatives. Roughly speaking, we will see that one can only obtain reasonable power locally across a family of distributions of fixed bounded dimension.

In order to make this precise, first consider the following normal model, which arises as the limiting experiment for testing goodness of fit in Section 16.4. The argument leading to the optimality result (16.42) was based on the fact that, for the 

[MISSING_PAGE_EMPTY:2424]

\[E_{t_{i}e_{i}}[\phi(Z_{1},\ldots,Z_{k})] =\int\phi(z_{1},\ldots,z_{k})\varphi(z_{i}-t_{i})\prod_{j\neq i} \varphi(z_{j})\prod_{i=1}^{k}dz_{i}\] \[=\int\phi(z_{1},\ldots,z_{k})\frac{\varphi(z_{i}-t_{i})}{\varphi( z_{i})}\prod_{i=1}^{k}\varphi(z_{i})dz_{i}\] \[=\int\phi(z_{1},\ldots,z_{k})\exp\left(t_{i}z_{i}-\frac{t_{i}^{2} }{2}\right)\prod_{i=1}^{k}\varphi(z_{i})dz_{i}\] \[=E_{0}[\phi(Z_{1},\ldots,Z_{k})Y_{i}]+\alpha\]

and so

\[E_{t_{i}e_{i}}[\phi(Z_{1},\ldots,Z_{k})]-\alpha=Cov_{0}(\phi,Y_{i})\.\]

Define

\[\beta_{i}=\begin{cases}\frac{Cor_{0}(\phi,Y_{i})}{Var_{0}(Y_{i})}&\text{if } Var_{0}(Y_{i})>0\\ 0&\text{otherwise.}\end{cases} \tag{16.69}\]

Note that, if \(t_{i}\neq 0\), then \(Var_{0}(Y_{i})>0\); if \(t_{i}=0\), then \(Y_{i}=0\) and \(\beta_{i}=0\). Define \(\tilde{\phi}\) by the relation

\[\phi(Z_{1},\ldots,Z_{k})-\alpha=\sum_{i=1}^{k}\beta_{i}Y_{i}+\tilde{\phi}\]

so that

\[E_{0}(\tilde{\phi})=0\,\quad E_{0}(\tilde{\phi}^{2})<\infty\]

and

\[Cov_{0}(\tilde{\phi},Y_{i})=0\quad i=1,\ldots n\.\]

This implies \(\tilde{\phi}\) is uncorrelated with \(\phi-\tilde{\phi}\), and so

\[Var_{0}(\phi)=Var_{0}(\tilde{\phi}+\phi-\tilde{\phi})=Var_{0}(\tilde{\phi})+ Var_{0}(\phi-\tilde{\phi})\.\]

Therefore,

\[Var_{0}(\phi-\tilde{\phi})\leq Var_{0}(\phi)=E_{0}(\phi^{2})-\alpha^{2}\leq \alpha(1-\alpha)\.\]

Also,

\[\sum_{i=1}^{k}\beta_{i}^{2}Var_{0}(Y_{i})=Var_{0}\left(\sum_{i=1}^{k}\beta_{i} Y_{i}\right)=Var_{0}(\phi-\tilde{\phi})\leq\alpha(1-\alpha). \tag{16.70}\]

But,

\[E_{t_{i}e_{i}}(\phi)-\alpha=\beta_{i}Var_{0}(Y_{i})\]implies

\[|E_{t_{i}e_{i}}(\phi)-\alpha|^{2}\leq\beta_{i}^{2}Var_{0}(Y_{i})\cdot Var_{0}(Y_{i })\leq\beta_{i}^{2}Var_{0}(Y_{i})(\exp(H^{2})-1)\.\]

Summing over \(i\) and using the bound (16.70) yields the result.

Notice that the bound on the right side of (16.68) does not depend on \(k\), the dimension of the parameter space. In fact, the same bound holds for tests based on an infinite sequence \(Z_{1},Z_{2},\ldots\). In order to avoid certain technical aspects of likelihoods on infinite product spaces, we restrict attention to the case of \(k\) finite.

We now use the previous lemma to show that, for the normal testing problem studied in Lemma 16.6.1, the power of any level \(\alpha\) test is poor, except possibly on a restricted range of alternatives. Thus, for fixed large \(k\), it is impossible to construct a test that has high power in all directions (which certainly implies the same conclusion for any larger \(k\) or when \(k=\infty\)). The following notation will be used. For a set \(V\) in \({\rm I\kern-1.8ptR}^{k}\), let \(V^{\perp}\) be defined as

\[V^{\perp}=\{x:\ \langle x,\,v\rangle=0\quad\mbox{for all}\ \ v\in V\}\.\]

**Theorem 16.6.1**: _Suppose \(Z_{1},\ldots,Z_{k}\) are independent, with \(Z_{i}\) normally distributed with mean \(h_{i}\) and variance one. The parameter \(h=(h_{1},\ldots,h_{k})^{\top}\) varies freely in \({\rm I\kern-1.8ptR}^{k}\). For testing \(h=0\) versus \(h\neq 0\), let \(\phi=\phi(Z_{1},\ldots,Z_{k})\) be any test with \(E_{0}(\phi)=\alpha\). Fix any \(\epsilon\) and any \(H>0\). Assume_

\[k>1+\epsilon^{-1}\alpha(1-\alpha)[\exp(H^{2})-1]. \tag{16.71}\]

_Then, there exists a linear subspace \(V\), whose dimension \(d\) is independent of \(k\) and \(\phi\), such that_

\[\sup\{|E_{h}(\phi)-\alpha|:\ h\in V^{\perp},\ |h|\leq H\}\leq\epsilon \tag{16.72}\]

_and_

\[d\leq 1+\epsilon^{-1}\alpha(1-\alpha)[\exp(H^{2})-1]. \tag{16.73}\]

_In other words, the power of \(\phi\) is poor on \(V^{\perp}\bigcap\{h:\ |h|\leq H\}\)._

Proof. Let \(V_{0}=\{0\}\). We will inductively choose linear subspaces \(V_{n}=\mbox{span}\{v_{1},\ldots,v_{n}\}\) of \({\rm I\kern-1.8ptR}^{k}\) as follows. Given \(v_{1},\ldots,v_{n}\), let \(v_{n+1}\) be orthogonal to \(v_{1}\),..., \(v_{n}\) and satisfy \(|v_{n+1}|=1\) and

\[\left[\sup|E_{tv}(\phi)-\alpha|:\ |t|\leq H,\ v\in V_{n}^{\perp},\ |v|=1 \right]^{2}\leq|E_{t_{n+1}v_{n+1}}(\phi)-\alpha|^{2}+\frac{\epsilon}{2^{n+1}}\.\]

Let \(b_{n+1}=|E_{t_{n+1}v_{n+1}}(\phi)-\alpha|^{2}\). Choose \(m\) to be the smallest positive integer satisfying

\[b_{m}+\frac{\epsilon}{2^{m}}\leq\epsilon. \tag{16.74}\]

[MISSING_PAGE_EMPTY:2427]

_(i)_

\[\limsup_{n}\sum_{i=1}^{k}\big{[}\sup|E_{t\varepsilon_{i}n^{-1/2}}(\phi_{n})-\alpha| :\ |t|\leq H\big{]}^{2} \tag{16.75}\]

\[\leq\alpha(1-\alpha)[\exp(H^{2})-1]\.\]

_(ii) There exists a subspace \(V\) of \(\hbox{\rm I\kern-2.0pt\hbox{\rm R}}^{k}\) whose dimension \(d\) satisfies (16.73) (independent of \(k\)) such that_

\[\limsup_{n}\sup\{|E_{hn^{-1/2}}(\phi_{n})-\alpha|:\ h\in V^{\perp},\ |h|\leq H\} \leq\epsilon. \tag{16.76}\]

Proof. The sequence of models \(P_{hn^{-1/2}}^{n}\) is asymptotically normal with identity covariance matrix \(I_{k}\), in the sense of Definition 15.4.1. Indeed, the family is an exponential family and hence is quadratic mean differentiable. In fact, as previously pointed out, the score vector for this model is given by (16.37) and is asymptotically multivariate normal with mean \(0\) and identity covariance matrix. The proof then follows from Theorem 15.4.1, which compares the limiting power of any test sequence with that of a test for the normal model studied in Lemma 16.6.1. For the limiting normal experiment, an upper bound for the sum of squared powers is given in Lemma 16.6.1, and so this bound must hold asymptotically. Similarly, (ii) follows by Theorem 16.6.1.

Of course, the theorem has implications for testing \(P=P_{0}\) against alternatives outside the parametric model (16.35). Indeed, since the right side of (16.75) does not depend on \(k\), we may take \(k=\infty\) on the left side and obtain the same result. That is, the squared infinite sum of deviations of power from \(\alpha\) remains bounded. We have stated the result first for finite \(k\) since our proof then only requires convergence to a normal experiment in a finite dimensional space (as we have not considered infinite dimensional spaces).

In fact, Janssen (2000a) shows that this result holds for each \(n\) as well; that is, one can simply delete the limsup in (16.75). Thus, the power of any test sequence is essentially flat outside a space of dimension \(d\), where \(d\) does not depend on \(n\).

To explain the result a little further, fix \(\theta\in\hbox{\rm I\kern-2.0pt\hbox{\rm R}}^{k}\) and consider the one-dimensional model indexed by \(t\) with density \(p_{t\theta}\) defined in (16.35). If we know that the actual distribution belongs to this one-dimensional exponential family submodel for some \(t>0\), then a UMP level \(\alpha\) test sequence exists for testing \(t=0\) against \(t>0\), which we now denote by \(\phi_{\theta}^{*}=\{\phi_{n,\theta}^{*}\}\); moreover,

\[\lim_{n}E_{t\theta n^{-1/2}}(\phi_{n,\theta}^{*})=1-\Phi(z_{1-\alpha}-t|\theta|) \tag{16.77}\]

(Problem 16.42). We will now connect the performance of an arbitrary test sequence \(\phi=\{\phi_{n}\}\) with the notion of asymptotic relative efficiency, as developed in Section 15.2. Let \(N_{\phi}(t,\theta,\alpha,\beta)\) be the smallest sample size required to achieve power at least \(\beta\) if the true density is \(p_{t\theta}\). In the case of \(\phi_{\theta}^{*}\), it follows from (16.77) (or Theorem 15.2.1(iii)) that, if \(|\theta|=1\),\[\lim_{t\to 0^{+}}t^{2}N_{\phi_{\theta}^{*}}(t,\theta,\alpha,\beta)=(z_{\alpha}-z_{\beta})^{2}\;. \tag{16.78}\]

With \(\alpha\) and \(\beta\) fixed, choose any small \(\delta>0\), any \(\epsilon\) satisfying \(0<\epsilon<\beta-\alpha\) and \(H>0\) large enough so that \((z_{\alpha}-z_{\beta})^{2}/H^{2}\leq\delta\). For an arbitrary test \(\phi\), Theorem 16.2(ii) implies that there exists \(V\subseteq\mathbb{R}^{k}\) of dimension \(d\) satisfying (16.73) such that, for all small \(t\) and \(\theta\in V^{\perp}\) with \(|\theta|=1\), the power function at \(t\theta\) is bounded above by \(\alpha+\epsilon<\beta\), at least for \(t\) such that \(tn^{1/2}\leq H\). This in turn implies that \(n\) must satisfy \(n^{1/2}t>H\) in order to achieve power \(\beta\); thus,

\[\liminf_{t\to 0^{+}}t^{2}N_{\phi}(t,\theta,\alpha,\beta)\geq H^{2}\;. \tag{16.79}\]

Combining (16.78) and (16.79) yields, for \(\theta\in V^{\perp}\),

\[\limsup_{t\to 0^{+}}\frac{N_{\phi_{\theta}^{*}}(t,\theta,\alpha,\beta)}{N_{\phi}(t,\theta,\alpha,\beta)}\leq\frac{(z_{\alpha}-z_{\beta})^{2}}{H^{2}}\leq\delta\;. \tag{16.80}\]

If the limsup on the left side of (16.80) is replaced by a limit, which is shown to exist, the limiting value would be the Pitman ARE of \(\phi\) with respect to \(\phi_{\theta}^{*}\) for the submodel \(P_{t\theta}\). While we are not claiming such a limit exists, the interpretation of the result is the following. Except on a set of \(\theta\) values of dimension \(d\) (independent of \(n\) and \(k\)), the test \(\phi_{\theta}^{*}\) requires approximately no more than a small proportion \(\delta\) of the sample size required by \(\phi\) to achieve power \(\beta\). Therefore, it is not possible to simultaneously have high power along all "directions" \(\theta\), at least from this local point of view.

The possibility of high power for parameter values far from \(0\) (corresponding to \(|t|>H\)) remains however, and so this result does not contradict the uniform consistency result, Theorem 16.2.2, of the Kolmogorov-Smirnov test; there, the power tends to one against nonlocal alternatives. But, for testing goodness of fit against a broad nonparametric class of alternatives, Lemma 16.3.1 and Theorem 16.6.1 imply that any test (sequence) performs well locally only in some fixed finite dimensional subset of alternatives, even as \(n\) increases. To put it another way, any test has a preferred set of alternatives (of bounded dimension) for which its power is locally high. Unfortunately, it may be difficult to analyze the preferred alternatives for any particular test. For certain classes of tests, such as the integral tests of Cramer-von Mises or Anderson and Darling, there exist principle component decompositions of the test statistics, which lead to useful power calculations; see Shorack and Wellner (1986), Chapter 5. For the Kolmogorov-Smirnov test, it is known that it is roughly speaking more powerful to deviations of the median; see Milbrodt and Strasser (1990) and Janssen (1995) for a more careful statement. Since any given test sequence can only perform well for some finite-dimensional set of alternatives, it seems natural to design tests that perform well on a given finite-dimensional set, which is exactly the approach taken in the construction of Neyman's smooth tests. A general theory of efficiency of goodness of fit tests is developed in Nikitin (1995), who also compares distinct notions of efficiency; also see Janssen and Pauls (2003b). Unfortunately, different efficiency notions give rise to different tests. It appears that a proper choice of test must be based on some knowledge of the possible set of alternatives for a given experiment. By restricting attention to families of densities with different degrees of smoothness, asymptotically maximin results have been obtained; see Ingster and Suslina (2003).

### Problems

#### Section 16.2

**Problem 16.1**: Verify (16.3).

**Problem 16.2**: (i) Let \(X_{1}\),..., \(X_{n}\) be i.i.d. real-valued random variables with c.d.f. \(F\). Consider testing \(F=F_{0}\) against \(F\neq F_{0}\) based on the Kolmogorov-Smirnov test. Fix \(F\) with \(n^{1/2}d_{K}(F,\,F_{0})>s_{n,1-\alpha}\). Show that

\[P_{F}\{T_{n}>s_{n,1-\alpha}\}\geq 1-\frac{1}{4|n^{1/2}d_{K}(F,\,F_{0})-s_{n,1- \alpha}|^{2}}\.\]

_Hint_: Use (16.6) and Chebyshev's inequality.

(ii) Derive the alternative lower bound to the power of the Kolmogorov-Smirnov test given by (16.8). Compare the two lower bounds.

**Problem 16.3**: For testing \(F=F_{0}\), where \(F_{0}\) is the uniform (0,1) c.d.f., consider alternatives \(F_{n}\) to \(F_{0}\) of the form

\[F_{n}(t)=(1-\lambda_{n})F_{0}(t)+\lambda_{n}G(t)\,\]

where \(G\neq F_{0}\) is some fixed distribution. Show that, if \(\lambda_{n}=\lambda n^{-1/2}\), then the limiting power of the Kolmogorov-Smirnov test is bounded away from \(\alpha\) if \(\lambda\) is large enough.

**Problem 16.4**: Suppose \(F_{n}\) satisfies \(n^{1/2}d_{K}(F_{n},\,F_{0})\to 0\). For testing \(F=F_{0}\) at level \(\alpha\), show that the limiting power of the Kolmogorov-Smirnov test against \(F_{n}\) is no better than \(\alpha\). In the case that \(F_{n}\) is continuous for every \(n\), show that the limiting power is equal to \(\alpha\).

**Problem 16.5**: (i) Suppose \(\{P_{\theta}\}\) is q.m.d. at \(\theta_{0}\), where \(P_{\theta}\) is a probability distribution on \(\mathbb{R}\) with corresponding c.d.f. \(F_{\theta}\). Show that there exists \(B=B_{\theta_{0}}(h)<\infty\) such that

\[\limsup_{n}nd_{K}^{2}(F_{\theta_{0}+hn^{-1/2}},\,\theta_{0})\leq B_{\theta_{0} }(h)\]

and \(B_{\theta_{0}}(h)\to 0\) as \(h\to 0\).

(ii) Construct a sequence of probability distributions \(P_{n}\) on the real line with corresponding c.d.f.s \(F_{n}\) satisfying \(d_{K}\left(F_{n},\,F_{0}\right)\to 0\) but \(H(P_{n},\,P_{0})\) is bounded away from 0, where \(H\) is the Hellinger metric. On the other hand, show that \(H(P_{n},\,P_{0})\to 0\) implies \(d_{K}(F_{n},\,F_{0})\to 0\).

**Problem 16.6** Let \(F_{0}\) be the uniform (0,1) c.d.f. and consider testing \(F=F_{0}\) by the Kolmogorov-Smirnov test.

(i) Construct a sequence of alternatives \(F_{n}\) to \(F_{0}\) satisfying \(n^{1/2}d_{K}\left(F_{n},\,F_{0}\right)\to\delta\) with \(0<\delta<\infty\) such that the limiting power against \(F_{n}\) is \(\alpha\), even though there exist tests whose limiting power against \(F_{n}\) exceeds \(\alpha\).

(ii) Construct a sequence of alternatives \(F_{n}\) to \(F_{0}\) satisfying \(n^{1/2}d_{K}(F_{n},\,F_{0})\to\delta\) with \(0<\delta<\infty\) such that the limiting power against \(F_{n}\) is one.

[_Hint_: Fix \(1>\gamma_{n}>0\) with \(n^{1/2}\gamma_{n}\to\delta>0\) and let \(F_{n}(t)\) be defined by

\[F_{n}(t)=\begin{cases}0&\text{if}\,t<\gamma_{n}\\ t&\text{if}\,\gamma_{n}\leq t\leq 1.\end{cases}\]

Note that \(d_{K}\left(F_{n},\,F_{0}\right)=\gamma_{n}\) by construction. Let \(U_{1},\,\ldots,\,U_{n}\) be i.i.d. according to the uniform distribution on (0, 1), and let \(\hat{G}_{n}(t)\) denote the empirical c.d.f. of the \(U_{i}\). Set

\[X_{i}=\begin{cases}U_{i}&\text{if}\,U_{i}\geq\gamma_{n}\\ \gamma_{n}&\text{if}\,U_{i}<\gamma_{n}\end{cases}\]

so that \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. with c.d.f. \(F_{n}\). Let \(\hat{F}_{n}(t)\) denote the empirical c.d.f. of the \(X_{i}\). Argue that

\[\sup_{t}|\hat{F}_{n}(t)-t|\leq\max\left[\sup_{t}|\hat{G}_{n}(t)-t|,\,\gamma_{ n}\right]\]

and

\[P_{F_{n}}\{T_{n}>s_{n,1-\alpha}\}\leq P\{n^{1/2}\sup_{t}|\hat{G}_{n}(t)-t|>s_{ n,1-\alpha}\}\]

if \(n^{1/2}\gamma_{n}<s_{n,1-\alpha}\). If \(\delta<s_{1-\alpha}\), then this last condition will be satisfied for large enough \(n\). Finally, the last displayed expression equals \(\alpha\).

**Problem 16.7** Let \(\mathbf{F}\) be the family of distributions having density \(F^{\prime}=f\) on (0, 1) and let \(F^{\prime}_{0}=f_{0}\) be the uniform density. Consider testing the null hypothesis that \(F=F_{0}\) based on the Kolmogorov-Smirnov test. Show that, if \(d_{k}(f,\,f_{0})\) is the sup distance between densities and \(0<c<1\), then for every \(n\),

\[\inf\,P_{F}\{T_{n}\geq s_{n,1-\alpha}:\,\,F\in\mathbf{F},\,\,d_{K}(F^{\prime}, \,f_{0})\geq c\}\leq\alpha\.\]

Argue that the result applies if \(d_{K}\) is replaced by the \(L^{2}\) distance between densities. _Hint_: Consider densities of the form \(f_{\theta}(t)=1+c\sin(2\pi\theta t)\). [Compare this result with Theorem 16.2.2. Ingster and Suslina (2003) argue that alternatives based on the sup distance between distribution functions are less natural than metrics between densities. This problem shows it is impossible for the Kolmogorov-Smirnov test to have power bounded away from \(\alpha\) against such alternatives. In fact, this is true for _any_ test; see Ingster (1993) and Section 16.6. However, by restricting the family of densities to have further smoothness properties, Ingster and Suslina (2003) have obtained positive results.]

**Problem 16.8**: Generalize Theorem 16.2.2 to any EDF test statistic of the form \(n^{1/2}d(\hat{F}_{n},\,F_{0})\), if \(d\) is a metric weaker than the Kolmogorov-Smirnov metric \(d_{k}\) in the sense

\[d(F,\,G)\leq Cd_{K}(F,\,G)\]

for some constant \(C\). In particular, show the result applies to the Cramer-von Mises test.

**Problem 16.9**: For testing the null hypothesis that \(X_{1}\),..., \(X_{n}\) are i.i.d. from a normal distribution with unknown mean \(\mu\) and unknown variance \(\sigma^{2}\), show that the null distribution of (16.13) does not depend on \((\mu,\,\sigma)\) (but it does depend on \(n\)). Describe a simulation method to approximate this null distribution. How can you construct a test that is exact level \(\alpha=0.05\) based on simulation? Generalize this problem to testing a general location-scale family.

**Problem 16.10**: Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. with c.d.f \(F\) on the real line. The problem is to test the null hypothesis \(H_{0}\) that the \(X_{i}\) are uniform on \((0,\,\theta]\) for some \(\theta\). Let \(\hat{\theta}_{n}=\max(X_{1},\,\ldots,\,X_{n})\), and let \(\hat{F}_{n}\) be the empirical distribution function. Let \(d_{K}(F,\,G)\) be the Kolmogorov-Smirnov distance between \(F\) and \(G\). Consider the test statistic

\[T_{n}=n^{1/2}d_{K}(\hat{F}_{n},\,F_{\hat{\theta}_{n}})\,\]

where \(F_{\theta}\) is the uniform \((0,\,\theta)\) c.d.f. Under \(H_{0}\), what is the limiting distribution of \(T_{n}\)?

**Problem 16.11**: Let \(X_{1},\,\cdots,\,X_{n}\) be a sample from the normal distribution with mean \(\theta\) and variance \(1\), with c.d.f. denoted by \(F_{\theta}(\cdot)\). Let \(\Phi(z)\) denote the standard normal c.d.f. so that \(F_{\theta}(t)=\Phi(t-\theta)\). For any two c.d.f.s F and G, let \(\|F-G\|\) denote \(\sup_{t}|F(t)-G(t)|\). Let \(\hat{\theta}_{n}\) be the estimator of \(\theta\) minimizing \(\|\hat{F}_{n}-F_{\theta}\|\), where \(\hat{F}_{n}(t)=n^{-1}\sum_{i=1}^{n}1(X_{i}\leq t)\) denotes the empirical c.d.f. In case you are worried about problems of existence or uniqueness, you may assume \(\hat{\theta}_{n}\) is any estimator satisfying

\[\|\hat{F}_{n}-F_{\hat{\theta}_{n}}\|\leq\inf_{\theta}\|\hat{F}_{n}-F_{\theta} \|+\epsilon_{n},\]

where \(\epsilon_{n}\) is any sequence of positive constants tending to \(0\).

(i) Prove \(\hat{\theta}_{n}\) is a consistent estimator of \(\theta\).

(ii) Suppose now the observations come from a c.d.f. \(F\), possibly nonnormal. The problem is to test the null hypothesis that \(F\) is normal with variance 1 against the alternative hypothesis that \(F\) is not. Consider the test statistic

\[T_{n}=\inf_{\theta}\|\hat{F}_{n}-F_{\theta}\|.\]

Argue, if \(F\) is \(N(\theta,\,1)\), then the distribution of \(T_{n}\) does not depend on \(\theta\).

(iii) If \(F\) is not normal with variance one, argue that \(T_{n}\) tends in probability to the constant \(\gamma_{F}=\inf_{\theta}\|F-F_{\theta}\|\), and \(\gamma_{F}>0\).

(iv) Find a sequence of constants \(c_{n}\) so that the test that rejects iff \(T_{n}\geq c_{n}\) has probability of a Type I error tending to 0, and has power tending to one for any fixed alternative \(F\). _Hint:_ Use the Dvoretzky, Kiefer, Wolfowitz Inequality.

_Section 16.3_

**Problem 16.12**: (i) Verify (16.19).

(ii) Verify (16.20).

**Problem 16.13**: Prove the convergence (16.21).

**Problem 16.14**: In the multinomial goodness of fit problem, calculate the Information matrix \(I(p)\) given by (16.22).

**Problem 16.15**: Prove part (iii) of Theorem 16.3.1.

**Problem 16.16**: Show that the result Theorem 16.3.2 (ii) holds for the likelihood ratio test.

**Problem 16.17**: Prove Lemma 16.3.1(i).

**Problem 16.18**: Recall \(M(k,\,h)\) defined by (16.27) and let \(F_{k}\) denote the c.d.f. of the central Chi-squared distribution with \(k\) degrees of freedom. Show that

\[M(k,\,h)=\alpha+\gamma_{k}\,\frac{h^{2}}{2}+o(h^{2})\quad\mbox{as $h\to 0$}\;,\]

where

\[\gamma_{k}=F_{k}(c_{k,1-\alpha})-F_{k+2}(c_{k,1-\alpha})\;.\]

**Problem 16.19**: As in Section 16.3.2, consider the Chi-squared test for testing uniformity on (0, 1) based on \(k+1\) cells; call if \(\phi_{n,k}^{*}\). Fix any \(B<\infty\) and \(\epsilon>0\). Let \(\mathcal{U}_{B}\) be the set of \(u\) with \(\int u=0\) and \(\int u^{2}\leq B\). For alternative sequences of the form (16.25) with \(b_{n}=n^{-1/2}\), show that, if \(k\) is large enough (but fixed), then

\[\limsup_{n}\sup_{u:u\in\mathcal{U}_{B}}E_{f_{n}}(\phi_{n,k}^{*})\leq\alpha+ \epsilon\.\]

**Problem 16.20**: Verify (16.33).

**Problem 16.21**: Under the setup of Problem 14.63, determine a Chi-squared test statistic, as well as its limiting distribution under the null hypothesis. [For a discussion of the Chi-squared test for testing independence in a two-way table, see Diaconis and Efron (1985) and Loh (1989).]

**Problem 16.22**: The Hardy-Weinberg law says the following. If gene frequencies are in equilibrium, the genotypes AA, Aa, and aa occur in a population with frequencies \(\theta^{2}\), \(2\theta(1-\theta)\), and \((1-\theta)^{2}\). In an i.i.d. sample of size \(n\), with each outcome being an AA, Aa, or aa with the above probabilities, let \(X_{1}\), \(X_{2}\), and \(X_{3}\) be the observed counts. For example, \(X_{1}\) is the number of trials where the observation is AA. Note that \(X_{1}+X_{2}+X_{3}=n\). The joint distribution of \((X_{1},X_{2},X_{3})\) is a trinomial distribution. Hence,

\[P_{\theta}\{X_{1}=x_{1},\,X_{2}=x_{2},\,X_{3}=x_{3}\}=\frac{n!}{x_{1}!x_{2}!x_ {3}!}(\theta^{2})^{x_{1}}[2\theta(1-\theta)]^{x_{2}}[(1-\theta)^{2}]^{x_{3}}\]

for any nonnegative integers \(x_{1}\), \(x_{2}\), and \(x_{3}\) summing to \(n\). Find the MLE and its limiting distribution (suitably normalized). Derive the likelihood ratio and Chi-squared tests to test the Hardy-Weinberg law.

**Problem 16.23**: In Example 16.3.1, verify (16.32) and determine the MLE \(\hat{\beta}_{n}\) for the linkage submodel being tested. Determine the limiting distribution of the Chi-squared statistic \(Q_{n}(\hat{\beta}_{n})\).

**Problem 16.24**: Consider the limit distribution of the Chi-squared goodness of fit statistic for testing normality if using the maximum likelihood estimators to estimate the unknown parameters. Specifically, suppose \(X_{1},\ldots,X_{n}\) are i.i.d. and the problem is to test whether the underlying distribution is \(N(\theta,1)\) for some \(\theta\). Group the observations into just two groups: positive observations and negative observations. Derive the limit distribution of the Chi-squared statistic using the sample mean to estimate \(\theta\) and show it is not Chi-squared.

### _Section 16.4_

**Problem 16.25**: Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(F\), and consider testing the null hypothesis that \(F\) is the uniform (0,1) c.d.f. For \(\theta=(\theta_{1},\theta_{2})\in{\rm I\kern-2.0ptR}^{2}\), consider a family of alternative densities of the form

\[p_{\theta}(x)=C(\theta)\exp[\theta_{1}T_{1}(x)+\theta_{2}T_{2}(x)],\quad 0<x<1\.\]

Assume this two-parameter exponential family is well-defined for all small enough \(|\theta|\), so that the family is a full-rank exponential family which is q.m.d. at \(\theta=0\) with Information matrix at \(\theta=0\) denoted by \(I(0)\). For the submodel with \(\theta_{2}=0\), what is the optimal limiting power for testing \(\theta_{1}=0\) against \(\theta_{1}=hn^{-1/2}\) at level \(\alpha\). Similarly, with \(\theta_{1}=0\), what is the optimal limiting power for testing \(\theta_{2}=0\) against \(\theta_{2}=hn^{-1/2}\). Prove that no level \(\alpha\) test sequence exists whose limiting power simultaneously achieves these optimal values. _Hint_: If \((Z_{1},Z_{2})\) is bivariate normal with \((h_{1},h_{2})\), then no UMP test exists for testing \((h_{1},h_{2})=(0,0)\).

**Problem 16.26**: In Example 16.4.3, show that the multinomial distribution can be written in the form (16.35) for the given orthogonal choice of functions \(T_{j}\).

**Problem 16.27**: Show that (16.42) holds with \(B=\infty\) if \(Var_{\theta}[T_{j}(X_{1})]\) is uniformly bounded in \(\theta\). _Hint_: Argue by contradiction. Suppose there exists \(h_{n}\) with \(|h_{n}|\geq b\) such that

\[E_{h_{n}n^{-1/2}}(\phi_{n}^{*})\to\ell\,\]

where \(\ell\) is less than the right side of (16.42). This is a contradiction if

\[E_{h_{n}n^{-1/2}}(\phi_{n}^{*})\to 1\]

if \(|h_{n}|\to\infty\). By taking subsequences if necessary, assume the \(j\)th component \(h_{n,j}\) of \(h_{n}\) satisfies \(|h_{n,j}|\to\infty\). Then,

\[E_{h_{n}n^{-1/2}}(\phi_{n}^{*})\geq P_{h_{n}n^{-1/2}}\{Z_{n,j}^{2}>c_{k,1- \alpha}\}\.\]

It now suffices to show \(|Z_{n,j}|\to\infty\) in probability under \(h_{n}n^{-1/2}\). But \(|E_{\theta}[T_{j}(X_{1})]|\) increases in \(\theta\) (using properties of exponential families) while the variance of \(Z_{n,j}\) remains bounded.

**Problem 16.28**: For testing \(P=P_{0}\) in the model of densities (16.35) with \(T_{j}\) the normalized Legendre polynomials, show that Neyman's smooth test is consistent in power against any distribution \(P\) as long as the first \(k\) moments of \(P\) are not all identical to the first \(k\) moments of \(P_{0}\).

**Problem 16.29**: Let \(X_{1}\),..., \(X_{n}\) be i.i.d. random variables on [0,1] with unknown distribution \(P\). The problem is to test \(P=P_{0}\), the uniform distribution on [0, 1]. Assume a parametric model with densities of the form (16.35) for some fixed positiveinteger \(k\). Set \(T_{0}(x)=1\) and assume the functions \(T_{1},\ldots,T_{k}\) are chosen so that \(T_{0},\ldots,T_{k}\) is a set of orthonormal functions on \(L^{2}(P_{0})\). Assume that

\[\sup_{x,\,j}|T_{j}(x)|\,<\infty\]

so that \(C_{k}(\theta)\) is well-defined for all \(k\)-vectors \(\theta\). Let \(\Lambda_{n}\) be a probability distribution over values of \(\theta\) and let \(A(\phi_{n},\,\Lambda_{n})\) denote the average power of a test \(\phi_{n}\) with respect to \(\Lambda_{n}\); that is,

\[A(\phi_{n},\,\Lambda_{n})=\int_{\theta}E_{\theta}(\phi_{n})d\Lambda_{n}(\theta )\.\]

In particular, let \(\Lambda_{n}\) be the \(k\)-dimensional normal distribution with mean vector \(0\) and covariance matrix equal to \(n^{-1}\) times the identity matrix. Among tests \(\phi_{n}\) such that \(E_{0}(\phi_{n})\rightarrow\alpha\), find one that maximizes

\[\lim_{n}A(\phi_{n},\,\Lambda_{n})\]

and find a simple expression for this limiting average power.

**Problem 16.30**: Use Minkowski's Inequality (Section A.3) to show (16.47).

**Problem 16.31**: Show (16.49).

**Problem 16.32**: Argue the validity of (16.51).

#### _Section 16.5_

**Problem 16.33**: In Theorem 16.5.1, show that \(W\) has a continuous, strictly increasing distribution function on \((0,\,\infty)\). _Hint_: Write \(W=a_{i}\,Z_{i}^{2}+R\) for some \(i\) with \(a_{i}>0\) and note that \(a_{i}\,Z_{i}^{2}\) has a density.

**Problem 16.34**: Show that the distribution of the Cramer-von Mises test statistic (16.57) under \(F_{0}\) is the same for all continuous distributions \(F_{0}\).

**Problem 16.35**: Show that the Cramer-von Mises test statistic \(C_{n}\) given by (16.57) can be computed by

\[C_{n}=\frac{1}{12n}+\sum_{i=1}^{n}\left[X_{(i)}-\frac{2i-1}{2n}\right]^{2}\,\]

where \(X_{(1)}\leq\cdots\leq X_{(n)}\) denote the order statistics; see D'Agostino and Stephens (1986), p.101 for computing formulas for other test statistics based on the empirical distribution function.

**Problem 16.36**: Let \(F\) be a c.d.f. on \((0,1)\). If

\[\int_{0}^{1}\cos(\pi jx)dF(x)=0\]

for all \(j=1\), \(2\), \(\ldots\), then \(F\) must be the uniform distribution on \((0,1)\). _Hint_: Integrate by parts and use the fact the functions \(\sqrt{2}\sin(\pi jx)\) form a complete, orthonormal system for \(L_{2}[0,1]\).

**Problem 16.37**: Show that the Anderson-Darling statistic (16.58) can be rewritten in the form (16.59).

**Problem 16.38**: Consider \(W_{n}\) with \(T_{j}(x)=\sqrt{2}\cos(\pi jx)\). Fix \(\gamma_{j}\geq 0\) with \(\gamma_{j}^{2}<\infty\). Let

\[q_{\theta}(x)=C(\theta)\exp\left[\theta\sum_{j=1}^{\infty}\gamma_{j}T_{j}(x) \right]\.\]

Show that, under \(\theta=hn^{-1/2}\),

\[W_{n}\xrightarrow{d}\sum_{j}a_{j}(Z_{j}+h\gamma_{j})^{2}\.\]

**Problem 16.39**: Verify the claims made in Example 16.5.4.

**Problem 16.40**: What is the characteristic function of the limiting random variable \(W\) of Theorem 16.5.1? As a special case, show that the characteristic function of the limiting null distribution of the Cramer-von Mises statistic is given by

\[\zeta(t)=\prod_{j=1}^{\infty}\left(1-\frac{2t}{\pi j}\right)^{-1/2}\.\]

(Note this characteristic function was inverted by Smirnov; see Durbin (1973), p. 32.)

**Problem 16.41**: Show that the expression (16.65) exceeds \(\alpha\) if there exists a \(j\) for which \(a_{j}>0\) and \(c_{j}\neq 0\). Also, show that (16.65) is an increasing function of \(|c_{j}|\).

### _Section 16.6_

**Problem 16.42**: Show why (16.77) is true.

**Problem 16.43**: Consider the setting of Problem 8.35 with \(\delta=\delta_{k}\to 0\) as \(k\to\infty\). At what rate should \(\delta_{k}\to 0\) as \(k\to\infty\) so that the limiting maximin power is strictly between \(\alpha\) and \(1\)?

### Notes

Goodness of fit tests based on the empirical distribution function were introduced by Cramer (1928), von Mises (1931), and Kolmogorov (1933). A classical reference for the asymptotic theory of such tests is Durbin (1973); also see Kendall and Stuart (1979, Chapter 30), Neuhaus (1979), and Tallis (1983). Readable accounts of many goodness of fit tests can be found in D'Agostino and Stephens (1986) and Read and Cressie (1988). Methods particularly suitable for testing normality are discussed for example in Shapiro et al. (1968), Hegazy and Green (1975), D'Agostino (1982), Hall and Welsh (1983), and Spiegelhalter (1983), and for testing exponentiality in Galambos (1982), Brain and Shapiro (1983), Spiegelhalter (1983), Deshpande (1983), Doksum and Yandell (1984), and Spurrier (1984). See also Kent and Quesenberry (1982). Modern treatments are provided by Shorack and Wellner (1986), van der Vaart and Wellner (1996), and Nikitin (1995). Some recent generalizations of the Kolmogorov-Smirnov test for testing goodness of fit are discussed in Beran and Millar (1986, 1988), Romano (1988), Khmaladze (1993), Cabana and Cabana (1997), Dumbgen (1998), and Polonik (1999).

The Chi-squared test was introduced by Pearson (1900). Cohen and Sackrowitz (1975) prove a finite-sample local optimality property of the Chi-squared test in the case of testing a simple null hypothesis of equal cell probabilities. In the context of testing a multinomial, Hoeffding (1965) compares the Chi-squared and likelihood ratio tests while letting \(\alpha\to 0\) as \(n\to\infty\); he finds the likelihood ratio test superior if the number of cells is fixed, but notes the situation can be reversed otherwise. As mentioned in Section 16.3, the use of the Chi-squared test for testing goodness of fit for continuous observations is hampered by the apparent loss of information through data grouping and the choice of the number of groups. The choice of the number of groups is considered, among others, by Quine and Robinson (1985) and by Kallenberg et al. (1985). A class of generalized Chi-squared tests is studied in Drost (1988, 1989), who uses the concept of Pitman asymptotic relative efficiency to study the effect of number of groups; a particular test, known as the Rao-Robson-Nikulin test, is advocated. In the case of nuisance parameters, Fisher (1924) argued that estimating nuisance parameters changes the limiting distribution of the Chi-squared statistic, contrary to early opinion. Chernoff and Lehmann (1954) showed that, when parameters are estimated by MLEs, the limiting distribution need not even be Chi-squared; also see de Wet and Randles (1987). For further discussion on the Chi-squared test, as well as its generalizations, see Kendall and Stuart (1979). A full account of the practical implementation of the Chi-squared test, including the accuracy of the Chi-squared approximation and choice of classes, as well as an extensive bibliography, are provided by Greenwood and Nikulin (1996).

Neyman's smooth tests were introduced in Neyman (1937b), which were seen to be a special case of the general score tests of Rao (1947). An elementary treatment is provided by Rayner and Best (1989), who also consider extensions to problems withnuisance parameters. The use of smooth tests for multinomial data with adaptive choice of order is advocated in Eubank (1997). For recent work on smooth tests for composite hypotheses, see Inglot et al. (1997), Pena (1998), and Fan and Lin (1998).

Goodness of fit tests based on the Kullback-Leibler divergence are studied in Barron (1989). Tests based on spacings are considered in Wells et al. (1993). Tests based on the likelihood ratio are given in Zhang (2002).

## Chapter 17 Permutation and Randomization Tests

### 17.1 Introduction

In this chapter, and Chapter 18, we shall deal with situations where both the null hypothesis and the class of alternatives may be nonparametric and so, as a result, it may be difficult even to construct tests (or confidence regions) that satisfactorily control the level (exactly or asymptotically). For such situations, we shall develop methods which achieve this modest goal under fairly general assumptions. A secondary aim will then be to obtain some idea of the power of the resulting tests.

In Section 17.2, we consider the class of randomization tests as a generalization of permutation tests. Under the randomization hypothesis (see Definition 17.2.1), the empirical distribution of the values of a given statistic recomputed over transformations of the data serves as a null distribution; this leads to exact control of the level in such models. When the randomization hypothesis holds, the construction applies broadly to any statistic. The appeal of these methods in semiparametric or nonparametric problems stems from their finite-sample validity, but their validity may not hold when the randomization hypothesis fails. Asymptotic analysis allows one to study the robustness of the methods when assumptions may not hold, and it also allows one to study power properties of the methods. Asymptotic efficiency properties ensue if the statistic is chosen appropriately. Section 17.3 discusses two-sample permutation tests in some depth. Further examples are provided in Section 17.4. Section 17.5 extends the use of randomization tests to problems in multiple testing.

### 17.2 Permutation and Randomization Tests

Permutation tests were introduced in Chapter 5 as a robust means of controlling the level of a test if the underlying parametric model only holds approximately. For example, the two-sample permutation \(t\)-test for testing equality of means studied inSection 5.11 of Chapter 5 has level \(\alpha\) whenever the two populations have the same distribution under the null hypothesis (without the assumption of normality). In this section, we consider the large-sample behavior of permutation tests and, more generally, randomization tests. The use of the term randomization here is distinct from its meaning in Section 5.10. There, randomization was used as a device prior to collecting data, for example, by randomly assigning experimental units to treatment or control. Such a device allows for a meaningful comparison after the data has been observed, by considering the behavior of a statistic recomputed over permutations in the data. Thus, the term randomization referred to both the experimental design and the analysis of data by recomputing a statistic over permutations or randomizations (sometimes called rerandomizations) of the data. It is this latter use of randomization that we now generalize. Thus, the term randomization test will refer to tests obtained by recomputing a test statistic over transformations (not necessarily permutations) of the data.

A general test construction will be presented that yields an exact level \(\alpha\) test for a fixed sample size, under a certain group invariance hypothesis. Then, two main questions will be addressed. First, we shall consider the robustness of the level. For example, in the two-sample problem just mentioned, the underlying populations may have the same mean under the null hypothesis, but differ in other ways, as in the classical Behrens-Fisher problem, where the underlying populations are normal but may not have the same variance. Then, the rejection probability under such populations is no longer \(\alpha\), and it becomes necessary to investigate the behavior of the rejection probability. In addition, we also consider the large-sample power of permutation and randomization tests. In the two-sample problem when the underlying populations are normal with common variance, for example, we should like to know whether there is a significant loss in power when using a permutation test as compared to the UMPU \(t\)-test.

#### The Basic Construction

Based on data \(X\) taking values in a sample space \(\mathcal{X}\), it is desired to test the null hypothesis \(H\) that the underlying probability law \(P\) generating \(X\) belongs to a certain family \(\Omega_{0}\) of distributions. Let \(\mathbf{G}\) be a finite group of transformations \(g\) of \(\mathcal{X}\) onto itself. The following assumption, which we will call the _randomization hypothesis_, allows for a general test construction.

**Definition 17.2.1** (**Randomization Hypothesis**) Under the null hypothesis, the distribution of \(X\) is invariant under the transformations in \(\mathbf{G}\), that is, for every \(g\) in \(\mathbf{G}\), \(gX\) and \(X\) have the same distribution whenever \(X\) has distribution \(P\) in \(\Omega_{0}\).

The randomization hypothesis asserts that the null hypothesis parameter space \(\Omega_{0}\) remains invariant under \(g\) in \(\mathbf{G}\). However, here we specifically do not require the alternative hypothesis parameter space to remain invariant (unlike what was assumed in Chapter 6).

As an example, consider testing the equality of distributions based on two independent samples (_Y_1,..., \(Y_{m}\)) and (_Z_1,..., \(Z_{n}\)), which was previously considered in Sections 5.8-5.11. Under the null hypothesis that the samples are generated from the same probability law, the observations can be permuted or assigned at random to either of the two groups, and the distribution of the permuted samples is the same as the distribution of the original samples. (Note that a test that is invariant with respect to all permutations of the data would be useless here.)

To describe the general construction of a randomization test, let _T_(_X_) be any real-valued test statistic for testing \(H\). Suppose the group **G** has \(M\) elements. Given \(X\) = \(x\), let

\[T^{(1)}(x) \leq T^{(2)}(x) \leq \cdots \leq T^{(M)}(x)\]

be the ordered values of _T_(_gx_) as \(g\) varies in **G**. Fix a nominal level \(a\), 0 < \(a\) < 1, and let \(k\) be defined by

\[k = M - [M\alpha ]\]

, where [_M_a_] denotes the largest integer less than or equal to _M_a_. Let _M_+(_x_) and \(M\)0(_x_) be the number of values _T_(_j_)(_x_) (_j_ = 1,..., _M_) which are greater than _T_(_k_)(_x_) and equal to _T_(_k_)(_x_), respectively. Set

\[a(x) = \frac{M\alpha - M^{+}(x)}{M^{0}(x)}\]

. Since **G** is a group, _T_(_k_)(_gx_) = _T_(_k_)(_x_), and similarly for the functions _M_+(*), \(M\)0(*), and _a_(*).

Generalizing the construction presented in Section 5.8, define the randomization test function _ph_(_x_) to be equal to 1, _a_(_x_), or 0 according to whether _T_(_x_) > _T_(_k_)(_x_), _T_(_x_) = _T_(_k_)(_x_), or _T_(_x_) < _T_(_k_)(_x_), respectively. By construction, for every \(x\) in \(X\),

\[\sum\limits_{g \in {\bf G}} \phi(gx) = M^{+}(x) + a(x)M^{0}(x) = M\alpha \]

. The following theorem shows that the resulting test is level \(a\), under the hypothesis that \(X\) and _gX_ have the same distribution whenever the distribution of \(X\) is in \(Q\)0. Note that this result is true for _any_ choice of test statistic \(T\).

**Theorem 17.2.1**: _Suppose \(X\) _has distribution \(P\) on \(X\) and the problem is to test the null hypothesis \(P\)  \(Q\)0. Let **G** be a finite group of transformations of \(X\) onto itself. Suppose the randomization hypothesis holds, so that, for every \(G\), \(X\) and _gX_ have the same distribution whenever \(X\) has a distribution \(P\) in \(Q\)0. Given a test statistic \(T\) = _T_(_X_), let _ph_ be the randomization test as described above. Then,_

\[E_{P}[\phi(X)] = \alpha \quad for\ \ all\ \ P \in \Omega_{0}\]Proof. To prove (17.3), by (17.2),

\[M\alpha=E_{P}[\sum_{g}\phi(gX)]=\sum_{g}E_{P}[\phi(gX)]\.\]

By hypothesis \(E_{P}[\phi(gX)]=E_{P}[\phi(X)]\), so that

\[M\alpha=\sum_{g}E_{P}[\phi(X)]=ME_{P}[\phi(X)]\,\]

and the result follows.

To gain further insight as to why the construction works, for any \(x\in{\cal X}\), let \({\bf G}^{x}\) denote the \({\bf G}\)-orbit of \(x\); that is,

\[{\bf G}^{x}=\{gx:\ g\in{\bf G}\}\.\]

Recall from Section 6.2 that these orbits partition the sample space. The hypothesis in Theorem 17.2.1 implies that the conditional distribution of \(X\) given \(X\in{\bf G}^{x}\) is uniform on \({\bf G}^{x}\), as will be seen in the next theorem. Since this conditional distribution is the same for all \(P\in\Omega_{0}\), a test can be constructed to be level \(\alpha\) conditionally, which is then level \(\alpha\) unconditionally as well. Because the event \(\{X\in{\bf G}^{x}\}\) typically has probability zero for all \(x\), we need to be careful about how we state a result. As \(x\) varies, the sets \({\bf G}^{x}\) form a partition of the sample space. Let \({\cal G}\) be the \(\sigma\)-field generated by this partition.

**Theorem 17.2.2**: _Under the null hypothesis of Theorem 17.2.1, for any real-valued statistic \(T=T(X)\), any \(P\in\Omega_{0}\), and any Borel subset \(B\) of the real line,_

\[P\{T(X)\in B|X\in{\cal G}\}=M^{-1}\sum_{g}I\{T(gX)\in B\} \tag{17.4}\]

_with probability one under \(P\). In particular, if the \(M\) values of \(T(gx)\) as \(g\) varies in \({\bf G}\) are all distinct, then the uniform distribution on these \(M\) values serves as a conditional distribution of \(T(X)\) given that \(X\in{\bf G}^{x}\)._

Proof. First, we claim that, for any \(g\in{\bf G}\) and \(E\in{\cal G}\), \(gE=E\). To see why, assume \(y\in E\). Then, \(g^{-1}y\in E\), because \(g^{-1}y\) is on the same orbit as \(y\). Then, \(gg^{-1}y\in gE\) or \(y\in gE\). A similar argument shows that, if \(y\in gE\), then \(y\in E\), so that \(gE=E\). Now, the right-hand side of (17.4) is clearly \({\cal G}\)-measurable, since the right-hand side is constant on any orbit. We need to prove, for any \(E\in{\cal G}\),

\[\int_{E}M^{-1}\sum_{g}I\{T(gx)\in B\}dP(x)=P\{T(X)\in B,\ X\in E\}\.\]But, the left-hand side is

\[M^{-1}\sum_{g}\int_{E}I\{T(gx)\in B\}d\,P(x)=M^{-1}\sum_{g}P\{T(gX)\in B,\ X\in E\}\]

\[=M^{-1}\sum_{g}P\{T(gX)\in B,\ gX\in gE\}=M^{-1}\sum_{g}P\{T(gX)\in B,\ gX\in E\}\;,\]

since \(gE=E\). Hence, this last expression becomes (by the randomization hypothesis)

\[M^{-1}\sum_{g}P\{T(X)\in B,\ X\in E\}=P\{T(X)\in B,\ X\in E\}\;,\]

as was to be shown.

**Example 17.2.1**: **(One-Sample Tests)** Let \(X=(X_{1},\ldots,X_{n})\), where the \(X_{i}\) are i.i.d. real-valued random variables. Suppose that, under the null hypothesis, the distribution of the \(X_{i}\) is symmetric about 0. This applies, for example, to the parametric normal location model when the null hypothesis specifies the mean is 0, but it also applies to the nonparametric model that consists of all distributions with the null hypothesis specifying the underlying distribution is symmetric about 0. For \(i=1,\ldots,n\), let \(\epsilon_{i}\) take on either the value 1 or \(-1\). Consider a transformation \(g=(\epsilon_{1},\ldots,\epsilon_{n})\) of \(\,{\bf R}^{\rm r}\) that takes \(x=(x_{1},\ldots,x_{n})\) to \((\epsilon_{1}x_{1},\ldots,\epsilon_{n}x_{n})\). Finally, let \({\bf G}\) be the \(M=2^{n}\) collection of such transformations. Then, the randomization hypothesis holds, i.e., \(X\) and \(gX\) have the same distribution under the null hypothesis.

**Example 17.2.2**: **(Two-Sample Tests)** Suppose \(Y_{1},\ldots,Y_{m}\) are i.i.d. observations from a distribution \(P_{Y}\) and, independently, \(Z_{1},\ldots,Z_{n}\) are i.i.d. observations from a distribution \(P_{Z}\). Here, \(X=(Y_{1},\ldots,Y_{m},Z_{1},\ldots,Z_{n})\). Suppose that, under the null hypothesis, \(P_{Y}=P_{Z}\). This applies, for example, to the parametric normal two-sample problem for testing equality of means when the populations have a common (possibly unknown) variance. Alternatively, it also applies to the parametric normal two-sample problem where the null hypothesis is that the means and variances are the same, but under the alternative either the means or the variances may differ; this model was advocated by Fisher (1935a, pp. 122-124). Lastly, this setup also applies to the nonparametric model where \(P_{Y}\) and \(P_{Z}\) may vary freely, but the null hypothesis is that \(P_{Y}=P_{Z}\). To describe an appropriate \({\bf G}\), let \(N=m+n\). For \(x=(x_{1},\ldots,x_{N})\in{\bf R}^{N}\), let \(gx\in{\bf R}^{N}\) be defined by \((x_{\pi(1)},\ldots,x_{\pi(N)})\), where \((\pi(1),\ldots,\pi(N))\) is a permutation of \(\{1,\ldots,N\}\). Let \({\bf G}\) be the collection of all such \(g\), so that \(M=N!\). Whenever \(P_{Y}=P_{Z}\), \(X\) and \(gX\) have the same distribution. In essence, each transformation \(g\) produces a new data set \(gx\), of which the first \(m\) elements are used as the \(Y\) sample and the remaining \(n\) as the \(Z\) sample to recompute the test statistic. Note that, if a test statistic is chosen that is invariant under permutations within each of the \(Y\) and \(Z\) samples (which makes sense by sufficiency), it is enough to consider the \(N\choose m\) transformed data sets obtained by taking \(m\) observations from all \(N\) as the \(Y\) observationsand the remaining \(n\) as the \(Z\) observations (which, of course, is equivalent to using a subgroup \({\bf G}^{\prime}\) of \({\bf G}\)).

As a special case, suppose the observations are real-valued and the underlying distribution is assumed continuous. Suppose \(T\) is any statistic that is a function of the ranks of the combined observations, so that \(T\) is a _rank statistic_ (previously studied in Sections 6.8 and 6.9). The randomization (or permutation) distribution can be obtained by recomputing \(T\) over all permutations of the ranks. In this sense, rank tests are special cases of permutation tests.

**Example 17.2.3**: (**Tests of Independence**) Suppose that \(X\) consists of i.i.d. random vectors \(X=((Y_{1},\,Z_{1}),\,\ldots,\,(Y_{n},\,Z_{n}))\) having common joint distribution \(P\) and marginal distributions \(P_{Y}\) and \(P_{Z}\). Assume, under the null hypothesis, \(Y_{i}\) and \(Z_{i}\) are independent, so that \(P\) is the product of \(P_{Y}\) and \(P_{Z}\). This applies to the parametric bivariate normal model when testing that the correlation is zero, but it also applies to the nonparametric model when the null hypothesis specifies \(Y_{i}\) and \(Z_{i}\) are independent with arbitrary marginal distributions. To describe an appropriate \({\bf G}\), let \((\pi(1),\,\ldots,\,\pi(n))\) be a permutation of \(\{1,\,\ldots\,n\}\). Let \(g\) be the transformation that takes \(((y_{1},\,z_{1}),\,\ldots,\,(y_{n},\,z_{n}))\) to the value \(((y_{1},\,z_{\pi(1)}),\,\ldots,\,(y_{n},\,z_{\pi(n)}))\). Let \({\bf G}\) be the collection of such transformations, so that \(M=n!\). Whenever \(Y_{i}\) and \(Z_{i}\) are independent, \(X\) and \(gX\) have the same distribution.

In general, one can define a \(p\)-value \(\hat{p}\) of a randomization test by

\[\hat{p}=\frac{1}{M}\sum_{g}I\{T(gX)\geq T(X)\}. \tag{17.5}\]

It can be shown (Problem 17.2) that \(\hat{p}\) satisfies, under the null hypothesis,

\[P\{\hat{p}\leq u\}\leq u\quad\mbox{for all }0\leq u\leq 1. \tag{17.6}\]

Therefore, the nonrandomized test that rejects when \(\hat{p}\leq\alpha\) is level \(\alpha\).

Because \({\bf G}\) may be large, one may resort to an approximation to construct the randomization test, for example, by randomly sampling transformations \(g\) from \({\bf G}\) with or without replacement. In the former case, for example, suppose \(g_{1},\,\ldots,\,g_{B-1}\) are i.i.d. and uniformly distributed on \({\bf G}\). Let

\[\tilde{p}=\frac{1}{B}\left[1+\sum_{i=1}^{B-1}I\{T(g_{i}X)\geq T(X)\}\right]. \tag{17.7}\]

Then, it can be shown (Problem 17.3) that, under the null hypothesis,

\[P\{\tilde{p}\leq u\}\leq u\quad\mbox{for all }0\leq u\leq 1\, \tag{17.8}\]

where this probability reflects variation in both \(X\) and the sampling of the \(g_{i}\). Note that (17.8) holds for any \(B\), and so the test that rejects when \(\tilde{p}\leq\alpha\) is level \(\alpha\) even when a stochastic approximation is employed. Of course, the larger the value of \(B\), the closer \(\hat{p}\) and \(\tilde{p}\) are to each other; in fact, \(\hat{p}-\tilde{p}\to 0\) in probability as \(B\to\infty\) (Problem 17.4). Approximations based on auxiliary randomization (such as the sampling of \(g_{i}\)) are known as stochastic approximations.

#### Asymptotic Results

We next study the limiting behavior of the randomization test in order to derive its large-sample power properties. For example, for testing the mean of a normal distribution is zero with unspecified variance, one would use the optimal \(t\)-test. But if we use the randomization test based on the transformations in Example 17.2.1, we will find that the randomization test has the same limiting power as the \(t\)-test against contiguous alternatives, and so is LAUMP. Of course, for testing the mean, the randomization test can be used without the assumption of normality, and we will study its asymptotic properties both when the underlying distribution is symmetric so that the randomization hypothesis holds, and also when the randomization hypothesis fails.

Consider a sequence of situations with \(X=X^{n}\), \(P=P_{n}\), \(\mathcal{X}=\mathcal{X}_{n}\), \(\mathbf{G}=\mathbf{G}_{n}\), \(T=T_{n}\), etc. defined for \(n=1\), \(2\), \(\ldots\); notice we use a superscript for the data \(X=X^{n}\). Typically, \(X=X^{n}=(X_{1}\), \(\ldots\), \(X_{n})\) consists of \(n\) i.i.d. observations and the goal is to consider the behavior of the randomization test sequence as \(n\to\infty\).

Let \(\hat{R}_{n}\) denote the _randomization distribution_ of \(T_{n}\) defined by

\[\hat{R}_{n}(t)=M_{n}^{-1}\sum_{g\in\mathbf{G}_{n}}I\{T_{n}(gX^{n})\leq t\}. \tag{17.9}\]

We seek the limiting behavior of \(\hat{R}_{n}(\cdot)\) and its \(1-\alpha\) quantile, which we now denote \(\hat{r}_{n}(1-\alpha)\) (but in the previous subsection was denoted by \(T^{(k)}(X)\)); thus,

\[\hat{r}_{n}(1-\alpha)=\hat{R}_{n}^{-1}(1-\alpha)=\inf\{t:\ \hat{R}_{n}(t)\geq 1- \alpha\}\.\]

We will study the behavior of \(\hat{R}_{n}\) under the null hypothesis and under a sequence of alternatives. First, observe that

\[E[\hat{R}_{n}(t)]=P\{T_{n}(G_{n}X^{n})\leq t\}\,\]

where \(G_{n}\) is a random variable that is uniform on \(\mathbf{G}_{n}\). So, in the case the randomization hypothesis holds, \(G_{n}X^{n}\) and \(X^{n}\) have the same distribution and so

\[E[\hat{R}_{n}(t)]=P\{T_{n}(X^{n})\leq t\}\.\]Then, if \(T_{n}\) converges in distribution to a c.d.f. \(R(\cdot)\) which is continuous at \(t\), it follows that

\[E[\hat{R}_{n}(t)]\,\to\,R(t)\.\]

In order to deduce \(\hat{R}_{n}(t)\stackrel{{ P}}{{\to}}R(t)\) (i.e., the randomization distribution asymptotically approximates the unconditional distribution of \(T_{n}\)), it is then enough to show \(Var[\hat{R}_{n}(t)]\to 0\). This approach for proving consistency of \(\hat{R}_{n}(t)\) and \(\hat{r}_{n}(1-\alpha)\) is used in the following result. The sufficiency part is due to Hoeffding (1952), and the necessity part is from Chung and Romano (2013). Note that the randomization hypothesis is not assumed.

**Theorem 17.2.3**: _Suppose \(X^{n}\) has distribution \(P_{n}\) in \(\mathcal{X}_{n}\), and \(\mathbf{G}_{n}\) is a finite group of transformations from \(\mathcal{X}_{n}\) to \(\mathcal{X}_{n}\). Let \(G_{n}\) be a random variable that is uniform on \(\mathbf{G}_{n}\). Also, let \(G_{n}^{\prime}\) have the same distribution as \(G_{n}\), with \(X^{n}\), \(G_{n}\), and \(G_{n}^{\prime}\) mutually independent. (i) Suppose, under \(P_{n}\),_

\[(T_{n}(G_{n}X^{n}),\,T_{n}(G_{n}^{\prime}X^{n}))\stackrel{{ d}}{{\to}}(T,\,T^{\prime})\, \tag{17.10}\]

_where \(T\) and \(T^{\prime}\) are independent, each with common c.d.f. \(R(\cdot)\). Then, under \(P_{n}\),_

\[\hat{R}_{n}(t)\stackrel{{ P}}{{\to}}R(t) \tag{17.11}\]

_for every \(t\) which is a continuity point of \(R(\cdot)\). Let_

\[r(1-\alpha)=\inf\{t:\ R(t)\geq 1-\alpha\}\.\]

_Suppose \(R(\cdot)\) is continuous and strictly increasing at \(r(1-\alpha)\). Then, under \(P_{n}\),_

\[\hat{r}_{n}(1-\alpha)\stackrel{{ P}}{{\to}}r(1-\alpha)\.\]

_(ii) Conversely, if (17.11) holds for some limiting c.d.f. \(R^{T}(\cdot)\) whenever \(t\) is a continuity point, then (17.10) holds._

Proof. To prove (i), let \(t\) be a continuity point of \(R(\cdot)\). Then,

\[E_{P_{n}}[\hat{R}_{n}(t)]=\,P_{n}\{T_{n}(G_{n}X^{n})\leq t\}\to\,R(t)\,\]

by the convergence hypothesis (17.10). It therefore suffices to show that \(Var_{P_{n}}[\hat{R}_{n}(t)]\to 0\) or, equivalently, that

\[E_{P_{n}}[\hat{R}_{n}^{2}(t)]\to\,R^{2}(t)\.\]

[MISSING_PAGE_FAIL:846]

By the bivariate Central Limit Theorem,

\[n^{-1/2}\sum_{i}(\epsilon_{i}X_{i},\epsilon_{i}^{\prime}X_{i})\stackrel{{ d}}{{\to}}(T,T^{\prime})\,\]

where \(T\) and \(T^{\prime}\) are independent, each distributed as \(N(0,\sigma^{2}(P))\). Hence, by Theorem 17.2.3, we conclude

\[\hat{R}_{n}(t)\stackrel{{ P}}{{\to}}\Phi(t/\sigma(P))\]

and

\[\hat{r}_{n}(1-\alpha)\stackrel{{ P}}{{\to}}\sigma(P)z_{1-\alpha}\.\]

Let \(\phi_{n}\) be the randomization test which rejects when \(T_{n}>\hat{r}_{n}(1-\alpha)\), accepts when \(T_{n}<\hat{r}_{n}(1-\alpha)\) and possibly randomizes when \(T_{n}=\hat{r}_{n}(1-\alpha)\). Since \(T_{n}\) is asymptotically normal, it follows by Slutsky's Theorem that

\[E_{P}(\phi_{n})=P\{T_{n}>\hat{r}_{n}(1-\alpha)\}+o(1)\to P\{\sigma(P)Z>\sigma( P)z_{1-\alpha}\}=\alpha\,\]

where \(Z\) denotes a standard normal variable. In other words, we have deduced the following for the problem of testing the mean of \(P\) is zero versus the mean exceeds zero. By Theorem 17.2.1, \(\phi_{n}\) is exact level \(\alpha\) if the underlying distribution is symmetric about \(0\); otherwise, it is at least asymptotically pointwise level \(\alpha\) as long as the variance is finite.

We now investigate the asymptotic power of \(\phi_{n}\) against the sequence of alternatives that the observations are \(N(hn^{-1/2},\sigma^{2})\). By the above, under \(N(0,\sigma^{2}),\hat{r}_{n}(1-\alpha)\to\sigma z_{1-\alpha}\) in probability. By contiguity, it follows that, under \(N(hn^{-1/2},\sigma^{2})\), \(\hat{r}_{n}(1-\alpha)\to\sigma z_{1-\alpha}\) in probability as well. Under \(N(hn^{-1/2},\sigma^{2})\), \(T_{n}\) is \(N(h,\sigma^{2})\). Therefore, by Slutsky's Theorem, the limiting power of \(\phi_{n}\) against \(N(hn^{-1/2},\sigma^{2})\) is then

\[E_{P_{n}}(\phi_{n})\to P\{\sigma Z+h>\sigma z_{1-\alpha}\}=1-\Phi\left(z_{1- \alpha}-\frac{h}{\sigma}\right)\.\]

In fact, this is also the limiting power of the optimal \(t\)-test for this problem. Thus, there is asymptotically no loss in efficiency when using the randomization test as opposed to the optimal \(t\)-test, but the randomization test has the advantage that its size is \(\alpha\) over all symmetric distributions. In the terminology of Section 15.2, the efficacy of the randomization test is \(1/\sigma\) and its ARE with respect to the \(t\)-test is \(1\). In fact, the ARE is \(1\) whenever the underlying family is a q.m.d. location family with finite variance (Problem 17.6).

Note that the randomization test that is based on \(T_{n}\) is identical to the randomization test that is based on the usual \(t\)-statistic \(t_{n}\). To see why, first observe that the randomization test based on \(T_{n}\) is identical to the randomization test based on \(S_{n}=T_{n}/(\sum_{i}X_{i}^{2})^{1/2}\), simply because all "randomizations" of the data have the same value for the sum of squares. But, as was seen in Section 5.2, \(t_{n}\) is an increasing function of \(S_{n}\) for positive \(S_{n}\). Hence, the one-sample \(t\)-test which rejects when \(t_{n}\) exceeds \(t_{n-1,1-\alpha}\), the \(1-\alpha\) quantile of the \(t\)-distribution with \(n-1\) degrees of freedom, is equivalent to a randomization test based on the statistic \(t_{n}\), except that \(t_{n-1,1-\alpha}\) is replaced by the data-dependent value. Such an analogy was previously made for the two-sample test in Section 5.8.

One benefit of the randomization test is that one does not have to assume normality. In addition, the asymptotic results allow one to avoid the exact computation of the randomization distribution by approximating the critical value by the normal quantile \(z_{1-\alpha}\) or even \(t_{n-1,1-\alpha}\). The problem of whether to use \(z_{1-\alpha}\) or \(t_{n-1,1-\alpha}\) is discussed in Diaconis and Holmes (1994), who also give algorithms for the exact evaluation of the randomization distribution. In practice, critical values should be obtained from the exact randomization distribution, or its Monte Carlo approximation by randomly sampling elements of \(\mathbf{G}\). In summary, two additional benefits are revealed by the asymptotics. First, the randomization test may be used in large samples even when the randomization hypothesis fails; in the one-sample case, this means the assumption of symmetry is not required. Second, asymptotics allow us to perform local power calculations and show that, even under normality, very little power is lost when using a randomization test as compared to the \(t\)-test; in fact, the randomization test and the \(t\)-test have the same limiting local power function against normal contiguous alternatives.

In the previous example, it was seen that the randomization distribution approximates the (unconditional) null distribution of \(T_{n}\) in the sense that

\[\hat{R}_{n}(t)-P\{T_{n}\leq t\}\stackrel{{ P}}{{\to}}0\]

if \(P\) has mean 0 and finite variance, since \(P\{T_{n}\leq t\}\to\Phi(t/\sigma(P))\). The following is a more general version of this result.

**Theorem 17.2.4**: _(i) Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. real-valued random variables with distribution \(P\), assumed symmetric about 0. Assume \(T_{n}\) is asymptotically linear in the sense that, for some function \(\psi_{P}\),_

\[T_{n}=n^{-1/2}\sum_{i=1}^{n}\psi_{P}(X_{i})+o_{P}(1)\, \tag{17.12}\]

_where \(E_{P}[\psi_{P}(X_{i})]=0\) and \(\tau_{P}^{2}=Var_{P}[\psi_{P}(X_{i})]<\infty\). Also, assume \(\psi_{P}\) is an odd function. Let \(\hat{R}_{n}\) denote the randomization distribution based on \(T_{n}\) and the group of sign changes in Example 17.2.1. Then, the hypotheses of Theorem 17.2.3 hold with \(P_{n}=P^{n}\) and \(R(t)=\Phi(t/\tau(P))\), and so_

\[\hat{R}_{n}(t)\stackrel{{ P}}{{\to}}\Phi(t/\tau(P))\.\]_(ii) If_ \(P\) _is not symmetric about 0, let_ \(F\) _denote its c.d.f. and define a symmetrized version_ \(\tilde{P}\) _of_ \(P\) _as the probability with c.d.f._

\[\tilde{F}(t)=\frac{1}{2}[F(t)+1-F(-t)]\.\]

_Assume_ \(T_{n}\) _satisfies (_17.12_) under_ \(\tilde{P}\)_. Then, under_ \(P\)_,_

\[\hat{R}_{n}(t)\stackrel{{ P}}{{\to}}\Phi(t/\tau(\tilde{P}))\quad \mbox{ and }\quad\hat{r}_{n}(1-\alpha)\stackrel{{ P}}{{\to}}\tau(\tilde{P})z_{1-\alpha}\.\]

Proof.: Independent of \(X^{n}=(X_{1},\ldots,X_{n})\) let \(\epsilon_{1},\ldots,\epsilon_{n}\) and \(\epsilon^{\prime}_{1},\ldots,\epsilon^{\prime}_{n}\) be mutually independent, each \(\pm 1\) with probability \(\frac{1}{2}\). Then, in the notation of Theorem 17.2.3, \(G_{n}X^{n}=(\epsilon_{1}X_{1},\ldots,\epsilon_{n}X_{n})\). Set

\[\delta_{n}(X_{1},\ldots,X_{n})=T_{n}-n^{-1/2}\sum\psi_{P}(X_{i})\]

so that \(\delta_{n}(X_{1},\ldots,X_{n})\stackrel{{ P}}{{\to}}0\). Since \(\epsilon_{i}X_{i}\) has the same distribution as \(X_{i}\), it follows that \(\delta_{n}(\epsilon_{1}X_{1},\ldots,\epsilon_{n}X_{n})\stackrel{{ P}}{{\to}}0\), and the same is true with \(\epsilon_{i}\) replaced by \(\epsilon^{\prime}_{i}\). Then,

\[\left(T_{n}(G_{n}X^{n}),\,T_{n}(G^{\prime}_{n}X^{n})\right)=n^{-1/2}\sum_{i=1} ^{n}\left(\psi_{P}(\epsilon_{i}X_{i}),\,\psi_{P}(\epsilon^{\prime}_{i}X_{i}) \right)+o_{P}(1)\.\]

But since \(\psi_{P}\) is odd, \(\psi_{P}(\epsilon_{i}X_{i})=\epsilon_{i}\psi_{P}(X_{i})\). By the bivariate CLT,

\[n^{-1/2}\sum_{i=1}^{n}\left(\epsilon_{i}\psi_{P}(X_{i}),\,\epsilon^{\prime}_{ i}\psi_{P}(X_{i})\right)\stackrel{{ d}}{{\to}}(T,\,T^{\prime})\,\]

where \((T,\,T^{\prime})\) is bivariate normal, each with mean 0 and variance \(\tau_{P}^{2}\), and

\[Cov(T,\,T^{\prime})=Cov\left(\epsilon_{i}\psi_{P}(X_{i}),\,\epsilon^{\prime}_{ i}\psi_{P}(X_{i})\right)=E(\epsilon_{i})E(\epsilon^{\prime}_{i})E_{P}[\psi_{P}^{2}(X_{i})]=0\,\]

and so (i) follows.

To prove (ii), observe that, if \(X\) has distribution \(P\) and \(\tilde{X}\) has distribution \(\tilde{P}\), then \(|X|\) and \(|\tilde{X}|\) have the same distribution. But, the construction of the randomization distribution only depends on the values \(|X_{1}|,\ldots,|X_{n}|\). Hence, the behavior of \(\hat{R}_{n}\) under \(P\) and \(\tilde{P}\) must be the same. But, the behavior of \(\hat{R}_{n}\) under \(\tilde{P}\) is given in (i).

**Example 17.2.5** (One-Sample Location Models): Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(f(x-\theta)\), where \(f\) is assumed symmetric about \(\theta_{0}=0\). Assume the family is q.m.d. at \(\theta_{0}\) with score statistic \(Z_{n}\). Thus, under \(\theta_{0}\), \(Z_{n}\stackrel{{ d}}{{\to}}N(0,\,I(\theta_{0}))\). Consider the randomization test based on \(T_{n}=Z_{n}\) (and the group of sign changes). It is exact level \(\alpha\) for all symmetric distributions. Moreover, \(Z_{n}=n^{-1/2}\sum_{i}\tilde{\eta}(X_{i},\,\theta_{0})\), where \(\tilde{\eta}\) can always be taken to be an odd function if \(f\) is even. So, the assumptions of Theorem 17.4 (i) hold. Hence, when \(\theta_{0}=0\),

\[\hat{r}_{n}(1-\alpha)\to I^{1/2}(\theta_{0})z_{1-\alpha}\.\]

By contiguity, the same is true under \(\theta_{n,h}=hn^{-1/2}\). By Theorem 15.1, the efficacy of the randomization test is \(I^{1/2}(\theta_{0})\). By Corollary 15.1, the ARE of the randomization test with respect to the Rao test that uses the critical value \(z_{1-\alpha}I^{1/2}(\theta_{0})\) (or even an exact critical value based on the true unconditional distribution of \(Z_{n}\) under \(\theta_{0}\)) is 1. Indeed, the randomization test is LAUMP. Therefore, there is no loss of efficiency in using the randomization test, and it has the advantage of being level \(\alpha\) across symmetric distributions.

### Two-Sample Permutation Tests

In this section, we derive the asymptotic behavior of some two-sample permutation tests introduced in Example 17.2.2. Recall the setup of Example 17.2.2 where \(Y_{1}\),..., \(Y_{m}\) are i.i.d. \(P_{Y}\) and, independently, \(Z_{1}\),..., \(Z_{n}\) are i.i.d. \(P_{Z}\), and \(P_{Y}\) and \(P_{Z}\) are now assumed to be distributions on the real line. Let \(\mu(P)\) and \(\sigma^{2}(P)\) denote the mean and variance, respectively, of a distribution \(P\). Consider the test statistic

\[T_{m,n}=m^{1/2}(\tilde{Y}_{m}-\tilde{Z}_{n})=m^{-1/2}[\sum_{i=1}^{m}Y_{i}- \frac{m}{n}\sum_{j=1}^{n}Z_{j}]. \tag{17.13}\]

Assume \(m/n\to\lambda\in(0,\infty)\) as \(m\), \(n\to\infty\). If the variances of \(P_{Y}\) and \(P_{Z}\) are finite and nonzero and \(\mu(P_{Y})=\mu(P_{Z})\), then

\[T_{m,n}\stackrel{{ d}}{{\to}}N\left(0,\sigma^{2}(P_{Y})+\lambda \sigma^{2}(P_{Z})\right). \tag{17.14}\]

We wish to study the limiting behavior of the randomization test based on the test statistic \(T_{m,n}\). If the null hypothesis implies that \(P_{Y}=P_{Z}\), then the randomization test is exact level \(\alpha\), though we may still require an approximation to its power. On the other hand, we may consider using the randomization test for testing the null hypothesis \(\mu(P_{Y})=\mu(P_{Z})\), and the randomization test is no longer exact if the distributions differ.

Let \(N=m+n\) and write

\[(X_{1},\ldots,X_{N})=(Y_{1},\ldots,Y_{m},Z_{1},\ldots,Z_{n})\.\]

Independent of the \(X\)s, let \((\pi(1),\ldots,\pi(N))\) and \((\pi^{\prime}(1),\ldots,\pi^{\prime}(N))\) be independent random permutations of \(1,\ldots,N\). In order to verify the conditions for Theorem 17.3, we need to determine the joint limiting behavior of \[(T_{m,n},\,T^{\prime}_{m,n})=m^{-1/2}(\sum_{i=1}^{N}X_{i}\,W_{i},\,\sum_{i=1}^{N}X_{ i}\,W^{\prime}_{i})\, \tag{17.15}\]

where \(W_{i}=1\) if \(\pi(i)\leq m\) and \(W_{i}=-m/n\) otherwise; \(W^{\prime}_{i}\) is defined with \(\pi\) replaced by \(\pi^{\prime}\). Note that \(E(W_{i})=E(X_{i}\,W_{i})=0\). Moreover, an easy calculation (Problem 17.8) gives

\[Var(T_{m,n})=\frac{m}{n}\sigma^{2}(P_{Y})+\sigma^{2}(P_{Z}) \tag{17.16}\]

and

\[Cov(T_{m,n},\,T^{\prime}_{m,n})=m^{-1}\sum_{i=1}^{N}\sum_{j=1}^{N}E(X_{i}\,X_{ j}\,W_{i}\,W^{\prime}_{j})=0\, \tag{17.17}\]

by the independence of the \(W_{i}\) and the \(W^{\prime}_{i}\). These calculations suggest the following result.

**Theorem 17.3.1**: _Assume the above setup with \(m/n\to\lambda\in(0,\,\infty)\). If \(\sigma^{2}(P_{Y})\) and \(\sigma^{2}(P_{Z})\) are finite and nonzero and \(\mu(P_{Y})=\mu(P_{Z})\), then (17.15) converges in law to a bivariate normal distribution with independent, identically distributed marginals having mean 0 and variance_

\[\tau^{2}=\lambda\sigma^{2}(P_{Y})+\sigma^{2}(P_{Z}). \tag{17.18}\]

_Hence, the randomization distribution \(R_{m,n}(\cdot)\) based on the statistic \(T_{m,n}\) satisfies_

\[\hat{R}_{m,n}(t)\stackrel{{ P}}{{\to}}\Phi(t/\tau)\.\]

Proof. Assume without loss of generality that \(\mu(P_{Y})=0\). By the Cramer-Wold device (Theorem 11.2.3), it suffices to show, for any \(a\) and \(b\) (not both 0)

\[m^{-1/2}\sum_{i=1}^{N}X_{i}\,(a\,W_{i}+b\,W^{\prime}_{i})\stackrel{{ d}}{{\to}}N\,\left(0,\,(a^{2}+b^{2})\tau^{2}\right)\.\]

Write the left side as

\[m^{-1/2}\sum_{i=1}^{m}Y_{i}(a\,W_{i}+b\,W^{\prime}_{i})+m^{-1/2}\sum_{j=1}^{n} Z_{j}(a\,W_{m+j}+b\,W^{\prime}_{m+j})\, \tag{17.19}\]

which conditional on the \(W_{i}\) and \(W^{\prime}_{i}\) is a sum of two independent terms, with each term a linear combination of independent variables. We can handle each of the two terms in (17.19) by appealing to Lemma 13.2.3. So, we must verify 

[MISSING_PAGE_EMPTY:2454]

Therefore,

\[Var\left(\frac{1}{m}\sum_{i=1}^{m}W_{i}W_{i}^{\prime}\right)=\frac{1}{m}\left[ \left(\frac{m}{n}\right)^{2}+m(m-1)\left(\frac{m^{2}}{n^{2}(N-1)^{2}}\right) \right]=O(\frac{1}{m})\to 0\.\]

It now follows that (17.21) converges in probability to \((a^{2}+b^{2})\lambda>0\), as was required. Thus, (17.20) holds and the left side of (17.19) converges in distribution to \(N(0,\lambda(a^{2}+b^{2})\sigma^{2}(P_{Y}))\). The right side of (17.19) is similar. Thus, Lemma 13.2.3 can be applied (conditionally) to each term in (17.19) and the result follows by Problem 11.73.

Note that the proof also shows that the result holds with \(\lambda=0\) as long as \(m\to\infty\). An alternative proof of the limiting behavior of the permutation distribution can be based on Theorem 12.2.3 (Problem 17.11).

The problem of testing equality of means in the two-sample problem without imposing parametric assumptions on the underlying distributions can be viewed as a nonparametric version of the Behrens-Fisher problem. Theorem 17.2.3 and Theorem 17.2.1 show that, under the null hypothesis that \(\mu(P_{Y})=\mu(P_{Z})\), the randomization distribution is, in large samples, approximately a normal distribution with mean \(0\) and variance \(\tau^{2}\). Hence, the critical value of the randomization test that rejects for large values of \(T_{m,n}\) converges in probability to \(z_{1-\alpha}\tau\). On the other hand, the true sampling distribution of \(T_{m,n}\) is approximately normal with mean \(0\) and variance

\[\sigma^{2}(P_{Y})+\lambda\sigma^{2}(P_{Z})\,\]

if \(\mu(P_{Y})=\mu(P_{Z})\). These two distributions are identical if and only if \(\lambda=1\) or \(\sigma^{2}(P_{Y})=\sigma^{2}(P_{Z})\). Therefore, for testing equality of means (and not distributions), the randomization test will be pointwise consistent in level even if \(P_{Y}\) and \(P_{Z}\) differ, as long as the variances of the populations are the same, or the sample sizes are roughly the same. In particular, when the underlying distributions have the same variance (as in the normal-theory model assumed in Section 5.3 for which the two-sample \(t\)-test is UMPU), the two-sample \(t\)-test is asymptotically equivalent to the corresponding randomization test. This equivalence is not limited to the behavior under the null hypothesis; see Problem 17.10.

In order to gain some insight into Theorem 17.2.1, the permutation distribution is invariant under permutations, and therefore its behavior under \(m\) observations from \(P_{Y}\) and \(n\) from \(P_{Z}\) should not be too different from the permutation distribution based on \(N=m+n\) observations from the mixture distribution, where each observation is taken from \(P_{Y}\) with probability \(m/N\) and from \(P_{Z}\) with probability \(n/N\). Consider the mixture distribution

\[\tilde{P}=\frac{\lambda}{1+\lambda}P_{Y}+\frac{1}{1+\lambda}P_{Z}. \tag{17.26}\]

Note that when \(\mu(P_{Y})=\mu(P_{Z})\),\[\sigma^{2}(\bar{P})=\frac{\lambda}{1+\lambda}\sigma^{2}(P_{Y})+\frac{1}{1+\lambda} \sigma^{2}(P_{Z}). \tag{17.27}\]

Since the permutation test is exact in the i.i.d. case when all \(N\) observations are from \(\bar{P}\), one might expect that the permutation distribution for \(T_{m,n}\) in this case behaves like its unconditional distribution. Its limiting distribution is (from 17.14) given by

\[N(0,\sigma^{2}(\bar{P})+\lambda\sigma^{2}(\bar{P}))=N(0,(1+\lambda)\sigma^{2} (\bar{P}))=N(0,\tau^{2})\,\]

which agrees with (17.18) in Theorem 17.3.1.

If the underlying variances differ and \(\lambda\neq 1\), the permutation test based on \(T_{m,n}\) given in (17.13) will have rejection probability that does not tend to \(\alpha\). Further results are given in Romano (1990). For example, two-sample permutations tests based on sample medians lead to tests that are not even pointwise consistent in level, unless the strict randomization hypothesis of equality of distributions holds. Thus, if testing equality of population medians based on the difference between sample medians, the asymptotic rejection probability of the randomization test need not be \(\alpha\) even with the underlying populations have the same median.

However, if one replaces \(T_{m,n}\) by the studentized version

\[\tilde{T}_{m,n}=T_{m,n}/D_{N}\, \tag{17.28}\]

where

\[D_{m.n}^{2}=D_{m,n}^{2}(X_{1},\ldots,X_{N})=S_{Y}^{2}+\frac{m}{n}S_{Z}^{2}\, \tag{17.29}\]

\[S_{Y}^{2}=\frac{1}{m}\sum_{i=1}^{m}(Y_{i}-\tilde{Y}_{m})^{2}\ \ \ \mbox{and}\ \ \ S_{Z}^{2}=\frac{1}{n}\sum_{j=1}^{n}(Z_{j}-\tilde{Z}_{n})^{2}\,\]

then the permutation test is pointwise consistent in level for testing equality of means, even when the underlying distributions have possibly different variances and the sample sizes differ. This important result is due to Janssen (1997) (with a different proof than the one below).

In order to prove this result, note that the unconditional distribution of (17.28) is asymptotically standard normal under the assumption of finite variances. Indeed, this is a simple exercise in applying Slutsky's Theorem. When considering the randomization distribution of (17.28), the following result can be viewed as Slutsky's Theorem for randomization distributions.

Given sequences of statistics \(T_{n}\), \(A_{n}\), and \(B_{n}\), let \(\hat{R}_{n}^{AT+B}(\cdot)\) denote the randomization distribution corresponding to the statistic sequence \(A_{n}T_{n}+B_{n}\), i.e., replace \(T_{n}\) in (17.9) by \(A_{n}T_{n}+B_{n}\), so

\[\hat{R}_{n}^{AT+B}(t)\equiv\frac{1}{|G_{n}|}\sum_{g\in G_{n}}I\{A_{n}(gX^{n})T _{n}(gX^{n})+B_{n}(gX^{n})\leq t\}. \tag{17.30}\]

**Theorem 17.3.2**: **(Slutsky's Theorem for Randomization Distributions)** _Let \(G_{n}\) and \(G^{\prime}_{n}\) be independent and uniformly distributed over \({\bf G}_{n}\) (and independent of \(X^{n}\)). Assume \(T_{n}\) satisfies (17.10). Further assume that, for constants \(a\) and \(b\),_

\[A_{n}(G_{n}X^{n})\stackrel{{ P}}{{\to}}a \tag{17.31}\]

_and_

\[B_{n}(G_{n}X^{n})\stackrel{{ P}}{{\to}}b. \tag{17.32}\]

_Let \(R^{aT+b}(\cdot)\) denote the distribution of \(aT+b\), where \(T\) is the limiting random variable in (17.10). Then,_

\[\hat{R}^{AT+B}_{n}(t)\stackrel{{ P}}{{\to}}R^{aT+b}(t)\,\]

_if the distribution \(R^{aT+b}(\cdot)\) of \(aT+b\) is continuous at \(t\). (Note \(R^{aT+b}(t)=R^{T}(\frac{t-b}{a})\) if \(a\not=0\).)_

Proof. The assumptions imply (see Problem 11.34) that

\[(T_{n}(G_{n}X^{n}),\,A_{n}(G_{n}X^{n}),\,B_{n}(G_{n}X^{n}),\,T_{n}(G^{\prime} _{n}X^{n}),\,A_{n}(G^{\prime}_{n}X^{n}),\,B_{n}(G^{\prime}_{n}X^{n}))\]

converges in distribution to (\(T\), \(a\), \(b\), \(T^{\prime}\), \(a\), \(b\), ), where (\(T\), \(T^{\prime}\)) are independent and given in Assumption (17.10). By the Continuous Mapping Theorem (Theorem 11.3.2), it follows that

\[(A_{n}(G_{n}X^{n})T_{n}(G_{n}X^{n})+B_{n}(G_{n}X^{n}),\,(A_{n}(G^{\prime}_{n}X ^{n})T_{n}(G^{\prime}_{n}X^{n})+B_{n}(G^{\prime}_{n}X^{n}))\]

converges in distribution to the distribution of (\(aT+b\), \(aT^{\prime}+b\)), so that the asymptotic independence condition in Theorem 17.2.3 holds. Therefore, the result follows by Theorem 17.2.3.

Returning to the two-sample problem studied in Theorem 17.3.1, we are now in a position to provide the limiting behavior of the randomization based on the studentized statistic \(\tilde{T}_{m,n}\) in (17.28). By Theorem 17.3.2, the problem is reduced to studying the statistic \(D_{m,n}\) given in (17.29).

**Theorem 17.3.3**: _Under the setup of Theorem 17.3.1, let \(\hat{R}^{\tilde{T}}_{m,n}\) denote the randomization distribution for \(\tilde{T}_{m,n}\). Assume \(H_{0}:\ \mu(P_{Y})=\mu(P_{Z})\). Then,_

\[\hat{R}^{\tilde{T}}_{m,n}(t)\stackrel{{ P}}{{\to}}\Phi(t). \tag{17.33}\]

_Since also_

\[\tilde{T}_{m,n}\stackrel{{ d}}{{\to}}N(0,1)\,\]

_the randomization test based on \(\tilde{T}_{m,n}\) is pointwise consistent in level._Proof. Combining Theorems 17.3.1 and 17.3.2, it suffices to show that, when \((\pi(1),\,\ldots,\,\pi(N))\) is a random permutation, then \(D_{m,n}\) given by (17.29) satisfies

\[D_{m,n}^{2}(X_{\pi(1)},\ldots,\,X_{\pi(N)})\stackrel{{ P}}{{ \rightarrow}}\tau^{2}\,\]

where \(\tau^{2}\) is given in (17.18). Equivalently, it suffices to show that each of \(S_{Y}^{2}\) and \(S_{Z}^{2}\) under random permutation converges in probability to \(\sigma^{2}(\bar{P})\) given in (17.27). By symmetry it suffices to look at a randomly permuted value of

\[S_{Y}^{2}=\frac{1}{m}\sum_{i=1}^{m}X_{i}^{2}-(\frac{1}{m}\sum_{i=1}^{m}X_{i})^{ 2}\.\]

Thus, it suffices to show that, for \(j=1,\,2\),

\[\frac{1}{m}\sum_{i=1}^{m}X_{\pi(i)}^{j}\stackrel{{ P}}{{ \rightarrow}}E(M^{j}), \tag{17.34}\]

where \(M\sim\bar{P}\). Looking at \(j=1\), it is easy to calculate (Problem 17.14) that

\[E\left(\frac{1}{m}\sum_{i=1}^{m}X_{\pi(i)}\right)=\frac{m}{N}\mu(P_{Y})+\frac {n}{N}\mu(P_{Z})\rightarrow\frac{\lambda}{1+\lambda}\mu(P_{Y})+\frac{1}{ \lambda}\mu(P_{Z})=\mu(\bar{P})\]

and that \(Var\left(\frac{1}{m}\sum_{i=1}^{m}X_{\pi(i)}\right)\to 0\), so that

\[\frac{1}{m}\sum_{i=1}^{m}X_{\pi(i)}\stackrel{{ P}}{{\rightarrow}} \mu(\bar{P})\.\]

The argument for \(j=2\) is left for Problem 17.14.

To summarize, Theorem 17.3.3 shows that for testing the equality of population means, the studentized permutation controls the probability of a Type 1 error asymptotically, but also retains exact Type 1 error control when the underlying distributions are equal (because in this case the randomization hypothesis holds). A test based on an asymptotic normal approximation does not have such a property.

In order to generalize Theorem 17.3.3 to other test statistics, it is important to understand the intuition behind the analysis. For a given test statistic \(T=T_{m,n}\), let \(J_{m,n}(P_{Y},\,P_{Z})\) denote the distribution of \(T_{m,n}\) based on \(m\) observations from \(P_{Y}\) and \(n\) from \(P_{Z}\). Just as in the case of the unstudentized test statistic, the asymptotic behavior of the randomization distribution \(\hat{R}_{m,n}^{T}\) based on \(m\) observations from \(P_{Y}\) and \(n\) observations from \(P_{Z}\) should be the same as when all observations are i.i.d. from the mixture distribution \(\bar{P}\) defined in (17.26). But the latter should be approximately equal to the true sampling distribution \(J_{m,n}(\bar{P},\,\bar{P})\) because the permutation test is exact under the randomization hypothesis that both distributions are the same. If we assume that \(J_{m,n}(P_{Y},\,P_{Z})\) converges to a limit law \(J\) which does not depend on \((P_{Y},\,P_{Z})\), then we have the randomization distribution \(\hat{R}_{m,n}^{T}\) is approximately equal to \(J\), which is also approximately equal to \(J_{m,n}(P_{Y},\,P_{Z})\). Therefore, the conclusion is that one should construct test statistics that have a limiting distribution free of any unknown parameters, at least under the null hypothesis. In the two-sample problem for testing differences of means, this is easily achieved by studentization. In fact, these ideas hold quite generally and apply to broad classes of test statistics; see Chung and Romano (2013, 2016a, 2016b).

### Further Examples

In this section, randomization and permutation tests are applied to some other situations.

**Example 17.4.1** (**Testing Means From Independent Observations**): Assume \(X_{1},\,\ldots,\,X_{n}\) are independent, but not necessarily i.i.d. Let \(E(X_{i})=\mu_{i}\), assumed to exist. Also, let

\[\bar{\mu}_{n}=\frac{1}{n}\sum_{i=1}^{n}\mu_{i}\,\]

\(Var(X_{i})=\sigma_{i}^{2}\) and

\[\bar{\sigma}_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}^{2}\.\]

First, examine the case where \(\mu_{i}=\mu\) for all \(i\). For testing \(H_{0}:\mu=0\), consider the group of sign changes as in Example 17.2.4. Even if the \(X_{i}\) have distinct distributions but these distributions are symmetric about \(\mu\), then the randomization hypothesis holds. Therefore, one may construct exact level \(\alpha\) randomization tests under symmetry.

The question we would now like to investigate is Type 1 error control when symmetry does not hold. Let \(T_{n}=\sqrt{n}\bar{X}_{n}\). Assume \(\bar{\sigma}_{n}^{2}\to\sigma_{\infty}^{2}>0\) and that, for some \(\delta>0\), \(\sup_{i}E(|X_{i}-\mu_{i}|^{2+\delta})<\infty\). By an argument similar to Example 17.2.4, one can show (Problem 17.18) that the conditions of Theorem 17.2.3 hold with the limit distribution \(R\) equal to \(N(0,\,\sigma_{\infty}^{2})\), where \(\sigma_{\infty}^{2}=\lim_{n}\bar{\sigma}_{n}^{2}\). On the other hand, by Example 11.2.2, under \(\mu=0\),

\[\sqrt{n}\bar{X}_{n}\stackrel{{ d}}{{\to}}N(0,\sigma_{\infty}^{2}). \tag{17.35}\]

Thus, the true sampling distribution and the randomization distribution are asymptotically equal, and therefore, under the above moment assumptions, the probability of a Type 1 error tends to the nominal level even under asymmetry.

Under the same setup and assumptions, we now consider the problem of testing the null hypothesis that \(\tilde{\mu}_{n}=0\) (so that the \(\mu_{i}\) may differ even under the null hypothesis). In this case, the randomization test has exact Type 1 error control when all the underlying distributions are symmetric about 0. So, we wish to also consider the asymptotic behavior under asymmetry as well as heterogeneity in means (and underlying distributions). Assume that

\[\frac{1}{n}\sum_{i=1}^{n}\mu_{i}^{2}\,\to\,v_{\infty}<\infty\.\]

Then, the conditions of Theorem 17.2.3 hold with \(R\) now equal to \(N(0,\sigma_{\infty}^{2}+\,v_{\infty})\). It then follows that the critical value, \(\hat{r}_{n}(1-\alpha)\), based on the randomization distribution satisfies

\[\hat{r}_{n}(1-\alpha)\,\stackrel{{ P}}{{\to}}\,z_{1-\alpha}\sqrt{ \sigma_{\infty}^{2}+v_{\infty}}\.\]

On the other hand, under the null hypothesis \(\tilde{\mu}_{n}=0\), (17.35) still holds. Hence, if \(v_{\infty}>0\), we see that these limiting distributions do not match. Therefore, under the null hypothesis, by Slutsky's Theorem,

\[P\left\{\sqrt{n}\tilde{X}_{n}\,>\hat{r}_{n}(1-\alpha)\right\}\,\to\,1-\Phi \left(z_{1-\alpha}\sqrt{1+\frac{v_{\infty}}{\sigma_{\infty}^{2}}}\right)\.\]

The limiting probability is \(\leq\alpha\) and only equals \(\alpha\) when \(v_{\infty}=0\), so that the resulting randomization test is in general conservative.

**Example 17.4.2** (**Matched Pairs**): In paired comparisons, data \((Y_{i},Z_{i}),\ i=1,\ldots,n,\) are observed. For example, pairing may arise in studies of twins, where one receives treatment and the other does not. Alternatively, paired data may represent before and after treatment of the same unit. In general, units may be matched according to other observed covariates, such as age, sex, blood pressure, etc. If the \(Z_{i}\)s represents the treated observations and \(Y_{i}\)s the untreated, then the differences \(D_{i}=Z_{i}-Y_{i}\) may be used to make inferences about the treatment effect \(E(D_{i})\). (More generally, matched pairs may be viewed as a special case of a randomized block design, where units are divided into subgroups or blocks according to covariates, so that smaller variability within blocks leads to more efficient estimate of treatment effects.)

Upon reduction to the \(D_{i}\)s, the one-sample tests studied in Examples 17.2.4 and 17.4.1 may apply. However, if observations are paired according to covariates, then the \(D_{i}\) may no longer be independent, and so the analysis becomes more involved. If observations are assigned to treatment at random within pairs once pairs are formed, then the \(D_{i}\) are conditionally independent given the covariates, in which case the analysis of Example 17.4.1 may be used a starting point. The details are beyond the scope here; see Bugni et al. (2019) and Bai et al. (2021).

**Example 17.4.3**: **(Hotelling Test for Multivariate Mean)** Let \(X_{1},\ldots,\)\(X_{n}\) be i.i.d. random vectors with distribution \(P\) on \(\mathbf{R}^{\mathbf{p}}\). Assume \(E_{P}|X_{i}|^{2}<\infty\). Let \(\mu=\mu(P)\) be the mean vector and let \(\Sigma=\Sigma(P)\) be the covariance matrix, assumed positive definite. The problem is to test the null hypothesis \(H_{0}:\mu(P)=0\) versus the alternative hypothesis \(H_{1}:\mu(P)\neq 0\) (or possibly a subset thereof).

Under the assumption of multivariate normality, one can perform an exact test using Hotelling's T-squared statistic. Specifically, let \(\tilde{X}_{n}\) denote the sample mean vector, and let \(\hat{\Sigma}_{n}\) denote the sample covariance matrix, defined as

\[\hat{\Sigma}_{n}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\tilde{X}_{n})(X_{i}-\tilde {X}_{n})^{\top}\.\]

Note that \(\hat{\Sigma}_{n}\) is invertible with probability one. Then, Hotelling's T-squared statistic is defined as

\[T_{n}=T_{n}(X_{1},\ldots,X_{n})=n\tilde{X}_{n}^{\top}\hat{\Sigma}_{n}^{-1} \tilde{X}_{n}\.\]

If \(H_{0}\) is true, then under the assumption of multivariate normality, \(T_{n}\) has Hotelling's T-squared distribution with parameters \(p\) and \(n-1\), from which exact critical values may be obtained. As \(n\to\infty\), this distribution tends to the Chi-squared distribution with \(p\) degrees of freedom.

Instead, we may construct exact tests without the multivariate normality assumption by constructing a randomization test. Under normality and if \(H_{0}\) is true, it follows that \(X_{i}\) and \(-X_{i}\) have the same distribution. But, this holds for many more distributions. In fact, exact Type 1 error control holds for distributions with mean \(0\) satisfying \(X_{i}\) and \(-X_{i}\) have the same distribution; that is, the randomization hypothesis holds for this family of distributions with respect to the group of sign changes. Of course, this holds for any \(p\), so that it applies even in the high-dimensional setting.

The question we now consider is the asymptotic behavior of the randomization test if the randomization hypothesis fails (for large \(n\), and fixed \(p\)). We wish to show that the probability of rejecting \(H_{0}\) tends to the nominal level \(\alpha\) as \(n\to\infty\), without assuming \(X_{i}\) and \(-X_{i}\) have the same distribution under \(H_{0}\).

To do this, let \(\epsilon_{1},\ldots,\epsilon_{n}\) and \(\epsilon_{1}^{\prime},\ldots,\epsilon_{n}^{\prime}\) be mutually independent and identically distributed, and independent of the \(X_{i}\)s, each \(\epsilon_{i}\) either \(1\) or \(-1\) with probability \(1/2\) each. We claim that

\[(T_{n}(\epsilon_{1}X_{1},\ldots,\epsilon_{n}X_{n}),\,T_{n}(\epsilon_{1}^{ \prime}X_{1},\ldots,\epsilon_{n}^{\prime}X_{n}))\stackrel{{ d}}{{\to}}(T,\,T^{\prime})\, \tag{17.36}\]

where \(T\) and \(T^{\prime}\) are i.i.d., each with the Chi-squared distribution with \(p\) degrees of freedom. The result then follows immediately from Theorem 17.2.3. To prove (17.36), we need the following lemma.

**Lemma 17.4.1**: _Assume \(X_{1},\ldots,X_{n}\) are i.i.d. \(P\) with positive definite covariance matrix \(\Sigma=\Sigma(P)\). Under the above assumptions, and if \(\mu(P)=0\), then_\[n^{-1/2}(\sum_{i=1}^{n}\epsilon_{i}X_{i},\,\sum_{i=1}^{n}\epsilon_{i}^{\prime}X_{i}) \stackrel{{ d}}{{\to}}(Z_{1},\,Z_{2})\,\]

_where \(Z_{1}\) and \(Z_{2}\) are i.i.d., each with the multivariate normal distribution with mean 0 and covariance matrix \(\Sigma\)._

**Remark 17.4.1**: Even if \(\mu(P)\neq 0\), then the same argument applies with the same result as long as \(\Sigma\) is replaced by the matrix with \((j,k)\) component given by \(E(X_{i,j}X_{i,k})\).

Proof of Lemma 17.4.1. Apply the Cramer-Wold Device. Fix vectors \(a\) and \(b\). It suffices to show the unconditional distribution of

\[n^{-1/2}\sum_{i=1}^{n}(\epsilon_{i}a^{\top}X_{i}+\epsilon_{i}^{\prime}b^{\top} X_{i})\]

tends in distribution to that of \(a^{\top}Z_{1}+b^{\top}Z_{2}\), which of course is \(N(0,a^{\top}\Sigma a+b^{\top}\Sigma b)\). But (17.38) is a normalized sum of i.i.d. real-valued random variables, with mean 0 and variance

\[E[(\epsilon_{i}a^{\top}X_{i}+\epsilon_{i}^{\prime}b^{\top}X_{i})^ {2}]\] \[=E\epsilon_{i}^{2}E[(a^{\top}X_{i})^{2}]+E((\epsilon_{i}^{\prime} )^{2})E[(b^{\top}X_{i})^{2}]+E\epsilon_{i}E\epsilon_{i}^{\prime}E(a^{\top}X_{i }b^{\top}X_{i})\] \[=E[(a^{\top}X_{i})^{2}]+E[(b^{\top}X_{i})^{2}]=E(a^{\top}X_{i}X_{i }^{\prime}a)+E(b^{\top}X_{i}X_{i}^{\prime}b)=a^{\top}\Sigma a+b^{\top}\Sigma b\.\]

The result follows from the ordinary Central Limit Theorem.

Next, we consider the randomization distribution based on a modified T-squared statistic. Instead of \(\hat{\Sigma}_{n}\), define \(\bar{\Sigma}_{n}\) to be

\[\bar{\Sigma}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}X_{i}^{\top}\.\]

Let

\[\bar{T}_{n}=\bar{T}_{n}(X_{1},\ldots,X_{n})=n\bar{X}_{n}^{\top}\bar{\Sigma}_{n }^{-1}\bar{X}_{n}\,\]

so that \(\hat{\Sigma}_{n}\) replaces \(\bar{\Sigma}_{n}\) in \(T_{n}\), and the denominator \(n-1\) is changed to \(n\). There are two reasons for considering this modification. First, \(\bar{\Sigma}_{n}\) is, under \(H_{0}\), an unbiased and consistent estimator for \(\Sigma\). More importantly for our purposes, \(\bar{\Sigma}_{n}\) is invariant with respect to sign changes of the observations; that is, replacing \(X_{i}\) by \(-X_{i}\) for any of the \(i\) results in the same estimator.

**Lemma 17.4.2**: _Under the assumptions of Lemma 17.4.1 and if \(\mu(P)=0\), we have_

\[(\bar{T}_{n}(\epsilon_{1}X_{1},\ldots,\epsilon_{n}X_{n}),\,\bar{T}_{n}( \epsilon_{1}^{\prime}X_{1},\ldots,\epsilon_{n}^{\prime}X_{n}))\stackrel{{ d}}{{\to}}(Z_{1}^{\top}\Sigma^{-1}Z_{1},\,Z_{2}^{\top}\Sigma^{-1}Z_{2})\,\]and hence the limiting joint distribution is a product of independent Chi-squared distributions each with \(p\) degrees of freedom._

Proof of Lemma 17.4.2. As mentioned, for any \(\epsilon_{i}\), \(\bar{\Sigma}_{n}(\epsilon_{1}X_{1},\ldots,\epsilon_{n}X_{n})=\bar{\Sigma}_{n}(X_ {1},\ldots,X_{n})\), with each term converging in probability to that of \(\Sigma\). The result follows by the Continuous Mapping Theorem.

Next, we consider Hotelling's T-squared statistic.

**Lemma 17.4.3**: _Under the assumptions of Lemma 17.4.1 and if \(\mu(P)=0\), we have_

\[(T_{n}(\epsilon_{1}X_{1},\ldots,\epsilon_{n}X_{n}),\,T_{n}(\epsilon_{1}^{ \prime}X_{1},\ldots,\epsilon_{n}^{\prime}X_{n}))\stackrel{{ d}}{{\to}}(Z_{1}^{\top}\Sigma^{-1}Z_{1},Z_{2}^{\top}\Sigma^{-1}Z_{2})\,\]

_and hence the limiting joint distribution is a product of independent Chi-squared distributions each with \(p\) degrees of freedom._

Proof of Lemma 17.4.3. Trivially,

\[\frac{n-1}{n}\hat{\Sigma}_{n}(\epsilon_{1}X_{1},\ldots,\epsilon_{n}X_{n})= \bar{\Sigma}_{n}-(\frac{1}{n}\sum_{i=1}^{n}\epsilon_{i}X_{i})(\frac{1}{n}\sum _{i=1}^{n}\epsilon_{i}X_{i})^{\top}\.\]

But, \(n^{-1}\sum_{i}\epsilon_{i}X_{i}\) is an average of mean 0 random vectors with finite second moments, and hence converges in probability to 0. Since we already established \(\bar{\Sigma}_{n}\) is consistent, it follows that \(\hat{\Sigma}_{n}\) is a consistent estimator of \(\Sigma\). The result then follows again by the Continuous Mapping Theorem.

Thus, the conditions for Theorem 17.2.3 have been verified and so the permutation distribution is asymptotically Chi-squared with \(p\) degrees of freedom. Since the true unconditional limiting distribution is also Chi-squared with \(p\) degrees of freedom, the conclusion is that the randomization test based on the T-squared statistic (or modified T-squared statistic) has rejection probability tending to the nominal level under any \(P\) with \(\mu(P)=0\) (and having second moments). Moreover, it retains exact control of the Type 1 error as long as the randomization hypothesis holds, i.e., \(X_{i}\) and \(-X_{i}\) have the same distribution (actually even if second moments don't exist).

**Example 17.4.4**: **(Maximum Test for Multivariate Mean)** Assume the same setup as Example 17.4.3. But rather than Hotelling's T-squared statistic, consider the maximum studentized statistic \(M_{n}\), given by

\[M_{n}=M_{n}(X_{1},\ldots,X_{n})=\max_{1\leq j\leq p}T_{n,j}\, \tag{17.39}\]

where

\[T_{n,j}=\frac{\sqrt{n}|\bar{X}_{n,j}|}{s_{n,j}}\, \tag{17.40}\]

\(\bar{X}_{n,j}\) is the \(j\)th component of the sample mean vector \(\bar{X}_{n}\), and \(s_{n,j}^{2}\) is the \((j,j)\) component of \(\hat{\Sigma}_{n}\). As before, when \(X_{i}\) and \(-X_{i}\) have the same distribution under the null hypothesis, the rejection probability is exactly the nominal level (and this is true for any choice of test statistic). We would like to examine the asymptotic Type 1 error rate of the randomization test based on the test statistic \(M_{n}\) under the null hypothesis \(H_{0}\) that specifies \(\mu(P)=0\). For this, we assume as in Example 17.4.3 that \(\Sigma\) exists, and also that all diagonal elements of \(\Sigma\) are positive.

Under \(H_{0}\), the true sampling distribution of \(M_{n}\) satisfies (Problem 17.19)

\[M_{n}\stackrel{{ d}}{{\to}}\max_{1\leq j\leq p}\left(|Y_{1}|,\, \ldots,|Y_{p}|\right)\,, \tag{17.41}\]

where \((Y_{1},\,\ldots,\,Y_{p})\) is multivariate normal with mean 0 and covariance matrix \(\Sigma^{\prime}\), where \(\Sigma^{\prime}\) is the correlation matrix corresponding to \(\Sigma\). In other words, the \((i,\,j)\) component of \(\Sigma^{\prime}\) is the \((i,\,j)\) component of \(\Sigma\) divided by \(\sigma_{i}\sigma_{j}\), and \(\sigma_{i}^{2}\) is the \(i\)th diagonal element of \(\Sigma\).

We claim that the randomization distribution asymptotically approximates the distribution of \(\max_{j}|Y_{j}|\) under \(H_{0}\). To see this, we again apply Theorem 17.2.3. First, note that, under \(H_{0}\),

\[\frac{n-1}{n}s_{n,j}^{2}\left(\epsilon_{1}X_{1},\,\ldots,\,\epsilon_{n}\,X_{n} \right)=\frac{1}{n}\sum_{i=1}^{n}X_{i,j}^{2}-\bar{X}_{n,j}^{2}\stackrel{{ P}}{{\to}}\sigma_{j}^{2}\,. \tag{17.42}\]

Therefore, by Lemma 17.4.1 and (17.42), we can apply the argument used in the proof of Theorem 17.3.2 to deduce that

\[\left(M_{n}(\epsilon_{1}X_{1},\,\ldots,\,\epsilon_{n}\,X_{n}),\,M_{n}(\epsilon_ {1}^{\prime}X_{1},\,\ldots,\,\epsilon_{n}^{\prime}\,X_{n})\right)\stackrel{{ d}}{{\to}}\left(M,\,M^{\prime}\right)\,, \tag{17.43}\]

where \(M\) and \(M^{\prime}\) are i.i.d. with distribution that of \(\max_{j}(|Y_{j}|)\) given in (17.41). Thus, the conditions of Theorem 17.2.3 hold. Therefore, similar conclusions apply to the randomization test based on \(M_{n}\) as for Hotelling's T-squared statistic. That is, the probability of a Type 1 error control is exactly the nominal level when \(X_{i}\) and \(-X_{i}\) have the same distribution, but otherwise, the probability of a Type 1 error tends to the nominal level.

### Randomization Tests and Multiple Testing

So far, randomization and permutation tests have been developed for tests of a single null hypothesis. Extensions to multiple testing are possible and desirable. Some obvious ways to do this are as follows:

* Since randomization tests can be used to generate \(p\)-values of individual tests, such as by (17.5) or (17.7), one can apply any of a number of multiple testing procedures based on marginal \(p\)-values. Many such tests were discussed in Chapter 9. For example, one may apply:* The Holm method to control the FWER (Theorem 9.1.2).
* The Benjamini-Yekutieli method to control the FDR (Theorem 9.3.2).
* Apply the closure method discussed in Section 9.2, where tests of intersection hypotheses are constructed via randomization tests.

**Example 17.5.1** (**Example** 17.4.3, **continued**) In Example 17.4.3, a randomization test based on Hotelling's T-squared statistic was discussed for testing the null hypothesis that a mean vector \(\mu(P)=0\). If one wishes to know which components of the mean vector might be nonzero, then the problem should be regarded as a multiple testing problem. Let \(H_{i}\) specify the ith component, \(\mu_{i}(P)\), of \(\mu(P)\) is 0. The closure method may be applied here as a means of constructing a procedure that controls the FWER. All that is needed are tests of the intersection hypotheses \(H_{K}\), where

\[H_{K}:\mu_{i}(P)=0\quad\text{for all }i\in K\.\]

Example 17.4.3 shows how to test \(H_{K}\) when \(K=\{1,\ldots,p\}\). For general \(K\), one can simply apply the same test, but just to those components specified by \(K\). If \(I\) denotes the set of indices \(i\) of hypotheses corresponding to \(\mu_{i}(P)=0\), and \(X_{I}=(X_{i},\,i\in I)\), then the FWER is controlled exactly if \(X_{I}\) and \(-X_{I}\) have the same distribution. Without the symmetry assumption, the FWER tends to \(\alpha\) (under the second moment assumption considered in Example 17.4.3).

In the previous example, the number of subsets \(K\) required to test may be of order \(2^{p}\). By using the maximum statistic, we now instead develop a stepdown method (as in Example 9.1.7) that is feasible for large \(p\). By using randomization tests, the assumption of multivariate normality is not needed. For power considerations when comparing the two test statistics, see Section 13.5.4.

**Example 17.5.2** (**Example** 17.4.4, **continued**) We now develop a stepdown method for testing means based on the maximum statistic \(M_{n}\) defined in (17.39). The method is a special case of Procedure 9.1.1, though we provide the details here. For \(i=1,\,\ldots,\,p\), \(H_{i}\) specifies \(\mu_{i}(P)=0\). With \(T_{n,\,j}\) defined in (17.40), order the observed test statistics as

\[T_{n,r_{1}}\geq T_{n,r_{2}}\geq\cdots\geq T_{n,r_{p}}\]

and let \(H_{(1)}\), \(H_{(2)}\),..., \(H_{(p)}\) be the corresponding hypotheses.

The stepdown procedure begins with the most significant test statistic \(T_{n,r_{1}}\), which is also \(M_{n}\). First, test the joint null hypothesis \(H_{\{1,\ldots,p\}}\) that all null hypotheses are true, using the randomization test based on the maximum statistic \(M_{n}\) described in Example 17.4.4. This hypothesis is rejected if \(T_{n,r_{1}}\) is large. If it is not large, accept all hypotheses; otherwise, reject the hypothesis \(H_{(1)}\) corresponding to the largest test statistic. Once a hypothesis is rejected, remove it and test the remaining hypotheses by rejecting for large values of the maximum of the remaining test statistics, and so We just need to specify the construction of critical values in each step. When testing \(H_{K}\), let \(\hat{c}_{n,K}(1-\alpha)\) denote the \(1-\alpha\) quantile of the randomization distribution corresponding to the statistic

\[M_{n,K}=\max_{j\in K}T_{n,j}\.\]

(Note that one could use the exact randomization test, which allows randomization in order to achieve exact level \(\alpha\), but for simplicity we opt for the slightly conservative procedure that does not randomize. The method can be adapted to maintain exact error control if desired.) Then, the stepdown algorithm can be described as follows.

**Procedure 1**: **(Stepdown Method Based on Randomization Tests)**__

1. Let \(K_{1}=\{1,\ldots,\,p\}\). If \(T_{n,r_{1}}\leq\hat{c}_{n,K_{1}}(1-\alpha)\), then accept all hypotheses and stop; otherwise, reject \(H_{(1)}\) and continue.
2. Let \(K_{2}\) be the indices of the hypotheses not previously rejected. If \(T_{n,r_{2}}\leq\hat{c}_{n,K_{2}}(1-\alpha)\), then accept all remaining hypotheses and stop; otherwise, reject \(H_{(2)}\) and continue. \[\vdots\]
2. Let \(K_{j}\) be the indices of the hypotheses not previously rejected. If \(T_{n,r_{j}}\leq\hat{c}_{n,K_{j}}(1-\alpha)\), then accept all remaining hypotheses and stop; otherwise, reject \(H_{(j)}\) and continue. \[\vdots\]
3. If \(T_{n,r_{p}}\leq\hat{c}_{n,K_{p}}(1-\alpha)\), then accept \(H_{(p)}\); otherwise, reject \(H_{(p)}\).

Theorem 9.1.3 shows that the FWER is controlled at level \(\alpha\) under the symmetry assumption that \(X_{I}\) and \(-X_{I}\) have the same distribution, just as in Example 17.5.1. Moreover, even if the symmetry assumption does not hold, the FWER tends to \(\alpha\) under the assumption of finite nonzero variances.

### Problems

#### Section 17.2

**Problem 17.1**: Generalize Theorem 17.2.1 to the case where \(\mathbf{G}\) is an infinite group.

**Problem 17.2**: With \(\hat{p}\) defined in (17.5), show that (17.6) holds.

**Problem 17.3**: (i) Suppose \(Y_{1},\ldots,Y_{B}\) are exchangeable real-valued random variables; that is, their joint distribution is invariant under permutations. Let \(\tilde{q}\) be defined \[\tilde{q}=\frac{1}{B}\left[1+\sum_{i=1}^{B-1}I\{Y_{i}\geq Y_{B}\}\right]\,.\]

Show, \(P\{\tilde{q}\leq u\}\leq u\) for all \(0\leq u\leq 1\). _Hint:_ Condition on the order statistics.

(ii) With \(\tilde{p}\) defined in (17.7), show that (17.8) holds.

(iii) How would you construct a \(p\)-value based on sampling without replacement from \({\bf G}\)?

**Problem 17.4**: With \(\hat{p}\) and \(\tilde{p}\) defined in (17.5) and (17.7), respectively, show that \(\hat{p}-\tilde{p}\to 0\) in probability.

**Problem 17.5**: As an approximation to (17.9), let \(g_{1}\),..., \(g_{B-1}\) be i.i.d. and uniform on \({\bf G}\). Also, set \(g_{B}\) to be the identity. Define

\[\tilde{R}_{n,\,B}(t)=\frac{1}{B}\sum_{i=1}^{B}I\{T_{n}(g_{i}X)\leq t\}\,.\]

Show, conditional on \(X\),

\[\sup_{t}|\tilde{R}_{n,\,B}(t)-\hat{R}_{n}(t)|\to 0\]

in probability as \(B\to\infty\), and so

\[\sup_{t}|\tilde{R}_{n,\,B}(t)-\hat{R}_{n}(t)|\to 0\]

in probability (unconditionally) as well. Do these results hold only under the null hypothesis? _Hint:_ Apply Theorem 11.4.3. For a similar result based on sampling without replacement, see Problem 12.15.

**Problem 17.6**: Suppose \(X_{1},\ldots,\,X_{n}\) are i.i.d. according to a q.m.d. location model with finite variance. Show the ARE of the one-sample \(t\)-test with respect to the randomization \(t\)-test (based on sign changes) is 1 (even if the underlying density is not normal).

**Problem 17.7**: In Theorem 17.2.4, show the conclusion may fail if \(\psi_{P}\) is not an odd function.

**Problem 17.8**: Verify (17.16) and (17.17). _Hint:_ Let \(S\) be the number of positive integers \(i\leq m\) with \(W_{i}=1\), and condition on \(S\).

**Problem 17.9**: (i) Assume \(X_{1},\,X_{2},\,\ldots\) are independent, with \(X_{i}\sim N(\mu_{i},\,1)\), with \(\mu_{i}\geq 0\). For testing the null hypothesis that all \(\mu_{i}=0\), compute the limiting power of the one-sided \(t\)-test test against alternatives \(\mu_{i}\) such that \(\sum_{i}\mu_{i}^{2}<\infty\). (Even though the variance is known, you are asked to consider the \(t\)-test.)

[MISSING_PAGE_EMPTY:2468]

test and the optimal UMPU test (5.75) are asymptotically equivalent in the sense of Problem 15.25.

(iii) Investigate what happens if the underlying distribution has correlation 0, but \(X_{i}\) and \(Y_{i}\) are dependent.

**Problem 17.17**: Prove a version of the Continuous Mapping Theorem (Theorem 11.2.10) for randomization distributions. That is, assume the randomization distribution \(\hat{R}_{n}(\cdot)\) of some test statistic \(T_{n}\) satisfies \(\hat{R}_{n}(t)\) converges in probability to \(R(t)\) for all \(t\) for which \(R(\cdot)\) is continuous. Let \(g\) be a continuous function, at least on a set of points where \(R\) has probability one. Prove a limit result for the randomization distribution based on the statistic \(g(T_{n})\).

#### _Section_ 17.4_

**Problem 17.18**: In Example 17.4.1, for testing the null hypothesis that all \(\mu_{i}\) = 0, verify the asymptotic behavior of the randomization distribution under the null hypothesis. _Hint: Problem_ 11.12.

**Problem 17.19**: Show (17.41).

**Problem 17.20**: Prove (17.43).

### Notes

Early references to permutations tests were provided at the end of Chapter 5. An elementary account is provided by Good (1994), who provides an extensive bibliography, and Edgington (1995). Multivariate permutation tests are developed in Pesarin (2001). The present large-sample approach originated in Hoeffding (1952). Applications to block experiments are discussed in Robinson (1973). Expansions for the power of rank and permutation tests in the one- and two-sample problems are obtained in Albers et al. (1976) and Bickel and Van Zwet (1978), respectively. A full account of the large-sample theory of rank statistics is given in Hajek et al. (1999). Robust two-sample permutation tests are obtained in Lambert (1985). The role of studentization in providing Type 1 error control for permutation tests was first recognized in Neuhaus (1993) and Janssen (1997, 1999), Janssen and Pauls (2005). A growing literature allows for application of randomization tests even when the randomization hypothesis fails. Some general results were provided in Chung and Romano (2013, 2016a, 2016b). Also, see Neubert and Brunner (2007) and Janssen and Pauls (2003a, 2005). Omelka and Pauly (2012) use permutation tests to compare correlations. Jentsch and Pauly (2015) apply randomization tests to testing equality of spectral densities. DiCiccio and Romano (2017) apply permutation tests for inference about correlation and regression coefficients. Bugni et al. (2019) apply randomization schemes to randomized controlled studies where units are stratified into a finite number of strata according to covariates. Bai et al. (2021) apply randomization methods to paired comparisons where the number of strata grows with sample size. Ritzwoller and Romano (2021) develop permutation tests for tests of streakiness in Bernoulli sequences with applications to the "hot hand" hypothesis. Romano and Tirlea (2020) consider permutation tests for dependence in time series models. Randomization methods have been extended to situations where the randomization hypothesis only holds in an asymptotic sense; see Canay et al. (2017).

## Chapter 18 Bootstrap and Subsampling Methods

### 18.1 Introduction

The bootstrap, subsampling, and other resampling methods provide methods for inference, especially in problems where large-sample approximations are not tractable. Such methods are not foolproof and require mathematical justification. In this chapter, fundamental properties of these methods are developed.

In Section 18.2 we first review some basic constructions of confidence regions and tests, which derive from the limiting distribution of an estimator or test sequence. This serves to motivate the bootstrap construction studied in Section 18.3; the bootstrap method offers a powerful approach to approximating the sampling distribution of a given statistic or estimator. The emphasis here is to find methods that control the level constraint, at least asymptotically. Like the randomization construction, the bootstrap approach will be asymptotically efficient if the given statistic is chosen appropriately; for example, see Theorem 18.3.2 and Corollary 18.3.1.

While the bootstrap is quite general, how does it compare in situations when other large-sample approaches apply as well? In Section 18.4, we provide some support to the claim that the bootstrap approach can improve upon methods that rely on a normal approximation. The use of the bootstrap in the context of hypothesis testing is studied in Section 18.5.

While the bootstrap method is quite broadly applicable, in some situations, it can be inconsistent. A more general approach based on subsampling is presented in Section 18.7. Together, these approaches serve as valuable tools for inference without having to make strong assumptions about the underlying distribution.

### 18.2 Basic Large-Sample Approximations

In the previous section, it was shown how permutation and randomization tests can be used in certain problems where the randomization hypothesis holds. Unfortunately, randomization tests only apply to a restricted class of problems. In this section,we discuss some generally used asymptotic approaches for constructing confidence regions or hypothesis tests based on data \(X=X^{n}\). In what follows, \(X^{n}=(X_{1},\ldots,X_{n})\) is typically a sample of \(n\) i.i.d. random variables taking values in a sample space \(S\) and having unknown probability distribution \(P\), where \(P\) is assumed to belong to a certain collection \(\mathbf{P}\) of distributions. Even outside the i.i.d. case, we think of the data \(X^{n}\) as coming from a model indexed by the unknown probability mechanism \(P\). The collection \(\mathbf{P}\) may be a parametric model indexed by a Euclidean parameter, but we will also consider nonparametric models.

We shall be interested in inferences concerning some parameter \(\theta(P)\). By the usual duality between the construction of confidence regions and hypothesis tests, we can restrict the discussion to the construction of confidence regions. Let the range of \(\theta\) be denoted by \(\Theta\), so that

\[\Theta=\{\theta(P):P\in\mathbf{P}\}\.\]

Typically, \(\Theta\) is a subset of the real line, but we also consider more general parameters. For example, the problem of estimating the entire cumulative distribution function (c.d.f.) of real-valued observations may be treated, so that \(\Theta\) is an appropriate function space.

This leads to considering a _root_\(R_{n}(X^{n},\theta(P))\), a term first coined by Beran (1984), which is just some real-valued functional depending on both \(X^{n}\) and \(\theta(P)\). The idea is that a confidence interval for \(\theta(P)\) could be constructed if the distribution of the root were known. For example, an estimator \(\hat{\theta}_{n}\) of a real-valued parameter \(\theta(P)\) might be given so that a natural choice is \(R_{n}(X^{n},\theta(P))=[\hat{\theta}_{n}-\theta(P)]\), or alternatively \(R_{n}(X^{n},\theta(P))=[\hat{\theta}_{n}-\theta(P)]/s_{n}\), where \(s_{n}\) is some estimate of the standard deviation of \(\hat{\theta}_{n}\).

When \(\mathbf{P}\) is suitably large so that the problem is nonparametric in nature, a natural construction for an estimator \(\hat{\theta}_{n}\) of \(\theta(P)\) is the plug-in estimator \(\hat{\theta}_{n}=\theta(\hat{P}_{n})\), where \(\hat{P}_{n}\) is the empirical distribution of the data, defined by

\[\hat{P}_{n}(E)=n^{-1}\sum_{i=1}^{n}I\{X_{i}\in E\}\.\]

Of course, this construction implicitly assumes that \(\theta(\cdot)\) is defined for empirical distributions so that \(\theta(\hat{P}_{n})\) is at least well defined. Alternatively, in parametric problems for which \(\mathbf{P}\) is indexed by a parameter \(\psi\) belonging to a subset \(\Psi\) of \(\mathbb{R}^{p}\) so that \(\mathbf{P}=\{P_{\psi}:\psi\in\Psi\}\), then \(\theta(P)\) can be described as a functional \(t(\psi)\). Hence, \(\hat{\theta}_{n}\) is often taken to be \(t(\hat{\psi}_{n})\), where \(\hat{\psi}_{n}\) is some desirable estimator of \(\psi\), such as an efficient likelihood estimator.

Let \(J_{n}(P)\) be the distribution of \(R_{n}(X^{n},\theta(P))\) under \(P\), and let \(J_{nIn order to construct a confidence region for \(\theta(P)\) based on the root \(R_{n}(X^{n},\theta(P))\), the sampling distribution \(J_{n}(P)\) or its appropriate quantiles must be known or estimated. Some standard methods, based on pivots and asymptotic approximations, are now briefly reviewed. Note that in many of the examples when the observations are real-valued, it is more convenient and customary to index the unknown family of distributions by the cumulative distribution function \(F\) rather than \(P\). We will freely use both, depending on the situation.

#### Pivotal Method

In certain exceptional cases, the distribution \(J_{n}(P)\) of \(R_{n}(X^{n},\theta(P))\) under \(P\) does not depend on \(P\). In this case, the root \(R_{n}(X^{n},\theta(P))\) is called a _pivotal quantity_ or a _pivot_ for short. Such quantities were previously considered in Section 6.12. From a pivot, a level \(1-\alpha\) confidence region for \(\theta(P)\) can be constructed by choosing constants \(c_{1}\) and \(c_{2}\) so that

\[P\{c_{1}\leq R_{n}(X^{n},\theta(P))\leq c_{2}\}\geq 1-\alpha. \tag{18.1}\]

Then, the confidence region

\[C_{n}=\{\theta\in\Theta:\ c_{1}\leq R_{n}(X^{n},\theta)\leq c_{2}\}\]

contains \(\theta(P)\) with probability under \(P\) at least \(1-\alpha\). Of course, the coverage probability is exactly \(1-\alpha\) if one has equality in (18.1).

Classical examples where confidence regions may be formed from a pivot are the following.

**Example 18.2.1** (**Location and Scale Families**) Suppose we are given an i.i.d. sample \(X^{n}=(X_{1},\ldots,X_{n})\) of \(n\) real-valued random variables, each having a distribution function of the form \(F[(x-\theta)/\sigma]\), where \(F\) is known, \(\theta\) is a location parameter, and \(\sigma\) is a scale parameter. More generally, suppose \(\hat{\theta}_{n}\) is location and scale equivariant in the sense that

\[\hat{\theta}_{n}(aX_{1}+b,\ldots,aX_{n}+b)=a\hat{\theta}_{n}(X_{1},\ldots,X_{n })+b\ ;\]

also suppose \(\hat{\sigma}_{n}\) is location invariant and scale equivariant in the sense that

\[\hat{\sigma}_{n}(aX_{1}+b,\ldots,aX_{n}+b)=|a|\hat{\sigma}_{n}(X_{1},\ldots,X_{ n})\.\]

Then, the root \(R_{n}(X^{n},\theta(P))=n^{1/2}[\hat{\theta}_{n}-\theta(P)]/\hat{\sigma}_{n}\) is a pivot (Problem 18.1). For example, in the case where \(F\) is the standard normal distribution function, \(\hat{\theta}_{n}\) is the sample mean and \(\hat{\sigma}_{n}^{2}\) is the usual unbiased estimate of variance, \(R_{n}\) has a \(t\)-distribution with \(n-1\) degrees of freedom. For another example, if \(\hat{\sigma}_{n}\) is location invariant and scale equivariant, then \(\hat{\sigma}_{n}/\sigma\) is also a pivot, since its distribution will not depend on \(\theta\) or \(\sigma\), but will of course depend on \(F\). When \(F\) is not normal, exact distribution theory may be difficult, but one may resort to Monte Carlo simulation of \(J_{n}(P)\) (discussed below). This example can be generalized to a class of parametric problems where group invariance considerations apply, and pivotal quantities lead to equivariant confidence sets; see Section 6.12 and Problems 6.71-6.74.

**Example 18.2.2**: (**Kolmogorov-Smirnov Confidence Bands**) Suppose that \(X^{n}=(X_{1},\,\cdots,\,X_{n})\) is a sample of \(n\) real-valued random variables having a distribution function \(F\). For a fixed value of \(x\), a (pointwise) confidence interval for \(F(x)\) can be based on the empirical distribution function \(\hat{F}_{n}(x)\), by using the fact that \(n\hat{F}_{n}(x)\) has a binomial distribution with parameters \(n\) and \(F(x)\). The goal now is to construct a uniform or simultaneous confidence band for \(\theta(F)=F\), so that it is required to find a set of distribution functions containing the true \(F(x)\) for all \(x\) (or uniformly in \(x\)) with coverage probability \(1-\alpha\). Toward this end, consider the root

\[R_{n}(X^{n},\,F)=n^{1/2}\sup_{x}|\hat{F}_{n}(x)-F(x)|.\]

Recall that, if \(F\) is continuous, then the distribution of \(R_{n}(X^{n},\,F)\) under \(F\) does not depend on \(F\) and so \(R_{n}(X^{n},\,F)\) is a pivot (Section 6.13 and Problem 11.68). As discussed in Sections 6.13 and 16.2, the finite-sample quantiles of this distribution have been tabled. Without the assumption that \(F\) is continuous, the distribution of \(R_{n}(X^{n},\,F)\) under \(F\) does depend on \(F\), both in finite samples and asymptotically.

In general, if \(R_{n}(X^{n},\,\theta(P))\) is a pivot, its distribution may not be explicitly computable or have a known tractable form. However, since there is only one distribution that needs to be known (and not an entire family indexed by \(P\)), the problem is much simpler than if the distribution depends on \(P\). One can resort to Monte Carlo simulation to approximate this distribution to any desired level of accuracy, by simulating the distribution of \(R_{n}(X^{n},\,\theta(P))\) under \(P\) for any choice of \(P\) in \(\mathbf{P}\). For further details, see Example 11.4.3.

#### Asymptotic Pivotal Method

In general, the above construction breaks down because \(R_{n}(X^{n},\,\theta(P))\) has a distribution \(J_{n}(P)\) which depends on the unknown probability distribution \(P\) generating the data. However, it is then sometimes the case that \(J_{n}(P)\) converges weakly to a limiting distribution \(J\) which is independent of \(P\). In this case, the root (sequence) \(R_{n}(X^{n},\,\theta(P))\) is called an _asymptotic pivot_, and then the quantiles of \(J\) may be used to construct an asymptotic confidence region for \(\theta(P)\).

**Example 18.2.3**: (**Parametric Models**) Suppose \(X^{n}=(X_{1},\,\ldots,\,X_{n})\) is a sample from a model \(\{P_{\theta},\,\,\theta\in\Omega\}\), where \(\Omega\) is a subset of \(\mathbb{R}^{k}\). To construct a confidence 

[MISSING_PAGE_FAIL:873]

the form of the asymptotic distribution as well as being able to construct consistent estimates of the unknown parameters upon which \(J(P)\) depends. Moreover, the method essentially consists of a double approximation; first, the finite sampling distribution \(J_{n}(P)\) is approximated by an asymptotic approximation \(J(P)\), and then \(J(P)\) is in turn approximated by \(J(\hat{P}_{n})\).

The most general situation occurs when the limiting distribution \(J(P)\) has an unknown form, and methods to handle this case will be treated in the subsequent sections.

**Example 18.2.5**: **(Nonparametric Mean, continued)** In the previous example, consider instead the non-studentized root

\[R_{n}(X^{n},\theta(F))=n^{1/2}[\bar{X}_{n}-\theta(F)]\.\]

In this case, \(J_{n}(F)\) converges weakly to \(J(F)\), the normal distribution with mean zero and variance \(\sigma^{2}(F)\). The resulting approximation to \(J_{n}(F)\) is the normal distribution with mean zero and variance \(S_{n}^{2}\). Alternatively, one can estimate the variance by any consistent estimator, such as the sample variance \(\sigma^{2}(\hat{F}_{n})\), where \(\hat{F}_{n}\) is the empirical distribution function. In effect, studentizing an asymptotically normal root converts it to an asymptotic pivot, and both methods lead to the same solution. (However, the bootstrap approach in the next section treats the roots differently.)

**Example 18.2.6**: **(Binomial \(p\))** As in Example 11.3.4, suppose \(X\) is binomial based on \(n\) trials and success probability \(p\). Let \(\hat{p}_{n}=X/n\). Like the previous example, the non-studentized root \(n^{1/2}(\hat{p}_{n}-p)\) and the studentized root \(n^{1/2}(\hat{p}_{n}-p)/[\hat{p}_{n}(1-\hat{p}_{n})]^{1/2}\) lead to the same approximate confidence interval given by (11.23). On the other hand, the Wilson interval (11.25) based on the root \(n^{1/2}(\hat{p}_{n}-p)/[p(1-p)]^{1/2}\) leads to a genuinely different solution which performs better in finite samples; see Brown et al. (2001).

**Example 18.2.7**: **(Trimmed mean)** Suppose \(X^{n}=(X_{1},\ldots,X_{n})\) is a sample of \(n\) real-valued random variables with unknown distribution function \(F\). Assume that \(F\) is symmetric about some unknown value \(\theta(F)\). Let \(\hat{\theta}_{n,\alpha}(X_{1},\ldots,X_{n})\) be the \(\alpha\)-trimmed mean; specifically,

\[\hat{\theta}_{n,\alpha}=\frac{1}{n-2[\alpha n]}\sum_{i=[\alpha n]+1}^{n-[ \alpha n]}X_{(i)}\,\]

where \(X_{(1)}\leq X_{(2)}\leq\cdots\leq X_{(n)}\) denote the order statistics and \(k=[\alpha n]\) is the greatest integer less than or equal to \(\alpha n\). Consider the root \(R_{n}(X^{n},\theta(F))=n^{1/2}[\hat{\theta}_{n,\alpha}-\theta(F)]\). Then, under reasonable smoothness conditions on \(F\) and assuming \(0\leq\alpha<1/2\), it is known that \(J_{n}(F)\) converges weakly to the normal distribution \(J(F)\) with mean zero and variance \(\sigma^{2}(\alpha,\,F)\), where \[\sigma^{2}(\alpha,F)= \tag{18.2}\] \[\frac{1}{(1-2\alpha)^{2}}\,[\int_{F^{-1}(\alpha)}^{F^{-1}(1-\alpha)} (t-\theta(F))^{2}dF(t)+2\alpha(F^{-1}(\alpha)-\theta(F))^{2}];\]

see Serfling (1980, p. 236). Then, a very simple first-order approximation to \(J(F)\) is \(J(\hat{F}_{n})\), where \(\hat{F}_{n}\) is the empirical distribution. The resulting \(J(\hat{F}_{n})\) is just the normal distribution with mean zero and variance \(\sigma^{2}(\alpha,\,\hat{F}_{n})\).

The use of the normal approximation in the previous example hinged on the availability of a consistent estimate of the asymptotic variance. The simple expression (18.2) easily led to a simple estimator. However, a closed form expression for the asymptotic variance may not exist. A fairly general approach to estimating the variance of a statistic is provided by the _jackknife_ estimator of variance, for which we refer the reader to Shao and Tu (1995, Chapter 2). However, the double approximation based on asymptotic normality and an estimate of the limiting variance may be poor. An alternative approach that more directly attempts to approximate the finite-sample distribution will be presented in the next section.

### Bootstrap Sampling Distributions

#### Introduction and Consistency

In this section, the bootstrap, due to Efron (1979), is introduced as a general method to approximate a sampling distribution of a statistic or a root (discussed in Section 18.2) in order to construct confidence regions for a parameter of interest. The use of the bootstrap to approximate a null distribution in the construction of hypothesis tests will be considered later as well.

The asymptotic approaches in the previous section are not always applicable, as when the limiting distribution does not have a tractable form. Even when a root has a known limiting distribution, the resulting approximation may be poor in finite samples. The bootstrap procedure discussed in this section is an alternative, more general, direct approach to approximate the sampling distribution \(J_{n}(P)\). An important aspect of the problem of estimating \(J_{n}(P)\) is that, unlike the usual problem of estimation of parameters, \(J_{n}(P)\) depends on \(n\).

The bootstrap method consists of directly estimating the exact finite sampling distribution \(J_{n}(P)\) by \(J_{n}(\hat{P}_{n})\), where \(\hat{P}_{n}\) is an estimate of \(P\) in \(\mathbf{P}\). In this light, the bootstrap estimate \(J_{n}(\hat{P}_{n})\) is a simple _plug-in_ estimate of \(J_{n}(P)\).

In nonparametric problems, \(\hat{P}_{n}\) is typically taken to be the empirical distribution of the data. In parametric problems where \(\mathbf{P}=\{P_{\psi}:\psi\in\Psi\}\), \(\hat{P}_{n}\) may be taken to be \(P_{\hat{\psi}_{n}}\), where \(\hat{\psi}_{n}\) is an estimate of \(\psi\).

In general, \(J_{n}(x,\,\hat{P}_{n})\) need not be continuous and strictly increasing in \(x\), so that unique and well-defined quantiles may not exist. To get around this and in analogy to (11.19), define

\[J_{n}^{-1}(1-\alpha,\,P)=\inf\{x:J_{n}(x,\,P)\geq 1-\alpha\}\;.\]

If \(J_{n}(\cdot,\,P)\) has a unique quantile \(J_{n}^{-1}(1-\alpha,\,P)\), then

\[P\{R_{n}(X^{n},\theta(P))\leq J_{n}^{-1}(1-\alpha,\,P)\}=1-\alpha\;;\]

in general, the probability on the left is at least \(1-\alpha\). If \(J_{n}^{-1}(1-\alpha,\,P)\) were known, then the region

\[\{\theta\in\Theta:\ R_{n}(X^{n},\theta)\leq J_{n}^{-1}(1-\alpha,\,P)\}\]

would be a level \(1-\alpha\) confidence region for \(\theta(P)\). The bootstrap simply replaces \(J_{n}^{-1}(1-\alpha,\,P)\) by \(J_{n}^{-1}(1-\alpha,\,\hat{P}_{n})\). The resulting bootstrap confidence region for \(\theta(P)\) of nominal level \(1-\alpha\) takes the form

\[B_{n}(1-\alpha,\,X^{n})=\{\theta\in\Theta:R_{n}(X^{n},\theta)\leq J_{n}^{-1}(1 -\alpha,\,\hat{P}_{n})\}\;. \tag{18.3}\]

Suppose the problem is to construct a confidence interval for a real-valued parameter \(\theta(P)\) based on the root \(|\hat{\theta}_{n}-\theta(P)|\) for some estimator \(\hat{\theta}_{n}\). The interval (18.3) would then be symmetric about \(\hat{\theta}_{n}\). An alternative equi-tailed interval can be based on the root \(\hat{\theta}_{n}-\theta(P)\) and uses both tails of \(J_{n}(\hat{P}_{n})\); it is given by

\[\{\theta\in\Theta:J_{n}^{-1}(\frac{\alpha}{2},\,\hat{P}_{n})\leq R_{n}(X^{n}, \theta)\leq J_{n}^{-1}(1-\frac{\alpha}{2},\,\hat{P}_{n})\}\;.\]

A comparison of the two approaches will be made in Section 18.4.

Outside certain exceptional cases, the bootstrap approximation \(J_{n}(x,\,\hat{P}_{n})\) cannot be calculated exactly. Even in the relatively simple case when \(\theta(P)\) is the mean of \(P\), the root is \(n^{1/2}[\tilde{X}_{n}-\theta(P)]\), and \(\hat{P}_{n}\) is the empirical distribution, the exact computation of the bootstrap distribution involves an \(n\)-fold convolution.1 Typically, one resorts to a Monte Carlo approximation to \(J_{n}(P)\), as introduced in Example 11.4.3. Specifically, conditional on the data \(X^{n}\), for \(j=1,\,\ldots,\,B\), let \(X^{n*}_{j}=(X^{*}_{1,j},\,\ldots,\,X^{*}_{n,j})\) be a sample of \(n\) i.i.d. observations from \(\hat{P}_{n}\); \(X^{n*}_{j}\) is referred to as the \(j\)th bootstrap sample of size \(n\). Of course, when \(\hat{P}_{n}\) is the empirical distribution, this amounts to resampling the original observations with replacement. The bootstrap estimator \(J_{n}(\hat{P}_{n})\) is then approximated by the empirical distribution of the \(B\) values \(R_{n}(X^{n*}_{j},\hat{\theta}_{n})\). Because \(B\) can be taken to be large (assuming enough computing power), the resulting approximation can be made arbitrarily close to \(J_{n}(\hat{P}_{n})\) (see Example 11.4.3), and so we will subsequently focus on the exact bootstrap estimator \(J_{n}(\hat{P}_{n})\) while keeping in mind it is usually only approximated by Monte Carlo simulation.

The bootstrap can then be viewed as a simple plug-in estimator of a distribution function. This simple idea, combined with Monte Carlo simulation, allows for quite a broad range of applications.

We will now discuss the consistency of the bootstrap estimator \(J_{n}(\hat{P}_{n})\) of the true sampling distribution \(J_{n}(P)\) of \(R_{n}(X^{n},\theta(P))\). Typically, one can show that \(J_{n}(P)\) converges weakly to a nondegenerate limit law \(J(P)\). Since the bootstrap replaces \(P\) by \(\hat{P}_{n}\) in \(J_{n}(\cdot)\), it is useful to study \(J_{n}(P_{n})\) under more general sequences \(\{P_{n}\}\). In order to understand the behavior of the random sequence of distributions \(J_{n}(\hat{P}_{n})\), it will be easier to first understand how \(J_{n}(P_{n})\) behaves for certain fixed sequences \(\{P_{n}\}\). For the bootstrap to be consistent, \(J_{n}(P)\) must be smooth in \(P\) since we are replacing \(P\) by \(\hat{P}_{n}\). Thus, we are led to studying the asymptotic behavior of \(J_{n}(P_{n})\) under fixed sequences of probabilities \(\{P_{n}\}\) which are "converging" to \(P\) in a certain sense. Once it is understood how \(J_{n}(P_{n})\) behaves for fixed sequences \(\{P_{n}\}\), it is easy to pass to random sequences \(\{\hat{P}_{n}\}\).

In the theorem below, the existence of a continuous limiting distribution is assumed, though its exact form need not be explicit. Although the conditions of the theorem appear strong, they can be verified in many interesting examples.

**Theorem 18.3.1**: _Let \({\bf C}_{P}\) be a set of sequences \(\{P_{n}\in{\bf P}\}\) containing the sequence \(\{P,\,P,\cdots\}\). Suppose that, for every sequence \(\{P_{n}\}\) in \({\bf C}_{P}\), \(J_{n}(P_{n})\) converges weakly to a common continuous limit law \(J(P)\) having distribution function \(J(x,\,P)\). Let \(X^{n}\) be a sample of size \(n\) from \(P\). Assume that \(\hat{P}_{n}\) is an estimate of \(P\) based on \(X^{n}\) such that \(\{\hat{P}_{n}\}\) falls in \({\bf C}_{P}\) with probability one. Then,_

\[\sup_{x}|J_{n}(x,\,P)-J_{n}(x,\,\hat{P}_{n})|\to 0\mbox{ with probability one.} \tag{18.4}\]

_If \(J(\cdot,\,P)\) is continuous and strictly increasing at \(J^{-1}(1-\alpha,\,P)\), then_

\[J_{n}^{-1}(1-\alpha,\,\hat{P}_{n})\to J^{-1}(1-\alpha,\,P)\mbox{ with probability one.} \tag{18.5}\]

_Also, the bootstrap confidence set \(B_{n}(1-\alpha,\,X^{n})\) given by Eq. (18.3) is pointwise consistent in level; that is,_

\[P\{\theta(P)\in B_{n}(1-\alpha,\,X^{n})\}\to 1-\alpha. \tag{18.6}\]

Proof. For the proof of part (18.4), note that the assumptions and Polya's Theorem (Theorem 11.2.9) imply that

\[\sup_{x}|J_{n}(x,\,P)-J_{n}(x,\,P_{n})|\to 0\]

for any sequence \(\{P_{n}\}\) in \({\bf C}_{P}\). Thus, since \(\{\hat{P}_{n}\}\in{\bf C}_{P}\) with probability one, (18.4) follows. Lemma 11.2.1 implies \(J_{n}^{-1}(1-\alpha,\,P_{n})\to J^{-1}(1-\alpha,\,P)\) whenever \(\mathbf{C}_{P}\); so (18.5) follows. In order to deduce (18.6), the probability on the left side of (18.6) is equal to

\[P\{R_{n}(X^{n},\theta(P))\leq J_{n}^{-1}(1-\alpha,\,\hat{P}_{n})\}. \tag{18.7}\]

Under \(P\), \(R_{n}(X^{n},\theta(P))\) has a limiting distribution \(J(\cdot,\,P)\) and, by (18.5), \(J_{n}^{-1}(1-\alpha,\,\hat{P}_{n})\to J^{-1}(1-\alpha,\,P)\) with probability one. Thus, by Slutsky's Theorem, (18.7) tends to \(J(J^{-1}(1-\alpha,\,P),\,P)=1-\alpha\).

Often, the set of sequences \(\mathbf{C}_{P}\) can be described as the set of sequences \(\{P_{n}\}\) such that \(d(P_{n},\,P)\to 0\), where \(d\) is an appropriate metric on the space of probabilities. Indeed, one should think of \(\mathbf{C}_{P}\) as a set of sequences \(\{P_{n}\}\) that are converging to \(P\) in an appropriate sense. Thus, the convergence of \(J_{n}(P_{n})\) to \(J(P)\) is locally uniform in the sense \(d(P_{n},\,P)\to 0\) implies \(J_{n}(P_{n})\) converges weakly to \(J(P)\). Note, however, that the appropriate metric \(d\) will depend on the precise nature of the root.

When the convergences (18.4) and (18.5) hold with probability one, we say the bootstrap is strongly consistent. If these convergences hold in probability, we say the bootstrap is weakly consistent. In any case, (18.6) holds even if (18.4) and (18.5) only hold in probability; see Problem 18.3. Furthermore, the conclusion (18.6) holds if \(J(\cdot,\,P)\) is continuous (and not necessarily strictly increasing); see Problem 18.6.

**Example 18.3.1**: (**Parametric Bootstrap**) Suppose \(X^{n}=(X_{1},\,\ldots,\,X_{n})\) is a sample from a q.m.d. model \(\{P_{\theta},\,\,\theta\in\Omega\}\), where \(\Omega\subseteq\mathbb{R}^{k}\). Suppose \(\hat{\theta}_{n}\) is an efficient likelihood estimator in the sense that (14.62) holds. Let \(g(\theta)\) be a differentiable map from \(\Omega\) to \(\mathbb{R}\) with nonzero gradient vector \(\dot{g}(\theta)\). Consider the root

\[R_{n}(X^{n},\theta)=n^{1/2}[g(\hat{\theta}_{n})-g(\theta)]\,\]

with distribution function \(J_{n}(x,\theta)\). By Theorem 14.4.1,

\[J_{n}(x,\theta)\to J(x,\theta)\,\]

where

\[J(x,\theta)=\Phi(x/\sigma_{\theta})\]

and

\[\sigma_{\theta}^{2}=\dot{g}(\theta)I^{-1}(\theta)\dot{g}(\theta)^{\top}\.\]

One approach to estimating the distribution of \(n^{1/2}[g(\hat{\theta}_{n})-g(\theta)]\) is to use the normal approximation \(N(0,\hat{\sigma}_{n}^{2})\), where \(\hat{\sigma}_{n}^{2}\) is a consistent estimator of \(\sigma_{\theta}^{2}\). For example, if \(\dot{g}(\theta)\) and \(I(\theta)\) are continuous in \(\theta\), then a weakly consistent estimator of \(\sigma_{\theta}^{2}\) is

\[\hat{\sigma}_{n}^{2}=\dot{g}(\hat{\theta}_{n})I^{-1}(\hat{\theta}_{n})\dot{g} (\hat{\theta}_{n})^{\top}\.\]

In order to calculate \(\hat{\sigma}_{n}^{2}\), the forms of \(\dot{g}(\cdot)\) and \(I(\cdot)\) must be known. This approach of using a normal approximation with an estimator of the limiting variance is a special case of asymptotic approximation discussed in Section 18.2.3. Because it may be difficult to calculate a consistent estimator of the limiting variance, and because the resulting approximation may be poor, it is interesting to consider the bootstrap method. A discussion of higher order asymptotic comparisons will be discussed in Section 18.4. For now, we show the bootstrap approximation \(J_{n}(x,\hat{\theta}_{n})\) to \(J(x,\theta)\) is weakly consistent.

**Theorem 18.3.2**: _Under the above setup, under \(\theta\),_

\[\sup_{x}|J_{n}(x,\theta)-J(x,\theta)|\to 0\]

_and_

\[\sup_{x}|J_{n}(x,\hat{\theta}_{n})-J_{n}(x,\theta)|\to 0 \tag{18.8}\]

_in probability; therefore, (18.6) holds._

Proof. For purposes of the proof, assume \(k=1\); the general case is left as an exercise. By Theorem 14.4.1, for any sequence \(\theta_{n}\) such that \(n^{1/2}(\theta_{n}-\theta)\to h\), \(J_{n}(x,\theta_{n})\to J(x,\theta)\). In trying to apply the previous theorem, define \({\bf C}_{\theta}\) as the set of sequences \(\{\theta_{n}\}\) satisfying \(n^{1/2}(\theta_{n}-\theta)\to h\), for some finite \(h\). (Rather than describe \({\bf C}_{P}\) as a set of sequences of distributions, we identify \(P_{\theta}\) with \(\theta\) and describe \({\bf C}_{\theta}\) as a set of sequences of parameter values.) Unfortunately, \(\hat{\theta}_{n}\) does not fall in \({\bf C}_{\theta}\) with probability one because \(n^{1/2}(\hat{\theta}_{n}-\theta)\) need not converge with probability one. However, we can modify the argument as follows. Since \(n^{1/2}(\hat{\theta}_{n}-\theta)\) converges in distribution, we can apply the Almost Sure Representation Theorem (Theorem 11.4.4). Thus, there exist random variables \(\tilde{\theta}_{n}\) and \(H\) defined on a common probability space such that \(\hat{\theta}_{n}\) and \(\tilde{\theta}_{n}\) have the same distribution and \(n^{1/2}(\tilde{\theta}_{n}-\theta)\to H\) almost surely. Then, \(\{\tilde{\theta}_{n}\}\in{\bf C}_{\theta}\) with probability one, and we can conclude

\[\sup_{x}|J_{n}(x,\tilde{\theta}_{n})-J_{n}(x,\theta)|\to 0\]

almost surely. Since \(\hat{\theta}_{n}\) and \(\tilde{\theta}_{n}\) have the same distributional properties, so do \(J_{n}(\hat{\theta}_{n})\) and \(J_{n}(\tilde{\theta}_{n})\), and the result (18.8) follows.

A one-sided bootstrap lower confidence bound for \(g(\theta)\) takes the form

\[g(\hat{\theta}_{n})-n^{-1/2}J_{n}^{-1}(1-\alpha,\hat{\theta}_{n})\.\]

The previous theorem implies, under \(\theta\),

\[J_{n}^{-1}(1-\alpha,\hat{\theta}_{n})\stackrel{{ P}}{{\to}}\sigma_{ \theta}z_{1-\alpha}\.\]

Suppose now the problem is to test \(g(\theta)=0\) versus \(g(\theta)>0\). By the duality between tests and confidence regions, one possibility is to reject the null hypothesis if the lower confidence bound exceeds zero, or equivalently when \(\alpha,\hat{\theta}_{n}\)). This test is pointwise asymptotically level \(\alpha\) because, by Slutsky's Theorem, \(n^{1/2}g(\hat{\theta}_{n})\) is asymptotically \(N(0,\sigma_{\theta}^{2})\) if \(g(\theta)=0\). The limiting power of this test against a contiguous sequence of alternatives is given in the following corollary.

**Corollary 18.3.1**: _Under the setup of Example 18.3.1 with \(\theta\) satisfying \(g(\theta)=0\), the limiting power of the test that rejects when \(n^{1/2}g(\hat{\theta}_{n})>J_{n}^{-1}(1-\alpha,\hat{\theta}_{n})\) against the sequence \(\theta_{n}=\theta+hn^{-1/2}\) satisfies_

\[P_{\theta_{n}}^{n}\{n^{1/2}g(\hat{\theta}_{n})>J_{n}^{-1}(1-\alpha,\hat{\theta }_{n})\}\to 1-\Phi(z_{1-\alpha}-\sigma_{\theta}^{-1}\langle\dot{g}(\theta)^{\top},h \rangle). \tag{18.9}\]

Proof. The left-hand side can be written as

\[P_{\theta_{n}}^{n}\{n^{1/2}[g(\hat{\theta}_{n})-g(\theta_{n})]>J_{n}^{-1}(1- \alpha,\hat{\theta}_{n})-n^{1/2}g(\theta_{n})\}. \tag{18.10}\]

Under \(P_{\theta}^{n}\), \(J_{n}^{-1}(1-\alpha,\hat{\theta}_{n})\) converges in probability to \(\sigma_{\theta}z_{1-\alpha}\); by contiguity, under \(P_{\theta_{n}}^{n}\), \(J_{n}^{-1}(1-\alpha,\hat{\theta}_{n})\) converges to the same constant. Also, by differentiability of \(g\) and the fact that \(g(\theta)=0\)

\[n^{1/2}g(\theta_{n})\to\langle\dot{g}(\theta)^{\top},h\rangle\.\]

By Theorem 14.4.1, the left-hand side of (18.10) is asymptotically \(N(0,\sigma_{\theta}^{2})\). Letting \(Z\) denote a standard normal variable, by Slutsky's Theorem, (18.10) converges to

\[P\{\sigma_{\theta}\,Z>\sigma_{\theta}z_{1-\alpha}-\langle\dot{g}(\theta)^{\top },h\rangle\}\,\]

and the result follows.

In fact, it follows from Theorem 15.5.1 that this limiting power is optimal. The moral is that the bootstrap can produce an asymptotically optimal test, but only if the initial estimator or test statistic is optimally chosen. Otherwise, if the root is based on a suboptimal estimator, the bootstrap approach to approximating the sampling distribution of a root is so good that the bootstrap will not be optimal. For example, in a normal location model \(N(\theta,1)\), the bootstrap distribution based on the root \(\bar{X}_{n}-\theta\) is exact as previously discussed (except possibly for simulation error), as is the bootstrap distribution for \(T_{n}-\theta\), where \(T_{n}\) is any location equivariant estimator. But, taking \(T_{n}\) equal to the sample median would not lead to an AUMP test, since the bootstrap is approximating the distribution of the sample median, a suboptimal statistic in this case. Furthermore, this leads to the observation that the bootstrap can be used adaptively to approximate several distributions, and then inference can be based on the one with better properties; see Leger and Romano (1990a; 1990b).

In general, one may base the choice of root by an initial estimator \(\hat{\theta}_{n}\) of \(\theta\), and then bootstrap the root using \(J_{n}(\tilde{\theta}_{n})\), where \(\hat{\theta}_{n}\) and \(\tilde{\theta}_{n}\) may differ. In some instances, the choice is important. For the problem of construction of confidence sets for a multivariate normal mean vector based on the James-Stein estimator, Beran (1995) shows the importance of proper choice of parametric bootstrap, at least when the dimension is moderately high.

#### The Nonparametric Mean

In this section, we consider the case of Example 18.2.4, confidence intervals for the nonparametric mean. This example deserves special attention because many statistics can be approximated by linear statistics. We will examine this case in detail, since similar considerations apply to more complicated situations. Given a sample \(X^{n}=(X_{1},\ldots,X_{n})\) from a distribution \(F\) on the real line, consider the problem of constructing a confidence interval for \(\theta(F)=E_{F}(X_{i})\). Let \(\sigma^{2}(F)\) denote the variance of \(F\). The conditions for Theorem 18.3.1 are verified in the following result.

**Theorem 18.3.3**: _Let \(F\) be a distribution on the line with finite, nonzero variance \(\sigma^{2}(F)\). Let \(J_{n}(F)\) be the distribution of the root \(R_{n}(X^{n},\theta(F))=n^{1/2}[\tilde{X}_{n}-\theta(F)]\)._

1. _Let_ \({\bf C}_{F}\) _be the set of sequences_ \(\{F_{n}\}\) _such that_ \(F_{n}\) _converges weakly to_ \(F\)_,_ \(\theta(F_{n})\to\theta(F)\)_, and_ \(\sigma^{2}(F_{n})\to\sigma^{2}(F)\)_. If_ \(\{F_{n}\}\in{\bf C}_{F}\)_, then_ \(J_{n}(F_{n})\) _converges weakly to_ \(J(F)\)_, where_ \(J(F)\) _is the normal distribution with mean zero and variance_ \(\sigma^{2}(F)\)_._
2. _Let_ \(X_{1},\ldots,X_{n}\) _be i.i.d._ \(F\)_, and let_ \(\hat{F}_{n}\) _denote the empirical distribution function. Then, the bootstrap estimator_ \(J_{n}(\hat{F}_{n})\) _is strongly consistent so that (_18.4_), (_18.5_), and (_18.6_) hold._

Proof of Theorem 18.3.3. For the purpose of proving (i), construct variables \(X_{n,1},\ldots,X_{n,n}\) which are independent with identical distribution \(F_{n}\), and set \(\tilde{X}_{n}=\sum_{i}X_{n,i}/n\). We must show that the law of \(n^{1/2}(\tilde{X}_{n}-\mu(F_{n}))\) converges weakly to \(J(F)\). It suffices to verify the Lindeberg Condition for \(Y_{n,i}\), where \(Y_{n,i}=X_{n,i}-\mu(F_{n})\). This entails showing that, for each \(\epsilon>0\),

\[\lim_{n\to\infty}E[Y_{n,1}^{2}1(Y_{n,1}^{2}>n\epsilon^{2})]=0. \tag{18.11}\]

Note that \(Y_{n,1}\stackrel{{ d}}{{\to}}Y\), where \(Y=X-\mu(F)\) and \(X\) has distribution \(F\), and \(E(Y_{n,1}^{2})\to E(Y^{2})\). By the continuous mapping theorem (Theorem 11.2.10), \(Y_{n,1}^{2}\stackrel{{ d}}{{\to}}Y^{2}\). Now, for any fixed \(\beta>0\) and all \(n>\beta/\epsilon^{2}\),

\[E[Y_{n,1}^{2}1(Y_{n,1}^{2}>n\epsilon^{2})]\leq E[Y_{n,1}^{2}1(Y_{n,1}^{2}>\beta )]\to E[Y^{2}1(Y^{2}>\beta)]\,\]

where the last convergence holds if \(\beta\) is a continuity point of the distribution of \(Y^{2}\), by (11.40). Since the set of continuity points of any distribution is dense and \(E[Y^{2}1(Y^{2}>\beta)]\downarrow 0\) as \(\beta\to\infty\), Lindeberg's Condition holds.

We now prove (ii) by applying Theorem 18.3.1; we must show that \(\{\hat{F}_{n}\}\in{\bf C}_{F}\) with probability one. By the Glivenko-Cantelli Theorem,

\[\sup_{x}|\hat{F}_{n}(x)-F(x)|\to 0\ \ \ \mbox{with probability one}\.\]

Also, by the Strong Law of Large Numbers, \(\theta(\hat{F}_{n})\to\theta(F)\) with probability one and \(\sigma^{2}(\hat{F}_{n})\to\sigma^{2}(F)\) with probability one. Thus, bootstrap confidence intervals forthe mean based on the root \(R_{n}(X^{n},\theta(F))=n^{1/2}(\bar{X}_{n}-\theta(F))\) are asymptotically consistent in the sense of the theorem.

**Remark 18.3.1**: Let \(F\) and \(G\) be two distribution functions on the real line and define \(d_{p}(F,\,G)\) to be the infimum of \(\{E[|X-Y|^{p}]\}^{1/p}\) over all pairs of random variables \(X\) and \(Y\) such that \(X\) has distribution \(F\) and \(Y\) has distribution \(G\). It can be shown that the infimum is attained and that \(d_{p}\) is a metric on the space of distributions having a \(p\)th moment. Further, if \(F\) has a finite variance \(\sigma^{2}(F)\), then \(d_{2}(F_{n},\,F)\to 0\) is equivalent to \(F_{n}\) converging weakly to \(F\) and \(\sigma^{2}(F_{n})\to\sigma^{2}(F)\). Hence, Theorem 18.3.3 may be restated as follows. If \(F\) has a finite variance \(\sigma^{2}(F)\) and \(d_{2}(F_{n},\,F)\to 0\), then \(J_{n}(F_{n})\) converges weakly to \(J(F)\). The metric \(d_{2}\) is known as Mallow's metric. For details, see Bickel and Freedman (1981).

Continuing the example of the nonparametric mean, it is of interest to consider roots other than \(n^{1/2}(\bar{X}_{n}-\theta(F))\). Specifically, consider the studentized root

\[R_{n}^{s}(X^{n},\theta(F))=n^{1/2}(\bar{X}_{n}-\theta(F))/\sigma(\hat{F}_{n})\, \tag{18.12}\]

where \(\sigma^{2}(\hat{F}_{n})\) is the usual bootstrap estimate of variance. To obtain consistency of the bootstrap method, called the bootstrap-\(t\), we appeal to the following result.

**Theorem 18.3.4**: _Suppose \(F\) is a c.d.f. with finite nonzero variance \(\sigma^{2}(F)\). Let \(K_{n}(F)\) be the distribution of the root (18.12) based on a sample of size \(n\) from \(F\)._

1. _Let_ \({\bf C}_{F}\) _be defined as in Theorem_ 18.3.3_. Then, for any sequence_ \(\{F_{n}\}\in{\bf C}_{F}\)_,_ \(K_{n}(F_{n})\) _converges weakly to the standard normal distribution._
2. _Hence, the bootstrap sampling distribution_ \(K_{n}(\hat{F}_{n})\) _is consistent in the sense that (18.4), (18.5), and (18.6) hold._

Before proving this theorem, we first need a weak law of large numbers for a triangular array that generalizes Theorem 11.3.1. The following lemma serves as a suitable version for our purposes.

**Lemma 18.3.1**: _Suppose \(Y_{n,1},\,\ldots,\,Y_{n,n}\) is a triangular array of independent random variables, the \(n\)-th row having c.d.f. \(G_{n}\). Assume \(G_{n}\) converges in distribution to \(G\) and_

\[E[|Y_{n,1}|]\to\,E[|Y|]<\infty\]

_as \(n\to\infty\), where \(Y\) has c.d.f. \(G\). Then,_

\[\bar{Y}_{n}\equiv n^{-1}\sum_{i=1}^{n}Y_{n,i}\stackrel{{ P}}{{\to}}E(Y)\]

_as \(n\to\infty\)._

Proof. Apply Lemma 13.4.2 and (11.40).

Proof of Theorem 18.3.4. For the proof, let \(X_{n,1},\ldots,\)\(X_{n,n}\) be independent with distribution \(F_{n}\). By Theorem 18.3.3 and Slutsky's Theorem, it is enough to show \(\sigma^{2}(\hat{F}_{n})\rightarrow\sigma^{2}(F)\) in probability under \(F_{n}\). But,

\[\sigma^{2}(\hat{F}_{n})=\frac{1}{n}\sum_{i}(X_{n,i}-\bar{X}_{n})^{2}\.\]

Now, apply Lemma 18.3.1 on the Weak Law of Large Numbers for a triangular array with \(Y_{n,i}=X_{n,i}\) and also with \(Y_{n,i}=X_{n,i}^{2}\). The consistency of the bootstrap method based on the root (18.12) now follows easily.

It is interesting to consider how the bootstrap behaves when the underlying distribution has an infinite variance (but well-defined mean). The short answer is that the bootstrap procedure considered thus far will fail, in the sense that the convergence in expression (18.4) does not hold. The failure of the bootstrap for the mean in the infinite variance case was first noted by Babu (1984); further elucidation is given in Athreya (1987) and Knight (1989). In fact, a striking theorem due to Gine and Zinn (1989) asserts that the simple bootstrap studied thus far will work for the mean in the sense of strong consistency if and only if the variance is finite. For a nice exposition of related results, see Gine (1997).

Related results for the studentized bootstrap based on approximating the distribution of the root (18.12) were considered by Csorgo and Mason (1989) and Hall (1990). The conclusion is that the bootstrap is strongly or almost surely consistent if and only if the variance is finite; the bootstrap is weakly consistent if and only if \(X_{i}\) is in the domain of attraction of the normal distribution.

In fact, it was realized by Athreya (1985) that the bootstrap can be modified so that consistency ensues even with infinite variance. The modification consists of reducing the bootstrap sample size. Further results are given in Arcones and Gine (1989, 1991). In other instances where the simple bootstrap fails, consistency can often be recovered by reducing the bootstrap sample size. The benefit of reducing the bootstrap sample size was recognized first in Bretagnolle (1983). An even more general approach based on subsampling will be considered later in Section 18.7.

#### Further Examples

**Example 18.3.2** (**Multivariate Mean**) Let \(X^{n}=(X_{1},\ldots,X_{n})\) be a sample of \(n\) observations from \(F\), where \(X_{i}\) takes values in \(\mathbb{R}\). Let \(\theta(F)=E_{F}(X_{i})\) be equal to the mean vector, and let

\[S_{n}(X^{n},\theta(F))=n^{1/2}(\bar{X}_{n}-\theta(F))\, \tag{18.13}\]

where \(\bar{X}_{n}=\sum_{i}X_{i}/n\) is the sample mean vector. Let\[R_{n}(X^{n},\theta(F))=\left\|S_{n}(X^{n},\theta(F))\right\|\,\]

where \(\|\cdot\|\) is any norm on \(\mathbf{R}^{k}\). The consistency of the bootstrap method based on the root \(R_{n}\) follows from the following theorem.

**Theorem 18.3.5**: _Let \(L_{n}(F)\) be the distribution (in \(\mathbf{R}^{k}\)) of \(S_{n}(X^{n},\theta(F))\) under \(F\), where \(S_{n}\) is defined in (18.13). Let \(\Sigma(F)\) be the covariance matrix of \(S_{n}\) under \(F\). Let \(\mathbf{C}_{F}\) be the set of sequences \(\{F_{n}\}\) such that \(F_{n}\) converges weakly to \(F\) and \(\Sigma(F_{n})\rightarrow\Sigma(F)\), so that each entry of the matrix \(\Sigma(F_{n})\) converges to the corresponding entry (assumed finite) of \(\Sigma(F)\)._

1. _Then,_ \(L_{n}(F_{n})\) _converges weakly to_ \(L(F)\)_, the multivariate normal distribution with mean zero and covariance matrix_ \(\Sigma(F)\)_._
2. _Assume_ \(\Sigma(F)\) _contains at least one nonzero component. Let_ \(\|\cdot\|\) _be any norm on_ \(\mathbf{R}\) _and let_ \(J_{n}(F)\) _be the distribution of_ \(R_{n}(X^{n},\theta(F))=\|S_{n}(X^{n},\theta(F))\|\) _under_ \(F\)_. Then,_ \(J_{n}(F_{n})\) _converges weakly to_ \(J(F)\)_, which is the distribution of_ \(\|Z\|\) _when_ \(Z\) _has distribution_ \(L(F)\)_._
3. _Suppose_ \(X_{1},\ldots,X_{n}\) _are i.i.d._ \(F\) _with empirical distribution_ \(\hat{F}_{n}\) _(in_ \(\mathbf{R}\)_). Then, the bootstrap approximation satisfies_ \[\rho(J_{n}(F),J_{n}(\hat{F}_{n}))\rightarrow0\mbox{ with probability one },\] _and bootstrap confidence regions based on the root_ \(R_{n}\) _are consistent in the sense that the convergences (_18.4_) to (_18.6_) hold._

Proof. The proof of (i) follows by the Cramer-Wold device (Theorem 11.2.3) and by Theorem 18.3.3 (i). To prove (ii), note that any norm \(\|\cdot\|\) on \(\mathbf{R}\) is continuous almost everywhere with respect to \(L(F)\). A proof of this statement can be based on the fact that, for any norm \(\|\cdot\|\), the set \(\{x\in\mathbf{R}:\|x\|=c\}\) has Lebesgue measure zero because it is the boundary of a convex set. So, the continuous mapping theorem applies and so \(J_{n}(F_{n})\) converges weakly to \(J(F)\).

Part (iii) follows because \(\{\hat{F}_{n}\}\in\mathbf{C}_{F}\) with probability one, by the Glivenko-Cantelli Theorem (on \(\mathbf{R}\)) and the strong law of large numbers.

Note the power of the bootstrap method. Analytical methods for approximating the distribution of the root \(R_{n}=\|S_{n}\|\) would depend heavily on the choice of norm \(\|\cdot\|\), but the bootstrap handles them all with equal ease.

Let \(\hat{\Sigma}_{n}=\Sigma(\hat{F})\) be the sample covariance matrix. As in the univariate case, one can also bootstrap the root defined by

\[\tilde{R}_{n}(X^{n},\theta(F))=\|\hat{\Sigma}_{n}^{-1/2}(\tilde{X}_{n}-\theta (F))\|, \tag{18.14}\]

provided \(\Sigma(F)\) is assumed positive definite. In the case where \(\|\cdot\|\) is the usual Euclidean norm, this root leads to confidence ellipsoid, i.e., a confidence set whose shape is an ellipsoid.

**Example 18.3.3**: **(Smooth Functions of Means)** Let \(X_{1}\),..., \(X_{n}\) be i.i.d. S-valued random variables with distribution \(P\). Suppose \(\theta=\theta(P)=(\theta_{1},\ldots,\theta_{p})\), where \(\theta_{j}=E_{P}[h_{j}(X_{i})]\) and the \(h_{j}\) are real-valued functions defined on \(S\). Interest focuses on \(\theta\) or some function \(f\) of \(\theta\). Let \(\hat{\theta}_{n}=(\hat{\theta}_{n,1},\ldots,\hat{\theta}_{n,p})\), where \(\hat{\theta}_{n,j}=\sum_{i=1}^{n}h_{j}(X_{i})/n\). Assume moment conditions on the \(h_{j}(X_{i})\). Then, by the multivariate mean case, the bootstrap approximation to the distribution of \(n^{1/2}(\hat{\theta}_{n}-\theta)\) is appropriately close in the sense

\[\rho\left({\cal L}_{P}(n^{1/2}(\hat{\theta}_{n}-\theta)),\,{\cal L}_{P_{n}^{*} }(n^{1/2}(\hat{\theta}_{n}^{*}-\hat{\theta}_{n}))\right)\to 0 \tag{18.15}\]

with probability one, where \(\rho\) is any metric metrizing weak convergence in \({\rm I\kern-1.8ptR}^{p}\) (such as the Bounded-Lipschitz metric introduced in Problem 11.24). Here, \(P_{n}^{*}\) refers to the distribution of the data resampled from the empirical distribution conditional on \(X_{1}\),...\(X_{n}\). Moreover,

\[\rho\left({\cal L}_{P}(n^{1/2}(\hat{\theta}_{n}-\theta)),\,{\cal L}(Z)\right) \to 0\;, \tag{18.16}\]

where \(Z\) is multivariate normal with mean zero and covariance matrix \(\Sigma\) having \((i,\,j)\)-th component

\[Cov(Z_{i},\,Z_{j})=Cov[h_{i}(X_{1}),\,h_{j}(X_{1})].\]

To see why, define \(Y_{i}\) to be the vector in \({\rm I\kern-1.8ptR}^{p}\) with \(j\)-th component \(h_{j}(X_{i})\), so that we are exactly back in the multivariate mean case. Now, suppose \(f\) is an appropriately smooth function from \({\rm I\kern-1.8ptR}^{p}\) to \({\rm I\kern-1.8ptR}^{q}\), and interest now focuses on the parameter \(\mu=f(\theta)\). Assume \(f=(f_{1},\ldots,\,f_{q})^{\top}\), where \(f_{i}(y_{1},\ldots,y_{p})\) is a real-valued function from \({\rm I\kern-1.8ptR}^{p}\) having a nonzero continuous differential at \((y_{1},\cdots,y_{p})=(\theta_{1},\ldots,\theta_{p})\). Let \(D\) be the \(q\times p\) matrix with \((i,\,j)\) entry \(\partial f_{i}(y_{1},\ldots,y_{p})/\partial y_{j}\) evaluated at \((\theta_{1},\ldots,\theta_{p})\). Then, the following is true.

**Theorem 18.3.6**: _Suppose \(f\) is a function satisfying the above smoothness assumptions. If \(E[h_{j}^{2}(X_{i})]<\infty\), then Eqs. (18.15) and (18.16) hold. Moreover,_

\[\rho\left({\cal L}_{P}(n^{1/2}[f(\hat{\theta}_{n})-f(\theta)]),\,{\cal L}_{P_{ n}^{*}}(n^{1/2}[f(\hat{\theta}_{n}^{*})-f(\hat{\theta}_{n})])\right)\to 0\]

_with probability one and_

\[\sup_{s}\Big{|}\,P\{\|f(\hat{\theta}_{n})-f(\theta)\|\leq s\}-\,P_{n}^{*}\{\|f (\hat{\theta}_{n}^{*})-f(\hat{\theta}_{n})\|\leq s\}\Big{|}\to 0\]

_with probability one._

Proof. The proof follows as Eqs. (18.15) and (18.16) are immediate from the multivariate mean case. The smoothness assumptions on \(f\) and the Delta Method implythat \(n^{1/2}[f(\hat{\theta}_{n})-f(\theta)]\) has a limiting multivariate normal distribution with mean \(0\) and covariance matrix \(D\Sigma D^{\top}\); see Theorem 11.3.4. Similar arguments apply to the bootstrap counterpart. Details are left to the reader (Problem 18.18).

**Example 18.3.4** (**Joint Confidence Rectangles**): Under the assumptions of Theorem 18.3.6, a joint confidence set can be constructed for \((f_{1}(\theta),\ldots,f_{q}(\theta))\) with asymptotic coverage \(1-\alpha\). In the case where \(\|x\|=\max|x_{i}|\), the set is a rectangle in \(\mbox{\rm I$\!$R}^{q}\). Such a set is easily described as

\[\{f(\theta):\ |f_{i}(\hat{\theta}_{n})-f_{i}(\theta)|\leq\hat{b}_{n}(1-\alpha) \ \ \mbox{for all}\ i\ \},\]

where \(\hat{b}_{n}(1-\alpha)\) is the bootstrap approximation to the \(1-\alpha\) quantile of the distribution of \(\max_{i}|f_{i}(\hat{\theta}_{n})-f_{i}(\theta)|\). Thus, a value for \(f_{i}(\theta)\) is included in the region if and only if \(f_{i}(\theta)\in f_{i}(\hat{\theta}_{n})\pm\hat{b}_{n}(1-\alpha)\). Note, however, the intervals \(f_{i}(\hat{\theta}_{n})\pm\hat{b}_{n}(1-\alpha)\) may be unbalanced in the sense that the limiting coverage probability for each marginal parameter \(f_{i}(\theta)\) may depend on \(i\). To fix this, one could instead bootstrap the distribution of \(\max_{i}|f_{i}(\hat{\theta}_{n})-f_{i}(\theta)|/\hat{\sigma}_{n,i}\), where \(\hat{\sigma}_{n,i}^{2}\) is some consistent estimate of the (\(i\), \(i\)) entry of the asymptotic covariance matrix \(D\Sigma D^{\top}\) for \(n^{1/2}f(\hat{\theta}_{n})\). For further discussion, see Beran (1988a), who employs a transformation called prepivoting to achieve balance.

**Example 18.3.5** (**Uniform Confidence Bands for a c.d.f. \(F\)**): Consider a sample \(X^{n}=(X_{1},\ldots,X_{n})\) of real-valued observations having c.d.f. \(F\). The empirical c.d.f. \(\hat{F}_{n}\) is then

\[\hat{F}_{n}(t)=n^{-1}\sum_{i=1}^{n}I\{X_{i}\leq t\}\.\]

For two distribution functions \(F\) and \(G\), define the Kolmogorov-Smirnov (or uniform) metric

\[d_{K}(F,G)=\sup_{t}|F(t)-G(t)|\.\]

Now, consider the root

\[R_{n}(X^{n},\theta(F))=n^{1/2}d_{K}(\hat{F}_{n},F)\,\]

whose distribution under \(F\) is denoted by \(J_{n}(F)\). As discussed in Example 11.4.2, \(J_{n}(F)\) has a continuous limiting distribution. In fact, the following triangular array convergence holds. If \(d_{K}(F_{n},F)\to 0\), then \(J_{n}(F_{n})\stackrel{{ d}}{{\rightarrow}}J(F)\); for a proof, see Politis et al. (1999, p. 20). Thus, we can define \({\bf C}_{F}\) to be the set of sequences \(\{F_{n}\}\) satisfying \(d_{K}(F_{n},F)\to 0\). By the Glivenko-Cantelli Theorem, \(d_{K}(\hat{F}_{n},F)\to 0\) with probability one, and strong consistency of the bootstrap follows. The resulting uniform confidence bands for \(F\) are then consistent in the sense that (18.6) holds, and no assumption on continuity of \(F\) is needed (unlike the classical limit theory). This example has been generalized considerably, and the proof depends on the behavior of \(n^{1/2}[\hat{F}_{n}(t)-F(t)]\), which can be viewed as a random function and is called the _empirical process_. The general theory of bootstrapping the empirical processes is developed in van der Vaart and Wellner (1996) and in Chapter 2 of Gine (1997). In particular, the theory generalizes to quite general spaces \(S\), so that the observations need not be real-valued. In the special case when \(S\) is \(k\)-dimensional Euclidean space, the \(k\)-dimensional empirical process was considered in Beran and Millar (1986). Confidence sets for a multivariate distribution based on the bootstrap can then be constructed which are pointwise consistent in level.

### 18.4 Higher Order Asymptotic Comparisons

One of the main reasons the bootstrap approach is so valuable is that it can be applied to approximate the sampling distribution of an estimator in situations where the finite-sample or large-sample distribution theory is intractable, or depends on unknown parameters. However, even in relatively simple situations, we will see that there are advantages to using a bootstrap approach. For example, consider the problem of constructing a confidence interval for a mean. Under the assumption of a finite variance, the standard normal theory interval and the bootstrap-\(t\) are each pointwise consistent in level. In order to compare them, we must consider higher order asymptotic properties. More generally, suppose \(I_{n}\) is a nominal \(1-\alpha\) level confidence interval for a parameter \(\theta(P)\). Its coverage error under \(P\) is

\[P\{\theta(P)\in I_{n}\}-(1-\alpha)\;\;,\]

and we would like to examine the rate at which this tends to zero. In typical problems, this coverage error is a power of \(n^{-1/2}\). It will be necessary to distinguish one-sided and two-sided confidence intervals because their orders of coverage error may differ.

Throughout this section, attention will focus on confidence intervals for the mean in a nonparametric setting. Specifically, we would like to compare some asymptotic methods based on the normal approximation and the bootstrap. Let \(X^{n}=(X_{1},\ldots,X_{n})\) be i.i.d. with c.d.f. \(F\), mean \(\theta(F)\), and variance \(\sigma^{2}(F)\). Also, let \(\hat{F}_{n}\) denote the empirical c.d.f., and let \(\hat{\sigma}_{n}=\sigma(\hat{F}_{n})\).

Before addressing coverage error, we recall from Section 13.3 the Edgeworth expansions for the distributions of the roots

\[R_{n}(X^{n},F)=n^{1/2}(\tilde{X}_{n}-\theta(F))\]

and

\[R_{n}^{s}(X^{n},F)=n^{1/2}(\tilde{X}_{n}-\theta(F))/\hat{\sigma}_{n}\;;\]

as in Section 18.3.2, their distribution functions under \(F\) are denoted by \(J_{n}(\cdot,F)\) and \(K_{n}(\cdot,F)\), respectively. Let \(\Phi\) and \(\varphi\) denote the standard normal c.d.f. and density, respectively.

**Theorem 18.4.1**: _Assume \(E_{F}(X_{i}^{4})<\infty\). Let \(\psi_{F}\) denote the characteristic function of \(F\), and assume_

\[\limsup_{|s|\to\infty}|\psi_{F}(s)|<1. \tag{18.17}\]

_Then,_

\[J_{n}(t,F)=\Phi(t/\sigma(F))-\frac{1}{6}\gamma(F)\varphi(t/\sigma(F))(\frac{t^ {2}}{\sigma^{2}(F)}-1)n^{-1/2}+\,O(n^{-1})\, \tag{18.18}\]

_where_

\[\gamma(F)=E_{F}[X_{1}-\theta(F)]^{3}/\sigma^{3}(F)\]

_is the skewness of \(F\). Moreover, the expansion holds uniformly in \(t\) in the sense that_

\[J_{n}(t,F)=[\Phi(t/\sigma(F))-\frac{1}{6}\gamma(F)\varphi(t/\sigma(F))(\frac{t ^{2}}{\sigma^{2}(F)}-1)n^{-1/2}]+\,R_{n}(t,F)\,\]

_where \(|R_{n}(t,F)|\leq C/n\) for all \(t\) and some \(C=C_{F}\) which depends on \(F\)._

**Theorem 18.4.2**: _Assume \(E_{F}(X_{i}^{4})<\infty\) and that \(F\) is absolutely continuous. Then, uniformly in \(t\),_

\[K_{n}(t,F)=\Phi(t)+\frac{1}{6}\gamma(F)\varphi(t)(2t^{2}+1)n^{-1/2}+\,O(n^{-1 }). \tag{18.19}\]

Note that the term of order \(n^{-1/2}\) is zero if and only if the underlying skewness \(\gamma(F)\) is zero, so that the dominant error in using a standard normal approximation to the distribution of the studentized statistic is due to skewness of the underlying distribution. We will use these expansions in order to derive some important properties of confidence intervals. Note, however, that the expansions are asymptotic results, and for finite \(n\), including the correction term (i.e., the term of order \(n^{-1/2}\)) may worsen the approximation.

Expansions for the distribution of a root such as (18.18) and (18.19) imply corresponding expansions for their quantiles, which are known as _Cornish-Fisher Expansions_. For example, \(K_{n}^{-1}(1-\alpha,F)\) is a value of \(t\) satisfying \(K_{n}(t,F)=1-\alpha\). Of course, \(K_{n}^{-1}(1-\alpha,F)\to\,z_{1-\alpha}\). We would like to determine \(c=c(\alpha,F)\) such that

\[K_{n}^{-1}(1-\alpha,F)=z_{1-\alpha}+cn^{-1/2}+\,O(n^{-1})\.\]

Set \(1-\alpha\) equal to the right-hand side of (18.19) with \(t=z_{1-\alpha}+cn^{-1/2}\), which yields

\[\Phi(z_{1-\alpha}+cn^{-1/2})+\frac{1}{6}\gamma(F)\varphi(z_{1-\alpha}+cn^{-1/2 })(2z_{1-\alpha}^{2}+1)n^{-1/2}+\,O(n^{-1})=1-\alpha\.\]By expanding \(\Phi\) and \(\varphi\) about \(z_{1-\alpha}\), we find that

\[c=-\frac{1}{6}\gamma(F)(2z_{1-\alpha}^{2}+1)\.\]

Thus,

\[K_{n}^{-1}(1-\alpha,F)=z_{1-\alpha}-\frac{1}{6}\gamma(F)(2z_{1-\alpha}^{2}+1)n^ {-1/2}+\mathit{O}(n^{-1}). \tag{18.20}\]

In fact, under the assumptions of Theorem 18.4.2, the expansion (18.19) holds uniformly in \(t\), and so the expansion (18.20) holds uniformly in \(\alpha\in[\epsilon,1-\epsilon]\), for any \(\epsilon>0\) (Problem 18.20). Similarly, one can show (Problem 18.21) that, under the assumptions of Theorem 18.4.1,

\[J_{n}^{-1}(1-\alpha,F)=\sigma(F)z_{1-\alpha}+\frac{1}{6}\sigma(F)\gamma(F)(z_{ 1-\alpha}^{2}-1)n^{-1/2}+\mathit{O}(n^{-1})\, \tag{18.21}\]

uniformly in \(\alpha\in[\epsilon,1-\epsilon]\).

_Normal Theory Intervals._ The most basic approximate upper one-sided confidence interval for the mean \(\theta(F)\) is given by

\[\tilde{X}_{n}+n^{-1/2}\hat{\sigma}_{n}z_{1-\alpha}\, \tag{18.22}\]

where \(\hat{\sigma}_{n}^{2}=\sigma^{2}(\hat{F}_{n})\) is the (biased) sample variance. Its one-sided coverage error is given by

\[P_{F}\{\theta(F)\leq\tilde{X}_{n}+n^{-1/2}\hat{\sigma}_{n}z_{1-\alpha}\}-(1- \alpha)\]

\[=\alpha-P_{F}\{n^{1/2}(\tilde{X}_{n}-\theta(F))/\hat{\sigma}_{n}<z_{\alpha}\}. \tag{18.23}\]

By (18.19), the one-sided coverage error of this normal theory interval is

\[-\frac{1}{6}\gamma(F)\varphi(z_{\alpha})(2z_{\alpha}^{2}+1)n^{-1/2}+\mathit{O} (n^{-1})=\mathit{O}(n^{-1/2}). \tag{18.24}\]

Analogously, the coverage error of the two-sided confidence interval of nominal level \(1-2\alpha\),

\[\tilde{X}_{n}\pm n^{-1/2}\hat{\sigma}_{n}z_{1-\alpha}\, \tag{18.25}\]

satisfies

\[P_{F}\{-z_{1-\alpha}\leq n^{1/2}(\tilde{X}_{n}-\theta(F))/\hat{\sigma}_{n}\leq z _{1-\alpha}\}-(1-2\alpha)\]

\[=P\{n^{1/2}(\tilde{X}_{n}-\theta(F))/\hat{\sigma}_{n}\leq z_{1-\alpha}\}-P\{n^ {1/2}(\tilde{X}_{n}-\theta(F))\hat{\sigma}_{n}<-z_{1-\alpha}\}-(1-2\alpha)\,\]

which by (18.19) is equal to 

[MISSING_PAGE_FAIL:890]

Thus, the bootstrap approximation \(J_{n}(t,\,\hat{F}_{n})\) to \(J_{n}(t,\,F)\) has the same order of error as that provided by the normal approximation.

Turning now to coverage error, consider the one-sided coverage error of the nominal level \(1-\alpha\) upper confidence bound \(\bar{X}_{n}-n^{-1/2}J_{n}^{-1}(\alpha,\,\hat{F}_{n})\), given by

\[P_{F}\{\theta(F)\leq\bar{X}_{n}-n^{-1/2}J_{n}^{-1}(\alpha,\,\hat{F}_{n})\}-(1-\alpha)\]

\[=\alpha-P_{F}\{n^{1/2}(\bar{X}_{n}-\theta(F))<J_{n}^{-1}(\alpha,\,\hat{F}_{n})\}\]

\[=\alpha-P_{F}\{n^{1/2}(\bar{X}_{n}-\theta(F))/\hat{\sigma}_{n}<z_{\alpha}+\frac {1}{6}\gamma(F)(z_{\alpha}^{2}-1)n^{-1/2}+O_{P}(n^{-1})\}\]

\[=\alpha-P_{F}\{n^{1/2}(\bar{X}_{n}-\theta(F))/\hat{\sigma}_{n}<z_{\alpha}+\frac {1}{6}\gamma(F)(z_{\alpha}^{2}-1)n^{-1/2}\}+O(n^{-1})\;.\]

The last equality, though plausible, requires a rigorous argument, but follows from Problem 18.22. The last expression, by (18.19) and a Taylor expansion, becomes

\[-\frac{1}{2}\gamma(F)\varphi(z_{\alpha})z_{\alpha}^{2}n^{-1/2}+O(n^{-1})\;,\]

so that the one-sided coverage error is of the same order as that provided by the basic normal approximation. Moreover, by similar reasoning, the two-sided bootstrap interval of nominal level \(1-2\alpha\), given by

\[[\bar{X}_{n}-n^{-1/2}J_{n}^{-1}(1-\alpha,\,\hat{F}_{n}),\,\bar{X}_{n}-n^{-1/2}J _{n}^{-1}(\alpha,\,\hat{F}_{n})]\;, \tag{18.29}\]

has coverage error \(O(n^{-1})\). Although these basic bootstrap intervals have the same orders of coverage error as those based on the normal approximation, there is evidence that the bootstrap does provide some improvement (in terms of the size of the constants); see Liu and Singh (1987).

_Bootstrap-\(t\) Confidence Intervals._ Next, we consider bootstrap confidence intervals for \(\theta(F)\) based on the studentized root

\[R_{n}^{s}(X^{n},\,\theta(F))=n^{1/2}(\bar{X}_{n}-\theta(F))/\hat{\sigma}_{n}\;, \tag{18.30}\]

whose distribution under \(F\) is denoted by \(K_{n}(\cdot,\,F)\). The bootstrap versions of the expansions (18.19) and (18.20) are

\[K_{n}(t,\,\hat{F}_{n})=\Phi(t)+\frac{1}{6}\gamma(\hat{F}_{n})\varphi(t)(2t^{2} +1)n^{-1/2}+O_{P}(n^{-1}) \tag{18.31}\]

and

\[K_{n}^{-1}(1-\alpha,\,\hat{F}_{n})=z_{1-\alpha}-\frac{1}{6}\gamma(\hat{F}_{n})( 2z_{1-\alpha}^{2}+1)n^{-1/2}+O_{P}(n^{-1})\;. \tag{18.32}\]

[MISSING_PAGE_EMPTY:2494]

Thus, the one-sided coverage error of the bootstrap-\(t\) interval is \(O(n^{-1})\) and is of smaller order than that provided by the normal approximation or the bootstrap based on a non-studentized root. Intervals with one-sided coverage error of order \(O(n^{-1})\) are said to be _second-order accurate_, while intervals with one-sided coverage error of order \(O(n^{-1/2})\) are only _first-order accurate_.

A heuristic reason why the bootstrap based on the root (18.30) outperforms the bootstrap based on the root (18.26) is as follows. In the case of (18.26), the bootstrap is estimating a distribution that has mean 0 and unknown variance \(\sigma^{2}(F)\). The main contribution to the estimation error is the implicit estimation of \(\sigma^{2}(F)\) by \(\sigma^{2}(\hat{F}_{n})\). On the other hand, the root (18.30) has a distribution that is nearly independent of \(F\) since it is an asymptotic pivot.

The two-sided interval of nominal level \(1-2\alpha\),

\[[\bar{X}_{n}-n^{-1/2}\hat{\sigma}_{n}K_{n}^{-1}(1-\alpha,\,\hat{F}_{n}),\, \bar{X}_{n}-n^{-1/2}\hat{\sigma}_{n}K_{n}^{-1}(\alpha,\,\hat{F}_{n})]\, \tag{18.35}\]

also has coverage error \(O(n^{-1})\) (Problem 18.24). This interval was formed by combining two one-sided intervals. Instead, consider the absolute studentized root

\[R_{n}^{t}(X^{n},\theta(F))=|n^{1/2}(\bar{X}_{n}-\theta(F))|/\hat{\sigma}_{n}\,\]

whose distribution and quantile functions under \(F\) are denoted by \(L_{n}(t,\,F)\) and \(L_{n}^{-1}(1-\alpha,\,F)\), respectively. An alternative two-sided bootstrap confidence interval for \(\theta(F)\) of nominal level \(1-\alpha\) is given by

\[\bar{X}_{n}\pm n^{-1/2}\hat{\sigma}_{n}L_{n}^{-1}(1-\alpha,\,\hat{F}_{n})\.\]

Note that this interval is symmetric about \(\bar{X}_{n}\). Its coverage error is actually \(O(n^{-2})\). The arguments for this claim are similar to the previous claims about coverage error, but more terms are required in expansions like (18.19).

_Bootstrap Calibration._ By considering a studentized statistic, the bootstrap-\(t\) yields one-sided confidence intervals with coverage error smaller than the non-studentized case. However, except in some simple problems, it may be difficult to standardize or studentize a statistic because an explicit estimate of the asymptotic variance may not be available. An alternative approach to improving coverage error is based on the following calibration idea of Loh (1987). Let \(I_{n}=I_{n}(1-\alpha)\) be any interval with nominal level \(1-\alpha\), such as one given by the bootstrap, or a simple normal approximation. Its coverage is defined to be

\[C_{n}(1-\alpha,\,F)=P_{F}\{\theta(F)\in I_{n}(1-\alpha)\}\.\]

We can estimate \(C_{n}(1-\alpha,\,F)\) by its bootstrap counterpart \(C_{n}(1-\alpha,\,\hat{F}_{n})\). Then, determine \(\hat{\alpha}_{n}\) to satisfy

\[C_{n}(1-\hat{\alpha}_{n},\,\hat{F}_{n})=1-\alpha\,\]so that \(\hat{\alpha}_{n}\) is the value that results in the estimated coverage to be the nominal level. The calibrated interval then is defined to be \(I_{n}(1-\hat{\alpha}_{n})\).

To fix ideas, suppose \(I_{n}(1-\alpha)\) is the one-sided normal theory interval \((-\infty,\tilde{X}_{n}+n^{-1/2}\hat{\sigma}_{n}z_{1-\alpha}]\). We argued its coverage error is \(O(n^{-1/2})\). More specifically,

\[C_{n}(1-\alpha,\,F)=P_{F}\{n^{1/2}(\tilde{X}_{n}-\theta(F))/\hat{\sigma}_{n}<z_ {\alpha}\}\]

\[=1-\alpha+\frac{1}{6}\varphi(z_{\alpha})(2z_{\alpha}^{2}+1)n^{-1/2}+O(n^{-1})\.\]

Under smoothness and moment assumptions, the bootstrap estimated coverage satisfies

\[C_{n}(1-\alpha,\,\hat{F}_{n})=1-\alpha+\frac{1}{6}\varphi(z_{\alpha})\gamma( \hat{F}_{n})(2z_{\alpha}^{2}+1)n^{-1/2}+O_{P}(n^{-1})\,\]

and the value of \(\hat{\alpha}_{n}\) is obtained by setting the estimated coverage equal to \(1-\alpha\). One can then show that

\[\hat{\alpha}_{n}-\alpha=-\frac{1}{6}\varphi(z_{\alpha})\gamma(F)(2z_{\alpha}^{ 2}+1)n^{-1/2}+O_{P}(n^{-1}). \tag{18.36}\]

By using this expansion and (18.19), it can be shown that the interval \(I_{n}(1-\hat{\alpha}_{n})\) has coverage \(1-\alpha+O(n^{-1})\), and hence is second-order accurate (Problem 18.25). Thus, calibration reduces the order of coverage error.

_Other Bootstrap Methods._ There are now many variations on the basic bootstrap idea that yield confidence regions that are second-order accurate, assuming the validity of Edgeworth Expansions like the ones used in this section. The calibration method described above is due to Loh (1987, 1991) and is essentially equivalent to Beran's (1987, 1988b) method of prepivoting (Problem 18.29). Given an interval \(I_{n}(1-\alpha)\) of nominal level \(1-\alpha\), calibration produces a new interval, say \(I_{n}^{1}(1-\alpha)=I_{n}(1-\hat{\alpha}_{n})\), where \(\hat{\alpha}_{n}\) is chosen by calibration. It is tempting to iterate this idea to further reduce coverage error. That is, now calibrate \(I_{n}^{1}\) to yield a new interval \(I_{n}^{2}\), and so on. Further reduction in coverage error is indeed possible (at the expense of increased computational effort). For further details on these and other methods such as Efron's \(BC_{a}\) method, see Hall and Martin (1988), Hall (1992) and Efron and Tibshirani (1993).

The analysis of this section was limited to methods for constructing confidence intervals for a mean, assuming the underlying distribution is smooth and has sufficiently many moments. But, many of the conclusions extend to smooth functions of means studied in Example 18.3.3. In particular, in order to reduce coverage error, it is desirable to use a root that is at least asymptotically pivotal, such as a studentized root that is asymptotically standard normal. Otherwise, the basic bootstrap interval (18.3) has the same order of coverage error as one based on approximating the asymptotic distribution. However, whether or not the root is asymptotically pivotal, bootstrap calibration reduces the order of coverage error. Of course, some qualifications are necessary. For one, even in the context of the mean, Cramer's condition may not hold, as in the context of a binomial proportion. Edgeworth expansions for such discrete distributions supported on a lattice are studied in Chapter 5 of Bhattacharya and Rao (1976) and Kolassa and McCullagh (1990); also see Brown et al. (2001), who study the binomial case. In other problems where smoothness is assumed, such as inference for a density or quantiles, Edgeworth expansions for appropriate statistics behave somewhat differently than they do for a mean. Such problems are treated in Hall (1992).

### Hypothesis Testing

In this section, we consider the use of the bootstrap for the construction of hypothesis tests. Assume the data \(X^{n}\) is generated from some unknown law \(P\). The null hypothesis \(H\) asserts that \(P\) belongs to a certain family of distributions \({\bf P_{0}}\), while the alternative hypothesis \(K\) asserts that \(P\) belongs to a family \({\bf P_{1}}\). Of course, we assume the intersection of \({\bf P_{0}}\) and \({\bf P_{1}}\) is the empty set, and the unknown law \(P\) belongs to \({\bf P}\), the union of \({\bf P_{0}}\) and \({\bf P_{1}}\).

There are several approaches one can take to construct a hypothesis test. First, consider the case when the null hypothesis can be expressed as a hypothesis about a real- or vector-valued parameter \(\theta(P)\). Then, one can exploit the familiar duality between confidence regions and hypothesis tests to test hypotheses about \(\theta(P)\). Thus, a consistent in level test of the null hypothesis that \(\theta(P)=\theta_{0}\) can be constructed by a consistent in level confidence region for \(\theta(P)\) by the rule: accept the null hypothesis if and only if the confidence region includes \(\theta_{0}\). Therefore, all the methods we have thus far discussed for constructing confidence regions may be utilized: methods based on a pivot, an asymptotic pivot, an asymptotic approximation, or the bootstrap. Indeed, this was the bootstrap approach already considered in Corollary 18.3.1, and it is also the basis for the multiple test construction in Section 18.6.

**Example 18.5.1** (**Testing Moment Inequalities Using Bootstrap**) Consider a nonparametric version of the moment inequality testing problem in Example 14.4.8. Assume the random vectors \(X_{1}\),..., \(X_{n}\) are i.i.d. \(P\) in \({\rm I\!R}\) with unknown invertible covariance matrix \(\Sigma=\Sigma(P)\) and mean vector \(\theta=\theta(P)\). The problem is to test the null hypothesis

\[H_{0}:\theta_{i}(P)\leq 0\ \ \ \mbox{for all}\ i=1,\ldots,k\.\]

One can exploit the duality between confidence sets and testing to first construct a bootstrap joint "lower" confidence set for \(\theta(P)\) as follows. Let \(\hat{P}_{n}\) be the empirical measure, let \(\tilde{X}\) be the sample mean vector, let \(\hat{\Sigma}=\Sigma(\hat{P}_{n})\) be the sample covariance matrix, and let \(\hat{\sigma}^{2}_{n,i}\) be the \(i\)th diagonal entry of \(\hat{\Sigma}\). Consider the root

\[J_{n}(x,P)=P\left\{\max_{1\leq i\leq k}\frac{\tilde{X}_{n,i}-\theta_{i}(P)}{ \hat{\sigma}_{n,i}}\leq x\right\}\.\]This leads to the bootstrap confidence set

\[\left\{\theta:\ \max_{1\leq i\leq k}\frac{\bar{X}_{n,i}-\theta_{i}}{\hat{\sigma}_{n, i}}\leq J_{n}^{-1}(1-\alpha,\,\hat{P}_{n})\right\}\,\]

which yields the joint lower confidence set as

\[[\bar{X}_{n,i}-\hat{\sigma}_{n,i}J_{n}^{-1}(1-\alpha,\,\hat{P}_{n}),\,\infty)\.\]

These \(k\) semi-infinite intervals simultaneously contain the true \(\theta_{i}(P)\) with asymptotic probability \(1-\alpha\). The argument is very similar to that in Example 18.3.4.

Returning to the moment inequality testing problem, a solution is to reject the null hypothesis \(H_{0}\) if the \(0\) vector is not included in the joint confidence set. Such a method asymptotically controls the probability of a Type \(1\) error. The power of this test is considered in Problem 18.30.

However, not all hypothesis testing problems fit nicely into the framework of testing parameters. For example, consider the problem of testing whether the data come from a certain parametric submodel (such as the family of normal distributions) of a nonparametric model, the so-called goodness of fit problem. Or, when \(X_{i}\) is vector-valued, consider the problem of testing whether \(X_{i}\) has a distribution that is spherically symmetric.

Given a test statistic \(T_{n}\), its distribution must be known, estimated, or approximated (at least under the null hypothesis), in order to construct a critical value. The approach taken in this section is to estimate the null distribution of \(T_{n}\) by resampling from a distribution obeying the constraints of the null hypothesis.

To be explicit, assume we wish to construct a test based on a real-valued test statistic \(T_{n}=T_{n}(X^{n})\) which is consistent in level and power. Large values of \(T_{n}\) reject the null hypothesis. Thus, having picked a suitable test statistic \(T_{n}\), our goal is to construct a critical value, say \(c_{n}(1-\alpha)\), so that the test which rejects if and only if \(T_{n}\) exceeds \(c_{n}(1-\alpha)\) satisfies

\[P\{T_{n}(X^{n})>c_{n}(1-\alpha)\}\rightarrow\alpha\ \mbox{as}\ n\rightarrow\infty\]

when \(P\in\mathbf{P_{0}}\). Furthermore, we require this rejection probability to tend to one when \(P\in\mathbf{P_{1}}\). Unlike the classical case, the critical value will be constructed to be data-dependent (as in the case of a permutation test). To see how the bootstrap can be used to determine a critical value, let the distribution of \(T_{n}\) under \(P\) be denoted by

\[G_{n}(x,\,P)=P\{T_{n}(X^{n})\leq x\}\.\]

Note that we have introduced \(G_{n}(\cdot,\,P)\) instead of utilizing \(J_{n}(\cdot,\,P)\) to distinguish from the case of confidence intervals where \(J_{n}(\cdot,\,P)\) represents the distribution of a root which may depend both on the data and on \(P\). In the hypothesis testing context, \(G_{n}(\cdot,\,P)\) represents the distribution of a statistic (and not a root) under \(P\). Let\[g_{n}(1-\alpha,\,P)=\inf\{x:G_{n}(x,\,P)\geq 1-\alpha\}\.\]

Typically, \(G_{n}(\cdot,\,P)\) will converge in distribution to a limit law \(G(\cdot,\,P)\), whose \(1-\alpha\) quantile is denoted by \(g(1-\alpha,\,P)\).

The bootstrap approach is to estimate the null sampling distribution by \(G_{n}(\cdot,\,\hat{Q}_{n})\), where \(\hat{Q}_{n}\) is an estimate of \(P\) in \({\bf P_{0}}\) so that \(\hat{Q}_{n}\) satisfies the constraints of the null hypothesis, since critical values should be determined as if the null hypothesis were true. A bootstrap critical value can then be defined by \(g_{n}(1-\alpha,\,\hat{Q}_{n})\). The resulting nominal level \(\alpha\) bootstrap test rejects \(H\) if and only if \(T_{n}>g_{n}(1-\alpha,\,\hat{Q}_{n})\).

Notice that we would not want to replace a \(\hat{Q}_{n}\) satisfying the null hypothesis constraints by the empirical distribution function \(\hat{P}_{n}\), the usual resampling mechanism of resampling the data with replacement. One might say that the bootstrap is so adept at estimating the distribution of a statistic that \(G_{n}(\cdot,\,\hat{P}_{n})\) is a good estimate of \(G_{n}(\cdot,\,P)\) whether or not \(P\) satisfies the null hypothesis constraints. Hence, the test that rejects when \(T_{n}\) exceeds \(g_{n}(1-\alpha,\,\hat{P}_{n})\) will (under suitable conditions) behave asymptotically like the test that rejects when \(T_{n}\) exceeds \(g_{n}(1-\alpha,\,P)\), and this test has an asymptotic probability of \(\alpha\) of rejecting the null hypothesis, even if \(P\in{\bf P_{1}}\). But, when \(P\in{\bf P_{1}}\), we would want the test to reject with probability that is approaching one.

Thus, the choice of resampling distribution \(\hat{Q}_{n}\) should satisfy the following. If \(P\in{\bf P_{0}}\), \(\hat{Q}_{n}\) should be near \(P\) so that \(G_{n}(\cdot,\,P)\approx G_{n}(\cdot,\,\hat{Q}_{n})\); then, \(g_{n}(1-\alpha,\,P)\approx g_{n}(1-\alpha,\,\hat{Q}_{n})\) and the asymptotic rejection probability approaches \(\alpha\). If, on the other hand, \(P\in{\bf P_{1}}\), \(\hat{Q}_{n}\) should not approach \(P\), but some \(P_{0}\) in \({\bf P_{0}}\). In this way, the critical value should satisfy

\[g_{n}(1-\alpha,\,\hat{Q}_{n})\approx g_{n}(1-\alpha,\,P_{0})\to\,g(1-\alpha,\, P_{0})<\infty\]

as \(n\to\infty\). Then, assuming the test statistic is constructed so that \(T_{n}\to\infty\) under \(P\) when \(P\in{\bf P_{1}}\), we will have

\[P\{T_{n}>g_{n}(1-\alpha,\,\hat{Q}_{n})\}\approx P\{T_{n}>g(1-\alpha,\,P_{0})\}\to 1\]

as \(n\to\infty\), by Slutsky's Theorem.

As in the construction of confidence intervals, \(G_{n}(\cdot,\,P)\) must be smooth in \(P\) in order for the bootstrap to succeed. In the theorem below, rather than specifying a set of sequences \({\bf C}_{P}\) as was done in Theorem 18.3.1, smoothness is described in terms of a metric \(d\), but either approach could be used. The proof is analogous to the proof of Theorem 18.3.1.

**Theorem 18.5.1**: _Let \(X^{n}\) be generated from a probability law \(P\in{\bf P_{0}}\). Assume the following triangular array convergence: \(d(P_{n},\,P)\to 0\) and \(P\in{\bf P_{0}}\) implies \(G_{n}(\cdot,\,P_{n})\) converges weakly to \(G(\cdot,\,P)\) with \(G(\cdot,\,P)\) continuous. Moreover, assume \(\hat{Q}_{n}\) is an estimator of \(P\) based on \(X^{n}\) which satisfies \(d(\hat{Q}_{n},\,P)\to 0\) in probability whenever \(P\in{\bf P_{0}}\). Then,_

\[P\{T_{n}>g_{n}(1-\alpha,\,\hat{Q}_{n})\}\to\alpha\quad\mbox{as $n\to\infty$}\.\]

**Example 18.5.2**: (**Normal Correlation**) Suppose \((Y_{i},\,Z_{i})\), \(i=1,\,\ldots,\,n\) are i.i.d. bivariate normal with unknown means, variances, and correlation \(\rho\). The null hypothesis specifies \(\rho=\rho_{0}\) versus \(\rho>\rho_{0}\). Let \(T_{n}=n^{1/2}\hat{\rho}_{n}\), where \(\hat{\rho}_{n}\) is the usual sample correlation. Under the null hypothesis, the distribution of \(T_{n}\) doesn't depend on any unknown parameters. So, if \(\hat{Q}_{n}\) is any bivariate normal distribution with \(\rho=\rho_{0}\), the bootstrap sampling distribution \(G_{n}(\cdot,\,\hat{Q}_{n})\) is exactly equal to the true null sampling distribution. Note, however, that inverting a parametric bootstrap confidence bound using the root \(n^{1/2}(\hat{\rho}_{n}-\rho)\) would not be exact.

**Example 18.5.3**: (**Likelihood Ratio Tests**) Suppose \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. according to a model \(\{P_{\theta},\,\,\theta\in\Omega\}\), where \(\Omega\) is an open subset of \(\mathbb{R}^{k}\). Assume \(\theta\) is partitioned as \((\xi,\,\mu)\), where \(\xi\) is a vector of length \(p\) and \(\mu\) is a vector of length \(k-p\). The null hypothesis parameter space \(\Omega_{0}\) specifies \(\xi=\xi_{0}\). Under the conditions of Theorem 14.4.2, the likelihood ratio statistic \(T_{n}=2\log(R_{n})\) is asymptotically \(\chi_{p}^{2}\) under the null hypothesis. Suppose \((\xi_{0},\,\hat{\mu}_{n,0})\) is an efficient likelihood estimator of \(\theta\) for the model \(\Omega_{0}\). Rather than using the critical value obtained from \(\chi_{p}^{2}\), one could bootstrap \(T_{n}\). So, let \(G_{n}(x,\,\theta)\) denote the distribution of \(T_{n}\) under \(\theta\). An appropriate parametric bootstrap test obeying the null hypothesis constraints is to reject the null when \(T_{n}\) exceeds the \(1-\alpha\) quantile of \(G_{n}(x,\,(\xi_{0},\,\hat{\mu}_{n,0}))\). Beran and Ducharme (1991) argue that, under regularity conditions, the bootstrap test has error in rejection probability equal to \(O(n^{-2})\), while the usual likelihood ratio test has error \(O(n^{-1})\). Moreover, the bootstrap test can be viewed as an analytical approximation to a Bartlett-corrected likelihood ratio test (see Section 14.4.4). In essence, the bootstrap automatically captures the Bartlett correction and avoids the need for analytical calculation. As an example, recall Example 14.4.7, where it was observed the Bartlett-corrected likelihood ratio test has error \(O(n^{-2})\). Here, the bootstrap test is exact (Problem 18.33).

**Example 18.5.4**: (**Behrens-Fisher Problem Revisited**) For \(j=1,\,2\), let \(X_{i,j},\,\,i=1,\,\ldots,\,n_{j}\) be independent with \(X_{i,j}\) distributed as \(N(\mu_{j},\,\sigma_{j}^{2})\). All four parameters are unknown and vary independently. The null hypothesis asserts \(\mu_{1}=\mu_{2}\) and the alternative is \(\mu_{1}>\mu_{2}\). Let \(n=n_{1}+n_{2}\), and for simplicity assume \(n_{1}\) to be the integer part of \(\lambda n\) for some \(0<\lambda<1\). Let \((\tilde{X}_{n,j},\,S_{n,j}^{2})\) be the usual unbiased estimators of \((\mu_{j},\,\sigma_{j}^{2})\) based on the \(j\)th sample. Consider the test statistic

\[T_{n}=(\tilde{X}_{1}-\tilde{X}_{2})/\sqrt{\frac{S_{n,1}^{2}}{n_{1}}+\frac{S_{n,2}^{2}}{n_{2}}}\,\,.\]

By Example 15.5.4, the test that rejects the null hypothesis when \(T_{n}>z_{1-\alpha}\) is efficient. However, we now study its actual rejection probability.

The null distribution of \(T_{n}\) depends only on \(\sigma^{2}=(\sigma_{1}^{2},\,\sigma_{2}^{2})\) through the ratio \(\sigma_{1}/\sigma_{2}\), and we denote this distribution by \(G_{n}(\cdot,\,\sigma^{2})\). Let \(S_{n}^{2}=(S_{n,1}^{2},\,S_{n,2}^{2})\). Like the method used in Problem 18.28, by conditioning on \(S_{n}^{2}\), we can write

\[G_{n}(x,\,\sigma^{2})=E[a(S_{n}^{2},\,\sigma^{2},\,x)]\,\,,\]where

\[a(S_{n}^{2},\sigma^{2},x)=\Phi[(1+\delta)^{1/2}x]\]

and

\[\delta=\sum_{j=1}^{2}n_{j}^{-1}(S_{n,j}^{2}-\sigma_{j}^{2})/\sum_{j=1}^{2}n_{j} ^{-1}\sigma_{j}^{2}\.\]

By Taylor expansion and the moments of \(S_{n}^{2}\), it follows that (Problem 18.34)

\[G_{n}(x,\sigma^{2})=\Phi(x)+\frac{1}{n}b_{n}(x,\sigma^{2})+O(n^{-2})\, \tag{18.37}\]

where

\[\frac{1}{n}b_{n}(x,\sigma^{2})=-(x+x^{3})\phi(x)\rho_{n}^{2}/4\]

is \(O(n^{-1})\) and

\[\rho_{n}^{2}=\sum_{j=1}^{2}(n_{j}-1)^{-1}n_{j}^{-2}\sigma_{j}^{4}/(\sum_{j=1}^ {2}n_{j}^{-1}\sigma_{j}^{2})^{2}\.\]

Correspondingly, the quantile function satisfies

\[G_{n}^{-1}(1-\alpha,\sigma^{2})=z_{1-\alpha}+(z_{1-\alpha}+z_{1-\alpha}^{3}) \rho_{n}^{2}/4+O(n^{-2}). \tag{18.38}\]

It follows that the rejection probability of the asymptotic test that rejects when \(T_{n}>z_{1-\alpha}\) is \(\alpha+O(n^{-1})\).

Consider next the (parametric) bootstrap-\(t\), which rejects when \(T_{n}>G_{n}^{-1}(1-\alpha,S_{n}^{2})\). Its rejection probability can be expressed as

\[1-E[a(S_{n}^{2},\sigma^{2},G_{n}^{-1}(1-\alpha,S_{n}^{2}))]\.\]

By Taylor expansion, it can be shown that the rejection probability of the test is \(\alpha+O(n^{-2})\) (Problem 18.35). Thus, the bootstrap-\(t\) improves upon the asymptotic expansion. In fact, bootstrap calibration (or the use of prepivoting) further reduces the error in rejection probability to \(O(n^{-3})\). Details are in Beran (1988), who further argues that the Welch method described in Section 13.2.1 behaves like the bootstrap-\(t\) method. Although the Welch approximation is based on elegant mathematics, the bootstrap approach essentially reproduces the analytical approximation automatically.

**Example 18.5.5** (**Nonparametric Mean**) Let \(X_{1}\),..., \(X_{n}\) be i.i.d. observations on the real line with probability law \(P\), mean \(\mu(P)\) and finite variance \(\sigma^{2}(P)\). The problem to test \(\mu(P)=0\) against either a one-sided or two-sided alternative. Let \(\mathbf{P_{0}}\) be the set of distributions with mean zero and finite variance. In the one-sided case, consider the test statistic \(T_{n}=n^{1/2}\widetilde{X}_{n}\), where \(\widetilde{X}_{n}\) is the sample mean, since test statistics based on \(\bar{X}_{n}\) were seen in Section 13.4 to possess a certain optimality property. We will also consider the studentized statistic \(T_{n}^{\prime}=n^{1/2}\bar{X}_{n}/S_{n}\), where we shall take \(S_{n}^{2}\) to be the unbiased estimate of variance. To apply Theorem 18.5.1, let \(\hat{Q}_{n}\) be the empirical distribution \(\hat{P}_{n}\) shifted by \(\bar{X}_{n}\) so it has mean 0. Then, the error in rejection probability will be \(O(n^{-1/2})\) for \(T_{n}\), and will be \(O(n^{-1})\) for \(T_{n}^{\prime}\), at least under the assumptions that \(F\) is smooth and has infinitely many moments; these statements follow from the results in Section 18.4 (Problem 18.37).

While shifting the empirical distribution works in this example, it is not easy to generalize when testing other parameters. Therefore, we consider the following alternative approach. The idea is to choose the distribution in \(\mathbf{P_{0}}\) that is in some sense closest to the empirical distribution \(\hat{P}_{n}\). One way to describe closeness is the following. For distributions \(P\) and \(Q\) on the real line, let \(\delta_{KL}(P,\,Q)\) be the (forward) Kullback-Leibler divergence between \(P\) and \(Q\) (studied in Example 11.3.1), defined by

\[\delta_{KL}(P,\,Q)=\int log(\frac{dP}{d\,Q})dP. \tag{18.39}\]

Note that \(\delta_{KL}(P,\,Q)\) may be \(\infty\), \(\delta_{KL}\) is not a metric, and it is not even symmetric in its arguments. Let \(\hat{Q}_{n}\) be the \(Q\) that minimizes \(\delta_{KL}(\hat{P}_{n},\,Q)\) over \(Q\) in \(\mathbf{P_{0}}\). This choice for \(\hat{Q}_{n}\) can be shown to be well-defined and corresponds to finding the nonparametric maximum likelihood estimator of \(P\) assuming \(P\) is constrained to have mean zero. (Another possibility is to minimize the (backward) Kullback-Leibler divergence \(\delta_{KL}(Q,\,\hat{P}_{n})\).) By Efron (1981) (Problem 18.38), \(\hat{Q}_{n}\) assigns mass \(w_{i}\) to \(X_{i}\), where \(w_{i}\) satisfies

\[w_{i}\,\propto\,\frac{(1+t\,X_{i})^{-1}}{\sum_{j=1}^{n}(1+t\,X_{j})^{-1}}\]

and \(t\) is chosen so that \(\sum_{i=1}^{n}w_{i}X_{i}=0\). Now, one could bootstrap either \(T_{n}\) or \(T_{n}^{\prime}\) from \(\hat{Q}_{n}\).

In fact, this approach suggests an alternative test statistic given by \(T_{n}^{\prime\prime}=n\delta_{KL}(\hat{P}_{n},\,\hat{Q}_{n})\), where \(\hat{Q}_{n}\) is the \(Q\) minimizing the Kullback-Leibler divergence \(\delta_{KL}(\hat{P}_{n},\,Q)\) over \(Q\) in \(\mathbf{P_{0}}\). This is equivalent to the test statistic used by Owen (1988, 2001) in his construction of empirical likelihood, who shows the limiting distribution of \(2T_{n}^{\prime\prime}\) under the null hypothesis is Chi-squared with 1 degree of freedom. The wide scope of empirical likelihood is presented in Owen (2001).

**Example 18.5.6** (**Goodness of fit**): The problem is to test whether the underlying probability distribution \(P\) belongs to a parametric family of distributions \(\mathbf{P_{0}}=\{P_{0},\,\theta\in\Theta_{0}\}\), where \(\Theta_{0}\) is an open subset of \(k\)-dimensional Euclidean space. Let \(\hat{P}_{n}\) be the empirical measure based on \(X_{1},\,\ldots,\,X_{n}\). Let \(\hat{\theta}_{n}\in\Theta_{0}\) be an estimator of \(\theta\). Consider the test statistic

\[T_{n}=n^{1/2}\delta(\hat{P}_{n},\,P_{\hat{\theta}_{n}})\,\]where \(\delta\) is some measure (typically a metric) between \(\hat{P}_{n}\) and \(P_{\hat{\theta}_{n}}\). (In fact, \(\delta\) need not even be symmetric, which is useful sometimes: for example, consider the Cramer-von Mises statistic.) Beran (1986) considers the case where \(\hat{\theta}_{n}\) is a minimum distance estimator, while Romano (1988) assumes that \(\hat{\theta}_{n}\) is some asymptotically linear estimator (like an efficient likelihood estimator). For the resampling mechanism, take \(\hat{Q}_{n}=P_{\hat{\theta}_{n}}\). Beran (1986) and Romano (1988) give different sets of conditions so that the above theorem is applicable, both requiring the machinery of empirical processes.

**Example 18.5.7**: **(Moment inequalties)** Consider testing moment inequalities as in Example 18.5.1 based on the test statistic

\[T_{n}=\max_{1\leq i\leq k}\frac{\bar{X}_{n,i}}{\hat{\sigma}_{n,i}}\.\]

To simplify the point of the example, assume a parametric model with \(P=P_{\theta}\) multivariate normal with unknown mean vector \(\theta\) and known covariance matrix \(\Sigma\). In order to apply Theorem 18.5.1, a reasonable choice for resampling distribution under the null hypothesis would be \(\hat{Q}_{n}=P_{\hat{\mu}_{n}}\), where \(\hat{\mu}_{n}\) is an estimator of \(\theta\) under \(H_{0}\). If we further assume \(\Sigma\) is the identity, then a reasonable choice for \(\hat{\mu}_{n}\) is the maximum likelihood estimator under the null hypothesis constraint; so, \(\hat{\mu}_{n}\) has \(i\)th component \(\hat{\mu}_{n,i}=\min(\bar{X}_{n,i},0)\). Then, the conditions in Theorem 18.5.1 do not hold, and the bootstrap is too liberal; see Problem 18.31. Intuitively, the level of the test would be controlled by using a critical value based on the distribution of the test statistic when \(\theta=0\). Instead, the bootstrap procedure sometimes uses a critical value based on the distribution of the test statistic under \(\hat{\mu}_{n}\), which is component-wise no bigger than \(\bar{X}_{n}\) and therefore leads to a smaller critical value.

### Stepdown Multiple Testing

Suppose data \(X=X^{n}\) is generated from some unknown probability distribution \(P\), where \(P\) belongs to a certain family of probability distributions \(\Omega\). For \(j=1\),..., \(s\), consider the problem of simultaneously testing hypotheses \(H_{j}:\ P\in\omega_{j}\).

For any subset \(K\subseteq\{1,\ldots,s\}\), let \(H_{K}=\bigcap_{j\in K}H_{j}\) be the hypothesis that \(P\in\bigcap_{j\in K}\omega_{j}\). Suppose that a test of the individual hypothesis \(H_{j}\) is based on a test statistic \(T_{n,j}\), with large values indicating evidence against the \(H_{j}\).

The goal is to construct a stepdown method that controls the familywise error rate (FWER). Recall that the FWER is the probability of rejecting at least one true null hypothesis. More specifically, if \(P\) is the true probability mechanism, let \(I=I(P)\subseteq\{1,\ldots,s\}\) denote the indices of the set of true hypotheses; that is, \(i\in I\) if and only \(P\in\omega_{i}\). Then, FWER is the probability under \(P\) that any \(H_{i}\) with \(i\in I\) is rejected. To show its dependence on \(P\), we may write FWER = FWER\({}_{P}\). We require that any procedure satisfy that the FWER be no bigger than \(\alpha\) (at least asymptotically).

Suppose \(H_{i}\) is specified by a real-valued parameter \(\beta_{i}(P)=0\). Then, one approach to constructing a multiple test is to invert a simultaneous confidence region. Under the setup of Example 18.3.4, with \(\beta_{i}(P)=f_{i}(\theta(P))\), any hypothesis \(H_{i}\) is rejected if \(f_{i}(\hat{\theta}_{n})>\hat{b}_{n}(1-\alpha)\). A procedure that uses a common critical value \(\hat{b}_{n}(1-\alpha)\) for all the hypotheses is called a single-step method.

Another approach is to compute (or approximate) a \(p\)-value for each individual test, and then use Holm's method discussed in Section 9.1, However, Holm's method, which makes no assumptions about the dependence structure of the test statistics, can be improved by methods that implicitly or explicitly estimate this dependence structure. In this section, we consider a stepdown procedure that incorporates the dependence structure and thereby improves upon the two methods just described.

Let

\[T_{n,r_{1}}\geq T_{n,r_{2}}\geq\cdots\geq T_{n,r_{s}} \tag{18.40}\]

denote the observed ordered test statistics, and let \(H_{r_{1}}\), \(H_{r_{2}}\),..., \(H_{r_{s}}\) be the corresponding hypotheses.

Recall the stepdown method presented in Procedure 9.1.1. The problem now is how to construct the \(\hat{c}_{n,K}(1-\alpha)\) so that the FWER is controlled, at least asymptotically. The following is an immediate consequence of Theorem 9.1.3, and reduces the multiple testing problem of asymptotically controlling the FWER to the single testing problem of asymptotically controlling the probability of a Type 1 error.

**Corollary 18.6.1**: _Let \(P\) denote the true distribution generating the data. Consider Procedure 9.1.1 based on critical values \(\hat{c}_{n,K}(1-\alpha)\) which satisfy the monotonicity requirement: for any \(K\supseteq I(P)\),_

\[\hat{c}_{n,K}(1-\alpha)\geq\hat{c}_{n,I(P)}(1-\alpha). \tag{18.41}\]

_If \(\hat{c}_{n,I(P)}(1-\alpha)\) satisfies_

\[\limsup_{n}P\{\max(T_{n,j}:\ j\in I(P))>\hat{c}_{n,I(P)}(1-\alpha)\}\leq\alpha\, \tag{18.42}\]

_then \(\limsup_{n}FWER_{P}\leq\alpha\) as \(n\to\infty\)._

Under the monotonicity requirement (18.41), the multiplicity problem is effectively reduced to testing a single intersection hypothesis at a time. So, the problem now is to construct intersection tests whose critical values are monotone and asymptotically control the rejection probability.

We now specialize a bit and develop a concrete construction based on the bootstrap. Suppose hypothesis \(H_{i}\) is specified by \(\{P:\ \theta_{i}(P)=0\}\) for some real-valued parameter \(\theta_{i}\), and \(\hat{\theta}_{n,i}\) is an estimate of \(\theta_{i}\). Also, let \(T_{n,i}=\tau_{n}|\hat{\theta}_{n,i}|\) for some non-negative (nonrandom) sequence \(\tau_{n}\to\infty\); usually, \(\tau_{n}=n^{1/2}\). The bootstrap method relies on its ability to approximate the joint distribution of \(\{\tau_{n}|\hat{\theta}_{n,i}\ -\theta_{i}(P)\}:\ i\in K\}\), whose distribution we denote by \(J_{n,K}(P)\). Also, let \(L_{n,K}(P)\) denote the distribution under \(P\) of \(\max\{\tau_{n}|\hat{\theta}_{n,i}\ -\theta_{i}(P)|:\ i\in K\}\), with corresponding distribution function \(L_{n,K}(x,P)\) and \(\alpha\)-quantile

\[b_{n,K}(\alpha,P)=\inf\{x:L_{n,K}(x,P)\geq\alpha\}\.\]

Let \(\hat{Q}_{n}\) be some estimate of \(P\). Then, a nominal \(1-\alpha\) level bootstrap confidence region for the subset of parameters \(\{\theta_{i}(P):\ i\in K\}\) is given by

\[\{(\theta_{i}:\ i\in K):\max_{i\in K}\tau_{n}|\hat{\theta}_{n,i}-\theta_{i}|\leq b _{n,K}(1-\alpha,\hat{Q}_{n})\}\.\]

So a value of \(0\) for \(\theta_{i}(P)\) falls outside the region iff

\[T_{n,i}=\tau_{n}|\hat{\theta}_{n,i}|>b_{n,K}(1-\alpha,\hat{Q}_{n})\.\]

By the usual duality of confidence sets and hypothesis tests, this suggests the use of the critical value

\[\hat{c}_{n,K}(1-\alpha)=b_{n,K}(1-\alpha,\hat{Q}_{n})\,\]

at least if the bootstrap is a valid asymptotic approach for confidence region construction.

Note that, regardless of asymptotic behavior, the monotonicity assumption (18.41) is always satisfied for the choice (18.43). Indeed, for any \(Q\) and if \(I\subseteq K\), \(b_{n,I}(1-\alpha,Q)\) is the \(1-\alpha\) quantile under \(Q\) of the maximum of \(|I|\) variables, while \(b_{n,K}(1-\alpha,Q)\) is the \(1-\alpha\) quantile of these same \(|I|\) variables together with \(|K|-|I|\) variables.

Therefore, in order to apply Theorem 18.6.1 to conclude \(\lim\sup_{n}\text{FWER}_{P}\leq\alpha\), it is now only necessary to study the asymptotic behavior of \(b_{n,K}(1-\alpha,\hat{Q}_{n})\) in the case \(K=I(P)\). For this, we assume the usual conditions for bootstrap consistency when testing the _single_ hypothesis that \(\theta_{i}(P)=0\) for all \(i\in I(P)\); that is, we assume the bootstrap consistently estimates the joint distribution of \(\tau_{n}[\hat{\theta}_{n,i}-\theta_{i}(P)]\) for \(i\in I(P)\). In particular, we assume

\[J_{n,I(P)}(P)\stackrel{{ d}}{{\rightarrow}}J_{I(P)}(P)\,\]

a nondegenerate limit law. Assumption (18.44) implies \(L_{n,I(P)}(P)\) has a limiting distribution \(L_{I(P)}(P)\), with c.d.f. denoted by \(L_{I(P)}(x,P)\). We will further assume \(L_{I(P)}(P)\) is continuous and strictly increasing on its support. It follows that

\[b_{n,I(P)}(1-\alpha,P)\to b_{I(P)}(1-\alpha,P)\,\]

where \(b_{I(P)}(\alpha,P)\) is the \(\alpha\)-quantile of the limiting distribution \(L_{I(P)}(P)\).

**Theorem 18.6.1**: _Fix \(P\) and assume (18.44) and that \(L_{I(P)}(P)\) is continuous and strictly increasing on its support. Let \(\hat{Q}_{n}\) be an estimate of \(P\) satisfying: for any metric \(\rho\) metrizing weak convergence on \(\mathbb{R}^{|I(P)|}\),_\[\rho\left(J_{n,I(P)}(P),J_{n,I(P)}(\hat{Q}_{n})\right)\stackrel{{ P}}{{\to}}0. \tag{18.46}\]

_Consider the generic stepdown method in Procedure 9.1.1 with \(c_{n,K}(1-\alpha)\) equal to \(b_{n,K}(1-\alpha,\hat{Q}_{n})\). Then, \(\limsup_{n}FWER_{P}\leq\alpha\)._

Proof. By the Continuous Mapping Theorem and a subsequence argument (Problem 18.40), the assumption (18.44) implies

\[\rho_{1}\left(L_{n,I(P)}(P),L_{n,I(P)}(\hat{Q}_{n})\right)\stackrel{{ P}}{{\to}}0\, \tag{18.47}\]

where \(\rho_{1}\) is any metric metrizing weak convergence on \(\mathbb{R}\). It follows from Problem 11.30, which is a generalization of Lemma 11.2.1, that

\[b_{n,I(P)}(1-\alpha,\hat{Q}_{n})\stackrel{{ P}}{{\to}}b_{I(P)}(1- \alpha,P)\.\]

By Slutsky's Theorem,

\[P\left\{\max(T_{n,j}:\ j\in I(P))\right\}>b_{n,I(P)}(1-\alpha,\hat{Q}_{n}) \}\to 1-L_{I(P)}(b_{I(P)}(1-\alpha,P),P),\]

and the last expression is \(\alpha\).

**Example 18.6.1**: (**Multivariate Mean**) Assume \(X_{i}=(X_{i,1},\ldots,X_{i,s})\) are \(n\) i.i.d. random vectors with \(E(|X_{i}|^{2})<\infty\) and mean vector \(\mu=(\mu_{1},\ldots,\mu_{s})\). Note that the vector \(X_{i}\) can have an arbitrary \(s\)-variate distribution, so that multivariate normality is not assumed as it was in Example 9.1.7. Suppose \(H_{i}\) specifies \(\mu_{i}=0\) and \(T_{n,i}=n^{-1/2}|\sum_{j=1}^{n}X_{j,i}|\). Then, the conditions of Theorem 18.6.1 are satisfied by Example 18.3.2. Alternatively, one can also consider the studentized test statistic \(t_{n,i}=T_{n,i}/S_{n,i}\), where \(S_{n,i}^{2}\) is the sample variance of the \(i\)th components of the data (Problem 18.41).

**Example 18.6.2**: (**Comparing Treatment Means**) For \(i=1,\ldots,k\), suppose we observe \(k\) independent samples, and the \(i\)th sample consists of \(n_{i}\) i.i.d. observations \(X_{i,1},\ldots,X_{i,n_{i}}\) with mean \(\mu_{i}\) and finite variance \(\sigma_{i}^{2}\). Hypothesis \(H_{i,j}\) specifies \(\mu_{i}=\mu_{j}\), so that the problem is to compare all \(s=\binom{k}{2}\) means. (Note that we are indexing hypotheses and test statistics now by 2 indices \(i\) and \(j\).) Let \(T_{n,i,j}=n^{1/2}|\bar{X}_{n,i}-\bar{X}_{n,j}|\), where \(\bar{X}_{n,i}=\sum_{j=1}^{n}X_{i,j}/n_{i}\). Let \(\hat{Q}_{n,i}\) be the empirical distribution of the \(i\)th sample. The bootstrap resampling scheme is to independently resample \(n_{i}\) observations from \(\hat{Q}_{n,i}\), \(i=1,\ldots,k\). Then, Theorem 18.6.1 applies and it also applies to appropriately studentized statistics (Problem 18.42). The setup can easily accommodate comparisons of \(k\) treatments with a control group (Problem 18.43).

**Example 18.6.3**: (**Testing Correlations**) Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. random vectors in \(\mathbb{R}^{k}\), so that \(X_{i}=(X_{i,1},\ldots,X_{i,k})\). Assume \(E|X_{i,j}|^{2}<\infty\) and \(Var(X_{i,j})>0\), so that the correlation between \(X_{1,i}\) and \(X_{1,j}\), namely \(\rho_{i,j}\) is well defined. Let \(H_{i,j}\)denote the hypothesis that \(\rho_{i,j}=0\), so that the multiple testing problem consists in testing all \(s={k\choose 2}\) pairwise correlations. Also let \(T_{n,i,j}\) denote the ordinary sample correlation between variables \(i\) and \(j\). (Note that we are indexing hypotheses and test statistics now by 2 indices \(i\) and \(j\).) By Example 18.3.3, the conditions for the bootstrap hold because correlations are smooth functions of means.

### Subsampling

In this section, a general theory for the construction of approximate confidence sets or hypothesis tests is presented, so the goal is the same as that of the bootstrap. The basic idea is to approximate the sampling distribution of a statistic based on the values of the statistic computed over smaller subsets of the data. For example, in the case where the data are \(n\) observations which are independent and identically distributed, a statistic \(\hat{\theta}_{n}\) is computed based on the entire data set and is recomputed over all \({n\choose b}\) data sets of size \(b\). Implicit is the notion of a statistic sequence, so that the statistic is defined for samples of size \(n\) and \(b\). These recomputed values of the statistic are suitably normalized to approximate the true sampling distribution.

This approach based on subsamples is perhaps the most general one for approximating a sampling distribution, in the sense that consistency holds under extremely weak conditions. That is, it will be seen that, under very weak assumptions on \(b\), the method is consistent whenever the original statistic, suitably normalized, has a limit distribution under the true model. The bootstrap, on the other hand, requires that the distribution of the statistic is somehow locally smooth as a function of the unknown model. In contrast, no such assumption is required in the theory for subsampling. Indeed, the method here is applicable even in the several known situations which represent counterexamples to the bootstrap. However, when both subsampling and the bootstrap are consistent, the bootstrap is typically more accurate.

To appreciate why subsampling behaves well under such weak assumptions, note that each subset of size \(b\) (taken without replacement from the original data) is indeed a sample of size \(b\) from the true model. If \(b\) is small compared to \(n\) (meaning \(b/n\to 0\)), then there are many (namely \({n\choose b}\)) subsamples of size \(b\) available. Hence, it should be intuitively clear that one can at least approximate the sampling distribution of the (normalized) statistic \(\hat{\theta}_{b}\) by recomputing the values of the statistic over all these subsamples. But, under the weak convergence hypothesis, the sampling distributions based on samples of size \(b\) and \(n\) should be close. The bootstrap, on the other hand, is based on recomputing a statistic over a sample of size \(n\) from some estimated model which is hopefully close to the true model.

The use of subsample values to approximate the variance of a statistic is well known. The Quenouille-Tukey jackknife estimates of bias and variance based on computing a statistic over all subsamples of size \(n-1\) has been well studied and is closely related to the mean and variance of our estimated sampling distribution with \(b=n-1\). For further history of subsampling methods, see Politis et al. (1999).

#### The Basic Theorem in the I.I.D. Case

Suppose \(X_{1}\),..., \(X_{n}\) is a sample of \(n\) i.i.d. random variables taking values in an arbitrary sample space \(S\). The common probability measure generating the observations is denoted by \(P\). The goal is to construct a confidence region for some parameter \(\theta(P)\). For now, assume \(\theta\) is real-valued, but this can and will be generalized to allow for the construction of confidence regions for multivariate parameters or confidence bands for functions.

Let \(\hat{\theta}_{n}=\hat{\theta}_{n}(X_{1},\)..., \(X_{n})\) be an estimator of \(\theta(P)\). It is desired to estimate the true sampling distribution of \(\hat{\theta}_{n}\) in order to make inferences about \(\theta(P)\). Nothing is assumed about the form of the estimator.

As in previous sections, let \(J_{n}(P)\) be the sampling distribution of the root \(\tau_{n}(\hat{\theta}_{n}-\theta(P))\) based on a sample of size \(n\) from \(P\), where \(\tau_{n}\) is a normalizing constant. Here, \(\tau_{n}\) is assumed known and does not depend on \(P\). Also define the corresponding cumulative distribution function:

\[J_{n}(x,\,P)=P\{\tau_{n}[\hat{\theta}_{n}(X_{1},\)..., \(X_{n})-\theta(P)]\leq x\}\.\]

Essentially, the only assumption that we will need to construct asymptotically valid confidence intervals for \(\theta(P)\) is the following.

**Assumption 18.7.1**: There exists a limiting distribution \(J(P)\) such that \(J_{n}(P)\) converges weakly to \(J(P)\) as \(n\rightarrow\infty\).

This assumption will be required to hold for some sequence \(\tau_{n}\). The most informative case occurs when \(\tau_{n}\) is such that the limit law \(J(P)\) is nondegenerate.

To describe the subsampling method, consider the \(N_{n}={n\choose b}\) subsets of size \(b\) of the data \(\{X_{1},\)..., \(X_{n}\}\); call them \(Y_{1}\),..., \(Y_{N_{n}}\), ordered in any fashion. Thus, each \(Y_{i}\) constitutes a sample of size \(b\) from \(P\). Of course, the \(Y_{i}\) depend on \(b\) and \(n\), but this notation has been suppressed. Only a very weak assumption on \(b\) will be required. In the consistency results that follow, it will be assumed that \(b/n\to 0\) and \(b\rightarrow\infty\) as \(n\rightarrow\infty\). Now, let \(\hat{\theta}_{n,b,i}\) be equal to the statistic \(\hat{\theta}_{b}\) evaluated at the data set \(Y_{i}\). The approximation to \(J_{n}(x,\,P)\) we study is defined by

\[L_{n,b}(x)=N_{n}^{-1}\sum_{i=1}^{N_{n}}I\{\tau_{b}(\hat{\theta}_{n,b,i}-\hat{ \theta}_{n})\leq x\}. \tag{18.48}\]

The motivation behind the method is the following. For any \(i\), \(Y_{i}\) is actually a random sample of \(b\) i.i.d. observations from \(P\). Hence, the _exact_ distribution of \(\tau_{b}(\hat{\theta}_{n,b,i}-\theta(P))\) is \(J_{b}(P)\). The empirical distribution of the \(N_{n}\) values of \(\tau_{b}(\hat{\theta}_{n,b,i}-\theta(P))\) should then serve as a good approximation to \(J_{n}(P)\). Of course, \(\theta(P)\) is unknown, so we replace \(\theta(P)\) by \(\hat{\theta}_{n}\), which is asymptotically permissible because \(\tau_{b}(\hat{\theta}_{n}-\theta(P))\) is of order \(\tau_{b}/\tau_{n}\), and \(\tau_{b}/\tau_{n}\) will be assumed to tend to zero.

**Theorem 18.7.1**: _Suppose Assumption 18.7.1 holds. Also, assume \(\tau_{b}/\tau_{n}\to 0\), \(b\to\infty\), and \(b/n\to 0\) as \(n\to\infty\)._

1. _If_ \(x\) _is a continuity point of_ \(J(\cdot,\,P)\)_, then_ \(L_{n,b}(x)\to J(x,\,P)\) _in probability._
2. _If_ \(J(\cdot,\,P)\) _is continuous, then_ \[\sup_{x}|L_{n,b}(x)-J_{n}(x,\,P)|\to 0\mbox{ in probability }.\] (18.49)
3. _Let_ \[c_{n,b}(1-\alpha)=\inf\{x\,:\,L_{n,b}(x)\geq 1-\alpha\}\;.\] _and_ \[c(1-\alpha,\,P)=\inf\{x\,:\,J(x,\,P)\geq 1-\alpha\}\;.\] _If_ \(J(\cdot,\,P)\) _is continuous at_ \(c(1-\alpha,\,P)\)_, then_ \[P\{\tau_{n}[\hat{\theta}_{n}-\theta(P)]\leq c_{n,b}(1-\alpha)\}\to\,1-\alpha \mbox{ as }n\to\infty\;.\] (18.50) _Therefore, the asymptotic coverage probability under_ \(P\) _of the confidence interval_ \([\hat{\theta}_{n}-\tau_{n}^{-1}c_{n,b}(1-\alpha),\,\infty)\) _is the nominal level_ \(1-\alpha\)_._

Proof. Let

\[U_{n}(x)=U_{n,b}(x,\,P)=N_{n}^{-1}\sum_{i=1}^{N_{n}}I\{\tau_{b}[\hat{\theta}_{n,b,i}-\theta(P)]\leq x\}\;. \tag{18.51}\]

Note that the dependence of \(U_{n}(x)\) on \(b\) and \(P\) will now be suppressed for notational convenience. To prove (i), it suffices to show \(U_{n}(x)\) converges in probability to \(J(x,\,P)\) for every continuity point \(x\) of \(J(x,\,P)\). To see why, note that

\[L_{n,b}(x)=N_{n}^{-1}\sum_{i}I\{\tau_{b}[\hat{\theta}_{n,b,i}-\theta(P)]+\tau_{ b}[\theta(P)-\hat{\theta}_{n}]\leq x\}\;,\]

so that for every \(\epsilon>0\),

\[U_{n}(x-\epsilon)I\{E_{n}\}\leq L_{n,b}(x)I\{E_{n}\}\leq U_{n}(x+\epsilon)I\{E _{n}\}\;,\]

where \(I\{E_{n}\}\) is the indicator of the event \(E_{n}\equiv\{\tau_{b}|\theta(P)-\hat{\theta}_{n}|\leq\epsilon\}\). But, the event \(E_{n}\) has probability tending to one. So, with probability tending to one,

\[U_{n}(x-\epsilon)\leq L_{n,b}(x)\leq U_{n}(x+\epsilon)\]

for any \(\epsilon>0\). Hence, if \(x+\epsilon\) and \(x-\epsilon\) are continuity points of \(J(\cdot,\,P)\), then \(U_{n}(x\pm\epsilon)\to J(x\pm\epsilon,\,P)\) in probability implie\[J(x-\epsilon,\,P)-\epsilon\,\leq L_{n,b}(x)\leq J(x+\epsilon,\,P)+\epsilon\]

with probability tending to one. Now, let \(\epsilon\,\rightarrow\,0\) so that \(x\,\pm\,\epsilon\) are continuity points of \(J(\cdot,\,P)\). Then, it suffices to show \(U_{n}(x)\,\rightarrow\,J(x,\,P)\) in probability for all continuity points \(x\) of \(J(\cdot,\,P)\). But, \(0\leq U_{n}(x)\leq 1\) and

\[E[U_{n}(x)]=J_{b}(x,\,P)\.\]

Since \(J_{b}(x,\,P)\,\rightarrow\,J(x,\,P)\), it suffices to show \(Var[U_{n}(x)]\,\rightarrow\,0\). To this end, suppose \(k\) is the greatest integer less than or equal to \(n/b\). For \(j=1,\ldots,k\), let \(R_{n,b,j}\) be equal to the statistic \(\hat{\theta}_{b}\) evaluated at the data set \(\hat{\theta}_{b}(X_{b(j-1)+1},\,X_{b(j-1)+2},\,\ldots,\,X_{b(j-1)+b})\) and set

\[\bar{U}_{n}(x)=k^{-1}\sum_{j=1}^{k}I\{\tau_{b}[R_{n,b,\,j}-\theta(P)]\leq x\}\.\]

Clearly, \(\bar{U}_{n}(x)\) and \(U_{n}(x)\) have the same expectation. But, since \(\bar{U}_{n}(x)\) is the average of \(k\) i.i.d. variables (each of which is bounded between 0 and 1), it follows that

\[Var[\bar{U}_{n}(x)]\leq\frac{1}{4k}\,\rightarrow\,0\]

as \(n\rightarrow\infty\). Intuitively, \(U_{n}(x)\) should have a smaller variance than \(\bar{U}_{n}(x)\), because \(\bar{U}_{n}(x)\) uses the ordering in the sample in an arbitrary way. Formally, we can write

\[U_{n}(x)=E[\bar{U}_{n}(x)|\bf{X_{n}}]\,\]

where \(\bf{X_{n}}\) is the information containing the original sample but without regard to their order. Applying the inequality \([E(Y)]^{2}\leq E(Y^{2})\) (conditionally) yields

\[E[U_{n}^{2}(x)]=E\{E[\bar{U}_{n}(x)|\bf{X_{n}}]\}^{2}\leq\{E[\bar{U}_{n}^{2}(x )|\bf{X_{n}}]\}=E[\bar{U}_{n}^{2}(x)]\.\]

Thus, \(Var[U_{n}(x)]\rightarrow\,0\) and (i) follows.

To prove (ii), given any subsequence \(\{n_{k}\}\), one can extract a further subsequence \(\{n_{k_{j}}\}\) so that \(L_{n_{k_{j}}}(x)\rightarrow\,J(x,\,P)\) almost surely. Therefore, \(L_{n_{k_{j}}}(x)\rightarrow\,J(x,\,P)\) almost surely for all \(x\) in some countable dense set of the real line. So, \(L_{n_{k_{j}}}\) tends weakly to \(J(x,\,P)\) and this convergence is uniform by Polya's Theorem. Hence, the result (ii) holds.

To prove (iii), if \(J(\cdot,\,P)\) is also assumed strictly increasing at \(c(1-\alpha,\,P)\), then

\[c_{n,b}(1-\alpha)\stackrel{{ P}}{{\rightarrow}}c(1-\alpha,\,P)\]by Problem 11.30, which is a generalization of Lemma 11.2.1. The limiting coverage probability now follows from Slutsky's Theorem. To complete the proof without the strictly increasing assumption, see Problem 18.6.

The assumptions \(b/n\to 0\) and \(b\to\infty\) need not imply \(\tau_{b}/\tau_{n}\to 0\). For example, in the unusual case \(\tau_{n}=\log(n)\), if \(b=n^{\gamma}\) and \(\gamma>0\), the assumption \(\tau_{b}/\tau_{n}\to 0\) is not satisfied. In fact, a slight modification of the method is consistent without assuming \(\tau_{b}/\tau_{n}\to 0\); see Politis et al. (1999), Corollary 2.2.1. In regular cases, \(\tau_{n}=n^{1/2}\), and the assumptions on \(b\) simplify to \(b/n\to 0\) and \(b\to\infty\).

The assumptions on \(b\) are as weak as possible under the weak assumptions of the theorem. However, in some cases, the choice \(b=O(n)\) yields similar results; this occurs in Wu (1990), where the statistic is approximately linear with an asymptotic normal distribution and \(\tau_{n}=n^{1/2}\). This choice will not work in general; see Example 18.7.2.

Assumption 18.7.1 is satisfied in numerous examples, including all previous examples considered by the bootstrap.

#### Comparison with the Bootstrap

The usual bootstrap approximation to \(J_{n}(x,P)\) is \(J_{n}(x,\hat{Q}_{n})\), where \(\hat{Q}_{n}\) is some estimate of \(P\). In many nonparametric i.i.d. situations, \(\hat{Q}_{n}\) is taken to be the empirical distribution of the sample \(X_{1},\ldots,X_{n}\). In Section 18.3, we proved results like (18.49) and (18.50) with \(L_{n,b}(x)\) replaced by \(J_{n}(x,\hat{Q}_{n})\). While the consistency of the bootstrap requires arguments specific to the problem at hand, the consistency of subsampling holds quite generally.

To elaborate a little further, we proved bootstrap limit results in the following manner. For some choice of metric (or pseudo-metric) \(d\) on the space of probability measures, it must be known that \(d(P_{n},P)\to 0\) implies \(J_{n}(P_{n})\) converges weakly to \(J(P)\). That is, Assumption 18.7.1 must be strengthened so that the convergence of \(J_{n}(P)\) to \(J(P)\) is suitably locally uniform in \(P\). In addition, the estimator \(\hat{Q}_{n}\) must then be known to satisfy \(d(\hat{Q}_{n},P)\to 0\) almost surely or in probability under \(P\). In contrast, no such strengthening of Assumption 18.7.1 is required in Theorem 18.7.1. In the known counterexamples to the bootstrap, it is precisely a certain lack of uniformity in convergence which leads to failure of the bootstrap.

In some special cases, it has been realized that a sample size trick can often remedy the inconsistency of the bootstrap. To describe how, focus on the case where \(\hat{Q}_{n}\) is the empirical measure, denoted by \(\hat{P}_{n}\). Rather than approximating \(J_{n}(P)\) by \(J_{n}(\hat{P}_{n})\), the suggestion is to approximate \(J_{n}(P)\) by \(J_{b}(\hat{P}_{n})\) for some \(b\) which usually satisfies \(b/n\to 0\) and \(b\to\infty\). The resulting estimator \(J_{b}(x,\hat{P}_{n})\) is obviously quite similar to our \(L_{n,b}(x)\) given in (2.1). In words, \(J_{b}(x,\hat{P}_{n})\) is the bootstrap approximation defined by the distribution (conditional on the data) of \(\tau_{b}[\hat{\phi}_{b}(X_{1}^{*},\ldots,X_{b}^{*})-\hat{\theta}_{n}]\), where \(X_{1}^{*},\ldots,X_{b}^{*}\) are chosen with replacement from \(X_{1},\ldots,X_{n}\). In contrast, \(L_{n,b}(x)\) is the distribution (conditional on the data) of \(\tau_{b}[\hat{\theta}_{b}(Y_{1}^{*},\ldots,Y_{b}^{*})-\hat{\theta}_{n}]\), where are chosen _without_ replacement from \(X_{1},\ldots,X_{n}\). Clearly, these two approaches must be similar if \(b\) is so small that sampling with and without replacement are essentially the same. Indeed, if one resamples \(b\) numbers (or indices) from the set \(\{1,\ldots,n\}\), then the chance that none of the indices is duplicated is \(\Pi_{i=1}^{b-1}(1-\frac{i}{n})\). This probability tends to \(0\) if \(b^{2}/n\to 0\). (To see why, take logs and do a Taylor expansion analysis.) Hence, the following is true.

**Corollary 18.7.1**: _Under the further assumption that \(b^{2}/n\to 0\), parts (i)-(iii) of Theorem 18.7.1 remain valid if \(L_{n,b}(x)\) is replaced by the bootstrap approximation \(J_{b}(x,\hat{P}_{n})\)._

The bootstrap approximation with smaller resample size, \(J_{b}(\hat{P}_{n})\), is further studied in Bickel, Gotze, and van Zwet (1997). In spite of the Corollary, we point out that \(L_{n,b}\) is more generally valid. Indeed, without the assumption \(b^{2}/n\to 0\), \(J_{b}(x,\hat{P}_{n})\) can be inconsistent. To see why, let \(P\) be any distribution on the real line with a density (with respect to Lebesgue measure). Consider any statistic \(\hat{\theta}_{n}\), \(\tau_{n}\), and \(\theta(P)\) satisfying Assumption 18.7.1. Even the sample mean will work here. Now, modify \(\hat{\theta}_{n}\) to \(\tilde{\theta}_{n}\) so that the statistic \(\tilde{\theta}_{n}(X_{1},\ldots,X_{n})\) completely misbehaves if any pair of the observations \(X_{1},\ldots,X_{n}\) are identical. The bootstrap approximation to the distribution of \(\tilde{\theta}_{n}\) must then misbehave as well unless \(b^{2}/n\to 0\), while the consistency of \(L_{n,b}\) remains intact.

The above example, though artificial, was designed to illustrate a point. We now consider some further examples.

**Example 18.7.1**: **(U-statistics of Degree 2)** Let \(X_{1},\ldots,X_{n}\) be i.i.d. on the line with c.d.f. \(F\). Denote by \(\hat{F}_{n}\) the empirical distribution of the data. Let

\[\theta(F)=\int\int\omega(x,y)dF(x)dF(y)\]

and assume \(\omega(x,y)=\omega(y,x)\). Assume

\[\int\omega^{2}(x,y)dF(x)dF(y)<\infty\.\]

Set \(\tau_{n}=n^{1/2}\) and

\[\hat{\theta}_{n}=\sum_{i<j}\omega(X_{i},X_{j})/{n\choose 2}\.\]

Then, by Theorem 12.3.2, \(J_{n}(F)\) converges weakly to \(J(F)\), the normal distribution with mean \(0\) and variance given by

\[v^{2}(F)=4\left\{\int[\omega(x,y)dF(y)]^{2}dF(x)-\theta^{2}(F)\right\}\,.\]

Hence, Assumption 18.7.1 holds. However, in order for the bootstrap to succeed, the additional condition \(\int\omega^{2}(x,x)dF(x)<\infty\) is required. Bickel and Freedman(1981) give a counterexample to show the inconsistency of the bootstrap without this additional condition.

Interestingly, the bootstrap may fail even if \(\int\omega^{2}(x,x)dF(x)<\infty\), stemming from the possibility that \(v^{2}(F)=0\). (Otherwise, Bickel and Freedman's argument justifies the bootstrap.) As an example, let \(w(x,y)=xy\). In this case,

\[\theta(\hat{F}_{n})=\bar{X}_{n}^{2}-S_{n}^{2}/n\,\]

where \(S_{n}^{2}\) is the usual unbiased sample variance. If \(\theta(F)=0\), then \(v(F)=0\). Then, \(n[\theta(\hat{F}_{n})-\theta(F)]\) converges weakly to \(\sigma^{2}(F)(Z^{2}-1)\), where \(Z\) denotes a standard normal random variable and \(\sigma^{2}(F)\) denotes the variance of \(F\). However, it is easy to see that the bootstrap approximation to the distribution of \(n[\theta(\hat{F}_{n})-\theta(F)]\) has a representation \(\sigma^{2}(F)Z^{2}+2Z\sigma(F)n^{1/2}\bar{X}_{n}\). Thus, failure of the bootstrap follows.

In the context of U-statistics, the possibility of using a reduced sample size in the resampling has been considered in Bretagnolle (1983); an alternative correction is given by Arcones (1991).

**Example 18.7.2** (Extreme Order Statistic): The following counterexample is taken from Bickel and Freedman (1981). If \(X_{1}\),..., \(X_{n}\) are i.i.d. according to a uniform distribution on \((0,\theta)\), let \(X_{(n)}\) be the maximum order statistic. Then, \(n[X_{(n)}-\theta]\) has a limit distribution given by the distribution of \(-\theta X\), where \(X\) is exponential with mean one. Hence, Assumption 18.7.1 is satisfied here. However, the usual bootstrap fails. To see why, let \(X_{1}^{*}\),..., \(X_{n}^{*}\) be \(n\) observations sampled from the data with replacement, and let \(X_{(n)}^{*}\) be the maximum of the bootstrap sample. The bootstrap approximation to the distribution of \(n[X_{(n)}-\theta]\) is the distribution of \(n[X_{(n)}^{*}-X_{(n)}]\), conditional on \(X_{1}\),..., \(X_{n}\). But, the probability mass at \(0\) for this bootstrap distribution is the probability that \(X_{(n)}^{*}=X_{(n)}\), which occurs with probability

\[1-(1-\frac{1}{n})^{n}\to 1-\exp(1)\.\]

However, the true limiting distribution is continuous. Note in Theorem 18.7.1 that the conditions on \(b\) (with \(\tau_{n}=n\)) reduce to \(b/n\to 0\) and \(b\to\infty\). In this example, at least, it is clear that we cannot assume \(b/n\to c\), where \(c>0\). Indeed, \(L_{n,b}(x)\) places mass \(b/n\) at \(0\). Thus, while it is sometimes true that, under further conditions such as Wu (1990) assumes, we can take \(b\) to be of the same order as \(n\), this example makes it clear that we cannot in general weaken our assumptions on \(b\) without imposing further structure.

**Example 18.7.3** (Superefficient Estimator): Assume \(X_{1}\),..., \(X_{n}\) are i.i.d. according the normal distribution with mean \(\theta(P)\) and variance one. Fix \(c>0\). Let \(\hat{\theta}_{n}=c\bar{X}_{n}\) if \(|\bar{X}_{n}|\leq n^{-1/4}\) and \(\hat{\theta}_{n}=\bar{X}_{n}\) otherwise. The resulting estimator is known as Hodges' superefficient estimator; see Lehmann and Casella (1998), p. 440 and Problem 14.70. It is easily checked that \(n^{1/2}(\hat{\theta}_{n}-\theta(P))\) has a limit distribution for every \(\theta\), so the conditions for Theorem 18.7.1 remain applicable. However, Beran(1984) showed that the distribution of \(n^{1/2}(\hat{\theta}_{n}-\theta(P))\) cannot be bootstrapped, even if one is willing to apply a parametric bootstrap! \(\blacksquare\)

We have claimed that subsampling is superior to the bootstrap in a first-order asymptotic sense, since it is more generally valid. However, in many typical situations, the bootstrap is far superior and has some compelling second-order asymptotic properties. Some of these were studied in Section 18.4; also see Hall (1992). In nice situations, such as when the statistic or root is a smooth function of sample means, a bootstrap approach is often very satisfactory. In other situations, especially those where it is not known that the bootstrap works even in a first-order asymptotic sense, subsampling is preferable. Still, in other situations (such as the mean in the infinite variance case), the bootstrap may work, but only with a reduced sample size. The issue becomes whether to sample with or without replacement (as well as the choice of resample size). Although this question is not yet answered unequivocally, some preliminary evidence in Bickel et al. (1997) suggests that the bootstrap approximation \(J_{b}(x,\hat{P}_{n})\) might be more accurate; more details on the issue of higher order accuracy of the subsampling approximation \(L_{n,b}(x)\) are given in Chapter 10 of Politis et al. (1999).

Because \({n\choose b}\) can be large, \(L_{n,b}\) may be difficult to compute. Instead, an approximation may be employed. For example, let \(I_{1},\ldots I_{B}\) be chosen randomly with or without replacement from \(\{1,2,\ldots,N_{n}\}\). Then, \(L_{n,b}(x)\) may be approximated by

\[\hat{L}_{n,b}(x)=\frac{1}{B}\sum_{i=1}^{B}I\{\tau_{b}(\hat{\theta}_{n,b,I_{i}} -\hat{\theta}_{n})\leq x\}. \tag{18.52}\]

**Corollary 18.7.2**: _Under the assumptions of Theorem 18.7.1 and the assumption \(B\to\infty\) as \(n\to\infty\), the results of Theorem 18.7.1 are valid if \(L_{n,b}(x)\) is replaced by \(\hat{L}_{n,b}(x)\)._

Proof. If the \(I_{i}\) are sampled with replacement, \(\sup_{x}|\hat{L}_{n,b}(x)-L_{n,b}(x)|\to 0\) in probability by the Dvoretzky, Kiefer, Wolfowitz inequality. This result is also true in the case the \(I_{i}\) are sampled without replacement; apply Proposition 4.1 of Romano (1989b). \(\blacksquare\)

An alternative approach, which also requires fewer computations, is the following. Rather than employing all \({n\choose b}\) subsamples of size \(b\) from \(X_{1},\ldots,X_{n}\), just use the \(n-b+1\) subsamples of size \(b\) of the form \(\{X_{i},X_{i+1},\ldots,X_{i+b-1}\}\). Notice that the ordering of the data is fixed and retained in the subsamples. Indeed, this is the approach that is applied for time series data; see Chapter 3 of Politis et al. (1999), where consistency results in data-dependent situations are given. Even when the i.i.d. assumption seems reasonable, this approach may be desirable to ensure robustness against possible serial correlation. Most inferential procedures based on i.i.d. models are simply not valid (i.e., not even first-order accurate) if the independence assumption is violated, so it seems worthwhile to account for possible dependencies in the data if we do not sacrifice too much in efficiency.

#### Hypothesis Testing

In this section, we consider the use of subsampling for the construction of hypothesis tests. As before, \(X_{1}\),..., \(X_{n}\) is a sample of \(n\) independent and identically distributed observations taking values in a sample space \(S\). The common unknown distribution generating the data is denoted by \(P\). This unknown law \(P\) is assumed to belong to a certain class of laws \(\mathbf{P}\). The null hypothesis \(H\) asserts \(P\in\mathbf{P_{0}}\), and the alternative hypothesis \(K\) is \(P\in\mathbf{P_{1}}\), where \(\mathbf{P_{i}}\subset\mathbf{P}\) and \(\mathbf{P_{0}}\bigcup\mathbf{P_{1}}=\mathbf{P}\).

The goal is to construct an asymptotically valid test based on a given test statistic,

\[T_{n}=\tau_{n}t_{n}(X_{1},\ldots,X_{n})\,\]

where, as before, \(\tau_{n}\) is a fixed nonrandom normalizing sequence. Let

\[G_{n}(x,P)=P\{\tau_{n}t_{n}(X_{1},\ldots,X_{n})\leq x\}\.\]

We will be assuming that \(G_{n}(\cdot,P)\) converges in distribution, at least for \(P\in\mathbf{P_{0}}\). Of course, this would imply (as long as \(\tau_{n}\to\infty\)) that \(t_{n}(X_{1},\ldots,X_{n})\to 0\) in probability for \(P\in\mathbf{P_{0}}\). Naturally, \(t_{n}\) should somehow be designed to distinguish between the competing hypotheses. The theorem we will present will assume \(t_{n}\) is constructed to satisfy the following: \(t_{n}(X_{1},\ldots,X_{n})\to t(P)\) in probability, where \(t(P)\) is a constant which satisfies \(t(P)=0\) if \(P\in\mathbf{P_{0}}\) and \(t(P)>0\) if \(P\in\mathbf{P_{1}}\). This assumption easily holds in typical examples.

To describe the test construction, as in Section 18.7.1, let \(Y_{1},\ldots,Y_{N_{n}}\) be equal to the \(N_{n}={n\choose b}\) subsets of \(\{X_{1},\ldots,X_{n}\}\), ordered in any fashion. Let \(t_{n,b,i}\) be equal to the statistic \(t_{b}\) evaluated at the data set \(Y_{i}\). The sampling distribution of \(T_{n}\) is then approximated by

\[\hat{G}_{n,b}(x)=N_{n}^{-1}\sum_{i=1}^{N_{n}}I\{\tau_{b}t_{n,b,i}\leq x\}. \tag{18.53}\]

Using this estimated sampling distribution, the critical value for the test is obtained as the \(1-\alpha\) quantile of \(\hat{G}_{n,b}(\cdot)\); specifically, define

\[g_{n,b}(1-\alpha)=\inf\{x:\hat{G}_{n,b}(x)\geq 1-\alpha\}. \tag{18.54}\]

Finally, the nominal level \(\alpha\) test rejects \(H\) if and only if \(T_{n}>g_{n,b}(1-\alpha)\).

The following theorem gives the asymptotic behavior of this procedure, showing the test is pointwise consistent in level and pointwise consistent in power. In addition, an expression for the limiting power of the test is obtained under a sequence of alternatives contiguous to a distribution in the null hypothesis.

**Theorem 18.7.2**: _Assume \(b/n\to 0\) and \(b\to\infty\) as \(n\to\infty\)._

1. _Assume, for_ \(P\in\mathbf{P_{0}}\)_,_ \(G_{n}(P)\) _converges weakly to a continuous limit law_ \(G(P)\)_, whose corresponding cumulative distribution function is_ \(G(\cdot,P)\) _and whose\(1-\alpha\) quantile is \(g(1-\alpha,P)\). If \(G(\cdot,P)\) is continuous at \(g(1-\alpha,P)\) and \(P\in\mathbf{P_{0}}\), then_ \[g_{n,b}(1-\alpha)\to g(1-\alpha,P)\text{ in probability}\] _and_ \[P\{T_{n}>g_{n,b}(1-\alpha)\}\to\alpha\text{ as }n\to\infty.\]
2. _Assume the test statistic is constructed so that_ \(t_{n}(X_{1},\ldots,X_{n})\to t(P)\) _in probability, where_ \(t(P)\) _is a constant which satisfies_ \(t(P)=0\) _if_ \(P\in\mathbf{P_{0}}\) _and_ \(t(P)>0\) _if_ \(P\in\mathbf{P_{1}}\)_. Assume_ \(\liminf_{n}(\tau_{n}/\tau_{b})>1\)_. Then, if_ \(P\in\mathbf{P_{1}}\)_, the rejection probability satisfies_ \[P\{T_{n}>g_{n,b}(1-\alpha)\}\to 1\text{ as }n\to\infty.\]
3. _Suppose_ \(P_{n}\) _is a sequence of alternatives such that, for some_ \(P_{0}\in\mathbf{P_{0}}\)_,_ \(\{P_{n}^{n}\}\) _is contiguous to_ \(\{P_{0}^{n}\}\)_. Then,_ \[g_{n,b}(1-\alpha)\to g(1-\alpha,P_{0})\text{ in }P_{n}^{n}\text{-probability.}\] _Hence, if_ \(T_{n}\) _converges in distribution to_ \(T\) _under_ \(P_{n}\) _and_ \(G(\cdot,P_{0})\) _is continuous at_ \(g(1-\alpha,P_{0})\)_, then_ \[P_{n}^{n}\{T_{n}>g_{n,b}(1-\alpha)\}\to Prob\{T>g(1-\alpha,P_{0})\}.\]

The proof is similar to that of Theorem 18.7.1 (Problem 18.45).

**Example 18.7.4**: Consider the special case of testing a real-valued parameter. Specifically, suppose \(\theta(\cdot)\) is a real-valued function from \(\mathbf{P}\) to the real line. The null hypothesis is specified by \(\mathbf{P_{0}}=\{P:\ \theta(P)=\theta_{0}\}\). Assume the alternative is one sided and is specified by \(\{P:\ \theta(P)>\theta_{0}\}\). Suppose we simply take

\[t_{n}(X_{1},\ldots,X_{n})=\hat{\theta}_{n}(X_{1},\ldots,X_{n})-\theta_{0}\.\]

If \(\hat{\theta}_{n}\) is a consistent estimator of \(\theta(P)\), then the hypothesis on \(t_{n}\) in part (ii) of the theorem is satisfied (just take the absolute value of \(t_{n}\) for a two-sided alternative). Thus, the hypothesis on \(t_{n}\) in part (ii) of the theorem boils down to verifying a consistency property and is rather weak, though this assumption can in fact be weakened further. The convergence hypothesis of part (i) is satisfied by typical test statistics; in regular situations, \(\tau_{n}=n^{1/2}\).

The interpretation of part (iii) of the theorem is the following. Suppose, instead of using the subsampling construction, one could use the test that rejects when \(T_{n}>g_{n}(1-\alpha,P)\), where \(g_{n}(1-\alpha,P)\) is the exact \(1-\alpha\) quantile of the true sampling distribution \(G_{n}(\cdot,P)\). Of course, this test is not available in general because \(P\) is unknown and so is \(g_{n}(1-\alpha,P)\). Then, the asymptotic power of the subsampling test against a sequence of contiguous alternatives \(\{P_{n}\}\) to \(P\) with \(P\) in \(\mathbf{P_{0}}\) is the same as the asymptotic power of this fictitious test against the same sequence of alternatives. Hence, to the order considered, there is no loss in efficiency in terms of power.

**Example 18.7.5** (**Moment Inequalities Using Subsampling**): _Reconsider the moment inequality testing problem in Example 18.5.1, where the problem is to test all components of a mean vector are less than or equal to zero. Let \(\tau_{n}=\sqrt{n}\) and_

\[t_{n}(X_{1},\ldots,X_{n})=\max_{1\leq i\leq k}\bar{X}_{n,i}\.\]

_The subsampling distribution is then defined as in (18.53). Theorem 18.7.2 applies and the subsampling test controls Type 1 error asymptotically. (In fact, it controls Type 1 error uniformly over a large collection of underlying distributions; see Romano and Shaikh (2008, 2012).) Looking at the local asymptotic power properties of this test, suppose \(\theta_{0}=(\theta_{0,1},\ldots,\theta_{0,k})^{\top}\) lies on the boundary of the parameter space, so that \(\theta_{0,i}\leq 0\) for all \(i\) and equal to \(0\) for some \(i\). Also, let_

\[I=\{i\,:\,\theta_{i}(P)=0\}\]

_and \(h=(h_{1},\ldots,h_{k})^{\top}\). To keep it simple, suppose \(P=P_{\theta}\) is multivariate normal with mean \(\theta\) and covariance matrix \(\Sigma\). Then, under \(\theta_{0}+hn^{-1/2}\),_

\[T_{n}=\sqrt{n}t_{n}\stackrel{{ d}}{{\rightarrow}}\max_{i\in I}(Z _{i}+h_{i})\, \tag{18.55}\]

_where \((Z_{1},\ldots,Z_{k})^{\top}\) is multivariate normal with mean \(0\) and covariance matrix \(\Sigma\). Under such a sequence, the subsampling distribution \(\hat{G}_{n,b}(x)\) satisfies, for any \(x\),_

\[\hat{G}_{n,b}(x)\stackrel{{ P}}{{\rightarrow}}P\{\max_{i\in I}Z_{i }\leq x\}. \tag{18.56}\]

_Therefore, if \(d_{I,1-\alpha}\) denotes the \(1-\alpha\) quantile of \(\max_{i\in I}Z_{i}\), then the subsampling quantile \(g_{n,b}(1-\alpha)\) satisfies_

\[g_{n,b}(1-\alpha)\stackrel{{ P}}{{\rightarrow}}d_{I,1-\alpha}\.\]

_Hence, the limiting power against \(\theta_{0}+hn^{-1/2}\) of the subsampling test can be expressed as (Problem 18.48)_

\[P\{\max_{i\in I}(Z_{i}+h_{i})>d_{I,1-\alpha}\}. \tag{18.57}\]

_In particular, this limiting power is greater than it would be if \(d_{I,1-\alpha}\) were replaced by \(d_{I_{0},1-\alpha}\), where \(I_{0}=\{1,\ldots,k\}\). In other words, subsampling implicitly is applying a moment selection procedure. Compare with the bootstrap in Problems 14.68 and 18.30._

### Problems

#### Section 18.2

**Problem 18.1**: Assume \(X_{1},\ldots,X_{n}\) are i.i.d. according to a location-scale model with distribution of the form \(F[(x-\theta)/\sigma]\), where \(F\) is known, \(\theta\) is a location parameter, and \(\sigma\) is a scale parameter. Suppose \(\hat{\theta}_{n}\) is a location and scale equivariant estimator and \(\hat{\sigma}_{n}\) is a location invariant, scale equivariant estimator. Then, show that the roots \([\hat{\theta}_{n}-\theta]/\hat{\sigma}_{n}\) and \(\hat{\sigma}_{n}/\sigma\) are pivots.

**Problem 18.2**: Let \(X=(X_{1},\ldots,X_{n})^{\top}\) and consider the linear model

\[X_{i}=\sum_{j=1}^{s}a_{i,j}\beta_{j}+\sigma\epsilon_{i}\,\]

where the \(\epsilon_{i}\) are i.i.d. \(F\), where \(F\) has mean 0 and variance 1. Here, the \(a_{i,j}\) are known, \(\beta=(\beta_{1},\ldots,\beta_{s})^{\top}\) and \(\sigma\) are unknown. Let \(A\) be the \(n\times s\) matrix with \((i,\,j)\) entry \(a_{i,j}\) and assume \(A\) has rank \(s\). As in Section 13.2.3, let \(\hat{\beta}_{n}=(A^{\top}A)^{-1}A^{\top}X\) be the least squares estimate of \(\beta\). Consider the test statistic

\[T_{n}=\frac{(n-s)(\hat{\beta}_{n}-\beta)(A^{\top}A)(\hat{\beta}_{n}-\beta)}{sS _{n}^{2}}\,\]

where \(S_{n}^{2}=(X-A\hat{\beta}_{n})^{\top}(X-A\hat{\beta}_{n})/(n-s)\). Is \(T_{n}\) a pivot when \(F\) is known?

#### Section 18.3

**Problem 18.3**: Suppose the convergence (18.4) only holds in probability and that \(J(\cdot,P)\) is continuous and strictly increasing at \(J^{-1}(1-\alpha,P)\). Show that (18.5) holds in probability. Then, show that (18.6) still holds.

**Problem 18.4**: In Theorem 18.3.1, one cannot deduce the uniform convergence result (18.4) without the assumption that the limit law \(J(P)\) is continuous. Show that, without the continuity assumption for \(J(P)\),

\[\rho_{L}(J_{n}(\hat{P}_{n}),J_{n}(P))\to 0\]

with probability one, where \(\rho_{L}\) is the Levy metric defined in Definition 11.2.3.

**Problem 18.5**: In Theorem 18.3.3 (i), show that the assumption that \(\theta(F_{n})\to\theta(F)\) actually follows from the other assumptions.

**Problem 18.6**: Under the assumptions of Theorem 18.3.1 but without the assumption that \(J(\cdot,\,P)\) is strictly increasing, show that the conclusion (18.6) still holds. [_Hint:_ See Problems 11.26 and 11.31.]

**Problem 18.7**: Reprove Theorem 18.3.3(ii) under the assumption \(E(|X_{i}|^{3})<\infty\) by using the Berry-Esseen Theorem.

**Problem 18.8**: Prove the following extension of Theorem 18.3.3 holds. Let \(\mathbf{D_{F}}\) be the set of sequences \(\{F_{n}\}\) such that \(F_{n}\) converges weakly to a distribution \(G\) and \(\sigma^{2}(F_{n})\rightarrow\sigma^{2}(G)=\sigma^{2}(F)\). Then, Theorem (18.3.3) holds with \(\mathbf{C}_{F}\) replaced by \(\mathbf{D_{F}}\). (Actually, one really only needs to define \(\mathbf{D_{F}}\) so that and sequence \(\{F_{n}\}\) is tight and any weakly convergent subsequence of \(\{F_{n}\}\) has the above property.) Thus, the possible choices for the resampling distribution are quite large in the sense that the bootstrap approximation \(J_{n}(\hat{G}_{n})\) can be consistent even if \(\hat{G}_{n}\) is not at all close to \(F\). For example, the choice where \(\hat{G}_{n}\) is normal with mean \(\bar{X}_{n}\) and variance equal to a consistent estimate of the sample variance results in consistency. Therefore, the normal approximation can in fact be viewed as a bootstrap procedure with a perverse choice of resampling distribution. Show the bootstrap can be inconsistent if \(\sigma^{2}(G)\neq\sigma^{2}(F)\).

**Problem 18.9**: Assume \(X_{1},\,\ldots,\,X_{n}\) are i.i.d. with c.d.f. \(F\), mean \(\mu(F)\) and variance \(\sigma^{2}(F)<\infty\). Let \(\bar{X}_{n}=\mu(\hat{F}_{n})\), where \(\hat{F}_{n}\) is the empirical c.d.f. Conditional on \(\hat{F}_{n}\), let \(X_{1}^{*},\,\ldots,\,X_{n}^{*}\) be i.i.d. according to \(\hat{F}_{n}\), with sample mean \(\bar{X}_{n}^{*}\). Find the (unconditional) joint limiting distribution of

\[n^{1/2}[\bar{X}_{n}^{*}-\bar{X}_{n},\,\bar{X}_{n}-\mu(F)]\;.\]

**Problem 18.10**: Under the setup of Problem 18.9, the problem now is to construct a confidence interval for \(\mu(F)\), but now it is known and assumed that \(\mu(F)\geq 0\). Inference is based on the estimator \(\hat{\mu}_{n}=\max(\bar{X}_{n},0)\). Consider the root \(R_{n}(X_{1},\,\ldots,\,X_{n},\,\mu(F))=n^{1/2}[\hat{\mu}_{n}-\mu(F)]\), with distribution \(J_{n}(F)\). Investigate bootstrap consistency. Separate out cases by \(\mu(F)>0\) and \(\mu(F)=0\). _Hint: In the case \(\mu(F)=0\), first find the limiting behavior of \(J_{n}(F)\). For any \(c>0\), if \(n^{1/2}\bar{X}_{n}<-c\), show that the bootstrap distribution is dominated in the limit by that of \(\sigma(F)\max(Z-c,0)\), where \(Z\sim N(0,\,1)\). Use the almost sure representation theorem to argue that the bootstrap fails, at least along a subsequence._

**Problem 18.11**: In the case that \(\theta(P)\) is real-valued, Efron initially proposed the following construction, called the bootstrap _percentile_ method. Let \(\hat{\theta}_{n}\) be an estimator of \(\theta(P)\), and let \(\bar{J}_{n}(P)\) be the distribution of \(\hat{\theta}_{n}\) under \(P\). Then, Efron's two-sided percentile interval of nominal level \(1-\alpha\) takes the form

\[[\bar{J}_{n}^{-1}(\frac{\alpha}{2},\,\hat{P}_{n}),\,\bar{J}_{n}^{-1}(1-\, \frac{\alpha}{2},\,\hat{P}_{n})]\;. \tag{18.58}\]

Also, consider the root \(R_{n}(X^{n},\,\theta(P))=n^{1/2}(\hat{\theta}_{n}-\theta(P))\), with distribution \(J_{n}(P)\). Write (18.58) as a function of \(\hat{\theta}_{n}\) and the quantiles of \(J_{n}(\hat{P}_{n})\), assuming \(\theta(\hat{P}_{n})=\hat{\theta}_{n}\).

Suppose Theorem 18.3.1 holds for the root \(R_{n}\), so that \(J_{n}(P)\) converges weakly to \(J(P)\). What must be assumed about \(J(P)\) so that \(P\{\theta(P)\in I_{n}\}\to 1-\alpha\)?

**Problem 18.12**: Let \(\hat{\theta}_{n}\) be an estimate of a real-valued parameter \(\theta(P)\). Suppose there exists an increasing transformation \(g\) such that

\[g(\hat{\theta}_{n})-g(\theta(P))\]

is a pivot, so that its distribution does not depend on \(P\). Also, assume this distribution is continuous, strictly increasing, and symmetric about zero.

(i) Show that Efron's percentile interval (18.58), which may be constructed without knowledge of \(g\), has exact coverage \(1-\alpha\).

(ii) Show that the percentile interval is transformation equivariant. That is, if \(\phi=m(\theta)\) is a monotone transformation of \(\theta\), then the percentile interval for \(\phi\) is the percentile interval for \(\theta\) transformed by \(m\) when \(\hat{\phi}_{n}\) is taken to be \(m(\hat{\theta}_{n})\). This holds true for the theoretical percentile interval as well as its approximation due to simulation.

(iii) If the parameter \(\theta\) only takes values in an interval \(I\) and \(\hat{\theta}_{n}\) does as well, then the percentile interval is range-preserving in the sense that the interval is always a subset of \(I\).

**Problem 18.13**: Suppose \(\hat{\theta}_{n}\) is an estimate of some real-valued parameter \(\theta(P)\). Let \(H_{n}(x,\theta)\) denote the c.d.f. of \(\hat{\theta}_{n}\) under \(\theta\), with inverse \(H_{n}^{-1}(1-\alpha,\theta)\). The percentile interval lower confidence bound of level \(1-\alpha\) is then \(H_{n}^{-1}(\alpha,\hat{\theta}_{n})\). Suppose that, for some increasing transformation \(g\), and constants \(z_{0}\) (called the _bias correction_) and \(a\) (called the _acceleration constant_),

\[P\{\frac{g(\hat{\theta}_{n})-g(\theta)}{1+ag(\theta)}+z_{0}\leq x\}=\Phi(x)\, \tag{18.59}\]

where \(\Phi\) is the standard normal c.d.f.

(i) Letting \(\hat{\phi}_{n}=g(\hat{\theta}_{n})\), show that \(\hat{\theta}_{n,L}\) given by

\[\hat{\theta}_{n,L}=g^{-1}\left[\hat{\phi}_{n}+(z_{\alpha}+z)(1+a\hat{\phi}_{n} )/[1-a(z_{\alpha}+z_{0})]\right\}\]

is an exact \(1-\alpha\) lower confidence bound for \(\theta\).

(ii) Because \(\hat{\theta}_{n,L}\) requires knowledge of \(g\), let

\[\hat{\theta}_{n,BC_{\alpha}}=H_{n}^{-1}(\beta,\hat{\theta}_{n})\,\]

where

\[\beta=\Phi(z+(z_{\alpha}+z_{0})/[1-a(z_{\alpha}+z_{0})]\.\]Show that \(\hat{\theta}_{n,BC_{a}}=\hat{\theta}_{n,L}\). [The lower bound \(\hat{\theta}_{n,BC_{a}}\) is called the \(BC_{a}\) lower bound and Efron shows one may take \(z=\Phi^{-1}(H_{n}(\hat{\theta}_{n},\hat{\theta}_{n}))\) and gives methods to estimate \(a\); see Efron and Tibshirani (1993, Chapter 14).]

**Problem 18.14**: Assume the setup of Problem 18.13 and condition (18.59). Let \(\theta_{0}\) be any value of \(\theta\) and let \(\theta_{1}=H_{n}^{-1}(1-\alpha,\theta_{0})\). Let

\[\hat{\theta}_{n,AP}=H_{n}^{-1}(\beta^{\prime},\hat{\theta}_{n})\,\]

where

\[\beta^{\prime}=H_{n}(\theta_{0},\theta_{1})\.\]

Show that \(\hat{\theta}_{n,AP}\) is an exact level \(1-\alpha\) lower confidence bound for \(\theta\). [This is called the _automatic percentile_ lower bound of DiCiccio and Romano (1989), and may be computed without knowledge of \(g\), \(a\) or \(z\). Its exactness holds under assumptions even weaker than (18.59).]

**Problem 18.15**: Let \(X_{1}\),..., \(X_{n_{X}}\) be i.i.d. with distribution \(F_{X}\), and let \(Y_{1}\),..., \(Y_{n_{Y}}\) be i.i.d. with distribution \(F_{Y}\). The two samples are independent. Let \(\mu(F)\) denote the mean of a distribution \(F\), and let \(\sigma^{2}(F)\) denote the variance of \(F\). Assume \(\sigma^{2}(F_{X})\) and \(\sigma^{2}(F_{Y})\) are finite. Suppose we are interested in \(\theta=\theta(F_{X},\,F_{Y})=\mu(F_{X})-\mu(F_{Y})\). Construct a bootstrap confidence interval for \(\theta\) of nominal level \(1-\alpha\), and prove that it asymptotically has the correct coverage probability, assuming \(\min(n_{X},\,n_{Y})\to\infty\).

**Problem 18.16**: Let \(X_{1}\),..., \(X_{n}\) be i.i.d. Bernoulli trials with success probability \(\theta\). (i). As explicitly as possible, find a uniformly most accurate upper confidence bound for \(\theta\) of nominal level \(1-\alpha\). State the bound explicitly in the case \(X_{i}=0\) for every \(i\).

(ii). Describe a bootstrap procedure to obtain an upper confidence bound for \(\theta\) of nominal level \(1-\alpha\). What does it reduce to for the previous data set?

(iii). Let \(\hat{B}_{1-\alpha}\) denote your upper bootstrap confidence bound for \(\theta\). Then, \(P_{\theta}(\theta\leq\hat{B}_{1-\alpha})\to 1-\alpha\) as \(n\to\infty\). Prove the following.

\[\sup_{\theta}|P_{\theta}(\theta\leq\hat{B}_{1-\alpha})-(1-\alpha)|\]

does not tend to \(0\) as \(n\to\infty\).

**Problem 18.17**: Let \(X_{1}\),..., \(X_{n}\) be i.i.d. with c.d.f. \(F\), mean \(\mu(F)\) and finite variance \(\sigma^{2}(F)\). Consider the root \(R_{n}=n^{1/2}(\tilde{X}_{n}^{2}-\mu^{2}(F))\) and the bootstrap approximation to its distribution \(J_{n}(\hat{F}_{n})\), where \(\hat{F}_{n}\) is the empirical c.d.f. Determine the asymptotic behavior of \(J_{n}(\hat{F}_{n})\). _Hint:_ Distinguish the cases \(\mu(F)=0\) and \(\mu(F)\neq 0\).

**Problem 18.18**: Prove the remaining details for Theorem 18.3.6. Furthermore, without assuming the differential of \(f\) is continuous, one can replace the convergence with probability one results by convergence in probability. (More general results are available in van der Vaart and Wellner (1996).)

**Problem 18.19**: Let \(\epsilon_{1}\), \(\epsilon_{2}\), \(\ldots\) be i.i.d. \(N(0,1)\). Let \(X_{i}=\mu+\epsilon_{i}+\beta\epsilon_{i+1}\) with \(\beta\) a fixed nonzero constant. The \(X_{i}\) form a moving-average process studied in Section 13.2.1.

(i) Examine the behavior of the nonparametric bootstrap method for estimating the mean using the root \(n^{1/2}(\bar{X}_{n}-\mu)\) and resampling from the empirical distribution. Show that the coverage probability need not tend to the nominal level under such a moving-average process.

(ii) Suppose \(n=bk\) for integers \(b\) and \(k\). Consider the following _moving blocks bootstrap_ resampling scheme. Let \(L_{i,b}=(X_{i},\,X_{i+1},\,\ldots,\,X_{i+b-1})\) be the block of \(b\) observations beginning at "time" \(i\). Let \(X_{1}^{*},\ldots,\,X_{n}^{*}\) be obtained by randomly choosing with replacement \(k\) of the \(n-b+1\) blocks \(L_{i,b}\); that is, \(X_{1}^{*},\,\ldots,\,X_{b}^{*}\) are the observations in the first sampled block, \(X_{b+1}^{*},\,\ldots,\,X_{2b}^{*}\) are the observations from the second sampled block, etc. Then, the distribution of \(n^{1/2}[\bar{X}_{n}-\mu]\) is approximated by the _moving blocks bootstrap_ distribution given by the distribution of \(n^{1/2}[\bar{X}_{n}^{*}-\bar{X}_{n}]\), where \(\bar{X}_{n}^{*}=\sum_{i=1}^{n}X_{i}^{*}/n\). If \(b\) is fixed, determine the mean and variance of this distribution as \(n\to\infty\). Now let \(b\to\infty\) as \(n\to\infty\). At what rate should \(b\to\infty\) so that the mean and variance of the moving blocks distribution tends to the same limiting values as the true mean and variance, at least in probability? [The moving blocks bootstrap was independently discovered by Kunsch (1989) and Liu and Singh (1992). The stationary bootstrap of Politis and Romano (1994a) and other methods designed for dependent data are studied in Lahiri (2003).]

### _Section 18.4_

**Problem 18.20**: Under the assumptions of Theorem 18.4.2, show that, for any \(\epsilon>0\), the expansion (18.20) holds uniformly in \(\alpha\in[\epsilon,\,1-\epsilon]\).

**Problem 18.21**: Under the assumptions of Theorem 18.4.1, show that, for any \(\epsilon>0\), the expansion (18.21) holds uniformly in \(\alpha\in[\epsilon,\,1-\epsilon]\).

**Problem 18.22**: Suppose \(Y_{n}\) is a sequence of random variables satisfying

\[P\{Y_{n}\leq t\}=g_{0}(t)+g_{1}(t)n^{-1/2}+O(n^{-1})\,\]

uniformly in \(t\), where \(g_{0}\) and \(g_{1}\) have uniformly bounded derivatives. If \(T_{n}=O_{P}(n^{-1})\), then show, for any fixed (nonrandom) sequence \(t_{n}\),

\[P\{Y_{n}\leq t_{n}+T_{n}\}=g_{0}(t_{n})+g_{1}(t_{n})n^{-1/2}+O(n^{-1})\.\]

**Problem 18.23**: Assuming the expansions in the section hold, show that the two-sided bootstrap interval (18.29) has coverage error of order \(n^{-1}\).

**Problem 18.24**: Assuming the expansions in the section hold, show that the two-sided bootstrap-\(t\) interval (18.35) has coverage error of order \(n^{-1}\).

**Problem 18.25**: Verify the expansion (18.36) and argue that the resulting interval \(I_{n}(1-\hat{\alpha}_{n})\) has coverage error \(O(n^{-1})\).

**Problem 18.26**: In the nonparametric mean setting, determine the one- and two-sided coverage errors of Efron's percentile method described in (18.58).

**Problem 18.27**: Assume \(F\) has infinitely many moments and is absolutely continuous. Under the notation of this section, argue that \(n^{1/2}[J_{n}(t,\hat{F}_{n})-J_{n}(t,F)]\) has an asymptotically normal limiting distribution, as does \(n[K_{n}(t,\hat{F}_{n})-K_{n}(t,F)]\).

**Problem 18.28**: (i) In a normal location model \(N(\mu,\sigma^{2})\), consider the root \(R_{n}=n^{1/2}(\bar{X}_{n}-\mu)\), which is not a pivot. Show that bootstrap calibration, by parametric resampling, produces an exact interval.

(ii) Next, consider the root \(n^{1/2}(S_{n}^{2}-\sigma^{2})\), where \(S_{n}^{2}\) is the usual unbiased estimate of variance. Show that bootstrap calibration, by parametric resampling, produces an exact interval.

**Problem 18.29**: (i) Show the bootstrap interval (18.3) can be written as

\[\{\theta\in\Theta:\ J_{n}(R_{n}(X^{n},\theta),\,\hat{P}_{n})\leq 1-\alpha\} \tag{18.60}\]

if, for the purposes of this problem, \(J_{n}(x,\,P)\) is defined as the left continuous c.d.f.

\[J_{n}(x,\,P)=P\{R_{n}(X^{n},\theta(P))<x\}\]

and \(J_{n}^{-1}(1-\alpha,\,P)\) is now defined as

\[J_{n}^{-1}(1-\alpha,\,P)=\sup\{x:\ J_{n}(x,\,P)\leq 1-\alpha\}\.\]

[_Hint:_ If a random variable \(Y\) has left continuous c.d.f. \(F(x)=P\{Y<x\}\) and \(F^{-1}(1-\alpha)\) is the largest \(1-\alpha\) quantile of \(F\), then the event \(\{X\leq F^{-1}(1-\alpha)\}\) is identical to \(\{F(X)\leq 1-\alpha\}\) for any random variable \(X\) (which need not have distribution \(F\)). Why?]

(ii) The bootstrap interval (18.60) pretends that

\[R_{n,1}(X^{n},\,\theta(P))\equiv J_{n}(R_{n}(X^{n},\,\theta(P)),\,\hat{P}_{n})\]

has the uniform distribution on \((0,\,1)\). Let \(J_{n,1}(P)\) be the actual distribution of \(R_{n,1}(X^{n},\,\theta(P))\) under \(P\), with left continuous c.d.f. denoted by \(J_{n,1}(x,\,P)\). This results in a new interval with \(R_{n}\) and \(J_{n}\) replaced by \(R_{n,1}\) and \(J_{n,1}\) in (18.60). Show that the resulting interval is equivalent to bootstrap calibration of the initial interval. [The mapping of \(R_{n}\) into \(R_{n,1}\) by estimated c.d.f. of the former is called _prepivoting_. Beran (1987, 1988b) argues that the interval based on \(R_{n,1}\) has better coverage properties than the interval based on \(R_{n}\).]

### Section 18.5

**Problem 18.30**: In Example 18.5.1, assume \(P\) is multivariate normal. Fix \(\theta_{0}\) on the boundary of the null hypothesis parameter space, so that \(\theta_{0}=(\theta_{0,1},\ldots,\theta_{0,k})^{\top}\) has \(\theta_{0,i}\leq 0\) for all \(i\) and equal to zero for at least one \(i\). Fix \(h=(h_{1},\ldots,h_{k})^{\top}\) and calculate the limiting power of the bootstrap test against alternatives \(\theta_{0}+hn^{-1/2}\). Compare with Problem 14.68.

**Problem 18.31**: Explain why the parametric bootstrap method described in Example 18.5.7 fails if \(k>1\). What happens if \(k=1\)? [By sufficiency, you may assume you observe \(X=(X_{1},\ldots,X_{k})^{\top}\) multivariate normal. In the case \(k=2\), plot the rejection region. Recall Examples 8.7.3 and 14.4.8).]

**Problem 18.32**: In Example 18.5.2, rather than exact evaluation of \(G_{n}(\cdot,\hat{Q}_{n})\), describe a simulation test of \(H\) that has exact level \(\alpha\).

**Problem 18.33**: In Example 18.5.3, why is the parametric bootstrap test exact for the special case of Example 14.4.7?

**Problem 18.34**: In the Behrens-Fisher problem, show that (18.37) and (18.38) hold.

**Problem 18.35**: In the Behrens-Fisher problem, verify the bootstrap-\(t\) has rejection probability equal to \(\alpha+O(n^{-2})\).

**Problem 18.36**: In the Behrens-Fisher problem, what is the order of error in rejection probability for the likelihood ratio test? What is the order of error in rejection probability if you bootstrap the non-studentized statistic \(n^{1/2}(\bar{X}_{n,1}-\bar{X}_{n,2})\).

**Problem 18.37**: In Example 18.5.5, with resampling from the empirical distribution shifted to have mean 0, what are the errors in rejection for the tests based on \(T_{n}\) and \(T_{n}^{\prime}\)? How do these tests differ from the corresponding tests obtained through inverting bootstrap confidence bounds?

**Problem 18.38**: Let \(X_{1}\),..., \(X_{n}\) be i.i.d. with a distribution \(P\) on the real line, and let \(\hat{P}_{n}\) be the empirical distribution function. Find \(Q\) that minimizes \(\delta_{KL}(\hat{P}_{n},\,Q)\), where \(\delta_{KL}\) is the Kullback-Leibler divergence defined by (18.39).

**Problem 18.39**: Suppose \(X_{1}\),..., \(X_{n}\) are i.i.d. real-valued with c.d.f. \(F\). The problem is to test the null hypothesis that \(F\) is \(N(\mu,\sigma^{2})\) for some \((\mu,\sigma^{2})\). Consider the test statistic

\[T_{n}=n^{1/2}\sup_{t}|\hat{F}_{n}(t)-\Phi((t-\bar{X}_{n})/\hat{\sigma}_{n})|\,\]

where \(\hat{F}_{n}\) is the empirical c.d.f. and \((\bar{X}_{n},\hat{\sigma}_{n}^{2})\) is the MLE for \((\mu,\sigma^{2})\) assuming normality. Argue that the distribution of \(T_{n}\) does not depend on \((\mu,\sigma^{2})\) and describe an exact bootstrap test construction. [Such problems are studied in Romano (1988)].

### _Section 18.6_

**Problem 18.40**: Show why (18.47) is true.

**Problem 18.41**: (i) Under the setup of Example 18.6.1, prove that Theorem 18.6.1 applies if studentized statistics are used.

(ii) In addition to the \(X_{1}\),..., \(X_{n}\), suppose i.i.d. \(Y_{1}\),..., \(Y_{n^{\prime}}\) are observed, with \(Y_{i}=(Y_{i,1}\),..., \(Y_{i,s})\). The distribution of \(Y_{i}\) need not be that of \(X_{i}\). Suppose the mean of \(Y_{i}\) is \((\mu^{\prime}_{1}\),..., \(\mu^{\prime}_{s})\). Generalize Example 18.6.1 to simultaneously test \(H_{i}:\mu_{i}=\mu^{\prime}_{i}\). Distinguish between two cases, first where the \(X_{i}\)s are independent of the \(Y_{j}\)s, and next where \((X_{i}\), \(Y_{i})\) are paired (so \(n=n^{\prime}\)) and \(X_{i}\) need not be independent of \(Y_{i}\).

**Problem 18.42**: Under the setup of Example 18.6.2, provide the details to show that the FWER is asymptotically controlled.

**Problem 18.43**: Under the setup of Example 18.6.2, suppose that there is also an i.i.d. control sample \(X_{0,1}\),..., \(X_{0,n_{0}}\), independent of the other \(X\)s. Let \(\mu_{0}\) denote the mean of the controls. Now consider testing \(H_{i}:\ \mu_{i}=\mu_{0}\). Describe a method that asymptotically controls the FWER.

**Problem 18.44**: Under the setup of Example 18.6.2, let \(F_{i}\) denote the distribution of the \(i\)th sample. Now, consider \(H^{\prime}_{i,j}:F_{i}=F_{j}\) based on the same test statistics. Describe a randomization test that controls the FWER.

### _Section 18.7_

**Problem 18.45**: Prove Theorem 18.7.2. [_Hint_: For (ii), rather than considering \(\hat{G}_{n,b}(x)\), just look at the empirical distribution, \(\hat{G}^{0}_{n,b}\), of the values of \(t_{n,b,i}\) (not scaled by \(\tau_{b}\)) and show \(\hat{G}^{0}_{n,b}(\cdot)\) converges in distribution to a point mass at \(t(P)\).]

**Problem 18.46**: Prove a general subsampling theorem for two-sample problems. Here, you observe \(X_{1}\),..., \(X_{m}\) i.i.d. \(P\) and independently \(Y_{1}\),..., \(Y_{n}\) are i.i.d. \(Q\). The problem is to get a confidence interval for \(\theta(Q)-\theta(P)\). Assume \(\min(m,n)\rightarrow\infty\). Describe the method, state a theorem, and prove it.

**Problem 18.47**: Prove a result for subsampling analogous to Theorem 18.6.1, but that does not require assumption (18.46). [Theorem 18.6.1 applies to testing real-valued parameters; a more general multiple testing procedure based on subsampling is given by Theorem 4.4 of Romano and Wolf (2005a).]

**Problem 18.48**: In Example 18.7.5, verify (18.55), (18.56), and (18.57).

**Problem 18.49**: To see how subsampling extends to a dependent time series model, assume \(X_{1},\ldots,X_{n}\) are sampled from a stationary time series model that is \(m\)-dependent. [Stationarity means the distribution of the \(X_{1},X_{2},\ldots\) is the same asthat of \(X_{t}\), \(X_{t+1}\), \(\ldots\) for any \(t\). The process is \(m\)-dependent if, for any \(t\) and \(m\), (\(X_{1}\), \(\ldots\), \(X_{t}\)) and (\(X_{t+m+1}\), \(X_{t+m+2}\), \(\ldots\)) are independent; that is, observations separated in time by more than \(m\) units are independent.] Suppose the sum in the definition (18.48) of \(L_{n,b}\) extends only over the \(n-b+1\) subsamples of size \(b\) of the form (\(X_{i}\), \(X_{i+1}\), \(\ldots\), \(X_{i+b-1}\)); call the resulting estimate \(\widetilde{L}_{n,b}\). Under the assumption of stationarity and \(m\)-dependence, prove a theorem analogous to Theorem 18.7.1. Then, extend the argument to strong mixing sequences, which were discussed in Section 12.4.

### Notes

The bootstrap was discovered by Efron (1979), who coined the name. Much of the theoretical foundations of the bootstrap are laid out in Bickel and Freedman (1981) and Singh (1981). The development in Section 18.3 is based on Beran (1984). The use of Edgeworth expansions to study the bootstrap was initiated in Singh (1981) and Babu and Singh (1983), and is used prominently in Hall (1992). There have since been hundreds of papers on the bootstrap, as well as several book length treatments, including Hall (1992), Efron and Tibshirani (1993), Shao and Tu (1995), Davison and Hinkley (1997) and Lahiri (2003). Comparisons of bootstrap and randomization tests are made in Romano (1989b) and Janssen and Pauls (2003b). Westfall and Young (1993) and van der Lann et al. (2004) apply resampling to multiple testing problems. Theorem 18.6.1 is based on Romano and Wolf (2005a). Efficient computation of adjusted \(p\)-values for resampling based stepdown multiple testing methods are discussed in Romano and Wolf (2016) and Clarke, Romano and Wolf (2020). Simultaneous bootstrap confidence intervals for differences as described in Example 18.6.2, form a basis for inference for ranks of populations; see Mogstad et al. (2020). Bootstrap results in high-dimensional problems are developed in Chernozhukov et al. (2017) and Xue and Yao (2020).

The method of empirical likelihood referred to in Example 18.5.5 is fully treated in Owen (2001). Similar to parametric models, the method of empirical likelihood can be improved through a Bartlett correction, yielding two-sided tests with error in rejection probability of \(O(n^{-2})\); see DiCiccio et al. (1991). Alternatively, rather than using the asymptotic Chi-squared distribution to get critical values, a direct bootstrap approach resamples from \(\hat{Q}_{n}\). Higher order properties of such procedures are considered in DiCiccio and Romano (1990).

The roots of subsampling can be traced to Quenouille's (1949) and Tukey's (1958a) jackknife. Hartigan (1969) and Wu (1990) used subsamples to construct confidence intervals, but in a very limited setting. A general theory for using subsampling to approximate a sampling distribution is presented in Politis and Romano (1994b), including i.i.d. and data-dependent settings. Multi-samples are treated in Politis and Romano (2008, 2010) and McMurry et al. (2012). A full treatment with numerous earlier references is given by Politis et al. (1999). Romano and Shaikh (2012) discuss the uniform asymptotic validity of both the bootstrap and subsampling.

## Appendix A Auxiliary Results

### Equivalence Relations; Groups

A relation: \(x\sim y\) among the points of a space \(\mathcal{X}\) is an equivalence relation if it is reflexive, symmetric, and transitive, that is, if

1. \(x\sim x\) for all \(x\in\mathcal{X}\);
2. \(x\sim y\) implies \(y\sim x\);
3. \(x\sim y\), \(y\sim z\) implies \(x\sim z\).

**Example A.1.1**: Consider a class of statistical decision procedures as a space, of which the individual procedures are the points. Then the relation defined by \(\delta\sim\delta^{\prime}\) if the procedures \(\delta\) and \(\delta^{\prime}\) have the same risk function is an equivalence relation. As another example consider all real-valued functions defined over the real line as points of a space. Then \(f\sim g\) if \(f(x)=g(x)\) a.e. is an equivalence relation.

Given an equivalence relation, let \(D_{x}\) denote the set of points of the space that are equivalent to \(x\). Then \(D_{x}=D_{y}\) if \(x\sim y\), and \(D_{x}\cap D_{y}=0\) otherwise. Since by (i) each point of the space lies in at least one of the sets \(D_{x}\), it follows that these sets, the _equivalence classes_ defined by the relation \(\sim\), constitute a partition of the space.

A set \(G\) of elements is called a _group_ if it satisfies the following conditions.

1. There is defined an operation, group multiplication, which with any two elements \(a,b\in G\) associates an element \(c\) of \(G\). The element \(c\) is called the product of \(a\) and \(b\) and is denoted by \(ab\).
2. Group multiplication obeys the associative law \[(ab)c=a(bc).\]
3. There exists an element \(e\in G\), called the _identity_, such that \[ae=ea=a\quad\quad\text{for all}\quad a\in G.\]4. For each element \(a\in G\), there exists an element \(a^{-1}\in G\), its _inverse_, such that \[aa^{-1}=a^{-1}a=e.\] Both the identity element and the inverse \(a^{-1}\) of any element \(a\) can be shown to be unique.

**Example A.1.2**: The set of all \(n\times n\) orthogonal matrices constitutes a group if matrix multiplication and inverse are taken as group multiplication and inverse respectively, and if the identity matrix is taken as the identity element of the group. With the same specification of the group operations, the class of all nonsingular \(n\times n\) matrices also forms a group. On the other hand, the class of all \(n\times n\) matrices fails to satisfy condition (iv).

If the elements of \(G\) are transformations of some space onto itself, with the group product \(ba\) defined as the result of applying first transformation a and following it by \(b\), then \(G\) is called a transformation group. Assumption (ii) is then satisfied automatically. For any transformation group defined over a space \(\mathcal{X}\) the relation between points of \(X\) given by

\[x\sim y\ \ \ \mbox{if}\ \ \ \mbox{there exists}\ a\in G\ \mbox{such that}\ y=ax\]

is an equivalence relation. That it satisfies conditions (i), (ii), and (iii) required of an equivalence follows respectively from the defining properties (iii), (iv), and (i) of a group.

Let \(\mathbb{C}\) be any class of \(1:1\) transformations of a space, and let \(G\) be the class of all finite products \(a_{1}^{\pm 1}a_{2}^{\pm 1}\ldots a_{m}^{\pm 1}\), with \(a_{1}\),..., \(a_{m}\in\mathbb{C}\), \(m=1\), \(2\),..., where each of the exponents can be \(+1\) or \(-1\) and where the elements \(a_{1}\), \(a_{2}\),... need not be distinct. Then it is easily checked that \(G\) is a group, and is in fact the smallest group containing \(\mathbb{C}\).

### Convergence of Functions; Metric Spaces

When studying convergence properties of functions it is frequently convenient to consider a class of functions as a realization of an abstract space \(\mathcal{F}\) of points \(f\) in which convergence of a sequence \(f_{n}\) to a limit \(f\), denoted by \(f_{n}\to f\), has been defined.

**Example A.2.1**: Let \(\mu\) be a measure over a measurable space \((\mathcal{X},\mathcal{A})\).

1. Let \(\mathcal{F}\) be the class of integrable functions. Then \(f_{n}\) converges to \(f\)_in the mean_ if1\[\int|f_{n}-f|\,d_{\mu}\to 0.\] (A.1)
2. Let \(\mathcal{F}\) be a uniformly bounded class of measurable functions. The sequence is said to converge to \(f\)_weakly_ if \[\int f_{n}p\,d\mu\to\int fp\,d\mu\] (A.2) for all functions \(p\) that are integrable \(\mu\).
3. Let \(\mathcal{F}\) be the class of measurable functions. Then \(f_{n}\) converges to \(f\)_pointwise_ if \[f_{n}(x)\to f(x)\ \ \ \ \ a.e.\ \mu.\] (A.3)

A subset of \(\mathcal{F}_{0}\) is _dense_ in \(\mathcal{F}\) if, given any \(f\in\mathcal{F}\), there exists a sequence in \(\mathcal{F}_{0}\) having \(f\) as its limit point. A space \(\mathcal{F}\) is _separable_ if there exists a countable dense subset of \(\mathcal{F}\). A space \(\mathcal{F}\) such that every sequence has a convergent subsequence whose limit point is in \(\mathcal{F}\) is compact.2 A space \(\mathcal{F}\) is a _metric space_ if, for every pair of points \(f\), \(g\) in \(\mathcal{F}\), there is defined a metric (or distance) \(d(f,\,g)\geq 0\) such that

Footnote 2: The term _compactness_ is more commonly used for an alternative concept. which coincides with the one given here in metric spares. The distinguishing term _sequential compactness_ is then sometimes given to the notion defined here.

1. \(d(f,\,g)=0\) if and only if \(f=g\);
2. \(d(f,\,g)=d(g,\,f)\);
3. \(d(f,\,g)+d(g,\,h)\geq d(f,h)\) for all \(f\), \(g\), \(h\).

The space is a _pseudometric_ space if (i) is replaced by

1. \(d(f,\,f)=0\) for all \(f\in\mathcal{F}\).

A pseudometric space can be converted into a metric space by introducing the equivalence relation \(f\sim g\) if \(d(f,\,g)=0\). The equivalence classes \(F\), \(G\), \(\ldots\) then constitute a metric space with respect to the metric \(D(F,\,G)=d(f,\,g)\) where \(f\in F\), \(g\in G\).

In any pseudometric space a natural convergence definition is obtained by putting \(f_{n}\to f\) if \(d(f_{n},\,f)\to 0\).

**Example A.2.2**: The space of integrable functions of Example A.2.1(i) becomes a pseudometric space if we put

\[d(f,\,g)=\int|f-g|\,d\mu\]

and the induced convergence definition is that given by (1).

**Example A.2.3**: Let \(\mathcal{P}\) be a family of probability distributions over \((\mathcal{X},\mathcal{A})\). Then \(\mathcal{P}\) is a metric space with respect to the metric

\[d(P,\,Q)=\sup_{A\in\mathcal{A}}|\,P(A)-Q(A)|.\] (A.4)

**Lemma A.2.1**: _If \(\mathcal{F}\) is a separable pseudometric space, then every subset of \(\mathcal{F}\) is also separable._

Proof. By assumption there exists a dense countable subset \(\{f_{n}\}\) of \(\mathcal{F}\). Let

\[S_{m,n}=\left\{f:d(f,\,f_{n})<\frac{1}{m}\right\},\]

and let \(A\) be any subset of \(\mathcal{F}\). Select one element from each of the intersections \(A\cap S_{m,n}\) that is nonempty, and denote this countable collection of elements by \(A_{0}\). If \(a\) is any element of \(A\) and \(m\) any positive integer, there exists an element \(f_{n_{m}}\) such that \(d(a,\,f_{n_{m}})<1/m\). Therefore \(a\) belongs to \(S_{m,n_{m}}\), the intersection \(A\cap S_{m,n_{m}}\) is nonempty, and there exists therefore an element of \(A_{0}\) whose distance to \(a\) is \(<2/m\). This shows that \(A_{0}\) is dense in \(A\), and hence that \(A\) is separable.

**Lemma A.2.2**: _A sequence \(f_{n}\) of integrable functions converges to \(f\) in the mean if and only if_

\[\int_{A}f_{n}\,d\mu\to\int_{A}f\,d\mu\quad\quad\text{uniformly for}\quad A\in \mathcal{A}.\] (A.5)

Proof. That (1) implies (5) is obvious, since for all \(A\in\mathcal{A}\)

\[\left|\int_{A}f_{n}\,d\mu-\int_{A}f\,d\mu\right|\leq\int|f_{n}-f|\,d\mu.\]

Conversely, suppose that (5) holds, and denote by \(A_{n}\) and \(A_{n}^{\prime}\) the set of points \(x\) for which \(f_{n}(x)>f(x)\) and \(f_{n}(x)<f(x)\) respectively. Then

\[\int|f_{n}-f|\,d\mu=\int_{A_{n}}(f_{n}-f)\,d\mu-\int_{A_{n}^{\prime}}(f_{n}-f) \,d\mu\to 0\.\]

**Lemma A.2.3**: _A sequence \(f_{n}\) of uniformly bounded functions converges to a bounded function \(f\) weakly if and only if_

\[\int_{A}f_{n}\,d\mu\to\int_{A}f\,d\mu\quad\quad\text{for all $A$ with $\mu(A)<\infty$}.\] (A.6)Proof. That weak convergence implies (6) is seen by taking for \(p\) in (2) the indicator function of a set \(A\), which is integrable if \(\mu(A)<\infty\). Conversely (6) implies that (2) holds if \(p\) is any simple function \(s=\sum a_{i}I_{A_{i}}\) with all the \(\mu(A_{i})<\infty\). Given any integrable function \(p\), there exists, by the definition of the integral, such a simple function \(s\) for which \(\int\,|p-s|\,d\mu<\epsilon/3M\), where \(M\) is a bound on the \(|f|\)'s. We then have

\[\left|\int\,(f_{n}-f)p\,d\mu\right|\leq\left|\int\,f_{n}(p-s)\,d\mu\right|+ \left|\int\,f(s-p)\,d\mu\right|+\left|\int\,(f_{n}-f)s\,d\mu\right|.\]

The first two terms on the right-hand side are \(<\epsilon/3\), and the third term tends to zero as \(n\) tends to infinity. Thus the left-hand side is \(<\epsilon\) for \(n\) sufficiently large, as was to be proved.

**Lemma A.2.4**: 3 _Let \(f\) and \(f_{n}\), \(n=1\), 2,..., be nonnegative integrable functions with_

\[\int\,f\,d\mu=\int f_{n}\,d\mu=1.\]

_Then pointwise convergence of \(f_{n}\) to \(f\) implies that \(f_{n}\to\,f\) in the mean._

Proof. If \(g_{n}=f_{n}-f\), then \(g\geq-f\), and the negative part \(g_{n}^{-}=\max(-g_{n},0)\) satisfies \(|g_{n}^{-}|\leq f\). Since \(g_{n}(x)\to 0\) (a.e. \(\mu\)), it follows from Theorem 2.2.2(ii) of Chapter 2 that \(\int\,g_{n}^{-}\,d\mu\to\,0\), and \(\int\,g_{n}^{+}\,d\mu\) then also tends to zero, since \(\int\,g_{n}\,d\mu=0\). Therefore \(\int\,|g_{n}|\,d\mu=\int(g_{n}^{+}+g_{n}^{-})\,d\mu\to\,0\), as was to be proved.

Let \(P\) and \(P_{n}\), \(n=1\), 2,... be probability distributions over \((\mathcal{X},\mathcal{A})\) with densities \(p_{n}\) and \(p\) with respect to \(\mu\). Consider the convergence definitions

* \(p_{n}\to\,p\) (a.e. \(\mu\));
* \(\int|p_{n}-p|\,d\mu\to\,0\);
* \(\int\,gp_{n}\,d\mu\to\,\int\,gp\,d\mu\) for all bounded measurable \(g\);

and

* \(P_{n}(A)\to\,P(A)\) uniformly for all \(A\in\mathcal{A}\);
* \(P_{n}(A)\to\,P(A)\) for all \(A\in\mathcal{A}\).

Then Lemmas A.2.2 and A.2.4 together with a slight modification of Lemma A.2.3 show that (a) implies (b) and (b) implies (c), and that (b) is equivalent to (b\({}^{\prime}\)) and (c) to (c\({}^{\prime}\)). It can further be shown that neither (a) and (b) nor (b) and (c) are equivalent.4

### Banach and Hilbert Spaces

A set V is called a _vector space_ (or linear space) over the reals if there exists a function \(+\) on \(V\times V\) to \(V\) and a function \(\cdot\) on \(\mathbf{R}\times V\) to \(V\) which satisfy for \(x\), \(y\), \(z\in V\),

(i) \(x+y=y+x\).

(ii) \((x+y)+z=z+(y+z)\).

(iii) There is a vector \(\underline{0}\in V\colon x+\underline{0}=x\) for all \(x\in V\).

(iv) \(\lambda(x+y)=\lambda x+\lambda y\) for any \(\lambda\in\mathbf{R}\).

(v) \((\lambda_{1}+\lambda_{2})x=\lambda_{1}x+\lambda_{2}x\) for \(\lambda_{i}\in\mathbf{R}\).

(vi) \(\lambda_{1}(\lambda_{2}x)=(\lambda_{1}\lambda_{2})x\) for \(\lambda_{i}\in\mathbf{R}\).

(vii) \(0\cdot x=\underline{0},\ 1\cdot x=x\).

The operation \(+\) is called addition by scalars and \(\cdot\) is multiplication by scalars. A nonnegative real-valued function \(\|\ \|\) defined on a vector space is called a norm if

(i) \(\|x\|=0\) if and only if \(x=\underline{0}\).

(ii) \(\|x+y\|\leq\|x\|+\|y\|\).

(iii)\(\|\lambda x\|=|\lambda|\|x\|\).

A vector space with norm \(\|\ \|\) is a then a metric space if we define the metric \(d\) to be \(d(x,\,y)=\|x-y\|\).

A sequence \(\{x_{n}\}\) of elements in a normed vector space \(V\) is called a Cauchy sequence if, given \(\epsilon>0\), there is an \(N\) such that for all \(m\), \(n\geq N\), we have \(\|x_{n}-x_{m}\|<\epsilon\). A Banach space is a normed vector space that is complete in the sense that every Cauchy sequence \(\{x_{n}\}\) satisfies \(\|x_{n}-x\|\to 0\) for some \(x\in V\).

**Example A.3.1**: **(\(L^{p}\) spaces.)** Let \(\mu\) be a measure over a measurable space \((\mathcal{X},\,\mathcal{A})\). Fix \(p>0\) and \(L^{p}[\mathcal{X},\,\mu]\) denote the measurable functions \(f\) such that \(\int|f|^{p}d\mu<\infty\). If we identify equivalence classes of functions that are equal almost everywhere \(\mu\), then, for \(p\geq 1\), this vector space becomes a normed vector space by defining

\[\|f\|=\|f\|_{p}=\left[\int|f|^{p}d\mu\right]^{1/p}\.\]

In this case, the triangle inequality

\[\|f+g\|_{p}\leq\|f\|_{p}+\|g\|_{p}\]

is known as _Minkowski's inequality_. Moreover, this space is a Banach space.5

Footnote 5: For proofs of the results in this section, see Chapter 5 of Dudley (1989).

A Hilbert space \(H\) is a Banach space for which there is defined a function \(\langle x,\,y\rangle\) on \(H\times H\) to \(\mathbf{R}\), called the inner product of \(x\) and \(y\), satisfying, for \(x_{i}\), \(y\in H,\lambda_{i}\in\mathbf{R}\),

(i) \(\langle\lambda_{1}x_{1}+\lambda_{2}x_{2},\,y\rangle=\lambda_{1}\langle x_{1},\,y\rangle+\lambda_{2}\langle x_{2},\,y\rangle\).

(ii) \(\langle x,\,y\rangle=\langle y,\,x\rangle\).

(iii) \(\langle x,\,x\rangle=\|x\|^{2}\).

Two vectors \(x\) and \(y\) of \(H\) are called orthogonal if \(\langle x,\,y\rangle=0\). A collection \(H_{0}\subset H\) of vectors is called an orthogonal system if any two elements in \(H_{0}\) are orthogonal. An orthogonal system is orthonormal if each vector in it has norm 1. An orthonormal system \(H_{0}\) is called complete if \(\langle x,\,h\rangle=0\) for all \(h\in H_{0}\) implies \(x=\underline{0}\). In a separable Hilbert space, every orthonormal system is countable and there exists a complete orthonormal system. Letting \(\{h_{1},h_{2},\ldots\}\) denote a complete orthonormal system, Parseval's identity says that, for any \(x\in H\),

\[\|x\|^{2}=\sum_{j=1}^{\infty}[\langle x,\,h_{j}\rangle]^{2}\.\]

**Example A.3.2**: **(\(L^{2}\) spaces.)** In example A.3.1 with \(p=2\), the equivalence classes of square integrable functions is a Hilbert space with inner product given by

\[\langle f_{1},\,f_{2}\rangle=\int f_{1}f_{2}d\mu\.\]

If \({\cal X}\) is [0, 1] and \(\mu\) is Lebesgue measure, then a complete orthonormal system is given by the functions \(f_{j}(u)=\sqrt{2}\sin(\pi ju)\), \(j=1,2,\ldots\). Therefore, for any square integrable function \(f\), Parseval's identity yields

\[\int_{0}^{1}f^{2}(u)du=2\sum_{j=1}^{\infty}\left[\int_{0}^{1}f(u)\sin(\pi ju) du\right]^{2}.\]

### Dominated Families of Distributions

Let \({\cal M}\) be a family of measures defined over a measurable space (\({\cal X}\), \({\cal A}\)). Then \({\cal M}\) is said to be _dominated_ by a \(\sigma\)-finite measure \(\mu\) defined over (\({\cal X}\), \({\cal A}\)) if each member of \({\cal M}\) is absolutely continuous with respect to \(\mu\). The family \({\cal M}\) is said to be _dominated_ if there exists a \(\sigma\)-finite measure dominating it. Actually, if \({\cal M}\) is dominated there always exists a finite dominating measure. For suppose that \({\cal M}\) is dominated by \(\mu\) and that \({\cal X}=\cup A_{i}\), with \(\mu(A_{i})\) finite for all \(i\). If the sets \(A_{i}\) are taken to be mutually exclusive, the measure \(\upsilon(A)=\sum\mu(A\cap A_{i})/2^{i}\mu(A_{i})\) also dominates \({\cal M}\) and is finite.

**Theorem A.4.1**6: _A family \({\cal P}\) of probability measures over a Euclidean space (\({\cal X}\), \({\cal A}\)) is dominated if and only if it is separable with respect to the metric (4) or equivalently with respect to the convergence definition_

\[P_{n}\to P\ \ \ \mbox{if}\ \ \ P_{n}(A)\to P(A)\ \ \ \mbox{uniformly for}\ \ \ A\in{\cal A}.\]Proof. Suppose first that \({\cal P}\) is separable and that the sequence \(\{P_{n}\}\) is dense in \({\cal P}\), and let \(\mu=\sum P_{n}/2^{n}\). Then \(\mu(A)=0\) implies \(P_{n}(A)=0\) for all \(n\), and hence \(P(A)=0\) for all \(P\in{\cal P}\). Conversely suppose that \({\cal P}\) is dominated by a measure \(\mu\), which without loss of generality can be assumed to be finite. Then we must show that the set of integrable functions \(d\,P/d\mu\) is separable with respect to the convergence definition (5) or, because of Lemma A.2.2, with respect to convergence in the mean. It follows from Lemma A.2.1 that it suffices to prove this separability for the class \({\cal F}\) of all functions \(f\) that are integrable \(\mu\). Since by the definition of the integral every integrable function can be approximated in the mean by simple functions, it is enough to prove this for the case that \({\cal F}\) is the class of all simple integrable functions. Any simple function can be approximated in the mean by simple functions taking on only rational values, so that it is sufficient to prove separability of the class of functions \(\sum r_{i}\,I_{A_{i}}\), where the \(r\)'s are rational and the \(A\)'s are Borel sets, with finite \(\mu\)-measure since the \(f\)'s are integrable. It is therefore finally enough to take for \({\cal F}\) the class of functions \(I_{A}\), which are indicator functions of Borel sets with finite measure. However, any such set can be approximated by finite unions of disjoint rectangles with rational endpoints. The class of all such unions is denumerable, and the associated indicator functions will therefore serve as the required countable dense subset of \({\cal F}\).

An examination of the proof shows that the Euclidean nature of the space \(({\cal X},\,{\cal A})\) was used only to establish the existence of a countable number of sets \(A_{i}\in{\cal A}\) such that for any \(A\in{\cal A}\) with finite measure there exists a subsequence \(A_{i}\) with \(\mu(A_{i})\to\mu(A)\). This property holds quite generally for any \(\sigma\)-field \({\cal A}\) which has a _countable number of generators_, that is, for which there exists a countable number of sets \(B_{i}\) such that \({\cal A}\) is the smallest \(\sigma\)-field containing the \(B_{i}\).7 It follows that Theorem A.4.1 holds for any \(\sigma\)-field with this property. Statistical applications of such \(\sigma\)-fields occur in sequential analysis, where the sample space \({\cal X}\) is the union \({\cal X}=\cup_{i}{\cal X}_{i}\) of Borel subsets \({\cal X}_{i}\) of \(i\)-dimensional Euclidean space. In these problems, \({\cal X}_{i}\) is the set of points \((x_{1},\ldots,x_{i})\) for which exactly \(i\) observations are taken. If \({\cal A}_{i}\) is the \(\sigma\)-field of Borel subsets of \({\cal X}_{i}\), one can take for \({\cal A}\), the \(\sigma\)-field generated by the \({\cal A}_{i}\), and since each \({\cal A}_{i}\) possesses a countable number of generators, so does \({\cal A}\).

Footnote 7: A proof of this is given for example by Halmos (1974, Theorem B of Section 40).

If \({\cal A}\) does not possess a countable number of generators, a somewhat weaker conclusion can be asserted. Two families of measures \({\cal M}\) and \({\cal N}\) are _equivalent_ if \(\mu(A)=0\) for all \(\mu\in{\cal M}\) implies \(\nu(A)=0\) for all \(\nu\in{\cal N}\) and vice versa.

**Theorem A.4.28**: _A family \({\cal P}\) of probability measures is dominated by a \(\sigma\)-finite measure if and only if \({\cal P}\) has a countable equivalent subset._

Proof. Suppose first that \({\cal P}\) has a countable equivalent subset \(\{P_{1},\,P_{2},\ldots\}\). Then \({\cal P}\) is dominated by \(\mu=\sum P_{n}/2^{n}\). Conversely, let \({\cal P}\) be dominated by a \(\sigma\)-finite measure \(\mu\), which without loss of generality can be assumed to be finite. Let \({\cal Q}\) be the class of all probability measures \(Q\) of the form \(\sum c_{i}\,P_{i}\), where \(P_{i}\in{\cal P}\), the \(c\)'s

[MISSING_PAGE_FAIL:933]

first the sequence of numbers \(\{\Phi_{n}(p_{1})\}\) which possesses a convergent subsequence \(\Phi_{n_{1}^{\prime}}(p_{1})\), \(\Phi_{n_{2}^{\prime\prime}}(p_{1})\),.... Next the sequence \(\Phi_{n_{1}^{\prime}}(p_{2})\), \(\Phi_{n_{2}}^{\prime}(p_{2})\),... has a convergent subsequence \(\Phi_{n_{1}^{\prime\prime}}(p_{2})\), \(\Phi_{n_{2}^{\prime\prime}}(p_{2})\),.... Continuing in this way, let \(n_{1}=n_{1}^{\prime}\), \(n_{2}=n_{2}^{\prime\prime}\), \(n_{3}^{\prime\prime\prime}\),.... Then \(n_{1}<n_{2}<\dots\), and the sequence \(\{\Phi_{n_{i}}\}\) converges for each \(p_{m}\). It follows from the inequality

\[\left|\int(\phi_{n_{j}}-\phi_{n_{i}})p\,d\mu\right|\leq\left|\int(\phi_{n_{j}}- \phi_{n_{i}})p_{m}\,d\mu\right|+2\int\left|p-p_{m}\right|d\mu\]

that \(\Phi_{n_{i}}(p)\) converges for all \(p\). Denote its limit by \(\Phi(p)\), and define a set function \(\Phi^{*}\) over \(\mathcal{A}\) by putting

\[\Phi^{*}(A)=\Phi(I_{A}).\]

Then \(\Phi^{*}\) is nonnegative and bounded, since for all \(A\), \(\Phi^{*}(A)\leq\mu(A)\). To see that it is also countably additive let \(A=\cup A_{k}\), where the \(A_{k}\) are disjoint. Then \(\Phi^{*}(A)=\lim\Phi_{n_{i}}^{*}(\cup A_{k})\) and

\[\left|\int_{\cup A_{k}}\phi_{n_{i}}\,d\mu-\sum\Phi^{*}(A_{k}) \right|\leq\left|\int_{\cup_{k=1}^{m}A_{k}}\phi_{n_{i}}\,d\mu-\sum_{k=1}^{m} \Phi^{*}(A_{k})\right|\] \[+\left|\int_{\cup_{k=m+1}^{\infty}A_{k}}\phi_{n_{i}}\,d\mu-\sum_{ k=m+1}^{\infty}\Phi^{*}(A_{k})\right|.\]

Here the second term is to be taken as zero in the case of a finite sum \(A=\cup_{k=1}^{m}A_{k}\), and otherwise does not exceed \(2\mu(\cup_{k=m+1}^{\infty}A_{k})\), which can be made arbitrarily small by taking \(m\) sufficiently large. For any fixed \(m\), the first term tends to zero as \(i\) tends to infinity. Thus \(\Phi^{*}\) is a finite measure over \((\mathcal{X},\,\mathcal{A})\). It is furthermore absolutely continuous with respect to \(\mu\), since \(\mu(A)=0\) implies \(\Phi_{n_{i}}(I_{A})=0\) for all \(i\), and therefore \(\Phi(I_{A})=\Phi^{*}(A)=0\) We can now apply the Radon-Nikodym Theorem to get

\[\Phi^{*}(A)=\int_{A}\phi\,d\mu\quad\text{for all }A\,,\]

with \(0\leq\phi\leq 1\). We then have

\[\int_{A}\phi_{n_{i}}\,d\mu\to\int_{A}\phi\,d\mu\quad\text{for all }A\,,\]

and weak convergence of the \(\phi_{n_{i}}\) to \(\phi\) follows from Lemma A.2.3.

## References

* Agresti (1992) Agresti, A. (1992). A survey of exact inference for contingency tables (with discussion). _Statistical Science_**7**, 131-177.
* Agresti (2002) Agresti, A. (2002). _Categorical Data Analysis_, 2nd edition. John Wiley, New York.
* Agresti and Coull (1998) Agresti, A. and Coull, B. (1998). Approximate is better than "exact" for interval estimation of binomial proportions. _American Statistician_**52**, 119-126.
* Aiyar et al. (1979) Aiyar, R. J., Guillier, C. L., and Albers, W. (1979). Asymptotic relative efficiencies of rank tests for trend alternatives. _Journal of the American Statistical Association_**74**, 226-231.
* Akritas et al. (1997) Akritas, M., Arnold, S. and Brunner, E. (1997). Nonparametric hypotheses and rank statistics for unbalanced factorial designs. _Journal of the American Statistical Association_**92**, 258-265.
* Albers (1978) Albers, W. (1978). Testing the mean of a normal population under dependence. _Annals of Statistics_**6**, 1337-1344.
* Albers et al. (1976) Albers, W., Bickel, P. and van Zwet, W. (1976). Asymptotic expansion for the power of distribution free tests in the one-sample problem. _Annals of Statistics_**4**, 108-156.
* Albert (1976) Albert, A. (1976). When is a sum of squares an analysis of variance? _Annals of Statistics_**4**, 775-778.
* Anderson (1967) Anderson, T. W. (1967). Confidence limits for the expected value of an arbitrary bounded random variable with a continuous distribution function. _Bull. ISI_**43**, 249-251.
* Anderson (2003) Anderson, T. W. (2003). _An Introduction to Multivariate Statistical Analysis_, 3rd edition. John Wiley, Hoboken, NJ. [Problem 6.20.]
* Andersson (1982) Andersson, S. (1982). Distributions of maximal invariants using quotient measures. _Annals of Statistics_**10**, 955-961.
* Andrews and Barwick (2012) Andrews, D. and Barwick, P. (2012). Inference for parameters defined by moment inequalities: a recommended moment selection procedure. _Econometrica_**80**, 2805-2826.
* Anscombe (1948) Anscombe, F. (1948). Transformations of Poisson, binomial and negative binomial data. _Biometrika_**35**, 246-254.
* (c) The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
* E. L. Lehmann and J. P. Romano, _Testing Statistical Hypotheses_, Springer Texts in Statistics, [https://doi.org/10.1007/978-3-030-70578-7](https://doi.org/10.1007/978-3-030-70578-7)* [Antille, Kersting, and ZucchiniAntille et al.1982] Antille, A., Kersting, G., and Zucchini, W. (1982). Testing symmetry. _Journal of the American Statistical Association_**77**, 639-651.
* [ArbuthnotArbuthnot1710] Arbuthnot, J. (1710). An argument for Divine Providence, taken from the constant regularity observ'd in the births of both sexes. _Phil. Trans._ **27**, 186-190.
* [ArconesArcones1991] Arcones, M. (1991). On the asymptotic theory of the bootstrap. Ph.D. thesis, The City University of New York.
* [Arcones and GineArcones and Gine2019] Arcones, M. and Gine, E. (1989). The bootstrap of the mean with arbitrary bootstrap sample size. _Annals of the Institute Henri Poincare_**25**, 457-481.
* [Arcones and GineArcones and Gine1991] Arcones, M. and Gine, E. (1991). Additions and correction to "the bootstrap of the mean with arbitrary bootstrap sample size". _Annals of the Institute Henri Poincare_**27**, 583-595.
* [Arias-Castro, Candes, and PlanArias-Castro et al.2011] Arias-Castro, E., Candes, E. and Plan, Y. (2011). Global testing under sparse alternatives: ANOVA, multiple. comparison and the higher criticism. _Annals of Statistics_**39**, 2533-2556.
* [ArmsenArmsen1955] Armsen, P. (1955). Tables for significance tests of 2 x 2 contingency tables. _Biometrika_**42**, 494-511.
* [ArnoldArnold1981] Arnold, S. (1981). _The Theory of Linear Models and Multivariate Analysis_. John Wiley, New York.
* [ArnoldArnold1984] Arnold, S. (1984). Pivotal quantities and invariant confidence regions. _Statistics and Decisions_**2**, 257-280.
* [Aronow, Gree, and LeeAronow et al.2014] Aronow, P., Gree, D. and Lee, D. (2014). Sharp bounds on the variance in randomized experiments. _The Annals of Statistics_**42**, 850-871.
* [ArrowArrow1960] Arrow, K. (1960). Decision theory and the choice of a level of significance for the \(t\)-test. In _Contributions to Probability and Statistics_ (Olkin et al., eds.) Stanford University Press, Stanford, California.
* [Arvesen and LayardArvesen and Layard1975] Arvesen, J. N. and Layard, M. W. J. (1975). Asymptotically robust tests in unbalanced variance component models. _Annals of Statistics_**3**, 1122-1134.
* [AthreyaAthreya1985] Athreya, K. (1985). Bootstrap of the mean in the infinite variance case, II. Technical Report 86-21, Department of Statistics, Iowa State University.
* [AthreyaAthreya1987] Athreya, K. (1987). Bootstrap of the mean in the infinite variance case. _Annals of Statistics_**15**, 724-731.
* [Atkinson and DonevAtkinson and Donev1992] Atkinson, A. and Donev, A. (1992). _Optimum Experimental Design_. Clarendon Press, Oxford.
* [Atkinson and RianiAtkinson and Riani2000] Atkinson, A. and Riani, M. (2000). _Robust Regression Analysis_. Springer-Verlag, New York.
* [Bababab1984] Babu, G. (1984). Bootstrapping statistics with linear combinations of chi-squares as weak limit. Sankhya Series A **56**, 85-93.
* [Bab and SinghBab and Singh1983] Babu, G. and Singh, K. (1983). Inference on means using the bootstrap. _Annals of Statistics_**11**, 999-1003.
* [BahadurBahadur1955] Bahadur, R. (1955). A characterization of sufficiency. _Annals of Mathematical Statistics_**26**, 286-293.
* [BahadurBahadur1960] Bahadur, R. (1960). Stochastic comparison of tests. _Annals of Mathematical Statistics_**31**, 279-295.
* [BahadurBahadur1965] Bahadur, R. (1965). An optimal property of the likelihood ratio statistic. In _Proc. 5th Berkeley Symposium on Probab. Theory and Math. Statist._**1**, Le Cam, L. and Neyman, J. (eds.), University of California Press, 13-26.

* Bahadur (1979) Bahadur, R. (1979). A note on UMV estimates and ancillary statistics. In _Contributions to Statistics_, J. Hajek Memorial Volume, Edited by Jureckova, Academia, Prague.
* Bahadur and Lehmann (1955) Bahadur, R. and Lehmann, E. L. (1955). Two comments on'sufficiency and statistical decision functions'. _Annals of Mathematical Statistics_**26**, 139-142. [Problem 2.5.]
* Bahadur and Savage (1956) Bahadur, R. and Savage, L. J. (1956). The nonexistence of certain statistical procedures in nonparametric problems. _Annals of Mathematical Statistics_**27**, 1115-1122.
* Bai et al. (2021) Bai, Y., Romano, J. and Shaikh, A. (2021). Inference in experiments with matched pairs. _Journal of the American Statistical Association_, to appear.
* Bain and Engelhardt (1975) Bain, L. J. and Engelhardt, M. E. (1975). A two-moment chi-square approximation for the statistic \(\log(\bar{X}/\bar{X})\). _Journal of the American Statistical Association_**70**, 948-950.
* Baker (1995) Baker, R. (1995). Two permutation tests of equality of variances. _Statistics and Computing_**5**, 351-361.
* Banach (1932) Banach, S. (1932). _Theorie des Operations Lineaires_. Funduszu Kultury Narodowej, Warszawa.
* Barber and Candes (2015) Barber, R. and Candes, E. (2015). Controlling the false discovery rate via knockoffs. _The Annals of Statistics_**43**, 2055-2085.
* Bar-Lev and Plachky (1989) Bar-Lev, S. and Plachky, D. (1989). Boundedly complete families which are not complete. _Metrika_**36**, 331-336.
* Bar-Lev and Reiser (1982) Bar-Lev, S. and Reiser, B. (1982). An exponential subfamily which admits UMPU tests based on a single test statistic. _Annals of Statistics_**10**, 979-989.
* Barankin and Maitra (1963) Barankin, E. W. and Maitra, A. P. (1963). Generalizations of the Fisher-Darmois-Koopman-Pitman theorem on sufficient statistics. _Sankhya Series A_**25**, 217-244.
* Barlow et al. (1972) Barlow, R. E., Bartholomew, D. J., Bremner, J. M., and Brunk, H. D. (1972). _Statistical Inference under Order Restrictions_, John Wiley, New York.
* Barnard (1976) Barnard, G. A. (1976). Conditional inference is not inefficient. _Scandinavian Journal of Statistics_**3**, 132-134. [Problem 10.27.]
* Barnard (1995) Barnard, G. A. (1995). Pivotal models and the fiducial argument. _International Statistical Review_**63**, 309-323.
* Barnard (1996) Barnard, G. A. (1996). Rejoinder, Pivotal models and structural models. _International Statistical Review_**64**, 235-236.
* Barndorff-Nielsen (1978) Barndorff-Nielsen, O. (1978). _Information and Exponential Families in Statistical Theory_. John Wiley, New York. [Provides a systematic discussion of various concepts of ancillarity with many examples.]
* Barndorff-Nielsen (1983) Barndorff-Nielsen, O. (1983). On a formula for the distribution of the maximum likelihood estimator. _Biometrika_**70**, 343-365.
* Barndorff-Nielsen et al. (1986) Barndorff-Nielsen, O., Cox, D. and Reid, N. (1986). Differential geometry in statistical theory. _International Statistics Review_**54**, 83-96.
* Barndorff-Nielsen and Hall (1988) Barndorff-Nielsen, O. and Hall, P. (1988). On the level-error after Bartlett adjustment of the likelihood ratio statistic. _Biometrika_**75**, 374-378.
* Barndorff-Nielsen and Pedersen (1968) Barndorff-Nielsen, O. and Pedersen, K. (1968). Sufficient data reduction and exponential families. _Math. Scand._**2**, 197-202.
* Barnett (1999) Barnett, V. (1999). _Comparative Statistical Inference_, 3rd edition. John Wiley, New York.
* Barnett (1999)Barron, A. (1989). Uniformly powerful goodness of fit tests. _Annals of Statistics_**17**, 107-124.
* Bartlett (1937) Bartlett, M. S. (1937). Properties of sufficiency and statistical tests. _Proc. Roy. Sec. London, Ser. A_**160**, 268-282. [Points out that _exact_ (that is, similar) tests can be obtained by combining the conditional tests given the different values of a sufficient statistic. Applications.]
* Bartlett (1957) Bartlett, M. S. (1957). A comment on D. V. Lindley's statistical paradox. _Biometrika_**44**, 533-534.
* Basu (1955) Basu, D. (1955). On statistics independent of a complete sufficient statistic. _Sankhya_**15**, 377-380.
* Basu (1958) Basu, D. (1958). On statistics independent of a sufficient statistic. _Sankhya_**20**, 223-226.
* Basu (1959) Basu, D. (1959). The family of ancillary statistics. _Sankhya_ (_A_) **21**. 247-256. [Problem 10.7.]
* Basu (1964) Basu, D. (1964). Recovery of ancillary information. _Sankhya_ (_A_) **26**, 3-16. [Problems 10.9, 10.11.]
* Basu (1978) Basu, D. (1978). On partial sufficiency: A review. _Journal of Statistical Planning and Inference_**2**, 1-13.
* Basu (1982) Basu, D. (1982). Basu theorems. In _Encycl. Statisti. Sci_**1**, 193-196.
* Basu (1999) Basu, S. (1999). Conservatism of the \(z\) confidence interval under symmetric and asymmetric departures from normality. _Annals of the Institute of Statistical Mathematics_**51**, 217-230.
* Basu and DasGupta (1995) Basu, S. and DasGupta, A. (1995). Robustness of standard confidence intervals for location parameters under departure from normality. _Annals of Statistics_**23**, 1433-1442.
* Bayarri and Berger (2000) Bayarri, M. and Berger, J. (2000). \(P\)-values for composite null hypotheses. _Journal of the American Statistical Association_**95**, 1127-1142.
* Bayarri and Berger (2004) Bayarri, M. and Berger, J. (2004). The interplay of Bayesian and frequentist analysis. _Statistical Science_**19**, 58-80.
* Becker (1997) Becker, B. (1997). Combination of \(p\)-values. _Encycl. Statist. update_**1**, 448-453.
* Becker and Gordon (1983) Becker, N. and Gordon, I. (1983). On Cox's criterion for discriminating between alternative ancillary statistics. _International Statistical Review_**51**. 89-92.
* Bednarski (1984) Bednarski, T. (1984). Minimax testing between Prohorov neighbourhoods. _Statistics and Decisions_**2**, 281-292.
* Behnen and Neuhaus (1989) Behnen, K. and Neuhaus, G. (1989). _Rank Tests With Estimated Scores and Their Applications_. (Teubner Skripten zur Mathematischen Stochastik) B. G. Teubner, Stuttgart.
* Bell et al. (1960) Bell, C. B., Blackwell, D. and Breiman, L. (1960). On the completeness of order statistics. _Annals of Mathematical Statistics_**31**, 794-797.
* Bell (1964) Bell, C. B. (1964). A characterization of multisample distribution-free statistics. _Annals of Mathematical Statistics_**35**, 735-738.
* Bell and Sen (1984) Bell, C. D. and Sen, P. (1984). Randomization procedures. In _Handbook of Statistics 4_ (Krishnaiah and Sen, eds.), Elsevier.
* Benichou et al. (1996) Benichou, J., Fears, T. and Gail, M. (1996). A reminder of the fallibility of the Wald statistic. _American Statistician_**50**, 226-227.
* Bening (2000) Bening, V. (2000). _Asymptotic Theory of Testing Statistical Hypotheses: Efficient Statistics, Optimality, Power Loss, and Deficiency._ VSP Publishing, The Netherlands.
* Benjamini (1983) Benjamini, Y. (1983). Is the \(t\)-test really conservative when the parent distribution is long-tailed? _Journal of the American Statistical Association_**78**, 645-654.

* Benjamini and Hochberg (1995) Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing, _Journal of the Royal Statistical Society Series B_**57**, 289-300.
* Benjamini and Yekutieli (2001) Benjamini, Y. and Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. _Annals of Statistics_**29**, 1165-1189.
* Bennett (1957) Bennett, B. (1957). On the performance characteristic of certain methods of determining confidence limits. _Sankhya_**18**, 1-12.
* Bentkus (2003) Bentkus, V. (2003). On the dependence of the Berry-Esseen bound on dimension. _Journal of Statistical Planning and Inference_**113**, 385-402.
* Bera and Bilias (2001) Bera, A. and Bilias, Y. (2001). Rao's score, Neyman's \(C(\alpha)\) and Silvey's LM tests: an essay on historical developments and some new results. _Journal of Statistical Planning and Inference_**97**, 9-44.
* Beran (1974) Beran, R. (1974). Asymptotically efficient adaptive rank estimates in location models. _Annals of Statistics_**2**, 63-74.
* Beran (1977) Beran, R. (1977). Minimum Hellinger distance estimates for parametric models. _Annals of Statistics_**5**, 445-463.
* Beran (1984) Beran, R. (1984). Bootstrap methods in statistics. _Jahresberichte des Deutschen Mathematischen Vereins_**86**, 14-30.
* Beran (1986) Beran, R. (1986). Simulated power function. _Annals of Statistics_**14**, 151-173.
* Beran (1987) Beran, R. (1987). Prepivoting to reduce level error of confidence sets. _Biometrika_**74**, 151-173.
* Beran (1988a) Beran, R. (1988a). Balanced simultaneous confidence sets. _Journal of the American Statistical Association_**83**, 679-686.
* Beran (1988b) Beran, R. (1988b). Prepivoting test statistics: a bootstrap view of asymptotic refinements. _Journal of the American Statistical Association_**83**, 687-697.
* Beran (1995) Beran, R. (1995). Stein confidence sets and the bootstrap. _Statistica Sinica_**5**, 109-127.
* Beran (1999) Beran, R. (1999). Hajek-Inagaki convolution theorem. In _Encyclopedia of Statistical Sciences_, Update **3**, 294-297. John Wiley, New York.
* Beran and Ducharme (1991) Beran, R. and Ducharme, G. (1991). _Asymptotic Theory for Bootstrap Methods in Statistics_. Centre de recherches mathematiques, University of Montreal, Quebec.
* Beran and Millar (1986) Beran, R. and Millar, W. (1986). Confidence sets for a multivariate distribution. _Annals of Statistics_**14**, 431-443.
* Beran and Millar (1988) Beran, R. and Millar, W. (1988). A stochastic minimum distance test for multivariate parametric models. _Annals of Statistics_**17**, 125-140.
* Beran and Srivastava (1985) Beran, R. and Srivastava, M. S. (1985). Bootstrap tests and confidence regions for functions of a covariance matrix. _Annals of Statistics_**13**, 95-115.
* Berger (1951a) Berger, A. (1951a). On uniformly consistent tests. _Annals of Mathematical Statistics_**22**, 289-293.
* Berger (1951b) Berger, A. (1951b). Remark on separable spaces of probability measures. _Annals of Mathematical Statistics_**22**, 119-120.
* Berger (1985a) Berger, J. (1985a). _Statistical Decision Theory and Bayesian Analysis_, 2nd edition. Springer, New York.
* Berger (1985b) Berger, J. (1985b). The frequentist viewpoint of conditioning. In _Proc. Berkeley Conf. in Honor of J. Neyman and J. Kiefer_ (Le Cam and Olshen, eds.), Wadsworth, Belmont, Calif.

* Berger (2003) Berger, J. (2003). Could Fisher, Jeffreys and Neyman have aged on testing? (with discussion). _Statistical Science_**18**, 1-32.
* Berger et al. (1997) Berger, J., Boukai, B. and Wang, Y. (1997). United frequentist and Bayesian testing of a precise hypothesis (with discussion). _Statistical Science_**12**, 133-160.
* Berger et al. (1994) Berger, J., Brown, L. D. and Wolpert, R. (1994). A unified conditional frequentist and Bayesian test for fixed and sequential simple hypothesis testing. _Annals of Statistics_**22**, 1787-1807.
* Berger et al. (1999) Berger, J., Liseo, B. and Wolpert, R. (1999). Integrated likelihood methods for eliminating nuisance parameters (with discussion). _Statistical Science_**14**, 1-28.
* Berger and Sellke (1987) Berger, J. and Sellke, T. (1987). Testing a point null-hypothesis: The irreconcilability of significance levels and evidence. _Journal of the American Statistical Association_**82**, 112-122.
* Berger and Wolpert (1988) Berger, J. and Wolpert, R. (1988). _The Likelihood Principle_, 2nd edition, IMS Lecture Notes-Monograph Series, Hayward, CA.
* Berger (1982) Berger, R. (1982). Multiparameter hypothesis testing and acceptance sampling. _Technometrics_, **24**, 295-300.
* Berger (1989) Berger, R. (1989). Uniformly more powerful tests for hypotheses concerning linear inequalities and normal means. _Journal of the American Statistical Association_**84**, 192-199.
* Berger and Boos (1994) Berger, R. and Boos, D. (1994). \(p\)-values maximized over a confidence set for the nuisance parameter. _Journal of the American Statistical Association_**89**, 1012-1016.
* Berger and Hsu (1996) Berger, R. and Hsu, J. (1996). Bioequivalence trials, intersection-union tests and equivalence confidence sets (with discussion). _Statistical Science_**11**, 283-319.
* Berk (1970) Berk, R. (1970). A remark on almost invariance. _Annals of Mathematical Statistics_**41**, 733-735.
* Berk and Bickel (1968) Berk, R. and Bickel, P. (1968). On invariance and almost invariance. _Annals of Mathematical Statistics_**39**, 1573-1576.
* Berk and Cohen (1979) Berk, R. and Cohen, A. (1979). Asymptotically optimal methods of combining tests. _Journal of the American Statistical Association_**74**, 812-814.
* Berk et al. (1996) Berk, R., Nogales, A. and Oyola, J. (1996). Some counterexamples concerning sufficiency and invariance. _Annals of Statistics_**24**, 902-905.
* Bernardo and Smith (1994) Bernardo, J. and Smith, A. (1994). _Bayesian Theory_. New York, John Wiley.
* Bernoulli (1734) Bernoulli, D. (1734). Quelle est la cause physique de l'inclinaison des plans des orbites des planetes par rapport au plan de l'equateur de la revolution du soleil autour de son axe; Et d'ou vient que les inclinaisons de ces orbites sont differentes entre elles. _Recueil des Pieces qui ont Remporte le Prix de l'Academie Royale des Sciences_**3**, 93-122.
* Bhat and Miller (2002) Bhat, U. and Miller, G. (2002). _Elements of Applied Stochastic Processes_, 3rd edition, John Wiley, New York.
* Bhattacharya et al. (1982) Bhattacharya, P. K., Gastwirth, J. L., and Wright, A. L. (1982). Two modified Wilcoxon tests for symmetry about an unknown location parameter. _Biometrika_**69**, 377-382.
* Bhattacharya and Ghosh (1978) Bhattacharya, R. and Ghosh, J. (1978). On the validity of the formal Edgeworth expansion. _Annals of Statistics_**6**, 434-451.
* Bhattacharya and Rao (1976) Bhattacharya, R. and Rao, R. (1976). _Normal Approximation and Asymptotic Expansions_. John Wiley, New York.
* Bickel (1974) Bickel, P. (1974). Edgeworth expansions in nonparametric statistics. _Annals of Statistics_**2**, 1-20.

* Bickel (1982) Bickel, P. (1982). On adaptive estimation. _Annals of Statistics_**10**, 647-671.
* Bickel (1984) Bickel, P. (1984). Parametric robustness: small biases can be worthwhile. _Annals of Statistics_**12**, 864-879.
* Bickel and Doksum (1981) Bickel, P. and Doksum, K. A. (1981). An analysis of transformations revisited. _Journal of the American Statistical Association_**76**, 296-311.
* Bickel and Doksum (2001) Bickel, P. and Doksum, K. A. (2001). _Mathematical Statistics_, volume I, 2nd edition. Prentice Hall, Upper Saddle River, New Jersey.
* Bickel and Freedman (1981) Bickel, P. and Freedman, D. (1981). Some asymptotic theory for the bootstrap. _Annals of Statistics_**9**, 1196-1217.
* a Bayesian argument. _Annals of Statistics_**18**, 1070-1090.
* Bickel et al. (1997) Bickel, P., Gotze, F. and van Zwet, W. R. (1997). Resampling fewer than \(n\) observations: Gains, losses, and remedies for losses. _Statistica Sinica_**7**, 1-31.
* Bickel et al. (1993) Bickel, P., Klaassen, C., Ritov, Y., and Wellner, J. (1993). _Efficient and Adaptive Estimation for Semiparametric Models_. The John Hopkins University Press, Baltimore, MD.
* Bickel and Van Zwet (1978) Bickel, P. and Van Zwet, W. R. (1978). Asymptotic expansions for the power of distribution free tests in the two-sample problem. _Annals of Statistics_**6**, 937-1004.
* Billingsley (1961) Billingsley, P. (1961). Statistical methods in Markov chains. _Annals of Mathematical Statistics_**32**, 12-40.
* Billingsley (1968) Billingsley, P. (1968). _Convergence of Probability Measures_. John Wiley, New York.
* Billingsley (1995) Billingsley, P. (1995). _Probability and Measure_, 3rd edition. John Wiley, New York.
* Birch (1964) Birch, M. W. (1964). The detection of partial association, I The 2 x 2 case. _Journal of the Royal Statistical Society Series B_, **26**, 313-324.
* Birnbaum (1954a) Birnbaum, A. (1954a). Statistical methods for Poisson processes and exponential populations. _Journal of the American Statistical Association_ **49**, 254-266.
* Birnbaum (1954b) Birnbaum, A. (1954b). Admissible test for the mean of a rectangular distribution. _Annals of Mathematical Statistics_**25** 157-161.
* Birnbaum (1955) Birnbaum, A. (1955). Characterization of complete classes of tests of some multiparameter hypotheses, with applications to likelihood ratio tests. _Annals of Mathematical Statistics_**26**, 21-36.
* Birnbaum (1962) Birnbaum, A. (1962). On the foundations of statistical inference (with discussion). _Journal of the American Statistical Association_**57**, 269-326.
* Birnbaum (1952) Birnbaum Z. W. (1952). Numerical tabulation of the distribution of Kolmogorov's statistic for finite sample size. _Journal of the American Statistical Association_**47**, 431.
* Birnbaum and Chapman (1950) Birnbaum, Z. W. and Chapman, D. G. (1950). On optimum selections from multinormal populations. _Annals of Mathematical Statistics_**21**, 433-447. [Problem 3.51]
* Bishop et al. (1975) Bishop, Y. M. M., Fienberg, S. E., and Holland, P. W. (1975). _Discrete Multivariate Analysis: Theory and Practice_, MIT. Press, Cambridge, Mass.
* Blackwell (1951) Blackwell, D. (1951). On a theorem of Lyapunov. _Annals of Mathematical Statistics_**22**, 112-114.
* Blackwell and Dubins (1975) Blackwell, D. and Dubins, L. E. (1975). On existence and non-existence of proper, regular conditional distributions. _Annals Probability_**3**, 741-752.

Blackwell, D. and Girshick, M. A. (1954). _Theory of Games and Statistical Decisions_. John Wiley, New York.
* Blackwell and Ramamoorth (1982) Blackwell, D. and Ramamoorthi, R. V. (1982). A Bayes but not classically sufficient statistic. _Annals of Statistics_**10**, 1025-1026.
* Blair and Higgins (1980) Blair, R. C. and Higgins, J. J. (1980). A comparison of the power of Wilcoxon's rank-sum statistic to that of Student's _t_-statistic under various nonnormal distributions. _Journal of Educational Statistics_**5**, 309-335.
* Blanchard and Roquain (2009) Blanchard, G. and Roquain, E. (2009). Adaptive false discovery rate control under independence and dependence. _Journal of Machine Learning Research_**10**, 2837-2871.
* Blyth (1970) Blyth, C. R. (1970). On the inference and decision models of statistics (with discussion). _Annals of Statistics_**41**, 1034-1058.
* Blyth (1986) Blyth, C. R. (1986). Approximate binomial confidence limits. _Journal of the American Statistical Association_**81**, 843-855.
* Blyth and Hutchinson (1960) Blyth, C. R. and Hutchinson, D. W. (1960). Tables of Neyman--shortest confidence intervals for the binomial parameter. _Biometrika_**47**, 481-491.
* Blyth and Staudte (1995) Blyth, C.R. and Staudte, R. (1995). Estimating statistical hypotheses. _Statistics and Probability Letters_**23**, 45-52.
* Blyth and Staudte (1997) Blyth, C.R. and Staudte, R. (1997). Hypothesis estimates and acceptability profiles for 2 x 2 contingency tables. _Journal of the American Statistical Association_**92**, 694-699.
* Blyth and Still (1983) Blyth, C. R. and Still, H. A. (1983). Binomial confidence intervals. _Journal of the American Statistical Association_**78**, 108-116.
* Bohrer (1973) Bohrer, R. (1973). An optimality property of Scheffe bounds. _Annals of Statistics_**1**, 766-772.
* Bondar (1977) Bondar, J. V. (1977). A conditional confidence principle. _Annals of Mathematical Statistics_**5**, 881-891.
* Bondar and Milnes (1981) Bondar, J. V. and Milnes, P. (1981). Amenability: A survey for statistical applications of Hunt-Stein and related conditions on groups. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_**57**, 103-128.
* Bondar and Milnes (1982) Bondar, J. V. and Milnes, P. (1982). A converse to the Hunt-Stein theorem. Unpublished.
* Bondessen (1983) Bondessen, L. (1983). Equivariant estimators. in _Encyclopedia of Statistical Sciences_, Vol. 2. John Wiley, New York.
* Boos (1982) Boos, D. (1982). A test for asymmetry associated with the Hodges-Lehmann estimator. _Journal of the American Statistical Association_**77**, 647-651.
* Boos and Brownie (1989) Boos, D. and Brownie, C. (1989). Bootstrap methods for testing homogeneity of variances. _Technometrics_**31**, 69-82.
* Boos and Hughes-Oliver (1998) Boos, D. and Hughes-Oliver, J. (1998). Applications of Basu's theorem. _American Statistician_**52**, 218-221.
* Boschloo (1970) Boschloo, R. D. (1970). Raised conditional level of significance for the 2 x 2 table when testing the equality of two probabilities. _Statistica Neerlandica_**24**, 1-35.
* Bowker (1948) Bowker, A. H. (1948). A test for symmetry in contingency tables. _Journal of the American Statistical Association_**43**, 572-574.
* Box (1953) Box, G. E. P. (1953). Non-normality and tests for variances. _Biometrika_**40**, 318-335.

* Box and Andersen (1955) Box, G. E. P. and Andersen, S. L. (1955). Permutation theory in the derivation of robust criteria and the study of departures from assumptions. _Journal of the Royal Statistical Society Series B_**17**, 1-34.
* Box and Cox (1964) Box, G. E. P. and Cox, D. R. (1964). An analysis of transformations. _Journal of the Royal Statistical Society Series B_**26**, 211-252.
* Box and Cox (1982) Box, G. E. P. and Cox, D. R. (1982). An analysis of transformations revisited, rebutted. _Journal of the American Statistical Association_**77**, 209-210.
* Box et al. (1978) Box, G. E. P., Hunter, W. G., and Hunter, J. S. (1978). _Statistics for Experimenters_. John Wiley, New York.
* Box and Tiao (1964) Box, G. E. P. and Tiao, G. C. (1964). A note on criterion robustness and inference robustness. _Biometrika_**51**, 169-173.
* Box and Tiao (1973) Box, G. E. P. and Tiao, G. C. (1973). _Bayesian Inference in Statistical Analysis_. Addison-Wesley, Reading, Mass.
* Box (1978) Box, J. F. (1978). _R. A. Fisher: The Life of a Scientist_. John Wiley, New York.
* Bradley (2007) Bradley, R.D. (2007). _Introduction to Strong Mixing Conditions, Vols. 1, 2, and 3_. Kendrick Press, Heber City, Utah.
* Brain and Shapiro (1983) Brain, C. W. and Shapiro, S. S. (1983). A regression test for exponentiality: Censored and complete samples. _Technometrics_**25**, 69-76.
* Braun (1994) Braun, H. (Ed.) (1994). _The collected works of John W. Tukey: Vo. VIII Multiple comparisons: 1948-1983_. Chapman & Hall, New York.
* Bretagnolle (1983) Bretagnolle, J. (1983). Limites du bootstrap de ceraines fonctionnelles. _Annals of the Institute Henri Poincare_**3**, 281-296.
* Brockwell and Davis (1991) Brockwell, P. J. and Davis, R. A. (1991). _Time Series: Theory and Models_, 2nd edition. Springer, New York.
* Broemeling (1985) Broemeling, L. D. (1985). _Bayesian Analysis of Linear Models_. Marcel Dekker, New York.
* Bross and Kasten (1957) Bross, I. D. J. and Kasten, E. L. (1957). Rapid analysis of \(2\times 2\) tables. _Journal of the American Statistical Association_**52**, 18-28.
* Brown (1984) Brown, K. G. (1984). On analysis of variance in the mixed model. _Annals of Statistics_**12**, 1488-1499.
* Brown (1964) Brown, L. D. (1964). Sufficient statistics in the case of independent random variables. _Annals of Mathematical Statistics_**35**, 1456-1474.
* Brown (1966) Brown, L. D. (1966). On the admissibility of invariant estimators of one or more location parameters. _Annals of Mathematical Statistics_**37**, 1087-1136.
* Brown (1967) Brown, L. D. (1967). The conditional level of Student's \(t\)-test. _Annals of Mathematical Statistics_**38**, 1068-1071.
* Brown (1978) Brown, L. D. (1978). An extension of Kiefer's theory of conditional confidence procedures. _Annals of Statistics_**6**, 59-71.
* Brown (1986) Brown, L. D. (1986). _Fundamentals of Statistical Exponential Families (With Application to Statistical Decision Theory)_. Institute of Statistical Mathematics Lecture Notes Monograph Series, **9**, Hayward, CA.
* Brown (1990) Brown, L. D. (1990). An ancillarity paradox which appears in multiple linear regression (with discussion). _Annals of Statistics_**18**, 471-538.

Brown, L. D. (1994). Minimaxity, more or less. In _Statistical Decision Theory and Related Topics V_, Gupta and Berger (eds.), 1-18. Springer-Verlag, New York.
* Brown (2000) Brown, L. D. (2000). Statistical decision theory. _Journal of the American Statistical Association_**95**, 1277-1281.
* Brown et al. (2001) Brown, L. D., Cai, T. and DasGupta, A. (2001). Interval estimation for a binomial proportion. _Statistical Science_**16**, 101-133.
* Brown et al. (2002) Brown, L. D., Cai, T. and DasGupta, A. (2002). Confidence intervals for a binomial proportion and asymptotic expansions. _Annals of Statistics_**30**, 160-201.
* Brown et al. (1995) Brown, L. D., Casella, G. and Hwang, J. (1995). Optimal confidence sets, bioequivalence, and the limacon of Pascal. _Journal of the American Statistical Association_**90**, 880-889.
* Brown et al. (1976) Brown, L. D., Cohen, A., and Strawderman, W. E. (1976). A complete class theorem for strict monotone likelihood ratio with applications. _Annals of Statistics_**4**, 712-722.
* Brown et al. (1981) Brown, L. D., Johnstone, I. M. and MacGibbon, K. G. (1981). Variation diminishing transformations: A direct approach to total positivity and its statistical applications. _Journal of the American Statistical Association_**76**, 824-832.
* Brown et al. (1997) Brown, L. D., Hwang, J. and Munk, A. (1997). An unbiased test for the bioequivalence problem. _Annals of Statistics_**25**, 2345-2367.
* Brown and Marden (1989) Brown, L. D. and Marden, J. (1989). Complete class results for hypothesis testing problems with simple null hypotheses. _Annals of Statistics_**17**, 209-235.
* Brown and Sackrowitz (1984) Brown, L. D. and Sackrowitz, H. (1984). An alternative to Student's \(t\)-test for problems with indifference zones. _Annals of Statistics_**12**, 451-469.
* Brown and Forsythe (1974a) Brown, M. B. and Forsythe, A. (1974a). The small sample behavior of some statistics which test the equality of several means. _Technometrics_**16**, 129-132.
* Brown and Forsythe (1974b) Brown, M. B. and Forsythe, A. (1974b). Robust tests for the equality of variances. _Journal of the American Statistical Association_**69**, 364-367.
* Brownie and Kiefer (1977) Brownie, C. and Kiefer, J. (1977). The ideas of conditional confidence in the simplest setting. _Comm. Statist._**A6**(10.8), 691-751.
* Buehler (1959) Buehler, R. (1959). Some validity criteria for statistical inferences. _Annals of Mathematical Statistics_**30**, 845-863. [The first systematic treatment of relevant subsets, including Example 10.4.1.]
* Buehler (1982) Buehler, R. (1982). Some ancillary statistics and their properties. _Journal of the American Statistical Association_**77**, 581-589. [A review of the principal examples of ancillaries.]
* Buehler (1983) Buehler, R. (1983). Fiducial inference. In _Encyclopedia of Statistical Sciences_, Vol. 3, John Wiley, New York
* Buehler and Feddersen (1963) Buehler, R. and Feddersen, A. P. (1963). Note on a conditional property of Student's \(t\). _Annals of Mathematical Statistics_**34**. 1098-1100.
* Bugni et al. (2018) Bugni, F., Canay, I. and Shaikh, A. (2018). Inference under covariate-adaptive randomization. _Journal of the American Statistical Association_**113**, 1784-1796.
* Buhlmann and van de Geer (1961) Buhlmann, P. and van de Geer, S. _Statistics for High-Dimensional Data_. Springer-Verlag, Berlin.
* Burkholder (1961) Burkholder, D. L. (1961). Sufficiency in the undominated case. _Annals of Mathematical Statistics_**32**, 1191-1200.
* Cabana and Cabana (1997) Cabana, A. and Cabana, E. (1997). Transformed empirical processes and modified Kolmogorov-Smirnov tests for multivariate distributions. _Annals of Statistics_**25**, 2388-2409.

* Canay _et al._ (2017) Canay, I., Romano, J. and Shaikh, A. (2017). Randomization tests under an approximate symmetry assumption, _Econometrica_, **85**, 1013-1030. Supplement to "Randomization tests under an approximate symmetry assumption", _Econometrica Supplemental Material_, **85**, [http://dx.doi.org/10.3982/ECTA12974](http://dx.doi.org/10.3982/ECTA12974).
* Canay and Shaikh (2017) Canay, I. and Shaikh, A. (2017). Practical and theoretical advances for inference in partially identified models. _Advances in Economics and Econometrics: 11th World Congress (Econometric Society Monographs)_, B. Honore, A. Pakes, M. Piazzesi and L. Samuelson, eds., 271-306.
* Casella (1987) Casella, G. (1987). Conditionally acceptable recentered set estimators. _Annals of Statistics_**15**, 1363-1371.
* Casella (1988) Casella, G. (1988). Conditionally acceptable frequentist solutions (with discussion). In _Statistical Decision Theory and Related Topics IV_**1**, 73-117.
* Castillo and Puig (1999) Castillo, J. and Puig, P. (1999). The best test of exponentiality against singly truncated normal alternatives. _Journal of the American Statistical Association_**94**, 529-532.
* Chambers and Cox (1967) Chambers, E. A. and Cox, D. R. (1967). Discrimination between alternative binary response models. _Biometrika_**54**, 573-578.
* Chatterjee (2014) Chatterjee, S. (2014). A short survey of Stein's method. In _Proceedings of the International Congress of Mathematicians_, **IV**, 1-24.
* Chatterjee and Hadi and Price (2000) Chatterjee, S., Hadi, A. and Price, B. (2000). _Regression Analysis By Example_, 3rd edition. John Wiley, New York.
* Chebychev (1890) Chebychev, P. (1890). Sur deux theoremes relatifs aux probabilities. _Acta. Math._**14**, 305-315.
* Chen (1995) Chen, L. (1995). Testing the mean of skewed distributions. _Journal of the American Statistical Association_**90**, 767-772.
* Chen _et al._ (2011) Chen, L., Goldstein, L. and Shao, Q. (2011). _Normal Approximation by Stein's Method_. Springer, New York.
* Chernoff (1949) Chernoff, H. (1949). Asymptotic studentization in testing of hypotheses. _Annals of Mathematical Statistics_**20**, 268-278.
* Chernoff (1954) Chernoff, H. (1954). On the distribution of the likelihood ratio statistic. _Annals of Mathematical Statistics_**25**, 579-586.
* Chernoff and Lehmann (1954) Chernoff, H. and Lehmann, E. L. (1954). The use of maximum likelihood estimates in \(\chi^{2}\) goodness of fit. _Annals of Mathematical Statistics_**25**, 579-586.
* Chernozhukov _et al._ (2017) Chernozhukov, V., Chetverikov, D. and Kato, K. (2017). Central limit theorems and bootstrap in high dimensions. _Annals of Probability_**45**, 2309-2352.
* Chhikara (1975) Chhikara, R. S. (1975). Optimum tests for the comparison of two inverse Gaussian distribution means. _Australian Journal of Statistics_**17**, 77-83.
* Chhikara and Folks (1976) Chhikara, R. S. and Folks, J. L. (1976). Optimum test procedures for the mean of first passage time distribution in Brownian motion with positive drift. _Technometrics_**18**, 189-193.
* Chmielewski (1981) Chmielewski, M. A. (1981). Elliptically symmetric distributions: A review and bibliography. _International Statistical Review_**49**, 67-74.
* Choi and Marden (1997) Choi, K. and Marden, J. (1997). An approach to multivariate rank tests in multivariate analysis of variance. _Journal of the American Statistical Association_**92**, 1581-1590.
* Choi _et al._ (1996) Choi, S., Hall, W. and Schick, A. (1996). Asymptotically uniformly most powerful tests in parametric and semiparametric models. _Annals of Statistics_**24**, 841-861.

- Theory and Methods_**13**, 2673-2678.
* Christensen (1989) Christensen, R. (1989). Lack-of-fit tests based on near or exact replicates. _Annals of Statistics_**17**, 673-683.
* Christensen (2000) Christensen, R. (2000). Linear and loglinear models. _Journal of the American Statistical Association_**95**, 1290-1293.
* Chung and Romano (2013) Chung, E. and Romano, J. P. (2013). Exact and asymptotically robust permutations tests. _The Annals of Statistics_**41**, 484-507.
* Chung and Romano (2016a) Chung, E. and Romano, J. P. (2016a). Multivariate and Multiple Permutation Tests, _Journal of Econometrics_, 76-91.
* Chung and Romano (2016b) Chung, E. and Romano, J. P. (2016b). Asymptotically valid and exact permutation tests based on two-sample \(U\)-statistics, _Journal of Statistical Planning and Inference_, 97-105.
* Theory and Methods_**A5**, 875-882.
* Clinch and Kesselman (1982) Clinch, J. C. and Kesselman, H. J. (1982). Parametric alternatives to the analysis of variance. _Journal of Educational Statistics_**7**, 207-214.
* Clarke et al. (2020) Clarke, D., Romano, J. and Wolf, M. (2020). The Romano-Wolf multiple hypothesis correction in Stata, _Stata Journal_, to appear.
* Clopper and Pearson (1934) Clopper, C. J. and Pearson, E. S. (1934). The use of confidence or fiducial limits illustrated in the case of the binomial. _Biometrika_**26**, 404-413.
* Cochran (1968) Cochran, W. G. (1968). Errors of measurement in statistics. _Technometrics_**10**, 637-666.
* Cohen (1972) Cohen, A. (1972). Improved confidence intervals for the variance of a normal distribution. _Journal of the American Statistical Association_**67**, 382-387.
* Cohen et al. (1983) Cohen, A., Gatsonis, C., and Marden, J. (1983). Hypothesis tests and optimality properties in discrete multivariate analysis. In _Studies in Econometrics, Time Series, and Multivariate Statistics_ (Karlin et al., eds.), 379-405. Academic Press, New York.
* Cohen et al. (1994) Cohen, A., Kemperman, J. and Sackrowitz, H. (1994). Unbiased testing in exponential family regression. _Annals of Statistics_**22**, 1931-1946.
* Cohen and Marden (1989) Cohen, A. and Marden, J. (1989). On the admissibility and consistency of tests for homogeneity of variances. _Annals of Statistics_**17**, 236-251.
* Cohen and Miller (1976) Cohen, A. and Miller, J. (1976). Some remarks on Scheffes two-way mixed model. _American Statistician_**30**, 36-37.
* Cohen and Sackrowitz (1975) Cohen, A. and Sackrowitz, H. (1975). Unbiasedness of the chi-square, likelihood ratio and other goodness of fit tests for the equal cell case. _Annals of Statistics_**3**, 959-964.
* Cohen and Sackrowitz (1992) Cohen, A. and Sackrowitz, H. (1992). Improved tests for comparing treatments against a control and other one-sided problems. _Journal of the American Statistical Association_**87**, 1137-1144.
* Cohen and Strawderman (1973) Cohen, A. and Strawderman, W. E. (1973). Admissibility implications for different criteria in confidence estimation. _Annals of Statistics_**1**, 363-366.
* Cohen (1962) Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. _J. Abnormal and Soc. Psychology_**65**, 145-153.

Cohen, J. (1977). _Statistical Power Analysis for the Behavioral Sciences_, revised edition. Academic Press, New York. [Advocates the consideration of power attainable against the alternatives of interest, and provides the tables needed for this purpose for some of the most common tests.]
* Cohen (1958) Cohen, L. (1958). On mixed single sample experiments. _Annals of Mathematical Statistics_**29**, 947-971.
* Conover et al. (1981) Conover, W. J., Johnson, M. E. and Johnson, M. M. (1981). A comparative study of tests for homogeneity of variances, with applications to the outer continental shelf bidding data. _Technometrics_**23**, 351-361.
* Cox (1958) Cox, D. R. (1958). Some problems connected with statistical inference. _Annals of Mathematical Statistics_**29**, 357-372.
* Cox (1959) Cox, D. R. (1959). _Planning of Experiments_. John Wiley, New York.
* Cox (1961) Cox, D. R. (1961). Tests of separate families of hypotheses. In _Proc. 4th Berkeley Symp._, Vol. 1, 105-123.
* Cox (1962) Cox, D. R. (1962). Further results on tests of separate families of hypotheses. _Journal of the Royal Statistical Society Series B_**24**, 406-423.
* Cox (1966) Cox, D. R. (1966). A simple example of a comparison involving quantal data. _Biometrika_**53**, 215-220.
* Cox (1970) Cox, D. R. (1970). _The Analysis of Binary Data_, Methuen, London. [An introduction to the problems treated in Sections 4.6-4.7 and some of their extensions.]
* Cox (1971) Cox, D. R. (1971). The choice between ancillary statistics. _Journal of the Royal Statistical Society_ (_B_) **33**, 251-255.
* Cox (1977) Cox, D. R. (1977). The role of significance tests. _Scandinavian Journal of Statistics_**4**, 49-62.
* Cramer (1928) Cramer, H. (1928). On the composition of elementary errors. _Skand. Aktuarietidskr._**11**, 13-74, 141-186.
* Cramer (1937) Cramer, H. (1937). _Random Variables and Probability Distributions._ Cambridge University Press, Cambridge.
* Cramer (1946) Cramer, H. (1946). _Mathematical Methods of Statistics_. Princeton University Press.
* Cressie (1980) Cressie, N. (1980). Relaxing assumptions in the one-sample \(t\)-test. _Australian Journal of Statistics_**22**, 143-153.
* Csorgo and Mason (1989) Csorgo, S. and Mason, D. (1989). Bootstrap empirical functions. _Annals of Statistics_**17**, 1447-1471.
* Cvitanic and Karatzas (2001) Cvitanic, J. and Karatzas, I. (2001). Generalized Neyman-Pearson lemma via convex duality. _Bernoulli_**7**, 79-97.
* Theory and Methods_**11**, 1671-1680.
* D'Agostino (1982) D'Agostino, R. (1982). Departures from normality, tests for. In _Encycl. Statist. Sci._ Vol. 2. John Wiley, New York.
* D'Agostino and Stephens (1986) D'Agostino, R. and Stephens, M. A. (1986). _Goodness-of-Fit Techniques_. Marcel Dekker, New York.
* Dantzig and Wald (1951) Dantzig, G. B. and Wald, A. (1951). On the fundamental lemma of Neyman and Pearson. _Annals of Mathematical Statistics_**22**, 87-93.

Darmois, G. (1935). Sur les lois de probabilite a estimation exhaustive. _C.R. Acad. Sci. Paris_**260**, 1265-1266.
* DasGupta (1991) DasGupta, A. (1991). Diameter and volume minimizing confidence sets in Bayes and classical problems. _Annals of Statistics_**19**, 1225-1243.
* Davenport and Webster (1975) Davenport, J. M. and Webster, J. T. (1975). The Behrens-Fisher problem. An old solution revisited. _Metrika_**22**, 47-54.
* David (1981) David, H. A. (1981). _Order Statistics_, 2nd edition. John Wiley, New York.
* Davison and Hinkley (1997) Davison, A. and Hinkley, D. (1997). _Bootstrap Methods and their Application_. Cambridge University Press, Cambridge.
* Davydov (1970) Davydov, Y. (1970). The invariance principle for stochastic processes. _Theory of Probability and its Applications_**14**, 487-498.
* Dawid (1975) Dawid, A. P. (1975). On the concepts of sufficiency and ancillarity in the presence of nuisance parameters. _Journal of the Royal Statistical Society Series B_**37**, 248-258.
* Dawid (1977) Dawid, A. P. (1977). Discussion of Wilkinson: On resolving the controversy in statistical inference. _Journal of the Royal Statistical Society_**39**, 151-152. [Problem 10.12.]
* Dayton (2003) Dayton, C. (2003). Information criteria for pairwise comparisons. _Psychological Methods_**8**, 61-71.
* Dedecker et al. (1990) Dedecker, J., Doukhan, P., Lang, G., Leon, J. Louhichi, S. and Prieur, C. _Weak Dependence: Models, Theory, and Applications. Lecture Notes in Statistics_**190**, Springer-Verlag, New York.
* DeGroot (1986) DeGroot, M. (1986). A conversation with Erich L. Lehmann. _Statistical Science_**1**, 243-258.
* de Leeuw (1992) de Leeuw, J. (1992). Introduction to Akaike's (1973) paper "Information theory and an extension of the maximum likelihood principle". Appeared in _Breakthroughs in Statistics_, volume I, Kotz, S. and Johnson, N. L. eds., Springer-Verlag, New York.
* de Moivre (1733) de Moivre, A. (1733). _The Doctrine of Chances_, 3rd edition (1756) has been reprinted by Chelsea, New York (1967).
* Dempster (1958) Dempster, A. P. (1958). A high dimensional two-sample significance test. _Annals of Mathematical Statistics_**29**, 995-1010.
* Deshpande (1983) Deshpande, J. V. (1983). A class of tests for exponentiality against increasing failure rate average alternatives. _Biometrika_**70**, 514-518.
* Deuchler (1914) Deuchler, G. (1914). Ueber die Methoden der Korrelationsrechnung in der Paedagogik und Psychologie. _Z. Padag. Psychol._**15**, 114-131, 145-159, 229-242.
* Devroye (1986) Devroye, L. (1986). _Non-Uniform Random Variate Generation._ Springer-Verlag, New York.
* de Wet and Randles (1987) de Wet, T. and Randles, R. (1987). On the effect of substituting parameter estimators in limiting \(\chi^{2}\)\(U\) and \(V\) statistics. _Annals of Statistics_**15**, 398-412.
* Diaconis (1988) Diaconis, P. (1988). _Group representations in probability and statistics._ IMS Lecture Notes, **11**, Institute of Statistical Mathematics, Hayward, CA.
* Diaconis and Efron (1985) Diaconis, P. and Efron, B. (1985). Testing for independence in a two-way table. New interpretations of the chi-square statistic (with discussion). _Annals of Statistics_**13**, 845-913.
* Diaconis and Holmes (1994) Diaconis, P. and Holmes, S. (1994). Gray codes for randomization procedures. _Statistics and Computing_**4**, 287-302.
* DiCiccio and Romano (2017) DiCiccio, C. and Romano, J. (2017). Robust permutation tests for correlation and regression coefficients. _Journal of the American Statistical Association._, **112**, 1211-1220. Supplement to Robust permutation tests for correlation and regression coefficients. _JASA_, electronically available [http://dx.doi.org/10.1080/01621459.2016.1202117](http://dx.doi.org/10.1080/01621459.2016.1202117)
* DiCiccio et al. (1991) DiCiccio, T., Hall, P., and Romano, J. P (1991). Empirical likelihood is Bartlett-correctable. _Annals of Statistics_**19**, 1053-1061.
* DiCiccio and Romano (1989) DiCiccio, T. and Romano, J. P. (1989). The automatic percentile method: accurate confidence limits in parametric models. _Canadian Journal of Statistics_**17**, 155-169.
* DiCiccio and Romano (1990) DiCiccio, T. and Romano, J. (1990). Nonparametric confidence limits by resampling and least favorable distributions. _International Statistical Review_**58**, 59-76.
* DiCiccio and Stern (1994) DiCiccio, T. and Stern, S. (1994). Frequentist and Bayesian Bartlett correction of test statistics based on adjusted profile likelihoods. _Journal of the Royal Statistical Society Series B_**56**, 397-408.
* Dickhaus (2014) Dickhaus, T. (2014). _Simultaneous Statistical Inference_, Springer-Verlag, Berlin.
* Ding (2017) Ding, P. (2017). A paradox from randomization-based causal inference (with Discussion). _Statistical Science_**32**, 331-345.
* Ding et al. (2016) Ding, P., Feller, A. and Miratrix, L. (2016). Randomization inference for treatment effect variation. _J. R. Statist. Soc., Series B_, **78**, 655-671.
* Dobson (1990) Dobson, A. (1990). _An Introduction to Generalized Linear Models_. Chapman & Hall, London.
* Doksum and Yandell (1984) Doksum, K. A. and Yandell, B. S. (1984). Tests for exponentiality. In _Handbook of Statistics_ (Krishnaiah and Sen, editors), Vol. 4, 579-611.
* Donoghue (2004) Donoghue, J. (2004). Implementing Shaffer's multiple comparison procedure for a large number of groups. To appear in _Recent Developments in Multiple Comparison Procedures_, IMS Lecture Notes Monograph Series.
* Donoho (1988) Donoho, D. (1988). One-sided inference about functionals of a density. _Annals of Statistics_**16**, 1390-1420.
* Donoho and Jin (2004) Donoho, D. and Jin, J. (2004). Higher criticism for detecting sparse heterogeneous mixtures. _Ann. Statist._**32**, 962-994.
* Donoho and Jin (2015) Donoho, D. and Jin, J. (2015). Higher Criticism for large-scale inference, especially for rare and weak effects. _Statistical Science_**30**, 1-25.
* Doukhan (1995) Doukhan, P. (1995). _Mixing: Properties and Examples_. Springer-Verlag, New York.
* Draper (1981) Draper, D. (1981). _Rank-Based Robust Analysis of Linear Models_, Ph.D. Thesis, Dept. of Statistics, University of California. Berkeley.
* Draper (1983) Draper, D. (1983). _Rank-Based Robust Analysis of Linear Models. I. Exposition and Background_, Tech. Report No. 17, Dept. of Statistics, University of California, Berkeley.
* Drost (1988) Drost, F. (1988). _Asymptotics for Generalized Chi-Square Goodness-of-Fit Tests_. Centrum voor Wiskunde en Informatica **48**, Amsterdam.
* Drost (1989) Drost, F. (1989). Generalized chi-square goodness-of-fit tests for location-scale models when the number of classes tends to infinity. _Annals of Statistics_**17**, 1285-1300.
* Dudley (1989) Dudley, R. (1989). _Real Analysis and Probability_. Wadsworth, Belmont.
* du Dudoit et al. (2003) du Dudoit, S., Shaffer, J. P. and Boldrick, J. (2003). Multiple hypothesis testing in microarray experiments. _Statistical Science_**18**, 71-103.
* Dumbgen (1998) Dumbgen, L. (1998). New goodness-of-fit tests and their application to nonparametric confidence sets. _Annals of Statistics_**26**, 288-314.

* Duncan (1955) Duncan, D. B. (1955). Multiple range and multiple \(F\)-tests. _Biometrics_**11**, 1-42.
* Durbin (1970) Durbin, J. (1970). On Bimbaum's theorem on the relation between sufficiency, conditionality, and likelihood. _Journal of the American Statistical Association_**65**, 395-398.
* Durbin (1973) Durbin, J. (1973). _Distribution theory for tests based on the sample distribution function._ SIAM Philadelphia, PA.
* Durbin and Knott (1972) Durbin, J. and Knott, M. (1972). Components of Cramer-von Mises statistics. Part I. _Journal of the Royal Statistical Society B_**34**, 290-307.
* Dvoretzky et al. (1953) Dvoretzky, A., Kiefer, J. and Wolfowitz, J. (1953). Sequential decision problems for processes with continuous time parameter. Testing hypotheses. _Annals of Mathematical Statistics_**24**, 254-264.
* Dvoretzky et al. (1956) Dvoretzky, A., Kiefer, J. and Wolfowitz. J. (1956). Asymptotic minimax character of the sample distribution function and the classical multinomial estimator. _Annals of Mathematical Statistics_**27**, 642-669.
* Dvoretzky et al. (1951) Dvoretzky, A., Wald, A. and Wolfowitz, J. (1951). Elimination of randomization in certain statistical decision procedures and zero-sum two-person games. _Annals of Mathematical Statistics_**22**, 1-21.
* Eaton (1983) Eaton, M. (1983). _Multivariate Statistics_. John Wiley, New York.
* Eaton (1989) Eaton, M. (1989). _Group Invariance Applications in Statistics_. Institute of Statistical Mathematics, Hayward, CA.
* Edelman (1990) Edelman, D. (1990). An inequality of optimal order for the tail probabilities of the \(T\) statistic under symmetry. _Journal of the American Statistical Association_**85**, 120-122.
* Edgeworth (1885) Edgeworth, F. Y. (1885). _Methods of Statistics_, Jubilee volume of the Statist. Soc., E. Stanford, London.
* Edgeworth (1905) Edgeworth, F. Y. (1905). The law of error. _Proc. Camb. Philos. Soc._**20**, 36-45.
* Edgeworth (1908-09) Edgeworth F. Y. (1908-09). On the probable errors of frequency constants. _J. Roy. Statist. Soc._**71**, 381-397, 499-512, 651-678; **72**, 81-90. [Edgeworth's work on maximum-likelihood estimation and its relation to the results of Fisher in the same area is reviewed by Pratt (1976). Stigler (1978) provides a systematic account of Edgeworth's many other important contributions to statistics.]
* Edgington (1995) Edgington, E. S. (1995). _Randomization Tests_, 3rd edition. Marcel Dekker, New York.
* Edwards (1963) Edwards, A. W. F. (1963). The measure of association in a \(2\times 2\) table. _Journal of the Royal Statistical Society Series B_**126**,109-114.
* Edwards (1983) Edwards, A. W. F. (1983). Fiducial distributions. In _Encycl. of Statist. Sci._, Vol. 3. John Wiley, New York.
* Efron (1969) Efron, B. (1969). Student's \(t\)-test under symmetry conditions. _Journal of the American Statistical Association_**64**, 1278-1302.
* Efron (1979) Efron, B. (1979). Bootstrap methods: Another look at the jackknife. _Annals of Statistics_**7**, 1-26.
* Efron (1981) Efron, B. (1981). Nonparametric standard errors and confidence intervals (with discussion). _Canadian Journal of Statistics_**9**, 139-172.
* Efron (1982) Efron, B. (1982). _The Jackknife, the Bootstrap and Other Resampling Plans_. SIAM, Philadelphia.
* Efron (2011) Efron, B. (2011). _Large-Scale Inference_, Cambridge University Press, New York.
* Efron and Tibshirani (1993) Efron, B. and Tibshirani, R. (1993). _An Introduction to the Bootstrap_. Chapman & Hall, New York.
* Elfving (1952) Elfving, G. (1952). Sufficiency and completeness. _Ann. Acad. Sci. Fennicae_ (_A_), No. 135.

* [21]Engelhardt and Bain (1977) Engelhardt, M. and Bain, L. J. (1977). Uniformly most powerful unbiased tests on the scale parameter of a gamma distribution with a nuisance shape parameter. _Technometrics_**19**, 77-81.
* [22]Engelhardt and Bain (1978) Engelhardt, M. and Bain, L. J. (1978). Construction of optimal unbiased inference proceures for the parameters of the gamma distribution. _Technometrics_**20**, 485-489.
* [23]Eubank (1997) Eubank, R. (1997). Testing goodness of fit with multinomial data. _Journal of the American Statistical Association_**92**, 1084-1093.
* [24]Eubank and LaRiccia (1992) Eubank, R. and LaRiccia, V. (1992). Asymptotic comparison of Cramer-von Mises and nonparametric function estimation techniques for testing goodness-of-fit. _Annals of Statistics_**20**, 2071-2086.
* [25]Falk and Kohne (1984) Falk, M. and Kohne, W. (1984). A robustification of the sign test under mixing conditions. _Annals of Statistics_**12**, 716-729.
* [26]Fan (1996) Fan, J. (1996). Test of significance based on wavelet thresholding and Neyman's truncation. _Journal of the American Statistical Association_**91**, 674-688.
* [27]Fan and Lin (1998) Fan, J. and Lin, S. (1998). Test of significance when data are curves. _Journal of the American Statistical Association_**93**, 1007-1021.
* [28]Fan et al. (2001) Fan, J., Zhang, C. and Zhang, J. (2001). Generalized likelihood ratio statistics and Wilks phenomenon. _Annals of Statistics_ **29**, 153-193.
* [29]Faraway and Sun (1995) Faraway, J. and Sun, J. (1995). Simultaneous confidence bands for linear regression with heteroscedastic errors. _Journal of the American Statistical Association_**90**, 1094-1098.
* [30]Farrell (1985a) Farrell, R. (1985a). _Multivariate Calculation: Use of the Continuous Groups_. Springer, Berlin.
* [31]Farrell (1985b) Farrell, R. (1985b). _Techniques of Multivariate Calculation_. Springer, Berlin.
* [32]Fears et al. (1996) Fears, T., Benichou, J. and Gail, M. (1996). A reminder of the fallibility of the Wald statistic. _American Statistician_**50**, 226-227.
* [33]Feller (1948) Feller, W. (1948). On the Kolmogorov-Smirnov limit theorems for empirical distributions. _Annals of Statistics_**19**, 177-189.
* [34]Feller (1968) Feller, W. (1968). _An Introduction to Probability Theory and its Applications_, 3rd edition, Vol. 1. John Wiley, New York.
* [35]Feller (1971) Feller, W. (1971). _An Introduction to Probability Theory and its Applications_, Vol. 2, 2nd edition. John Wiley, New York.
* [36]Fenstad (1983) Fenstad, G. U. (1983). A comparison between the \(U\) and \(V\) tests in the Behrens-Fisher problem. _Biometrika_**70**, 300-302.
* [37]Ferguson (1967) Ferguson, T. S. (1967). _Mathematical Statistics: A Decision Theoretic Approach_. Academic Press, New York.
* [38]Ferguson (1996) Ferguson, T. S. (1996). _A Course in Large Sample Theory_. Chapman & Hall, New York.
* [39]Fienberg (1980) Fienberg, S. (1980). _The Analysis of Cross-Classified Categorical Data_, 2nd edition. MIT Press, Cambridge, Massachusetts.
* [40]Fienberg and Tanur (1996) Fienberg, S. and Tanur, J. (1996). Reconsidering the fundamental contributions of Fisher and Neyman in experimentation and sampling. _International Statistical Review_**64**, 237-253.
* [41]Finch (1979) Finch, P. D. (1979). Description and analogy in the practice of statistics (with discussion). _Biometrika_**66**, 195-208.
* [42]Finner (1994) Finner, H. (1994). Two-sided tests and one-sided confidence bounds. _Annals of Statistics_ **22**, 1502-1516.
* [43]* [Finner1999] Finner, H. (1999). Stepwise multiple test procedures and control of directional errors. _Annals of Statistics_**27**, 274-289.
* [Finner and Roters1998] Finner, H. and Roters, M. (1998). Asymptotic comparison of step-down and step-up multiple test procedures based on exchangeable test statistics. _Annals of Statistics_**26**, 505-524.
* [Finner and Roters2001] Finner, H. and Roters, M. (2001). On the false discovery rate and expected type I errors, _Biometric Journal_**43**, 995-1005.
* [Finner and Strassburger2002] Finner, H. and Strassburger, K. (2002). The partioning principle: a powerful tool in multiple decision theory. _Annals of Statistics_**40**, 224-250.
* [Finney1948] Finney, D. J. (1948). The Fisher-Yates test of significance in \(2\times 2\) contingency tables. _Biometrika_**35**, 145-156.
* [Finney et al.1963] Finney, D. J., Latscha, R., Bennett, B., Hsu, P. and Horst, C. (1963, 1966). _Tables for Testing Significance in a \(2\times 2\) Contingency Table_, Cambridge U.P.
* [Fisher1922] Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. _Phil. Trans. Roy. Soc. London Series A_**222**, 309-368.
* [Fisher1924] Fisher, R. A. (1924). The conditions under which chi square measures the discrepancy between observation and hypothesis. _Journal of the Royal Statistical Society_**87**, 442-450.
* [Fisher1925a] Fisher, R.A. (1925a). Theory of statistical estimation. _Proc. Cambridge Phil. Soc._**22**, 700-725. [These papers develop a theory of point estimation (based on the maximum likelihood principle) and the concept of sufficiency. The factorization theorem is given in a form which is formally weaker but essentially equivalent to (1.20). First use of term ancillary.]
* [Fisher1925b] Fisher, R. A. (1925b). _Statistical Methods for Research Workers_, 1st edition (14th edition, 1970), Oliver and Boyd, Edinburgh.
* [Fisher1928a] Fisher, R. A. (1928a). The general sampling distribution of the multiple correlation coefficient. _Proc. Roy. Soc. Series A_**121**, 654-673. [Derives the noncentral \(\chi^{2}\)- and noncentral beta-distributions and the distribution of the sample multiple correlation coefficient for arbitrary values of the population multiple correlation coefficient.]
* [Fisher1928b] Fisher R. A. (1928b). On a property connecting the \(\chi^{2}\) measure of discrepancy with the method of maximum likelihood. Atti de Congresso Internazionale dei Mathematici, Bologna **6**, 94-100.
* [Fisher1930] Fisher, R. A. (1930). Inverse probability. _Proc. Cambridge Philos. Soc._**26**, 528-535.
* [Fisher1934a] Fisher, R. A. (1934a). _Statistical Methods for Research Workers_, 5th and subsequent eds., Oliver and Boyd, Edinburgh, Section 21.02. [Proposes the conditional tests for the hypothesis of independence in a \(2\times 2\) table.]
* [Fisher1934b] Fisher, R. A. (1934b). Two new properties of mathematical likelihood. _Proc. Roy. Soc._ (_A_) **144**, 285-307. [Introduces the idea of conditioning on ancillary statistics and applies it to the estimation of location parameters.]
* [Fisher1935a] Fisher R. A. (1935a). _The Design of Experiments_, 1st edition (8th edition, 1966). Oliver and Boyd, Edinburgh. [Contains the basic ideas concerning permutation tests. In particular, points out how randomization provides a basis for inference and proposes the permutation version of the \(t\)-test as not requiring the assumption of normality.]
* [Fisher1935b] Fisher R. A. (1935b). The logic of inductive inference (with discussion). _Journal of the Royal Statistical Society_**98**, 39-82.
* [Fisher1936] Fisher R. A. (1936). Uncertain inference. _Proc. Amer. Acad. Arts and Sci._**71**, 245-258.

* Fisher (1956a) Fisher, R. A. (1956a). On a test of significance in Pearson's Biometrika tables (No. 11). _Journal of the Royal Statistical Society_ (_B_) **18**, 56-60. (See also the discussion of this paper by Neyman, Bartlett, and Welch in the same volume, pp. 288-302.) [Exhibits a negatively biased relevant subset for the Welch-Aspin solution of the Behren-Fisher problem.]
* Fisher (1956b) Fisher, R. A. (1956b, 1959, 1973). _Statistical Methods and Scientific Inference_. Oliver and Boyd, Edinburgh (1956, 1959); Hafner, New York (1973). [In Chapter IV the author gives his views on hypothesis testing and in particular discusses his ideas on the Behrens-Fisher problem. Contains Fisher's last comprehensive statement of his views on many topics, including ancillarity and the Behrens-Fisher problem.]
* Fisher (1971-1973) Fisher, R. A. (1971-1973). _Collected Papers_ (J. H. Bennett, ed.), University of Adelaide.
* Fisher (1973) Fisher, R. A. (1973). _Statistical Methods and Scientific Inference_, 3rd edition, Hafner, New York.
* Folks and Chhikara (1978) Folks, J. L. and Chhikara, R. S. (1978). The inverse Gaussian distribution and its statistical applications--a review (with discussion). _Journal of the Royal Statistical Society Series B_**40**, 263-289.
* Forsythe and Hartigan (1970) Forsythe, A. and Hartigan, J. A. (1970). Efficiency of confidence intervals generated by repeated subsample calculations. _Biometrika_**57**, 629-639.
* Fourier (1826) Fourier, J. B. J. (1826). _Recherches Statistiques sur la Ville de Paris el le Departement de la Seine, Vol._ 3.
* Franck (1981) Franck, W. E. (1981). The most powerful invariant test of normal versus Cauchy with applications to stable alternatives. _Journal of the American Statistical Association_**76**, 1002-1005.
* Fraser (1953) Fraser, D. A. S. (1953). _Canadian Journal of Mathematics_**6**, 42-45.
* Fraser (1956) Fraser, D. A. S. (1956). Sufficient statistics with nuisance parameters. _Annals of Mathematical Statistics_**27**, 838-842.
* Fraser (1996) Fraser, D. (1996). Comment on "Pivotal inference and the fiducial argument." _International Statistical Review_**64**, 231-235.
* Freedman and Lane (1982) Freedman, D. and Lane, D. (1982). Significance testing in a nonstochastic setting. In _Festschrift for Erich L. Lehmann_ (Bickel. Doksum, and Hodges, eds.), Wadsworth, Belmont, Calif.
* Freeman and Tukey (1950) Freeman, M. F. and Tukey, J. W. (1950). Transformations related to the angular and the square root. _Annals of Mathematical Statistics_**21**, 607-611.
* Freiman et al. (1978) Freiman, J. A., Chalmers, T. C., Smith, H. and Kuebler, R. R. (1978). The importance of beta, the type II error and sample size in the design and interpretation of the randomized control trial. _New England Journal of Medicine_**299**, 690-694.
* Frisen (1980) Frisen, M. (1980). Consequences of the use of conditional inference in the analysis of a correlated contingency table. _Biometrika_**67**, 23-30.
* Fuller (1996) Fuller, W. (1996). _Introduction to Statistical Time Series_, 2nd Edition, John Wiley, New York.
* Gabriel (1964) Gabriel, K. R. (1964). A procedure for testing the homogeneity of all sets of means in analysis of variance. _Biometrics_**20**, 459-477.
* Gabriel and Hall (1983) Gabriel, K. R. and Hall W. J. (1983). Rerandomization inference on regression and shift effects: Computationally feasible methods. _Journal of the American Statistical Association_**78**, 827-836.
* Gabriel and Hsu (1983) Gabriel, K. R. and Hsu, C. F. (1983). Evaluation of the power of rerandomization tests, with application to weather modification experiments. _Journal of the American Statistical Association_**78**, 766-775.

* Galambos (1977) Galambos, J. (1977). _The asymptotic Theory of Extreme Order Statistics_, Academic Press, New York.
* Galambos (1982) Galambos, J. (1982). Exponential distribution. In _Encycl. Statist. Sci._, Vol. 2, John Wiley, New York.
* Gan and Jiang (1999) Gan, L. and Jiang, J. (1999). A test for global maximum. _Journal of the American Statistical Association_**94**, 847-854.
* Garside and Mack (1976) Garside, G. R. and Mack, C. (1976). Actual type 1 error probabilities for various tests in the homogeneity case of the 2 x 2 contingency table. _American Statistician_**30**, 18-21.
* Gart (1970) Gart, J. J. (1970). Point and interval estimation of the common odds ratio in the combination of 2 x 2 tables with fixed marginals. _Biometrika_**57**, 471-475.
* Garthwaite (1996) Garthwaite, P. (1996). Confidence intervals from randomization tests. _Biometrics_**52**, 1387-1393.
* Gastwirth and Rubin (1971) Gastwirth, J. L. and Rubin, H. (1971). Effect of dependence on the level of some one-sample tests. _Journal of the American Statistical Association_**66**, 816-820.
* Gauss (1809) Gauss, C. F. (1809). _Theoria motus corporum coelestium in sectionibus conicis solem ambientium_. Hamburg.
* Gauss (1816) Gauss, C. F. (1816). Bestimmung der Genauigkeit der Beobachtungen. _Z. Astron. and Verw. Wiss_**1**. (Reprinted in Gauss' collected works, Vol. 4, pp. 109-119.)
* Gavarret (1840) Gavarret, J. (1840). _Principes Generaux de Statistique Medicale_, Paris.
* George and Casella (1994) George, E. I. and Casella, G. (1994). An empirical Bayes confidence report. _Statistica Sinica_**4**, 617-638.
* Ghosh (1961) Ghosh, J. (1961). On the relation among shortest confidence intervals of different types. _Calcutta Statist. Assoc. Bull._ 147-152.
* Ghosh et al. (1981) Ghosh, J., Morimoto, H. and Yamada, S. (1981). Neyman factorization and minimality of pairwise sufficient subfields. _Annals of Statistics_**9**, 514-530.
* Ghosh (1948) Ghosh, M. (1948). On the problem of similar regions. _Sankhya_**8**, 329-338.
* Gibbons (1986) Gibbons, J. (1986). Ranking procedures. In _Encycl. Statist. Sci._**7**, 588-592.
* Gibbons (1988) Gibbons, J. (1988). Selection procedures. In _Encycl. Statist. Sci._**8**, 337-345.
* Gibbons and Chakraborti (1992) Gibbons, J. and Chakraborti, S. (1992). _Nonparametric statistical inference_, 3rd edition. Marcel Dekker, New York.
* Giesbrecht and Gumpertz (2004) Giesbrecht, F. and Gumpertz, M. (2004). _Planning, Construction, and Statistical Analysis of Comparative Experiments_. John Wiley, New York.
* Gine (1997) Gine, E. (1997). _Lectures on Some Aspects of the Bootstrap_. Ecole d'Ete de Calcul de Probabilites de Saint-Flour.
* Gine and Zinn (1989) Gine, E. and Zinn, J. (1989). Necessary conditions for the bootstrap of the mean. _Annals of Statistics_**17**, 684-691.
* Giraud (2015) Giraud, C. (2015). _Introduction to High-Dimensional Statistics_. CRC Press, Boca Raton, Florida.
* Giri and Kiefer (1964) Giri, N. and Kiefer, J. (1964). Local and asymptotic minimax properties of multivariate tests. _Annals of Mathematical Statistics_**35**, 21-35.
* Giri et al. (1963) Giri, N., Kiefer, J. and Stein, C. M. (1963). Minimax character of Hotelling's \(T^{2}\) test in the simplest case. _Annals of Mathematical Statistics_**34**, 1524-1535.

* Girshick et al. (1946) Girshick, M. A., Mosteller, F. and Savage, L. J. (1946). Unbiased estimates for certain binomial sampling problems with applications. _Annals of Mathematical Statistics_ **17**, 13-23. [Problem 4.12.]
* Glaser (1976) Glaser, R. E. (1976). The ratio of the geometric mean to the arithmetic mean for a random sample from a gamma distribution. _Journal of the American Statistical Association_**71**, 481-487.
* Glaser (1982) Glaser, R. E. (1982). Bartlett's test of homogeneity of variances. _Encycl. Statist. Sci._**1**, 189-191.
* Gleser (1985) Gleser, L. J. (1985). Exact power of goodness-of-fit tests of Kolmogorov type for discontinuous distributions. _Journal of the American Statistical Association_**80**, 954-958.
* Gleser and Hwang (1987) Gleser, L. J. and Hwang, J. (1987). The nonexistence of \(100(1-\alpha)\%\) confidence sets of finite expected diameter in errors-in-variables and related models. _Annals of Statistics_**15**, 1351-1362.
* Gokhale and Johnson (1978) Gokhale, D. V. and Johnson, N. S. (1978). A class of alternatives to independence in contingency tables. _Journal of the American Statistical Association_**73**, 800-804.
* Good (1994) Good, P. (1994). _Permutation Tests, A Practical Guide to Resampling Methods for Testing Hypotheses_. Springer-Verlag, New York.
* Goodman and Kruskal (1954) Goodman, L. A. and Kruskal, W. (1954, 1959). Measures of association for cross classification. _Journal of the American Statistical Association_**49**, 732-764; **54**, 123-163.
* Goutis and Casella (1991) Goutis, C. and Casella, G. (1991). Improved invariant confidence intervals for a normal variance. _Annals of Statistics_**19**, 2015-2031.
* Goutis and Casella (1992) Goutis, C. and Casella, G. (1992). Increasing the confidence in student's \(t\) interval. _Annals of Statistics_**20**, 1501-1513.
* Grandhi et al. (2019) Grandhi, A, Guo, W. and Romano, J. (2019). Control of directional errors in fixed sequence multiple testing. _Statistica Sinica_, **29**, 1047-1064.
* Graybill (1976) Graybill, F. A. (1976). _Theory and Application of the Linear Model_. Duxbury Press, North Scituate, Mass.
* Green (1977) Green, B. F. (1977). A practical interactive program for randomization tests of location. _American Statistician_**31**, 37-39.
* Greenwood and Nikulin (1996) Greenwood, P. and Nikulin, M. (1996). _A Guide to Chi-Squared Testing_. John Wiley, New York.
* Grenander (1981) Grenander, U. (1981). _Abstract Inference_. John Wiley, New York.
* Groeneboom (1980) Groeneboom, P. (1980). _Large Deviations and Asymptotic Efficiencies_. Mathematisch Centrum, Amsterdam, The Netherlands.
* Groeneboom and Oosterhoof (1981) Groeneboom, P. and Oosterhoof, J. (1981). Bahadur efficiency and small sample efficiency. _International Statistical Review_**49**, 127-141.
* Guenther (1978) Guenther, W. C. (1978). Some remarks on the runs tests and the use of the hypergeometric distribution. _American Statistician_**32**, 71-73.
* Guo et al. (2014) Guo, W., He, L., and Sarkar, S. (2014). Further results on controlling the false discovery proportion. _Annals of Statistics_**42**, 1070-1101.
* Guo and Romano (2006) Guo, W. and Romano, J. (2006). A generalized Sidak procedure and control of generalized error rates under independence. _Statistical Applications in Genetics and Molecular Biology_, **6**, Issue 1, Article 3.
* Guo and Romano (2015) Guo, W. and Romano, J. (2015). On stepwise control of directional errors under independence and some dependence., _Journal of Statistical Planning and Inference_, **163**, 21-33.

* Gupta and Vermeire (1986) Gupta, A. and Vermeire, L. (1986). Locally optimal tests for multiparameter hypotheses. _Journal of the American Statistical Association_**81**, 819-825.
* Haberman (1974) Haberman, S. J. (1974). _The Analysis of Frequency Data._ University of Chicago Press.
* Haberman (1982) Haberman, S. J. (1982). Association, Measures of In _Encycl. Statist. Sci._, Vol. 1. John Wiley, New York, 130-136.
* Hajek (1960) Hajek, J. (1960). Limiting distributions in simple random sampling from a finite population. _Publications of the Mathematics Institute of the Hungarian Academy of Science_**5**, 361-374.
* Hajek (1962) Hajek, J. (1962). Asymptotically most powerful rank order tests. _Annals of Mathematical Statistics_**33**, 1124-1147.
* Hajek (1967) Hajek, J. (1967). On basic concepts of statistics, In _Proc. Fifth Berkeley Symp. Math. Statist. and Probab._, Univ. of Calif. Press, Berkeley.
* Hajek (1968) Hajek, J. (1968). Asymptotic normality of simple rank statistics under alternatives. _Annals of Matematical Statistics_**39**, 325-346.
* Hajek (1970) Hajek, J. (1970). A characterization of limiting distributions of regular estimates. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_**14**, 323-330.
* Hajek (1972) Hajek, J. (1972). Local asymptotic minimax and admissibility in estimation. _Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability_**1**, 175-194.
* Hajek and Sidak (1967) Hajek, J. and Sidak, Z. (1967). _Theory of Rank Tests._ C.S.A.V. Prague and Academic Press.
* Hajek and Sen (1999) Hajek, J. Sidak, Z. and Sen, P. (1999). _Theory of Rank Tests_, 2nd edition. Academic Press, San Diego.
* Hald (1990) Hald, A. (1990). _A History of Probability and Statistics (and Their Applications Before 1750)._ John Wiley, New York.
* Hald (1998) Hald, A. (1998). _A History of Mathematical Statistics (from 1750 to 1930)._ John Wiley, New York.
* Hall (1982) Hall, P. (1982). Improving the normal approximation when constructing one-sided confidence intervals for binomial or Poisson parameters. _Biometrika_**69**, 647-652.
* Hall (1986) Hall, P. (1986). On the bootstrap and confidence intervals. _Annals of Statistics_**14**, 1431-1452.
* Hall (1990) Hall, P. (1990). Asymptotic properties of the bootstrap for heavy-tailed distributions. _Annals of Probability_**18**, 1342-1360.
* Hall (1992) Hall, P. (1992). _The Bootstrap and Edgeworth Expansion_. Springer, New York.
* Hall and Heyde (1980) Hall, P. and Heyde, C. (1980). _Martingale Limit Theory and Its Application._ Academic Press, San Diego.
* Hall and Jing (1995) Hall, P. and Jing, B. (1995). Uniform coverage bounds for confidence intervals and Berry-Esseen theorems for Edgeworth expansions. _Annals of Statistics_**23**, 363-375.
* Hall and Martin (1988) Hall, P. and Martin, M. (1988). On bootstrap resampling and iteration. _Biometrika_**75**, 661-671.
* Hall and Padmanabhan (1997) Hall, P. and Padmanabhan, A. (1997). Adaptive inference and the two sample scale problem. _Technometrics_**39**, 412-422.
* Hall and Welsh (1983) Hall, P. and Welsh, A. H. (1983). A test for normality based on the empirical characteristic function. _Biometrika_**70**, 485-489.
* Hall and Mathiason (1990) Hall, W. and Mathiason, D. (1990). On large-sample estimation and testing in parametric models. _International Statistical Review_**58**, 77-97.

* Hall et al. (1965) Hall, W., Wijsman, R. and Ghosh, J. (1965). The relationship between sufficiency and invariance with applications in sequential analysis. _Annals of Mathematical Statistics_**36**, 575-614.
* Hallin et al. (1999) Hallin, M., Taniguchi, M., Serroukh, A. and Choy, K. (1999). Local asymptotic normality for regression models with long memory disturbance. _Annals of Statistics_**27**, 2054-2080.
* Halmos (1974) Halmos, P. R. (1974). _Measure Theory_. Springer, New York.
* Halmos and Savage (1949) Halmos, P. R. and Savage, L. J. (1949). Application of the Radon-Nikodym theorem to the theory of sufficient statistics. _Annals of Mathematical Statistics_**20**, 225-241. [First abstract treatment of sufficient statistics; the factorization theorem. Problem 10.]
* Hamilton (1994) Hamilton, J. D. (1994). _Time Series Analysis_. Princeton University Press, Princeton.
* Hartigan (1969) Hartigan, J. A. (1969). Using subsample values as typical values. _Journal of the American Statistical Association_**64**, 1303-1317.
* Harville (1978) Harville, D. A. (1978). Alternative formulations and procedures for the two-way mixed model. _Biometrics_**34**, 441-454.
* Hastie and Tibshirani (1990) Hastie, T. and Tibshirani, R. (1990). _Generalized Additive Models._ Chapman & Hall, London.
* Hastie and Tibshirani (1997) Hastie, T. and Tibshirani, R. (1997). Generalized additive models. In _Encycl. Statist. Sci. update_**1**, 261-269.
* Haytner and Hsu (1994) Haytner, A. and Hsu, J. (1994). On the relationship between stepwise decision procedures and confidence sets. _Journal of the American Statistical Association_**89**, 128-136.
* Hedges and Olkin (1985) Hedges, L. and Olkin, I. (1985). _Statistical methods for meta-analysis_. Academic Press, Orlando.
* Hegazy and Green (1975) Hegazy, Y. A. S. and Green, J. R. (1975). Some new goodness-of-fit tests using order statistics. _Applied Statistics_**24**, 299-308.
* Hegemann and Johnson (1976) Hegemann, V. and Johnson, D. E. (1976). The power of two tests for nonadditivity. _Journal of the American Statistical Association_**71**, 945-948.
* Heritier and Ronchetti (1994) Heritier, S. and Ronchetti, E. (1994). Robust bounded-influence tests in general parametric models. _Journal of the American Statistical Association_**89**, 897-904.
* Hettmansperger (1984) Hettmansperger, T. P. (1984). _Statistical Inference Based on Ranks_. John Wiley, New York.
* Hettmansperger and McKean (1998) Hettmansperger, T. and McKean, J. (1998). _Robust Nonparametric Statistical Methods._ Arnold, London.
* Hettmansperger et al. (2000) Hettmansperger, T., McKean, J. and Sheather, S. (2000). Robust nonparametric methods. _Journal of the American Statistical Association_**95**, 1308-1312.
* Hettmansper et al. (1997) Hettmansperger, T., Mottonen, J. and Oja, H. (1997). Affine-invariant multivariate one-sample signed-rank tests. _Journal of the American Statistical Association_**92**, 1591-1600.
* Hinkley (1977) Hinkley, D. (1977). Conditional inference about a normal mean with known coefficient of variation. _Biometrika_, **64**, 105-108.
* Hinkley and Runger (1984) Hinkley, D. and Runger, G. (1984). The analysis of transformed data. (with discussion). _Journal of the American Statistical Association_**79**, 302-320.
* Hipp (1974) Hipp, C. (1974). Sufficient statistics and exponential families. _Annals of Statistics_**2**, 1283-1292.
* Hobson (1927) Hobson, E. W. (1927). _Theory of Functions of a Real Variable_, 3rd edition, Vol. 1. Cambridge University Press, p. 194.
* Hochberg and Tamhane (1987) Hochberg, Y. and Tamhane, A. (1987). _Multiple Comparison Procedures_. John Wiley, New York.
* Held and Rivest (1975)* Hocking (1973) Hocking, R. R. (1973). A discussion of the two-way mixed model. _American Statistician_**27**, 148-152.
* Hocking (2003) Hocking, R. R. (2003). _Methods and Applications of Linear Models_, 2nd edition. John Wiley, New York.
* Hocking and Speed (1975) Hocking, R. R. and Speed, F. M. (1975). A full rank analysis of some linear model problems. _Journal of the American Statistical Association_**70**, 706-712.
* Hodges (1957) Hodges, J. L., Jr. (1957). The significance probability of the Smirnov two-sample test. _Arkiv fur Matematik_**3**, 469-486.
* Hodges and Lehmann (1954) Hodges, J. L., Jr. and Lehmann, E. L. (1954). Testing the approximate validity of statistical hypotheses. _Journal of the Royal Statistical Society Series B_**16**, 261-268.
* Hodges and Lehmann (1956) Hodges, J. L., Jr. and Lehmann, E. L. (1956). The efficiency of some nonparametric competitors of the \(t\)-test. _Annals of Mathematical Statistics_**27**, 324-335.
* Hodges and Lehmann (1970) Hodges, J. L., Jr. and Lehmann, E.L. (1970). Deficiency. _Annals of Mathematical Statistics_**41**, 783-801.
* Hoeffding (1948) Hoeffding, W. (1948). A class of statistics with asymptotically normal distributions. _Annals of Mathematical Statistics_ **19**, 293-325.
* Hoeffding (1951) Hoeffding W. (1951). 'Optimum' nonparametric tests. in _Proc. 2nd Berkeley Symposium on Mathematical Statistics and Probability_, Univ. of Calif. Press., Berkeley, 83-92.
* Hoeffding (1952) Hoeffding, W. (1952). The large-sample power of tests based on permutations of observations. _Annals of Mathematical Statistics_ **23**, 169-192.
* Hoeffding (1956) Hoeffding, W. (1956). The role of assumptions in statistical decisions. In _Proc. Third Berkeley Symposium on Mathematical Statistics and Probability_, edited by Neyman, University of California Press, Berkeley, CA.
* Hoeffding (1965) Hoeffding, W. (1965). Asymptotically optimal tests for multinomial distributions (with discussion). _Annals of Mathematical Statistics_**36**, 369-408.
* Hoeffding (1977) Hoeffding, W. (1977). Some incomplete and boundedly complete families of distributions. _Annals of Statistics_**5**, 278-291.
* Hoeffding and Robbins (1957) Hoeffding, W. and Robbins, H. The central limit theorem for dependent random variables. _Duke Math J._**15**, 773-780.
* Hoel (1948) Hoel, P. G. (1948). On the uniqueness of similar regions. _Annals of Mathematical Statistics_**19**, 66-71. [Theorem 4.3.1 under regularity assumptions.]
* Hogg (1972) Hogg, R. V. (1972). More light on the kurtosis and related statistics. _Journal of the American Statistical Association_**67**, 422-424.
* Holm (1979) Holm, S. (1979). A simple sequentially rejective multiple test procedure. _Scandinavian Journal of Statistics_**6**, 65-70.
* Holm (1999) Holm, S. (1999). Multiple confidence sets based on stagewise tests. _Journal of the American Statistical Association_**94**, 489-495.
* Hommel (1988) Hommel, G. (1988). A comparison of two modified Bonferroni procedures. _Biometrika_**75**, 383-386.
* Hooper (1982a) Hooper, P. M. (1982a). Sufficiency and invariance in confidence set estimation. _Annals of Statistics_**10**, 549-555.

Hooper, P. M. (1982b). Invariant confidence sets with smallest expected measure. _Annals of Statistics_**10**, 1283-1294.
* Hotelling (1931) Hotelling, H. (1931). The generalization of Student's ratio. _Annals of Mathematical Statistics_**2**, 360-378.
* Hotelling (1936) Hotelling, H. (1936). Relations between two sets of variates. _Biometrika_**28**, 321-377. [One of the early papers making explicit use of invariance considerations.]
* Hotelling (1953) Hotelling, H. (1953). New light on the correlation coefficient and its transforms. _Journal of the Royal Statistical Society Series B_**15**, 193-224.
* Hotelling (1961) Hotelling, H. (1961). The behavior of some standard statistical tests under non-standard conditions. _Proceedings of the Fourth Berkeley Symposium of Mathematical Statistics_ Prob. **1**, 319-360.
* Hsu (1940) Hsu, C. T. (1940). On samples from a normal bivariate population. _Annals of Mathematical Statistics_**11**, 410-426.
* Hsu (1996) Hsu, J. (1996). _Multiple Comparisons: Theory and Methods_. Chapman & Hall, London.
* Hsu (1941) Hsu, P. (1941). Analysis of variance from the power function stand-point. _Biometrika_**32**, 62-69. [Shows that the test (7.7) is UMP among all tests whose power function depends only on the noncentrality parameter.]
* Hsu (1945) Hsu, P. (1945). On the former function of the \(E^{2}\)-test and the \(T^{2}\)-test. _Annals of Mathematical Statistics_**16**, 278-286. [Obtains a result on best average power for the \(T^{2}\)-test analogous to that of Chapter 7, Problem 7.5.]
* Huang and Ghosh (1982) Huang, J. S. and Ghosh, M. (1982). A note on strong unimodality of order statistics. _Journal of the American Statistical Association_**77**, 929-930.
* Huber (1965) Huber, P. J. (1965). A robust version of the probability ratio test. _Annals of Mathematical Statistics_**36**, 1753-1758.
* Huber (1973) Huber, P. J. (1973). Robust regression: Asymptotics, conjectures and Monte Carlo. _Annals of Statistics_**1**, 799-821. [Obtains the robustness conditions (13.18) and (13.20); related results are given by Eicker (1963).]
* Huber (1981) Huber, P. J. (1981). _Robust Statistics_. John Wiley, New York.
* Huber and Strassen (1973) Huber, P. J. and Strassen, V. (1973, 1974). Minimax tests and the Neyman-Pearson lemma for capacities. _Annals of Statistics_**1**, 251-263; **2**, 223-224.
* Hunt and Stein (1946) Hunt, G. and Stein, C. M. (1946). Most stringent tests of statistical hypotheses. [In this paper. which unfortunately was never published, a general theory of invariance is developed for hypothesis testing.]
* Hwang and Brown (1991) Hwang, J. and Brown, L. D. (1991). Estimated confidence under the validity constraint. _Annals of Statistics_**19**, 1964-1977.
* Hwang and Casella (1982) Hwang, J. and Casella, G. (1982). Minimax confidence sets for the mean of a multivariate normal distribution. _Annals of Statistics_**10**, 868-881.
* Hwang et al. (1992) Hwang, J., Casella, G., Robert, C., Wells, M. and Farrell, R. (1992). Estimation of accuracy in testing. _Annals of Statistics_**20**, 490-509.
* Ibragimov (1956) Ibragimov, J. A. (1956). On the composition of unimodal distributions (Russian). _Teoriya Veroyatnostey_**1**, 283-288; Eng]. transl., _Theor. Probab. Appl._**1** (1956), 255-260.
* Ibragimov (1962) Ibragimov, J. A. (1962). Some limit theorems for stationary processes (Translated by R.A. Silverman). _Theory of Probability and its Applications_**VII**, 349-382.

* Ibragimov and Has'minskii (1981) Ibragimov, I. and Has'minskii, R. (1981). _Statistical Estimation._ Springer-Verlag, New York.
* Imbens and Rubin (2015) Imbens, G. and Rubin, D. (2015). _Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction._ New York: Cambridge University Press.
* Inglot et al. (1997) Inglot, T., Kallenberg, W. and Ledwina, T. (1997). Data driven smooth tests for composite hypotheses. _Annals of Statistics_**25**, 1222-1250.
* Inglot et al. (2000) Inglot, T., Kallenberg, W. and Ledwina, T. (2000). Vanishing shortcoming and asymptotic relative efficiency. _Annals of Statistics_**28**, 215-238.
* Inglot and Ledwina (1996) Inglot, T. and Ledwina, T. (1996). Asymptotic optimality of data-driven Neyman's tests for uniformity. _Annals of Statistics_ **24**, 1982-2019.
* Ingster (1993) Ingster, Y. (1993). Asymptotically minimax hypothesis tests for nonparametric alternatives I, II, III. _Math. Methods Statist._ **2**, 85-114, 171-189, 249-268.
* Ingster (1999) Ingster, Y. (1999). Minimax detection of a signal for \(l_{n}^{P}\)-balls. _Math. Maethods Statist._**7**, 401-428.
* Ingster and Suslina (2003) Ingster, Y. and Suslina, I. (2003). _Nonparametric Goodness-of-Fit Testing Under Gaussian Models._ Springer Lecture Notes in Statistics **169**, Springer-Verlag, New York.
* Ingster and Tsybakov (2010) Ingster, I. and Tsybakov, A. (2010). Detection boundary in sparse regression. _Electronic Journal of Statistics_**4**, 1476-1526.
* Isaacson (1951) Isaacson, S. L. (1951). On the theory of unbiased tests of simple statistical hypotheses specifying the values of two or more parameters. _Annals of Mathematical Statistics_**22**, 217-234. [Introduces type \(D\) and \(E\) tests.]
* Jagers (1980) Jagers, P. (1980). Invariance in the linear model--an argument for \(\chi^{2}\) and \(F\) in nonnormal situations. _Statistics_**11**, 455-464.
* James (1954) James, A. T. (1954). Normal multivariate analysis and the orthogonal group. _Annals of Mathematical Statistics_**25**, 40-75.
* James (1951) James, G. S. (1951). The comparison of several groups of observations when the ratios of the population variances are unknown. _Biometrika_**38**, 324-329.
* James (1954) James, G. S. (1954). Tests of linear hypotheses in univariate and multivariate analysis when the ratios of the population variances are unknown. _Biometrika_**41**, 19-43.
* Janssen (1995) Janssen, A. (1995). Principal component decomposition of non-parametric tests. _Probability Theory and Related Fields_**101**, 193-209.
* Janssen (1997) Janssen, A. (1997). Studentized permutation tests for non-i.i.d. hypotheses and the generalized Behrens-Fisher problem. _Statistics and Probability Letters_**36**, 9-21.
* Janssen (1999) Janssen, A. (1999). Testing nonparametric statistical functionals with applications to rank tests. _Journal of Statistical Planning and Inference_**81**, 71-93, Erratum **92**,
* Janssen (2000a) Janssen, A. (2000a). Global power functions of goodness of fit tests. _Annals of Statistics_ **28**, 239-253.
* Janssen (2000b) Janssen, A. (2000b). Nonparametric bioequivalence for tests for statistical functionals and their efficient power functions. _Statistics and Decisions_**18**, 49-78.
* Janssen (2003) Janssen, A. (2003). Which power of goodness of fit tests can really be expected: intermediate versus contiguous alternatives. _Statistics and Decisions_ **21**, 301-325.
* Janssen and Pauls (2003) Janssen, A. and Pauls, T. (2003). How do bootstrap and permutation tests work? _Annals of Statistics_**31**, 768-806.

* Janssen and Pauls (2005) Janssen, A. and Pauls, T. (2005). A Monte Carlo comparison of studentized bootstrap and permutation tests for heteroscedastic two-sample problems. _Comput. Statist._**20**, 369-383,
* Jensen (1993) Jensen, J. (1993). A historical sketch and some new results on the improved log likelihood ratio statistic. _Scandinavian Journal of Statistics_**20**, 1-15.
* Jentsch and Pauly (2015) Jentsch, C. and Pauly, M. (2015). Testing equality of spectral densities using randomization techniques. _Bernoulli_**21**, 697-739.
* Jin (2003) Jin, J. (2003). Detecting and estimating sparse mixtures. Ph.D. thesis, Dept. Statistics, Stanford University, Stanford, CA.
* Jockel (1986) Jockel, K. (1986). Finite sample properties and asymptotic efficiency of Monte Carlo tests. _Annals of Statistics_**14**, 336-347.
* Johansen (1979) Johansen, S. (1979). _Introduction to the Theory of Regular Exponential Families_, Lecture Notes, No. 3, Inst. of Math. Statist., University of Copenhagen.
* Johansen (1980) Johansen, S. (1980). The Welch-James approximation to the distribution of the residual sum of squares in a weighted linear regression. _Biometrika_**67**, 85-92.
* John and Robinson (1983a) John, R. D. and Robinson, J. (1983a). Edgeworth expansions for the power of permutation tests. _Annals of Statistics_**11**, 625-631.
* John and Robinson (1983b) John, R. D. and Robinson, J. (1983b). Significance levels and confidence intervals for permutation tests. _Journal of Statistical Computation and Simulation_**16**, 161-173.
* Johnson and Kotz (1969) Johnson, N. L. and Kotz, S. (1969). _Distributions in Statistics: Discrete Distributions_. Houghton Mifflin, New York.
* Johnson and Kotz (1970) Johnson, N. L. and Kotz, S. (1970). _Distributions in Statistics: Continuous Univariate Distributions_ (2 vols.). Houghton Mifflin,New York.
* Johnson et al. (1995) Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995). _Continuous Univariate Distributions_**2**, 2nd edition. John Wiley, New York.
* Johnson et al. (1992) Johnson, N. L., Kotz, S. and Kemp, A. (1992). _Univariate Discrete Distributions_, 2nd edition. John Wiley, New York.
* Johnstone and Silverman (2004) Johnstone, I. and Silverman, B. (2004). Needles and straw in haystacks: empirical Bayes estimates of possibly sparse sequences. _Annals of Statistics_**32**, 1594-1649.
* Joshi (1982) Joshi, V. (1982). Admissibility. In _Encycl. Statist. Sci._**1**, 25-29.
* Kabe and Laurent (1981) Kabe, D. G. and Laurent, A. G. (1981). On some nuisance parameter free uniformly most powerful tests. _Biometrics Journal_**23**, 245-250.
* Kakutani (1948) Kakutani, S. (1948). On the equivalence of infinite product measures. _Annals of Mathematical Statistics_**49**, 214-224.
* Kalbfleisch (1975) Kalbfleisch, J. D. (1975). Sufficiency and conditionality (with discussion). _Biometrika_**62**, 251-259.
* Kallenberg (1982) Kallenberg, W. (1982). Chernoff efficiency and deficiency. _Annals of Statistics_**10**, 583-594.
* Kallenberg (1983) Kallenberg, W. (1983). Intermediate efficiency, theory and examples. _Annals of Statistics_**11**, 170-1-82.
* Kallenberg et al. (1984) Kallenberg, W. C. M. et al. (1984). _Testing Statistical Hypotheses: Worked Solutions_, CWI Syllabus No. 3, Centrum voor Wiskunde en Informatien, Amsterdam.
* Kallenberg and Ledwina (1995) Kallenberg, W. and Ledwina, T. (1995). Consistency and Monte Carlo simulation of a data driven version of smooth goodness-of-fit tests. _Annals of Statistics_**23**, 1594-1608.

Kallenberg, W. and Ledwina, T. (1999). Data-driven rank tests for independence. _Journal of the American Statistical Association_**94**, 285-301.
* Kallenberg et al. (1985) Kallenberg, W. C. M., Oosterhoff J. and Schriever B. F. (1985). The number of classes in chi-squared goodness-of-fit tests. _Journal of the American Statistical Association_**80**, 959-968.
* Kanoh and Kusunoki (1984) Kanoh, S. and Kusunoki, U. (1984). One sided simultaneous bounds in linear regression. _Journal of the American Statistical Association_**79**, 715-719.
* Kappenman (1975) Kappenman, R. F, (1975). Conditional confidence intervals for the double exponential distribution parameters. _Technometrics_**17**, 233-235.
* Kariya (1981) Kariya, T. (1981). Robustness of multivariate tests. _Annals of Statistics_**9**, 1267-1275.
* Kariya (1985) Kariya, T. (1985). _Testing in the Multivariate Linear Model_. Kinokuniya, Tokyo.
* Kariya and Sinha (1985) Kariya, T. and Sinha, B. (1985). Nonnull and optimality robustness of some tests. _Annals of Statistics_**13**, 1182-1197.
* Karlin (1957) Karlin, S. (1957). Polya type distributions. II. _Annals of Mathematical Statistics_**28**, 281-308.
* Karlin (1968) Karlin, S. (1968). _Total Positivity_, Vol. I, Stanford U.P. Stanford, Calif. [Properties of TP distributions, including Problems 3.55-3.58.]
* Karlin and Rubin (1956) Karlin, S. and Rubin, H. (1956). The theory of decision procedures for distributions with monotone likelihood ratio. _Annals of Mathematical Statistics_**27**. 272-299. [General theory of families with monotone likelihood ratio, including Theorem 3.4.2. For further developments of this theory, see Brown, Cohen, and Strawderman (1976).]
* Karlin and Taylor (1975) Karlin, S. and Taylor, H. (1975). _A First Course in Stochastic Processes_, 2nd ed., Academic Press, San Diego, CA.
* Kempthorne (1955) Kempthorne, O. (1955). The randomization theory of experimental inference. _Journal of the American Statistical Association_**50**, 946-967.
* Kempthorne (1988) Kempthorne, P. (1988). Controlling risks under different loss functions: The compromise decision problem. _Annals of Statistics_**16**, 1594-1608.
* Kendall (1970) Kendall, M. G. (1970). _Rank Correlation Methods_, 4th edition. Griffin, London.
* Kendall and Stuart (1979) Kendall M. G. and Stuart, A. (1979). _The Advanced Theory of Statistics_, 4th edition, Vol. 2. MacMilan, New York.
* Kent and Quesenberry (1982) Kent, J. and Quesenberry, C. P. (1982). Selecting among probability distributions used in reliability. _Technometrics_**24**, 59-65.
* Khmaladze (1993) Khmaladze, E. (1993). Goodness of fit problem and scanning innovation martingales. _Annals of Statistics._**21**, 798-829.
* Kiefer (1958) Kiefer, J. (1958). On the nonrandomized optimality and randomized nonoptimality of symmetrical designs. _Annals of Mathematical Statistics_**29**, 675-699. [Problem 8.9(ii).]
* Kiefer (1977a) Kiefer, J. (1977a). Conditional confidence statements and confidence estimators (with discussion). _Journal of the American Statistical Association_**72**, 789-827. [The key paper in Kiefer's proposed conditional confidence approach.]
* Kiefer (1977b) Kiefer, J. (1977b). Conditional confidence and estimated confidence in multi-decision problems (with applications to selections and ranking). _Multivariate Analysis_**IV**, 143-158.
* Kiefer and Schwartz (1965) Kiefer, J. and Schwartz, R. (1965). Admissible Bayes character of \(T^{2}\)-, \(R^{2}\)-, and other fully invariant tests for classical multivariate normal problems. _Annals of Mathematical Statistics_**36**, 747-770.

* King and Hillier (1985) King, M. L. and Hillier, G. H. (1985). Locally best invariance tests of the error covariance matrix of the linear regression model. _Journal of the Royal Statistical Society Series B_**47**, 98-102.
* Knight (1989) Knight, K. (1989). On the bootstrap of the sample mean in the infinite variance case. _Annals of Statistics_**17**, 1168-1175.
* Koehn and Thomas (1975) Koehn, U. and Thomas, D. L. (1975). On statistics independent of a sufficient statistic: Basu's Lemma. _American Statistician_**29**, 40-41.
* Kolassa and McCullagh (1990) Kolassa, J. and McCullagh, P. (1990). Edgeworth series for lattice distributions. _Annals of Statistics_**18**, 981-985.
* Kolmogorov (1933) Kolmogorov, A. (1933). Sulla adeterminazione empirica di una legge di distribuzione. _Giorn. Inst. Ital. Attuari_**4**, 83-91.
* Kolmogorov (1942) Kolmogorov, A. (1942). Sur l'estimation statistique des parametres de la loi de Gauss. _Bull. Acad. Sci. URSS Ser. Math._**6**, 3-32. (Russian-French summary.) [Definition of sufficiency in terms of distributions for the parameters.]
* Kolodziejczyk (1935) Kolodziejczyk, S. (1935). An important class of statistical hypotheses. _Biometrika_**37**, 161-190. [Discussion of the general linear univariate hypothesis from the likelihood-ratio point of view.]
* Koopman (1936) Koopman, B. (1936). On distributions admitting a sufficient statistic. _Trans. Amer. Math. Soc._**39**, 399-409.
* Korn et al. (2004) Korn, E., Troendle, J., McShane, L. and Simon, R. (2004). Controlling the number of false discoveries: Applications to high-dimensional genomic data. _Journal of Statistical Planning and Inference_**124**, 379-398.
* Koschat (1987) Koschat, M. (1987). A characterization of the Fieller solution. _Annals of Statistics_**15**, 462-468.
* Koshevnik and Levit (1976) Koshevnik, Y. and Levit, B. (1976). On a non-parametric analogue of the information matrix. _Theory of Probability and its Applications_ **21**, 738-753.
* Kotz et al. (1990) Kotz, S., Wang, Q., and Hung, K. (1990). Interrelations among various definitions of bivariate positive dependence. In _Topics in Statistical Dependence_. Block, Sampson and Savits, eds. (1990). IMS Lecture Notes **16**, Hayward, CA.
* Kowalski (1995) Kowalski, J. (1995). Complete classes of tests for regularly varying distributions. _Annals of the Institute of Statistical Mathematics_**47**, 321-350.
* Kowalski and Tu (2008) Kowalski, J. and Tu, X. (2008). _Modern Applied U-statistics_, John Wiley and Sons, Hoboken, NJ.
* Koziol (1983) Koziol, J. A. (1983). Tests for symmetry about an unknown value based on the empirical distribution function. _Communications in Statistics_**12**, 2823-2846.
* Krafft and Witting (1967) Krafft, O. and Witting, H. (1967). Optimale tests under ungunistigsten Verteilungen. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_**7**, 289-302.
* Kraft (1955) Kraft, C. (1955). Some conditions for consistency and uniform consistency of statistical procedures. _Univ. of Calif. Publ. in Statist._**2**, 125-142.
* Kruskal (1954) Kruskal, W. (1954). The monotonicity of the ratio of two non-central \(t\) density functions. _Annals of Mathematical Statistics_**25**, 162-165.
* Kruskal (1957) Kruskal, W. H. (1957). Historical notes on the Wilcoxon unpaired two-sample test. _Journal of the American Statistical Association_**52**, 356-360.
* Kruskal (1978) Kruskal, W. H. (1978). Significance, Tests of. In _International Encyclopedia of Statistics_, Free Press and Macmillan, New York and London.

Kuffner, T. and Walker, S. (2019). Why are p-values controversial? _The American Statistician_**73**, 1-3.
* Kunsch (1989) Kunsch, H. R. (1989). The jackknife and the bootstrap for general stationary observations. _Annals of Statistics_**17**, 1217-1241.
* Lahiri (2003) Lahiri, S. N. (2003). _Resampling Methods for Dependent Data._ Springer, New York.
* Lambert (1985) Lambert, D. (1985). Robust two-sample permutation tests. _Annals of Statistics_**13**, 606-625.
* Lambert and Hall (1982) Lambert, D. and Hall, W. (1982). Asymptotic lognormality of \(p\)-values. _Annals of Statistics_**10**, 44-64.
* Laplace (1773) Laplace, P. S. (1773). Memoire sur l'inclinaison moyenne des orbites des cometes. _Mem. Acad. Roy. Sci. Paris_**7** (1776). 503-524.
* Laplace (1812) Laplace, P. S. (1812). _Theorie Analytique des Probabilites_, Paris. (The 3rd edition of 1820 is reprinted as Vol. 7 of Laplace's collected works.)
* Lawless (1972) Lawless, J. F. (1972). Conditional confidence interval procedures for the location and scale parameters of the Cauchy and logistic distributions. _Biometrika_**59**, 377-386.
* Lawless (1973) Lawless, J. F. (1973). Conditional versus unconditional confidence intervals for the parameters of the Weibull distribution. _Journal of the American Statistical Association_**68**, 655-669.
* Lawless (1978) Lawless, J. F. (1978). Confidence interval estimation for the Weibull and extreme value distributions. _Technometrics_**20**, 355-368.
* Le Cam (1953) Le Cam, L. (1953). On some asymptotic properties of maximum likelihood estimates and related Bayes estimates. In _Univ. Calif. Publs. Statistics_, Vol. 1, pp. 277-329, Univ. of California Press, Berkeley and Los Angeles.
* Le Cam (1956) Le Cam, L. (1956). On the asymptotic theory of estimation and testing hypotheses. _Proc. 3rd Berkeley Symposium I_, 129-156.
* Le Cam (1958) Le Cam, L. (1958). Les propietes asymptotiques des solutions de Bayes. _Publ. Inst. Statist. Univ. Paris_. **VII** (3-4, 17-35.
* Le Cam (1960) Le Cam, L. (1960). Locally asymptotically normal families of distributions. _Univ. California Publ. Statist._**3**, 37-98.
* Le Cam (1964) Le Cam, L. (1964). Sufficiency and approximate sufficiency. _Annals of Mathematical Statistics_**35**, 1419-1455.
* Le Cam (1969) Le Cam, L. (1969). _Theorie Asymptotique de la Decision Statistique._ Presses de l'Universite de Montreal.
* Le Cam (1970) Le Cam, L. (1970). On the assumptions used to prove asymptotic normality of maximum likelihood estimators. _Annals of Mathematical Statistics_**41**, 802-828.
* Le Cam (1972) Le Cam, L. (1972). Limits of experiments. _Proc. 6th Berkeley Symp. on Math. Stat. and Prob. I_, 245-261.
* Le Cam (1979) Le Cam, L. (1979). On a theorem of J. Hajek. In _Contributions to Statistics: J. Hajek Memorial Volume_ (Jureckova, ed.), Academia, Prague. [Rigorous and very general treatment of the large-sample theory of maximum-likelihood estimates, with a survey of the large previous literature on the subject.]
* Le Cam (1986) Le Cam, L. (1986). _Asymptotic Methods in Statistical Decision Theory_. Springer-Verlag, New York.
* Le Cam (1990) Le Cam, L. (1990). On the standard asymptotic confidence ellipsoids of Wald. _International Statistical Review_. **58**, 129-152.

* Le Cam and Yang (2000) Le Cam, L. and Yang, G. (2000). _Asymptotics in Statistics, Some Basic Concepts_, 2nd edition. Springer-Verlag, New York.
* Ledwina (1994) Ledwina, T. (1994). Data-driven version of Neyman's smooth test of fit. _Journal of the American Statistical Association_**89**, 1000-1005.
* Lee (1990) Lee, A. (1990). _U-Statistics_, Marcel Dekker, New York, NY.
* Leger and Romano (1990a) Leger, C. and Romano, J. P. (1990a). Bootstrap adaptive estimation: the trimmed-mean example. _Canadian Journal of Statistics_**18**, 297-314.
* Leger and Romano (1990b) Leger, C. and Romano, J. P. (1990b). Bootstrap choice of tuning parameters. _Annals of the Institute of Statistical Mathematics_**42**, 709-735.
* Lehmann (1949) Lehmann, E. L. (1949). Some comments on large sample tests. In _Proceedings of the Berkeley Symposium on Mathematical Statistics and Probability_, University of CA Press, Berkeley. [Problem 15.27.]
* Lehmann (1950) Lehmann, E. L. (1950). Some principles of the theory of testing hypotheses. _Annals of Mathematical Statistics_**21**, 1-26.
* Lehmann (1951a) Lehmann, E. L. (1951a). A general concept of unbiasedness. _Annals of Mathematical Statistics_**22**, 587-597. [Definition (1.8); Problems 1.2, 1.3, 1.4, 1.6, 1.7, and 1.14.]
* Lehmann (1951b) Lehmann, E. L. (1951b). Consistency and unbiasedness of certain nonparametric tests. _Annals of Mathematical Statistics_**22**, 165-179.
* Lehmann (1952a) Lehmann, E. L. (1952a). Testing multiparameter hypotheses. _Annals of Mathematical Statistics_**23**, 541-552.
* Lehmann (1952b) Lehmann, E. L. (1952b). On the existence of least favorable distributions. _Annals of Mathematical Statistics_**23**, 408-416.
* Lehmann (1955) Lehmann, E. L. (1955). Ordered families of distributions. _Annals of Mathematical Statistics_**26**, 399-419. [Lemma 8.2.1; Problems 8.4, 8.12 (This problem is a corrected version of Theorem 8.5.1 of the paper in question. Thanks to R. Blumenthal for pointing out an error in the statement of this theorem in the paper.) and 8.13.]
* Lehmann (1957) Lehmann, E. L. (1957). A theory of some multiple decision procedures. (Parts I and II). _Annals of Mathematical Statistics_**28**, 1-25, 547-572.
* Lehmann (1958) Lehmann, E. L. (1958). Significance level and power. _Annals of Mathematical Statistics_**29**, 1167-1176.
* Lehmann (1961) Lehmann, E. L. (1961). Some model I problems of selection. _Annals of Mathematical Statistics_**32**, 990-1012.
* Lehmann (1980) Lehmann, E. L. (1980). An interpretation of completeness and Basu's theorem. _Journal of the American Statistical Association_**76**, 335-340. [Problem 5.70.]
* Lehmann (1985a) Lehmann, E. L. (1985a). The Neyman-Pearson theory after 50 years. In _Proc. Neyman-Kiefer Conference_ (LeCam and Olshen. eds.), Wadsworth, Belmont, CA.
* Lehmann (1985b) Lehmann, E. L. (1985b). The Neyman-Pearson Lemma. In _Encycl. Stat. Sci._**6**, 224-230.
* Lehmann (1993) Lehmann, E. L. (1993). The Fisher, Neyman-Pearson theories of testing hypotheses: one theory or two? _Journal of the American Statistical Association_**78**, 1242-1249.
* Lehmann (1997) Lehmann, E. L. (1997). Testing statistical hypotheses: the story of a book. _Statistical Science_**12**, 48-52.

* Lehmann (1998) Lehmann, E. L. (1998). _Nonparametrics: Statistical Methods Based on Ranks_, revised first edition. Prentice Hall, Upper Saddle River, New Jersey. [Previous edition by Holden-Day (1975).]
* Lehmann (1999) Lehmann, E. L. (1999). _Elements of Large-Sample Theory_. Springer-Verlag, New York.
* Lehmann (2008) Lehmann, E. L. (2008). _Reminiscences of a Statistician: The Company I Kept_. Springer, New York.
* Lehmann and Casella (1998) Lehmann, E. L. and Casella, G. (1998). _Theory of Point Estimation_, Second Edition, Springer-Verlag, New York.
* Lehmann and Loh (1990) Lehmann, E. L. and Loh, W-Y. (1990). Pointwise versus uniform robustness in some large-sample tests and confidence intervals. _Scandinavian Journal of Statistics_**17**, 177-187.
* Lehmann and Rojo (1992) Lehmann, E. L. and Rojo, J. (1992). Invariant directional orderings. _Annals of Statistics_**20**, 2100-2110.
* Lehmann and Romano (2005) Lehmann, E. L. and Romano, J. P. (2005). Generalizations of the familywise error rate. _Annals of Statistics_**33**, 1138-1154.
* Lehmann et al. (2005) Lehmann, E. L., Romano, J. P., and Shaffer, J. P. (2005). On optimality of stepdown and stepup procedures. _Annals of Statistics_**33**, 1084-1108.
* Lehmann and Scheffe (1950) Lehmann, E. L. and Scheffe, H. (1950, 1955). Completeness, similar regions, and unbiased estimation. _Sankhya_**10**, 305-340; **15**, 219-236. [Introduces the concept of completeness. Theorem 4.4.1 and applications.]
* Lehmann and Shaffer (1979) Lehmann, E. L. and Shaffer, J. P. (1979). Optimal significance levels for multistage comparison procedures. _Annals of Statistics_**7**, 27-45.
* Lehmann and Stein (1948) Lehmann, E. L. and Stein, C. M. (1948). Most powerful tests of composite hypotheses. _Annals of Mathematical Statistics_**19**, 495-516. [Theorem 3.8.1 and applications.]
* Lehmann and Stein (1949) Lehmann, E. L. and Stein, C. M. (1949). On the theory of some non-parametric hypotheses. _Annals of Mathematical Statistics_**20**, 28-45. [Develops the theory of optimum permutation tests, Problem 8.38.]
* Lehmann and Stein (1953) Lehmann, E. L. and Stein, C. M. (1953). The admissibility of certain invariant statistical tests involving a translation parameter. _Annals of Mathematical Statistics_**24**, 473-479.
* Lentner and Buehler (1963) Lentner, M. M. and Buehler, R. (1963). Some inferences about gamma parameters with an application to a reliability problem. _Journal of the American Statistical Association_**58**, 670-677.
* Levy and Narula (1974) Levy, K. J. and Narula, S. C. (1974). Shortest confidence intervals for the ratio of two normal variances. _Canadian Journal of Statistics_**2**, 83-87.
* Lexis (1875) Lexis, W. (1875). _Einleitung in die Theorie der Bevilkerungsstatistik_, Strassburg.
* Lexis (1877) Lexis, W. (1877). _Zur Theorie der Massenerscheinungen in der Menschlichen Gesellschaft_, Freiburg.
* Li and Ding (2017) Li, X. and Ding, P. (2017). General forms of finite population central limit theorems with applications to causal inference. _Journal of the American Statistical Association_**112**, 1759-1769.
* Liang (1984) Liang, K. Y. (1984). The asymptotic efficiency of conditional likelihood methods. _Biometrika_**71**, 305-313.
* Liang and Self (1985) Liang, K.Y. and Self, S. G. (1985). Tests for homogeneity of odds ratio when the data are sparse. _Biometrika_**72**, 353-358.
* Lieberman and Owen (1961) Lieberman, G. J. and Owen, D. B. (1961). _Tables of the Hypergeometric Probability Distribution_, Stanford University Press.

* Lindley (1957) Lindley, D. V. (1957). A statistical paradox. _Biometrika_**44**, 187-192.
* Linnik _et al._ (1968) Linnik, Y. V., Pliss, V. A. and Salaevskii, O. V. (1968). On the theory of Hotelling's test (Russian). Dok. AN SSSR **168**, 743-746.
* Littell and Louv (1981) Littell, R. C. and Louv, W. C. (1981). Confidence regions based on methods of combining test statistics. _Journal of the American Statistical Association_**76**, 125-130.
* Liu and Berger (1995) Liu, H. and Berger, R. (1995). Uniformly more powerful one-sided tests for hypotheses about linear inequalities. _Annals of Statistics_**23**, 55-72.
* Liu and Singh (1987) Liu, R. Y. and Singh,K. (1987). On a partial correction by the bootstrap. _Annals of Statistics_**15**, 1713-1718.
* Liu and Singh (1992) Liu, R. Y. and Singh, K. (1992). Moving blocks jackknife and bootstrap capture weak dependence. In _Exploring the Limits of Bootstrap_, 225-248. Edited by LePage, R. and Billard, L., John Wiley, New York.
* Liu and Singh (1997) Liu, R. Y. and Singh, K. (1997). Notions of limiting \(P\)-values based on data depth and bootstrap. _Journal of the American Statistical Association_**92**, 266-277.
* Loh (1984a) Loh, W.-Y. (1984a). Strong unimodality and scale mixtures. _Annals of the Institute of Statistical Mathematics_**36**, 441-450.
* Loh (1984b) Loh, W.-Y. (1984b). Bounds on ARE's for restricted classes of distributions defined via tail-orderings. _Annals of Statistics_**12**, 685-701.
* Loh (1985) Loh, W.-Y. (1985). A new method for testing separate families of hypotheses. _Journal of the American Statistical Association_**80**, 362-368.
* Loh (1987) Loh, W.-Y. (1987). Calibrating confidence coefficients. _Journal of the American Statistical Association_**82**, 155-162.
* Loh (1989) Loh, W.-Y. (1989). Bounds on the size of the \(\chi^{2}\)-test of independence in a contingency table. _Annals of Statistics_**17**, 1709-1722.
* Loh (1991) Loh, W.-Y. (1991). Bootstrap calibration for confidence interval construction and selection. _Statistica Sinica_**1**, 479-495.
* Loomis (1953) Loomis, L. H. (1953). _An Introduction to Abstract Harmonic Analysis_. Van Nostrand, New York.
* Theory and Methods_**13**, 2601-2623.
* Lou (1996) Lou, W. (1996). On runs and longest run tests: a method of finite Markov chain embedding. _Journal of the American Statistical Association_**91**, 1595-1601.
* Low (1997) Low, M. (1997). On nonparametric confidence intervals. _Annals of Statistics_**25**, 2547-2554.
* Lyapounov (1940) Lyapounov, A. M. (1940). Sur les fonctions-vecteurs completement additives, _Izv. Akad. Nauk SSSR Ser. Mat._**4**, 465-478.
* Maatta and Casella (1987) Maatta, J. and Casella, G. (1987). Conditional properties of interval estimators of the normal variance. _Annals of Statistics_**15**, 1372-1388.
* Mack and Skillings (1980) Mack, G. A. and Skillings, J. H. (1980). A Friedman type rank test for main effects in a two-factor ANOVA. _Journal of the American Statistical Association_**75**, 947-951.
* Madansky (1962) Madansky, A. (1962). More on length of confidence intervals. _Journal of the American Statistical Association_**57**, 586-599.
* Madansky (1963)* Mandelbaum and Ruschendorf (1987) Mandelbaum, A. and Ruschendorf, L. (1987). Complete and symmetrically complete families of distributions. _Annals of Statistics_**15**, 1229-1244.
* Mann (1945) Mann, H. (1945). Nonparametric tests against trend. _Econometrica_**13**, 245-259.
* Mann and Wald (1942) Mann, H. and Wald, A. (1942). On the choice of the number of intervals in the application of the chi-square test. _Annals of Mathematical Statistics_**13**, 306-317.
* Mantel (1987) Mantel, N. (1987). Understanding Wald's test for exponential families. _American Statistician_**41**, 147-149.
* Marasinghe and Johnson (1981) Marasinghe, M. C. and Johnson, D. E. (1981). Testing subhypotheses in the multiplicative interaction model. _Technometrics_**23**, 385-393.
* Marcus et al. (1976) Marcus, R., Peritz, E. and Gabriel, K. R. (1976). On closed testing procedures with special reference to ordered analysis of variance. _Biometrika_**63**, 655-660.
* Marden (1982a) Marden, J. (1982a). Minimal complete classes of tests of hypotheses with multivariate one-sided alternatives. _Annals of Statistics_**10**, 962-970.
* Marden (1982b) Marden, J. (1982b). Combining independent noncentral chi-squared or \(F\)-tests. _Annals of Statistics_**10**, 266-270.
* Marden (1985) Marden, J. (1985). Combining independent one-sided noncentral \(t\) or normal mean tests. _Annals of Statistics_**13**, 1535-1553.
* Marden (1991) Marden, J. (1991). Sensitive and sturdy \(p\)-values. _Annals of Statistics_**19**, 918-934.
* Marden (2000) Marden, J. (2000). Hypothesis testing: from \(p\)-values to Bayes factors. _Journal of the American Statistical Association_**95**, 1316-1320.
* Marden and Muyot (1995) Marden, J. and Muyot, M. (1995). Rank tests for main and interaction effects in analysis of variance. _Journal of the American Statistical Association_**90**, 1388-1398.
* Marden and Perlman (1980) Marden, J. and Perlman, M. (1980). Invariant tests for means with covariates. _Annals of Statistics_**8**, 25-63.
* Mardia and Zemroch (1978) Mardia, K. V. and Zemroch, P. J. (1978). _Tables of the \(F\)- and Related Distributions with Algorithms_. Academic Press, London. [Extensive tables of critical values for the central \(F\)- and related distributions.]
* Maritz (1979) Maritz, J. S. (1979). A note on exact robust confidence intervals for location. _Biometrika_**66**, 163-166. [Problem 5.46(ii).]
* Marshall and Olkin (1979) Marshall, A. W. and Olkin, I. (1979). _Inequalities: Theory of Majorization and Its Applications_. Academic Press, New York.
* Martin and Tapia (1998) Martin, A. and Tapia, J. (1998). On determining the \(p\)-value in 2 x 2 multinomial trials. _Journal of Statistical Planning and Inference_**69**, 33-49.
* Massart (1990) Massart, P. (1990). The tight constant in the Dvoretsky-Kiefer-Wolfowitz inequality. _Annals of Probability_**18**, 1269-1283.
* Massey (1950) Massey, F. J. (1950). A note on the power of a non-parametric test. _Annals of Mathematical Statistics_**21**, 440-443.
* Mathew and Sinha (1988a) Mathew, M. and Sinha, B. (1988a). Optimum tests for fixed effects and variance components in balanced models. _Journal of the American Statistical Association_**83**, 133-135.
* Mathew and Sinha (1988b) Mathew, M. and Sinha, B. (1988b). Optimum tests in unbalanced two-way models without interaction. _Annals of Statistics_**16**, 1727-1740.

* Mattner (1993) Mattner, L. (1993). Some incomplete but boundedly complete location families. _Annals of Statistics_ **21**, 2158-2162.
* Mattner (1996) Mattner, L. (1996). Complete order statistics in parametric models. _Annals of Statistics_**24**, 1265-1282.
* McCullagh (1985) McCullagh, P. (1985). On the asymptotic distribution of Pearson's statistic in linear exponential-family models. _International Statistical Review_**53**, 61-67.
* McCullagh (1986) McCullagh, P. (1986). The conditional distribution of goodness-of-fit statistics for discrete data. _Journal of the American Statistical Association_ **81**, 104-107.
* McCullagh and Nelder (1989) McCullagh, P. and Nelder, J. (1989). _Generalized Linear Models_, 2nd edition. Chapman & Hall, London.
* McCulloch and Searle (2001) McCulloch, C. and Searle, S. (2001). _Generalized, Linear, and Mixed Models_. John Wiley, New York.
* McDermott and Wang (2002) McDermott, M. and Wang, Y. (2002). Construction of uniformly more powerful tests for hypotheses about linear inequalities. _Journal of Statistical Planning and Inference_**107**, 207-217.
* McDonald et al. (1977) McDonald, L. L., Davis, B. M. and Milliken, G. A. (1977). A nonrandomized unconditional test for comparing two proportions in \(2\times 2\) contingency tables. _Technometrics_**19**, 145-158.
* McElroy and Politis (2020) McElroy, T. and Politis, D. (2020). _Time Series: A First Course with Bootstrap Starter_. Chapman & Hall/CRC, Boca Raton, Florida.
* McKean and Schrader (1982) McKean, J. and Schrader, R. M. (1982). The use and interpretation of robust analysis of variance. In _Modern Data Analysis_ (Launer and Siegel. eds.). Academic Press, New York.
* McMurry et al. (2012) McMurry, T.L., Politis, D.N. and Romano, J.P. (2012) Subsampling inference with \(K\) populations and a nonstandard Behrens-Fisher problem. _International Statistical Review_**80**, 149-175.
* Mee (1990) Mee, R. (1990). Confidence intervals for probabilities and tolerance regions based on a generalization of the Mann-Whitney statistic. _Journal of the American Statistical Association_**85**, 793-800.
* Meeks and D'Agostino (1983) Meeks, S. L. and D'Agostino, R. (1983). A note on the use of confidence limits following rejection of a null hypothesis. _American Statistician_**37**, 134-136.
* Theory and Methods_**43**, 2830-2843.
* Meng (1994) Meng, X. (1994). Posterior predictive \(p\)-values. _Annals of Statistics_**22**, 1142-1160.
* Michel (1979) Michel, R. (1979). On the asymptotic efficiency of conditional tests for exponential families. _Annals of Statistics_**7**, 1256-1263.
* Milbrodt and Strasser (1990) Milbrodt, H. and Strasser, H. (1990). On the asymptotic power of the two-sided Kolmogorov-Smirnov test. _Journal of Statistical Planning and Inference_**26**, 1-23.
* Millar (1983) Millar, W. (1983). The minimax principle in asymptotic statistical theory. In _Ecole d'Ete de Probabilites de Saint Flour XI 1981_ (P.L. Hennequin, ed.), 75-266. Lecture Notes in Mathematics **976**, Springer-Verlag, Berlin.
* Millar (1985) Millar, W. (1985). Nonparametric applications of an infinite dimensional convolution theorem. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_**68**, 545-556.
* Miller et al. (1998) Miller, F. R., Neill, J. and Sherfey, B. (1998). Maximin clusters for near-replicate regression lack of fit tests. _Annals of Statistics_**26**, 1411-1433.
* Miller and Quesenberry (1979) Miller, F. L. and Quesenberry, C. (1979). Power studies of tests for uniformity II. _Communications in Statist. Simulation. Comput._**8**, 271-290.

* Miller (1977a) Miller, J. (1977a). Asymptotic properties of maximum likelihood estimates in the mixed model of the analysis of variance. _Annals of Statistics_**5**, 746-762.
* Miller (1977b) Miller, R. G. (1977b). Developments in multiple comparisons 1966-1976. _Journal of the American Statistical Association_**72**, 779-788.
* Miller (1981) Miller, R. G. (1981). _Simultaneous Statistical Inference_, 2nd edition. Springer, New York.
* Miller (1986) Miller, R. G. (1986). _Beyond Anova_. John Wiley, New York.
* Miwa and Hayter (1999) Miwa, T. and Hayter, T. (1999). Combining the advantages of one-sided and two-sided test procedures for comparing several treatment effects. _Journal of the American Statistical Association_**94**, 302-307.
* Mogstad et al. (2020) Mogstad, M., Romano, J., Shaikh, A. and Wilhelm, D. (2020). Inference for ranks with applications to mobility across neighborhoods and academic achievement across countries. National Bureau of Economics Research Working Paper Series, Working Paper 26883.
* Mokkadem (2001) Mokkadem, A. Mixing properties of ARMA processes. _Stochastic Processes and their Applications_**29**, 309-315.
* Montgomery (2001) Montgomery, D. (2001). _Design and Analysis of Experiments_, 5th edition. John Wiley, New York.
* Morgan (1939) Morgan, W. A. (1939). A test for the significance of the difference between the two variances in a sample from a normal bivariate population. _Biometrika_**31**, 13-19.
* Morgenstern (1956) Morgenstern, D. (1956). Einfache Beispiele zweidimensionaler Verteilungen, _Mitteil. Math. Statistick_**8**, 234-235.
* Mosteller and Tukey (1977) Mosteller, F. and Tukey, J. W. (1977). _Data Analysis and Regression: A Second Course in Statistics_, Addison-Wesley, MA.
* Mottonen et al. (1997) Mottonen, J., Oja, H. and Tienari, J. (1997). On the efficiency of multivariate spatial sign and rank tests. _Annals of Statistics_**25**, 542-552.
* Mudholkar (1983) Mudholkar, G. S. (1983). Fisher's \(z\)-transformation. _Encyclopedia of Statistical Science_**3**, 130-135. (S. Kotz, N.L. Johnson, C.B. Read, eds.)
* Muller (1998) Muller, C. (1998). Optimum robust testing in linear models. _Annals of Statistics_**26**, 1126-1146.
* Murphy and van der Vaart (1997) Murphy, S. and van der Vaart, A. (1997). Semiparametric likelihood ratio inference. _Annals of Statistics_**25**, 1471-1509.
* Nachbin (1965) Nachbin, L. (1965). _The Haar Integral_. Van Nostrand, New York.
* Naiman (1984a) Naiman, D. Q. (1984a). Average width optimality of simultaneous confidence bounds. _Annals of Statistics_**12**, 1199-1214.
* Naiman (1984b) Naiman, D. Q. (1984b). Optimal simultaneous confidence bounds. _Annals of Statistics_**12**, 702-715.
* Neubert and Brunner (2007) Neubert, K. and Brunner, E. (2007). A studentized permutation test for the non-parametric Behrens-Fisher problem. _Comput. Statist. Data Anal._**51**, 5192-5204.
* Neuhaus (1979) Neuhaus, G. (1979). Asymptotic theory of goodness of fit tests when parameters are present: A survey. _Statistics_**10**, 479-494.
* Neuhaus (1993) Neuhaus, G. (1993). Conditional rank tests for the two-sample problem under random censorship. _Ann. Statist._**21**, 1760-1779.
* Neumann (2013) Neumann, M. (2013). A central limit theorem for triangular arrays of weakly dependent random variables, with applications in statistics. _ESAIM: Probability and Statistics_**17**, 120-134.

Neyman, J. (1923). On the application of probability theory to agriculture experiments. Essay on Principles. Section 9. Translated and edited by D. Dabrowska and T. Speed in (1990), _Statistical Science_**5**, 465-480, with comments by D. Rubin. The Polish original appeared in Roczniki Nauk Rolniczych Tom X (1923), 1-51 (_Annals of Agricultural Sciences_).
* Neyman (1935a) Neyman, J. (1935a). Sur un teorema concernente le cosidette statistiche sufficienti. _Giorn. Ist. Ital. At._**6**. 320-334.
* Neyman (1935b) Neyman, J. (1935b). Sur la verification des hypotheses statistiques composees. _Bull. Soc. Math. France_**63**, 246-266. [Defines, and shows how to derive, tests of type B, that is, tests which are LMP among locally unbiased tests in the presence of nuisance parameters.]
* Neyman (1935c) Neyman, J. (1935c). Statistical problems in agricultural experimentation (with Discussion). _Suppl. J. R. Statist. Soc._**2**, 107-180.
* Neyman (1937a) Neyman, J. (1937a). Outline of a theory of statistical estimation based on the classical theory of probability. _Phil. Trans. Roy. Soc. Ser. A._**236**, 333-380.
* Neyman (1937b) Neyman, J. (1937b). Smooth test for goodness of fit. _Skand. Aktuarietidskr._**20**, 150-199.
* Neyman (1938a) Neyman, J. (1938a). L'estimation statistique traitee comme un probleme classique de probabilite. _Actualites Sci. et Ind._**739**, 25-57.
* Neyman (1938b) Neyman, J. (1938b). _Lectures and Conferences on Mathematical Statistics and Probability_, 1st edition (2nd edition, 1952), Graduate School, U.S. Dept. of Agriculture, Washington.
* Neyman (1939) Neyman, J. (1939). On statistics the distribution of which is independent of the parameters involved in the original probability law of the observed variables, _Statist. Res. Mem._**2**, 59-89. [Essentially Theorem 5.1.2 under regularity assumptions.]
* Neyman (1941a) Neyman, J. (1941a). On a statistical problem arising in routine analyses and in sampling inspection of mass distributions. _Annals of Mathematical Statistics_**12**, 46-76. [Theory of tests of composite hypotheses that are locally unbiased and locally most powerful.]
* Neyman (1941b) Neyman, J. (1941b). Fiducial argument and the theory of confidence intervals. _Biometrika_**32**, 128-150.
* Neyman (1949) Neyman, J. (1949). Contribution to the theory of the \(\chi^{2}\) test. In _Proc. Berkeley Symposium on Mathematical Statistics and Probability_, Univ. of California Press, Berkeley, 239-273. [Gives a theory of \(\chi^{2}\) tests with restricted alternatives.]
* Neyman (1952) Neyman, J. (1952). _Lectures and Conferences on Mathematical Statistics_, 2nd edition Washington Graduat School, U.S. Dept. of Agriculture, 43-66. [An account of various approaches to the problem of hypothesis testing.]
* Neyman (1967) Neyman, J. (1967). _A Selection of Early Statistical Papers of J. Neyman_, Univ. of California Press, Berkeley. [Puts forth the point of view that statistics is primarily concerned with how to behave under uncertainty rather than with determining the values of unknown parameters, with inductive behavior rather than with inductive inference.]
* Neyman and Pearson (1928) Neyman, J. and Pearson, E. S. (1928). On the use and interpretation of certain test criteria for purposes of statistical inference. _Biometrika_**20A**, 175-240, 263-295.
* Neyman and Pearson (1933a) Neyman, J. and Pearson, E. S. (1933a). On the testing of statistical hypotheses in relation to probability a priori. _Proc. Cambridge Phil. Soc._**29**, 492-510.
* Neyman and Pearson (1933b) Neyman, J. and Pearson, E. S. (1933b). On the problem of the most efficient tests of statistical hypotheses. _Phil. Trans Roy. Soc. Ser. A_**231**, 289-337.
* Neyman and Pearson (1936a) Neyman, J. and Pearson, E. S. (1936a). Contributions to the theory of testing statistical hypotheses. I. Unbiased critical regions of type A and type A\({}_{1}\). _Statist. Res. Mem._**1**. 1-37.

Neyman, J. and Pearson, E. S. (1936b). Sufficient statistics and uniformly most powerful tests of statistical hypotheses. _Statist. Res. Mem._**1**, 113-137. [Problem 3.4(ii).]
* Neyman and Pearson (1936) Neyman, J. and Pearson, E. S. (1936, 1938). Contributions to the theory of testing statistical hypotheses. _Statist. Res. Mem._**1**, 1-37; **2**, 25-57. [Defines unbiasedness and determines both locally and UMP unbiased tests of certain classes of simple hypotheses. Discusses tests of types A, that is, tests which are LMP among locally unbiased tests when no nuisance parameters are present.]
* Neyman and Pearson (1967) Neyman, J. and Pearson, E. S. (1967). _Joint Statistical Papers of J. Neyman and E. S. Pearson_, Univ. of California Press, Berkeley. [In connection with the problem of hypothesis testing, suggests assigning weights for the various possible wrong decisions and the use of the minimax principle.]
* Nicolaou (1993) Nicolaou, A. (1993). Bayesian intervals with good frequentist behaviour in the presence of nuisance parameters. _Journal of the Royal Statistical Society Series B_**55**, 377-390.
* Niederhausen (1981) Niederhausen, H. (1981). Scheffer polynomials for computing exact Kolmogorov-Smirnov and Renyi type distributions. _Annals of Statistics_**9**, 923-944.
* Nikitin (1995) Nikitin, Y. (1995). _Asymptotic Efficiency of Nonparametric Tests._ Cambridge University Press.
* Noether (1955) Noether, G. (1955). On a theorem of Pitman. _Annals of Mathematical Statistics_**26**, 64-68.
* Nogales and Oyola (1996) Nogales, A. and Oyola, J. (1996). Some remarks on sufficiency, invariance and conditional independence. _Annals of Statistics_**24**, 906-909.
* Nogales et al. (2000) Nogales, A., Oyola, J. and Perez, P. (2000). Invariance, almost invariance and sufficiency. _Statistica_, LX, 277-286.
* Nolle and Plachky (1967) Nolle, G. and Plachky, D. (1967). Zur schwachen Folgenkompaktheit von Testfunktionen. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_**8**, 182-184.
* Nomakuchi and Sakata (1987) Nomakuchi, K. and Sakata, T. (1987). A note on testing two-dimensional normal mean. _Annals of the Institute of Statistical Mathematics_**39**, 489-495.
* Oden and Wedel (1975) Oden, A. and Wedel, H. (1975). Arguments for Fisher's permutation test. _Annals of Statistics_**3**, 518-520.
* Olshen (1973) Olshen, R. A. (1973). The conditional level of the \(F\)-test. _Journal of the American Statistical Association_**68**, 692-698.
* Omelka and Pauly (2012) Omelka, M. and Pauly, M. (2012). Testing equality of correlation coefficients in two populations via permutation methods. _Journal of Statistical Planning and Inference_**142**, 1396-1406.
* Oosterhoff and van Zwet (1979) Oosterhoff, J. and van Zwet, W. (1979). A note on contiguity and Hellinger distance. _Contributions to Statistics_, Reidel, Dordrecht-Boston, Mass. London, 157-166.
* Owen (1988) Owen, A. (1988). Empirical likelihood ratio confidence intervals for a single functional. _Biometrika_**72**, 45-58.
* Owen (1995) Owen, A. (1995). Nonparametric likelihood confidence bounds for a distribution function. _Journal of the American Statistical Association_**90**, 516-521.
* Owen (2001) Owen, A. (2001). _Empirical Likelihood_. Chapman & Hall, New York.
* Owen (1985) Owen, D. B. (1985). Noncentral \(t\)-distribution. _Encycl. Statist. Sci._**6**, 286-290.
* Pace and Salvan (1990) Pace, L. and Salvan, A. (1990). Best conditional tests for separate families of hypotheses. _Journal of the Royal Statistical Society Series B_**52**, 125-134.
* Pachares (1961) Pachares, J. (1961). Tables for unbiased tests on the variance of a normal population. _Annals of Mathematical Statistics_**32**, 84-87.

* Patel and Read (1982) Patel, J. K. and Read, C. B. (1982). _Handbook of the Normal Distribution_. Marcel Dekker, New York.
* Pawitan (2000) Pawitan, Y. (2000). A reminder of the fallibility of the Wald statistic: likelihood explanation. _American Statistician_**54**, 54-56.
* Pearson (1929) Pearson, E. S. (1929). Some notes on sampling tests with two variables. _Biometrika_**21**. 337-360.
* Pearson (1966) Pearson, E. S. (1966). The Neyman-Pearson story: 1926-1934. In _Research Papers in Statistics: Festschrift for J. Neyman_ (F. N. David, ed.), John Wiley, New York.
* Pearson and Hartley (1972) Pearson, E. S. and Hartley, H. O. (1972). _Biometrika Tables for Statisticians_. Cambridge University Press, Cambridge.
* Pearson (1900) Pearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. _Philosophical Magazine, Series 5_**50**, 157-175. (Reprinted in: _Karl Pearson's Early Statistical Papers_, Cambridge University Press, 1956). [The \(\chi^{2}\)-test is proposed for testing a simple multinomial hypothesis, and the limiting distribution of the test criterion is obtained under the hypothesis. The test is extended to composite hypotheses but contains an error in the degrees of freedom of the limiting distribution; a correct solution for the general case was found by Fisher (1924a). Applications.]
* Pena (1998) Pena, E. (1998). Smooth goodness-of-fit tests for composite hypothesis in hazard based models. _Annals of Statistics_**26**, 1935-1971.
* Pereira (1977) Pereira, B. (1977). Discriminating among separate models: A bibliography. _International Statistical Review_**45**, 163-172.
* Peritz (1965) Peritz, E. (1965). On inferring order relations in analysis of variance. _Biometrics_**21**, 337-344.
* Perlman (1969) Perlman, M. (1969). One-sided testing problems in multivariate analysis. _Annals of Mathematical Statistics_**40**, 549-567. [Correction: _Annals of Mathematical Statistics_**42** (1971), 1777.]
* Perlman (1972) Perlman, M. (1972). On the strong consistency of approximate maximum likelihood estimators. _Proceedings of the Sixth Berkeley Symposium in Mathematical Statistics_**1**, University of California Press, 263-281.
* Perlman and Wu (1999) Perlman, M. and Wu, L. (1999). The Emperor's new tests. _Statistical Science_**14**, 355-381.
* Pesarin (2001) Pesarin, F. (2001). _Multivariate Permutation Tests With Applications in Biostatistics_. John Wiley, Chichester, England.
* Peters and Randles (1991) Peters, D. and Randles, R. (1991). A bivariate signed rank test for the two-sample location problem. _Journal of the Royal Statistical Society Series B_**53**, 493-504.
* Pfanzagl (1967) Pfanzagl, J. (1967). A technical lemma for monotone likelihood ratio families. _Annals of Mathematical Statistics_**38**, 611-613.
* Pfanzagl (1968) Pfanzagl, J. (1968). A characterization of the one parameter exponential family by existence of uniformly most powerful tests. _Sankhya Series A_ **30**, 147-156.
* Pfanzagl (1974) Pfanzagl, J. (1974). On the Behrens-Fisher problem. _Biometrika_**61**, 39-47.
* Pfanzagl (1979) Pfanzagl, J. (1979). On optimal median unbiased estimators in the presence of nuisance parameters. _Annals of Statistics_**7**, 187-193.
* Pfanzagl (1982) Pfanzagl, J. (with the assistance of W. Wefelmeyer) (1982). _Contributions to a General Asymptotic Theory_. Springer-Verlag, New York.

Pfanzagl, J. (with the assistance of W. Wefelmeyer) (1985). _Asymptotic Expansions for General Statistical Models._ Springer-Verlag, New York, NY.
* Piegorsch (1985a) Piegorsch, W. W. (1985a). Admissible and optimal confidence bounds in simple linear regression. _Annals of Statistics_**13**, 801-817.
* Piegorsch (1985b) Piegorsch, W. W. (1985b). Average width optimality for confidence bands in simple linear regression. _Journal of the American Statistical Association_**80**, 692-697.
* Pierce (1973) Pierce, D. A. (1973). On some difficulties in a frequency theory of inference. _Annals of Statistics_**1**, 241-250.
* Pitman (1937) Pitman, E. J. G. (1937, 1938a). Significance tests which may be applied to samples from any population, _J. Roy. Statist. Soc. Suppl._**4**, 119-130, 225-232; Biometrika **29**, 322-335.
* Pitman (1938b) Pitman, E. J. G. (1938b). The estimation of the location and scale parameters of a continuous population of any given form. _Biometrika_**30**, 391-421.
* Pitman (1939a) Pitman, E. J. G. (1939a). A note on normal correlation. _Biometrika_**31**, 9-12. [Problem 5.39(i).]
* Pitman (1939b) Pitman, E. J. G. (1939b). Tests of hypotheses concerning location and scale parameters. _Biometrika_**31**, 200-215. [In these papers the restriction to invariant procedures is introduced for estimation and testing problems involving location and scale parameters.]
* Pitman (1949) Pitman, E. J. G. (1949). Lecture notes on nonparametric statistical inference, unpublished. [Develops the concept of relative asymptotic efficiency and applies it to several examples including the Wilcoxon test.]
* Plackett (1977) Plackett, R. L. (1977). The marginal totals of a \(2\times 2\) table. _Biometrika_**64**. 37-42. [Discusses the fact that the marginals of a \(2\times 2\) table supply some, but only little, information concerning the odds ratio. See also Barndorff-Nielsen (1978), Example 10.8.]
* Plackett (1981) Plackett, R. L. (1981). _The Analysis of Categorical Data_, 2nd edition. MacMillan, New York.
* Politis and Romano (1994a) Politis, D. N. and Romano, J. P. (1994a). The stationary bootstrap. _Journal of the American Statistical Association_**89**, 1303-1313.
* Politis and Romano (1994b) Politis, D. N. and Romano, J. P. (1994b). Large sample confidence regions based on subsamples under minimal assumptions. _Annals of Statistics_**22**, 2031-2050.
* Politis and Romano (2008) Politis, D. N. and Romano, J. P. (2008). \(K\)-sample subsampling. In _Proceedings of the First International Workshop on Functional and Operational Statistics, Toulouse, June 19-21, 2008_, Springer Verlag, Berlin.
* Politis and Romano (2010) Politis, D. N. and Romano, J. P. (2010). \(K\)-sample subsampling in general spaces: the case of independent time series. _Journal of Multivariate Analysis_**101**, 316-326.
* Politis and Romano and Wolf (1999) Politis, D. N., Romano, J. P. and Wolf, M. (1999). _Subsampling_. Springer, New York.
* Pollard (1984) Pollard, D. (1984). _Convergence of Stochastic Processes._ Springer-Verlag, New York.
* Pollard (1997) Pollard, D. (1997). Another look at differentiability in quadratic mean. In _Festschrift for Lucien Le Cam_, 305-314. Springer-Verlag, New York.
* Polonik (1999) Polonik, W. (1999). Concentration and goodness-of-fit in higher dimensions: (Asymptotically) distribution-free methods. _Annals of Statistics_**27**, 1210-1229.
* Posten _et al._ (1982) Posten, H. O., Yeh, H. C. and Owen, D. B. (1982). Robustness of the two-sample \(t\)-test under violations of the homogeneity of variance assumption. _Communications in Statistics_**11**, 109-126.

* Pratt (1958) Pratt, J. W. (1958). Admissible one-sided tests for the mean of a rectangular distribution. _Annals of Mathematical Statistics_**29**, 1268-1271.
* Pratt (1961a) Pratt, J. W. (1961a). Length of confidence intervals. _Journal of the American Statistical Association_**56**, 549-567.
* Pratt (1961b) Pratt, J. W. (1961b). Review of Testing Statistical Hypotheses by E L. Lehmann. _Journal of the American Statistical Association_**56**. 163-167. [Problems 10.27, 10.28.]
* Pratt (1962) Pratt, J. W. (1962). A note on unbiased tests. _Annals of Mathematical Statistics_**33**, 292-294.
* Pratt (1964) Pratt, J. W. (1964). Robustness of some procedures for the two-sample location problem. _Journal of the American Statistical Association_**59**, 665-680. [Proposes and illustrates approach (ii) of Section 1.]
* Prescott (1975) Prescott, P. (1975). A simple alternative to Student's \(t\). _Applied Statistics_**24**, 210-217.
* Pukelsheim (1993) Pukelsheim, F. (1993). _Optimal Design of Experiments_. John Wiley, New York.
* Quenouille (1949) Quenouille, M. (1949). Approximate tests of correlation in time series. _Journal of the Royal Statistical Society Series B_**11**, 68-84.
* Quesenberry and Starbuck (1976) Quesenberry, C. P. and Starbuck, R. R. (1976). On optimal tests for separate hypotheses and conditional probability integral transformations. _Communications in Statistics (A)_**1**, 507-524.
* Quine and Robinson (1985) Quine, M. P. and Robinson, J. (1985). Efficiencies of chi-square and likelihood ratio goodness-of-fit tests. _Annals of Statistics_ 13, 727-742.
* Radlow and Alf (1975) Radlow, R. and Alf, E. (1975). An alternative multinomial assessment of the accuracy of the chi-squared test of goodness of fit. _Journal of the American Statistical Association_**70**, 811-813.
* Ramachandran (1958) Ramachandran, K. V. (1958). A test of variances. _Journal of the American Statistical Association_**53**, 741-747.
* Ramsey (1980) Ramsey, P. H. (1980). _Journal of Educational Statistics_**5**, 337-349.
* Randles and Wolfe (1979) Randles, R. and Wolfe, D. A. (1979). _Introduction to the Theory of Nonparametric Statistics_. John Wiley, New York.
* Rao (1947) Rao, C. R. (1947). Large sample tests of statistical hypotheses concerning several parameters with applications to problems of estimation. _Proc. Camb. Phil. Soc._**44**, 50-57.
* Rao (1963) Rao, C. R. (1963). Criteria of estimation in large samples. _Sankhya_**25**, 189-206.
* Rao and Wu (2001) Rao, C. R. and Wu, Y. (2001). On model selection (with discussion). In _Model Selection_, Lahiri, P. ed., IMS Lecture Notes-Monograph Series, volume 38.
* Rao (1968) Rao, P. (1968). Estimation of the location of the cusp of a continuous density. _Annals of Mathematical Statistics_**39**, 76-87.
* Rayner and Best (1989) Rayner, J. and Best, D. (1989). _Smooth Tests of Goodness of Fit_. Oxford University Press, Oxford.
* Read and Cressie (1988) Read, T. and Cressie, N. (1988). _Goodness-of-Fit Statistics for Discrete Multivariate Data_. Springer-Verlag, New York.
* Reid (1982) Reid, C. (1982). _Neyman from Life_. Springer, New York.
* Reinhardt (1961) Reinhardt, H. E. (1961). The use of least favorable distributions in testing composite hypotheses. _Annals of Mathematical Statistics_**32**, 1034-1041.
* Richmond (1982) Richmond, J. (1982). A general method for constructing simultaneous confidence intervals. _Journal of the American Statistical Association_**77**, 455-460.
* Rie and Zahn (1982)* Rieder (1977) Rieder, H. (1977). Least favorable pairs for special capacities. _Annals of Statistics_**5**, 909-921.
* Rieder (1994) Rieder, H. (1994). _Robust Asymptotic Statistics._ Springer-Verlag, New York.
* Ripley (1987) Ripley, B. (1987). _Stochastic Simulation._ Wiley, New York.
* Ritzwoller and Romano (2021) Ritzwoller, D. and Romano, J. (2021). Uncertainty in the hot hand fallacy: detecting streaky alternatives to random bernoulli sequences. _The Review of Economic Studies_, to appear.
* Robbins (1948) Robbins, H. (1948). Convergence of distributions. _Annals of Mathematical Statistics_ **19**, 72-76.
* Robert (1993) Robert, C. (1993). A note on Jeffreys-Lindley paradox. _Statistica Sinica_**3**, 603-605.
* Robert (1994) Robert, C. (1994). _The Bayesian Choice._ Springer-Verlag, New York.
* Robertson et al. (1988) Robertson, T., Wright, F. and Dykstra, R. (1988). _Order Restricted Statistical Inference._ John Wiley, New York.
* Robins et al. (2000) Robins, J., van der Vaart, A. and Ventura, V. (2000). Asymptotic distribution of \(p\)-values in composite null models. _Journal of the American Statistical Association_**95**, 1143-1172.
* Robinson (1976) Robinson, G. (1976). Properties of Student's t and of the Behrens-Fisher solution to the two means problem. _Annals of Statistics_**4**, 963-971. [Correction (1982). _Ann. Statist._**10**, 321.]
* Robinson (1979a) Robinson, G. (1979a). Conditional properties of statistical procedures. _Annals of Statistics_**7**, 742-755.
* Robinson (1979b) Robinson, G. (1979b). Conditional properties of statistical procedures for location and scale parameters. _Annals of Statistics_**7**, 756-771. [Basic results concerning the existence of relevant and semirelevant subsets for location and scale parameters, including Example 10.4.1.]
* Robinson (1982) Robinson, G. (1982). Behrens-Fisher problem. In _Encycl. Statist. Sci._**1**, 205-209.
* Robinson (1973) Robinson, J. (1973). The large-sample power of permutation tests for randomization models. _Annals of Statistics_**1**, 291-296.
* Robinson (1983) Robinson, J. (1983). Approximations to some test statistics for permutation tests in a completely randomized design. _Australian Journal of Statistics_ **25**, 358-369. [Discusses the asymptotic performance of the permutation version of the \(F\)-test in randomized block experiments.]
* Rojo (1983) Rojo, J. (1983). _On Lehmann's General Concept of Unbiasedness and Some of Its Applications_, Ph.D. Thesis. University of California, Berkeley.
* A glimpse into his life and work. _The Annals of Statistics_**39**, 2244-2265.
* Romano (1988) Romano, J. P. (1988). A bootstrap revival of some nonparametric distance tests. _Journal of the American Statistical Association_ **83**, 698-708.
* Romano (1989a) Romano, J. P. (1989a). Do boostrap confidence procedures behave well uniformly in \(P\)? _Canadian Journal of Statistics_**17**, 75-80.
* Romano (1989b) Romano, J. P. (1989b). Bootstrap and randomization tests of some nonparametric hypotheses. _Annals of Statistics_**17**, 141-159.
* Romano (1990) Romano, J. P. (1990). On the behavior of randomization tests without a group invariance assumption. _Journal of the American Statistical Association_**85**, 686-692.
* Romano (2004) Romano, J. P. (2004). On nonparametric testing, the uniform behavior of the \(t\)-test, and related problems. _Scandinavian Journal of Statistics_, **31**, 567-584.
* Romano (2005) Romano, J. P. (2005). Optimal testing of equivalence hypothesis. _Annals of Statistics_, **33**, 1036-1047.

* Romano (2012) Romano, J. P. (2012). On the third edition of _Testing Statistical Hypotheses_, In _Selected Works of E.L.Lehmann_, J.Rojo, ed., Springer-Verlag, NY, 1089-1092.
* Romano and Shaikh (2006a) Romano, J. P. and Shaikh, A. (2006a). On stepdown control of the false discovery proportion. Appeared in IMS Lecture Notes-Monograph Series, _2nd Lehmann Symposium-Optimality_, edited by J. Rojo, 33-50.
* Romano and Shaikh (2006b) Romano, J. P. and Shaikh, A. (2006b). Stepup procedures for control of generalizations of the familywise error rate. _Annals of Statistics_, **34**, 1850-1873.
* Romano and Shaikh (2008) Romano, J. P. and Shaikh, A. (2008). Inference for identifiable parameters in partially identified econometric models. _Journal of Statistical Planning and Inference_, **138**, 2786-2807.
* Romano and Shaikh (2010) Romano, J. P. and Shaikh, A. (2010). Inference for the identified set in partially identified econometric models. _Econometrica_, **78**, 169-211.
* Romano and Shaikh (2012) Romano, J. P. and Shaikh, A. (2012). On the uniform asymptotic validity of subsampling and the bootstrap. _Annals of Statistics_, **40**, 2798-2822. Supplement to "On the uniform asymptotic validity of subsampling and the bootstrap." [https://doi.org/10.1214/12-AOS1041SUPP](https://doi.org/10.1214/12-AOS1041SUPP).
* Romano et al. (2008a) Romano, J. P., Shaikh, A. and Wolf, M. (2008a). Control of the false discovery rate under dependence using the bootstrap and subsampling (invited with discussion). _Test_, **17**, 417-442 and 461-471.
* Romano et al. (2008b) Romano, J. P., Shaikh, A. and Wolf, M. (2008b). Formalized data snooping based on generalized error rates. _Econometric Theory_, **24**, 404-447.
* Romano et al. (2011) Romano, J., Shaikh, A. and Wolf, M. (2011). Consonance and the closure method in multiple testing. _International Journal of Biostatistics_**7**, Issue 1, Article 12.
* Romano et al. (2014) Romano, J. P., Shaikh, A. and Wolf, M. (2014). A practical two-step method for testing moment inequalities with application to inference in partially identified models. _Econometrica_, 1979-2002. Supplement to "A practical two-step method for testing moment inequalities with application to inference in partially identified models," _Econometrica Supplemental Material_, 82, [http://www.econometricsociety.org/ecta/supmat/11011_miscellaneous.pdf](http://www.econometricsociety.org/ecta/supmat/11011_miscellaneous.pdf)
* Romano and Siegel (1986) Romano, J. P. and Siegel, A.F. (1986). _Counterexamples in Probability and Statistics_. Wadsworth, Belmont.
* Romano and Thombs (1996) Romano, J. P. and Thombs, L. A. (1996). Inference for autocorrelations under weak assumptions. _Journal of the American Statistical Association_**91**, 590-600.
* Romano and Tirlea (2020) Romano, J. P. and Tirlea, M. A. (2020). Permutation testing for dependence in time series. Stanford University Department of Statistics Technical Report 2020-11.
* Romano and Wolf (2000) Romano, J. P. and Wolf, M. (2000). Finite sample nonparametric inference and large sample efficiency. _Annals of Statistics_**28**, 756-778.
* Romano and Wolf (2005a) Romano, J. P. and Wolf, M. (2005a). Exact and approximate stepdown methods for multiple testing. _Journal of the American Statistical Association_, **100**, 94-108.
* Romano and Wolf (2005b) Romano, J. P. and Wolf, M. (2005b). Stepwise multiple testing as formalized data snooping. _Econometrica_**73**, 1237-1282.
* Romano and Wolf (2007) Romano, J. P. and Wolf, M. (2007). Control of generalized error rates in multiple testing. _Annals of Statistics_, **35**, 1378-1408.
* Romano and Wolf (2010) Romano, J. P. and Wolf, M. (2010). Balanced control of generalized error rates. _Annals of Statistics_, **38**, 598-633.
* Romano and Wolf (2016) Romano, J. P. and Wolf, M. (2016). Efficient computation of adjusted \(p\)-values for resampling-based multiple testing. _Statistics and Probability Letters_**113**, 38-40.

* Ronchetti (1982) Ronchetti, E. (1982). Robust alternatives to the \(F\)-test for the linear model. In _Probability and Statistical Inference_ (Grossman, Pflug, and Wertz, eds.), D. Reidel, Dordrecht.
* Rosenblatt (1956) Rosenblatt, M. (1956). A central limit theorem and a strong mixing condition. _Proceedings of the National Academy of Sciences_**42**, 43-47.
* Rosenthal and Rubin (1985) Rosenthal, R. and Rubin, D. B. (1985). Statistical analysis: summarizing evidence versus establishing facts. _Psych. Bull_**97**, 527-529,
* Ross (2011) Ross, N. (2011). Fundamentals of Stein's method. _Probability Surveys_, **8**, 210-293.
* Ross (1996) Ross, S. (1996). _Stochastic Processes_, 2nd edition. John Wiley, New York.
* Rothenberg (1984) Rothenberg, T. J. (1984). Hypothesis testing in linear models when the error covariance matrix is nonscalar. _Econometrica_**52**, 827-842.
* Roussas (1972) Roussas, G. (1972) _Contiguous Probability Measures: Some Applications in Statistics_, Cambridge University Press.
* Roy and Ramamoorthi (1979) Roy, K. K. and Ramamoorthi, R. V. (1979). Relationship between Bayes, classical and decision theoretic sufficiency. _Sankhya_**41**, 48-58.
* Roy and Bose (1953) Roy, S. N. and Bose, R. C. (1953). Simultaneous confidence interval estimation. _Annals of Mathematical Statistics_**24**, 513-536.
* Royden (1988) Royden, H. L. (1988). _Real Analysis_. 3rd ed., Macmillan, New York.
* Rubin (1974) Rubin, D. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. _J. Educ. Psychol._**66**, 688-701.
* Rukhin (1993) Rukhin, A. (1993). Bahadur efficiency of tests of separate hypotheses and adaptive test statistics. _Journal of the American Statistical Association_**88**, 161-165.
* Runger and Eaton (1992) Runger, G. and Eaton, M. (1992). Most powerful invariant permutation tests. _Journal of Multivariate Analysis_**42**, 202-209.
* Ruppert et al. (2003) Ruppert, D., Wand, M. P. and Carroll, R. J. (2003). _Semiparametric Regression_. Cambridge University Press.
* expected \(p\)-values. _American Statistician_**53**, 326-331.
* Sahai and Khurshid (1995) Sahai, H. and Khurshid, A. (1995). _Statistics in Epidemiology: Methods, Techniques and Applications_. CRC Press, Boca Raton, Florida.
* Sahai and Ojeda (2004) Sahai, H. and Ojeda, M. (2004). _Analysis of Variance for Random Models_. Birkhauser, Boston.
* Salaevskii (1971) Salaevskii, Y. (1971). _Essay in Investigations in Classical Problems of Probability Theory and Mathematical Statistics_ (V. M. Kalinin and O. V. Salaevskii, eds.) (Russian), Leningrad Seminars in Math., Vol. 13, Steklov Math. Inst.; Engl. transl., Consultants Bureau, New York.
* Sanathanan (1974) Sanathanan, L. (1974). Critical power function and decision making. _Journal of the American Statistical Association_**69**, 398-402.
* Sarkar (1998) Sarkar, S. K. (1998). Some probability inequalities for ordered \(MT\,P_{s}\) random variables: A proof of the Simes conjecture. _Annals of Statistics_**26**, 494-504.
* Sarkar (2002) Sarkar, S. K. (2002). Some results on false discovery rate in stepwise multiple testing procedures, _Annals of Statistics_**30**, 239-257.
* Sarkar (2007) Sarkar, S. K. (2007). Stepup procedures controlling generalized FWER and generalized FDR. _Ann. Statist._**35**, 2405-2420.

* Sarkar, S. K., Chen, J. and Guo, W. (2013). Multiple testing in two-stage adaptive design with combination tests controlling FDR. _Journal of the American Statistical Association_**108**, 1385-1401.
* Sarkar and Chang, C.-K. (1997). The Simes method for multiple hypothesis testing with positively dependent test statistics. _J. Amer. Statist. Assoc._**92**, 1601-1608.
* Sarkar and Guo, W. (2010). Procedures controlling the \(k\)-FDR using bivariate distributions of the null \(p\)-values. _Statistica Sinica_**20**, 1227-1238.
* Sarkar, S. K., Guo, W. and Finner, H. (2012). On adaptive procedures controlling the familywise error rate. _Journal of Statistical Planning and Inference_**142**, 65-78.
* Savage, L. J. (1962). _The Foundations of Statistical Inference_. Methuen, London.
* Savage, L. J. (1972). _The Foundations of Statistics_, 2nd edition. Dover, New York.
* Savage, L. J. (1976). On rereading R. A. Fisher (with discussion). _Annals of Statistics_**4**, 441-500.
* Schafer, G. (1982). Lindley's paradox (with discussion). _Journal of the American Statistical Association_**77**, 325-351.
* Schafer, G. (1988). Sharp null hypotheses. In _Encycl. Statist. Sci._**8**, 433-436.
* Scheffe, H. (1942). On the ratio of the variances of two normal populations. _Annals of Mathematical Statistics_**13**, 371-388.
* Scheffe, H. (1943). On a measure problem arising in the theory of non-parametric tests. _Annals of Mathematical Statistics_**14**, 227-233. [Proves the completeness of order statistics.]
* Scheffe, H. (1947). A useful convergence theorem for probability distribution functions. _Annals of Mathematical Statistics_**18**, 434-438.
* Scheffe, H. (1956). A'mixed model' for the analysis of variance. _Annals of Mathematical Statistics_**27**, 23-36 and 251-271.
* Scheffe, H. (1959). _Analysis of Variance_. John Wiley, New York.
* Scheffe, H. (1970). Practical solutions of the Behrens-Fisher problem. _Journal of the American Statistical Association_**65**, 1501-1504. [Introduces the idea of logarithmically shortest confidence intervals for ratios of scale parameters.]
* Scheffe, H. (1977). A note on a reformulation of the \(S\)-method of multiple comparison (with discussion). _Journal of the American Statistical Association_**72**, 143-146. [Problem 7.18.]
* Schervish, M. (1995). _Theory of Statistics_. Springer-Verlag, New York.
* Scholz, F. W. (1982). Combining independent \(P\)-values. In _A Festschrift for Erich L. Lehmann_ (Bickel, Doksum, and Hodges, eds.), Wadsworth, Belmont. Calif.
* Schuirmann, D. (1981). On hypothesis testing to determine if the mean of a normal distribution is contained in a known interval. _Biometrics_**37**, 617.
* Schwartz, R. E. (1967a). Locally minimax tests. _Annals of Mathematical Statistics_**38**, 340-360.
* Schwartz, R. (1967b). Admissible tests in multivariate analysis of variance. _Annals of Mathematical Statistics_**38**, 698-710.
* Schwartz, R. (1969). Invariant proper Bayes tests for exponential families. _Amer. Math. Statist._**40**, 270-283.
* Schweder, T. (1988). A significance version of the basic Neyman-Pearson theory for scientific hypothesis testing. _Scandinavian Journal of Statistics_**15**, 225-242.

Schweder, T. and Spjstvoll, E. (1982). Plots of P-values to evaluate many tests simultaneously. _Biometrika_**69**, 493-502.
* [Seal1967] Seal, H. L. (1967). Studies in the history of probability and statistics XV. The historical development of the Gauss linear model. _Biometrika_**54**, 1-24.
* [Searle1987] Searle, S. (1987). _Linear Models and Unbalanced Data_. John Wiley, New York.
* [Seber1977] Seber, G. A. F. (1977). _Linear Regression Analysis_. John Wiley, New York.
* [Seber1984] Seber, G. A. F. (1984). _Multivariate Observations_. John Wiley, New York.
* [Seidenfeld1992] Seidenfeld, T. (1992). R. A. Fisher's fiducial argument and Bayes' theorem. _Statistical Science_**7**, 358-368.
* [Sellke, Bayarri, and Berger2001] Sellke, T., Bayarri, J. and Berger, J. (2001). Calibration of \(p\)-values for testing precise null hypotheses. _American Statistician_**55**, 62-71.
* [Serfling1980] Serfling R. H. (1980). _Approximation Theorems of Mathematical Statistics_. John Wiley, New York.
* [Severini1993] Severini, T. (1993). Bayesian interval estimates which are also confidence intervals. _Journal of the Royal Statistical Society Series B_**55**, 533-540.
* [Shaffer1973] Shaffer, J. P. (1973). Defining and testing hypotheses in multi-dimensional contingency tables. _Psych. Bull._**79**, 127-141.
* [Shaffer1977a] Shaffer, J. P. (1977a). Multiple comparisons emphasizing selected contrasts: An extension and generalization of Dunnett's procedure. _Biometrics_**33**, 293-303.
* [Shaffer1977b] Shaffer, J. P. (1977b). Reorganization of variables in analysis of variance and multidimensional contingency tables. _Psych. Bull._**84**, 220-228.
* [Shaffer1980] Shaffer, J. P. (1980). Control of directional errors with stagewise multiple test procedures. _Annals of Statistics_**8**, 1342-1347.
* [Shaffer1981] Shaffer, J. P. (1981). Complexity: an interpretability criterion for multiple comparisons. _Journal of the American Statistical Association_**76**, 395-401.
* [Shaffer1984] Shaffer, J. P. (1984). Issues arising in multiple comparisons among populations. In _Proc. Seventh Conference on Probab. Theory_ (Iosifescu, ed.). Edit. Acad. Republ. Soc. Romania. Bucharest.
* [Shaffer1986] Shaffer, J. P. (1986). Modified sequentially rejective multiple test procedures. _Journal of the American Statistical Association_**81**, 826-831.
* [Shaffer1990] Shaffer, J. P. (1990). Control of direction error with stagewise multiple test procedures. _Annals of Statistics_**8**, 1342-1347.
* [Shaffer1995] Shaffer, J. P. (1995). Multiple hypothesis testing: A review. _Annual Review of Psychology_**46**, 561-584.
* Optimality_, Rojo and Perez-Abren (eds.), IMS Lecture Notes **44**, Beachwood, Ohio.
* [Shao1999] Shao, J. (1999). _Mathematical Statistics_. Springer, New York.
* [Shao and Tu1995] Shao, J. and Tu, D. (1995). _The Jackknife and the Bootstrap_. Springer, New York.
* [Shapiro, Wilk and Chen1968] Shapiro, S. S., Wilk M. B. and Chen H. J. (1968). A comparative study of various tests of normality. _Journal of the American Statistical Association_**63**, 1343-1372.
* [Shevstova2014] Shevstova, I. (2014). On the absolute constants in the Berry-Esseen-type inequalities. _Doklady Mathematics_**89**, 378-381.

- new experimental results. _Journal of the American Statistical Association_**23**, 144-153.
* Shorack (1972) Shorack, G. (1972). The best test of exponentiality against gamma alternatives. _Journal of the American Statistical Association_**67**, 213-214.
* Shorack and Wellner (1986) Shorack, G. and Wellner, J. (1986). _Empirical Processes with Applications to Statistics_. John Wiley, New York.
* Shorrock (1990) Shorrock, G. (1990). Improved confidence intervals for a normal variance. _Annals of Statistics_**18**, 972-980.
* Shuster (1968) Shuster, J. (1968). On the inverse Gaussian distribution function. _Journal of the American Statistical Association_**63**, 1514-1516.
* Siegmund (1985) Siegmund, D. (1985). _Sequential Analysis: Tests and Confidence Intervals_. Springer-Verlag, New York.
* Siegmund (1986) Siegmund, D. (1986). Boundary crossing probabilities and statistical applications. _Annals of Statistics_**14**, 361-404.
* Sierpinski (1920) Sierpinski, W. (1920). Sur les fonctions convexes measurables. _Fundamenta Math._**1**, 125-129.
* Silvapulle and Silvapulle (1995) Silvapulle, M. and Silvapulle, P. (1995). A score test against one-sided alternatives. _Journal of the American Statistical Association_**90**, 342-349.
* Silvey (1959) Silvey, S. D. (1959). The Lagrangian multiplier test. _Annals of Mathematical Statistics_**30**, 389-407.
* Silvey (1980) Silvey, S. D. (1980). _Optimal Design: An Introduction to the Theory of Parameter Estimation_, Chapman & Hall, London.
* Simes (1986) Simes, R. (1986). An improved Bonferroni procedure for multiple tests of significance. _Biometrika_**73**, 751-754.
* Simpson (1989) Simpson, D. (1989). Hellinger deviance tests: efficiency, breakdown points, and examples. _Journal of the American Statistical Association_**84**, 107-113.
* Singh (1981) Singh, K. (1981). On the asymptotic accuracy of Efron's bootstrap. _Annals of Statistics_**9**, 1187-1195.
* Small et al. (2000) Small, C., Wang, J. and Yang, Z. (2000). Eliminating multiple root problems in estimation (with discussion). _Statistical Science_**15**, 313-341.
* Smirnov (1948) Smirnov, N. V. (1948). Tables for estimating the goodness of fit of empirical distributions. _Annals of Mathematical Statistics_**19**, 279-281.
* Smith and Murray (1984) Smith, D. W. and Murray, L. W. (1984). An alternative to Eisenhart's Model II and mixed model in the case of negative variance estimates. _Journal of the American Statistical Association_**79**, 145-151.
* Sonnemann (1982) Sonnemann, E. (1982). Allgemeine Losungen multipler Testprobleme. _EVD in Medizin und Biologie_**13**, 120-128.
* Sonnemann (2008) Sonnemann, E. (2008). General solutions to multiple testing problems. _Biometrical Journal_ **50**, 641-656. (English translation of Sonnemann (1982) with minor corrections, prepared by H. Finner.)
* Sonnemann and Finner (1988) Sonnemann, E. and Finner, H. (1988). Vollstandigkeitssatze fur multiple Testprobleme. In Bauer, P., Hommel, G. and Sonnemann, E., editors, _Multiple Hypothesperufung_, 121-135, Springer, Berlin.
* Sophister (1928) Sophister (G. Story) (1928). Discussion of small samples drawn from an infinite skew population. _Biometrika_ 20A, 389-423.

* Speed et al. (1979) Speed, F. M., Hocking, R. R. and Hackney, O. P. (1979). Methods of analysis of linear models with unbalanced data. _Journal of the American Statistical Association_**73**, 105-112.
* Speed (1987) Speed, T. (1987). What is an analysis of variance? (with discussion). _Annals of Statistics_ **15**, 885-941.
* Speed (1990) Speed, T. (1990). Introductory remarks on Neyman (1923). _Statistical Science_**5**, 463-464.
* Spiegelhalter (1983) Spiegelhalter, D. J. (1983). Diagnostic tests of distributional shape. _Biometrika_**70**, 401-409.
* Spjstvoll (1967) Spjstvoll, E. (1967). Optimum invariant tests in unbalanced variance components models. _Annals of Mathematical Statistics_**38**, 422-428.
* Spjstvoll (1972) Spjstvoll, E. (1972). On the optimality of some multiple comparison procedures. _Annals of Mathematical Statistics_**43**, 398-411.
* Spjstvoll (1974) Spjstvoll, E. (1974). Multiple testing in analysis of variance. _Scandinavian Journal of Statistics_**1**, 97-114,
* Sprott (1975) Sprott, D. A. (1975). Marginal and conditional sufficiency. _Biometrika_**62**, 599-605.
* Theory and Methods_**13**, 1635-1654.
* Spurrier (1999) Spurrier, J. D. (1999). Exact confidence bounds for all contrasts of three or more regression lines. _Journal of the American Statistical Association_**94**, 483-488.
* Stein (1951) Stein, C. M. (1951). A property of some tests of composite hypotheses. _Annals of Mathematical Statistics_**22**, 475-476. [Problem 3.63.]
* Stein (1956a) Stein, C. M. (1956a). The admissibility of Hotelling's \(T^{2}\)-test. _Annals of Mathematical Statistics_**27**, 616-623.
* Stein (1956b) Stein, C. M. (1956b). Efficient nonparametric testing and estimation. in _Proc. 3rd Berkeley Symp. Math. Statist. and Probab._ Univ. of Calif. Press, Berkeley.
* Stein (1962) Stein, C. M. (1962). Confidence sets for the mean of a multivariate normal distribution. _Journal of the Royal Statistical Society Series B_**24**, 265-296.
* Stein (1972) Stein, C. M. (1972). A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In _Proceedings of Sixth Berkeley Symposium_, **2**, 586-602.
* Stein (1981) Stein, C. M. (1981). Estimation of the mean of a multivariate normal distribution. _Annals of Statistics_**9**, 1135-1151.
* Stein (1986) Stein, C. M. (1986). _Approximate Computation of Expectations_. Institute of Mathematical Statistics, Lecture Notes-Monograph Series, Volume 7, Hayward, CA.
* Stephens (1974) Stephens, M. (1974). EDF Statistics for goodness-of-fit and some comparisons. _Journal of the American Statistical Association_**69**, 730-737.
* Stephens (1976) Stephens, M. (1976). Asymptotic results for goodness-of-fit statistics with unknown parameters. _Annals of Statistics_**4**, 357-369.
* Stigler (1977) Stigler, S. M. (1977). Eight centuries of sampling inspection: The trial of the Pyx. _Journal of the American Statistical Association_**72**, 493-500.
* Stigler (1978) Stigler, S. M. (1978). Francis Ysidro Edgeworth, Statistician (with discussion). _Journal of the Royal Statistical Society Series A_**141**, 287-322.
* Stigler (1986) Stigler, S. M. (1986). Laplace's 1774 memoir on inverse probability. _Statistical Science_**1**, 359-378.
* Stell (1986)* Stone (1975) Stone, C. J. (1975). Adaptive maximum likelihood estimators of a location parameter. _Annals of Statistics_**3**, 267-294.
* Stone (1981) Stone, C. J. (1981). Admissible selection of an accurate and parsimonious normal linear regression model. _Annals of Statistics_**9**, 475-485.
* Stone and von Randow (1968) Stone, M. and von Randow, R. (1968). Statistically inspired conditions on the group structure of invariant experiments and their relationships with other conditions on locally compact topological groups. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_**10**, 70-78.
* Strasser (1985) Strasser, H. (1985). _Mathematical Theory of Statistics_. Walter de Gruyter, Berlin.
* Stuart and Ord (1987) Stuart, A. and Ord, J. (1987). _Kendall's Advanced Theory of Statistics_, Vol. 1, 5th edition. Oxford University Press, New York.
* Stuart and Ord (1991) Stuart, A. and Ord, J. (1991). _Kendall's Advanced Theory of Statistics_, Vol. 2, 5th edition. Oxford University Press, New York.
* Stuart and Ord (1999) Stuart, A., Ord., J. and Arnold, S. (1999). _Kendall's Advanced Theory of Statistics_, Vol. 2A, 6th edition. Oxford University Press, New York..
* Student (1908) Student (W.S. Gosset) (1908). On the probable error of the mean. _Biometrika_**6**, 1-25.
* Student (1927) Student (W.S. Gosset) (1927). Errors of routine analysis. _Biometrika_**19**, 151-164.
* Sugiura (1965) Sugiura, N. (1965). An example of the two-sided Wilcoxon test which is not unbiased. _Annals of the Institute of Statistical Mathematics_**17**, 261-263
* Sutton (1993) Sutton, C. (1993). Computer-intensive methods for tests about the mean of an asymmetrical distribution. _Journal of the American Statistical Association_**88**, 802-810.
* Swed and Eisenhart (1943) Swed, F. S. and Eisenhart, C. (1943). Tables for testing randomness of grouping in a sequence of alternatives. _Annals of Mathematical Statistics_**14**, 66-87.
* Takeuchi (1969) Takeuchi, K. (1969). A note on the test for the location parameter of an exponential distribution. _Annals of Mathematical Statistics_**40**, 1838-1839.
* Tallis (1983) Tallis, G. M. (1983). Goodness of fit. In _Encycl. Statist. Sci._, Vol. 3. John Wiley, New York.
* Theory and Methods_**11**, 2485-2511.
* Tang (1994) Tang, D. (1994). Uniformly more powerful tests in a one-sided multivariate problem. _Journal of the American Statistical Association_**89**, 1006-1011.
* Tate and Klett (1959) Tate, R. F. and Klett, G. W. (1959). Optimal confidence intervals for the variance of a normal distribution. _Journal of the American Statistical Association_**54**, 674-682.
* Taylor and Karlin (1998) Taylor, H. and Karlin, S. (1998). _An Introduction to Stochastic Modeling_, 3rd edition. Academic Press, San Diego, CA.
* Tiku (1967) Tiku, M. L. (1967). Tables of the power of the \(F\)-test. _Journal of the American Statistical Association_**62**, 525-539.
* Tiku (1972) Tiku, M. L. (1972). More tables of the power of the \(F\)-test. _Journal of the American Statistical Association_**67**, 709-710.
* Tiku (1985a) Tiku, M. L. (1985a). Noncentral chi-square distribution. _Encycl. Statist. Sci._**6**, 276-280.
* Tiku (1985b) Tiku, M. L. (1985b). Noncentral \(F\)-distribution. _Encycl. Statist. Sci._**6**, 280-294.

- Theory and Methods_**13**, 2143-2159.
* Theory and Methods_**A10**, 2057-2071.
* [TongTong1980] Tong, Y. L. (1980). _Probability Inequalities in Multivariate Distributions_. Academic Press, New York.
* [TritchlerTritchler1984] Tritchler, D. (1984). On inverting permutation tests. _Journal of the American Statistical Association_**79**, 200-207.
* [TroendleTroendle1995] Troendle, J. (1995). A stepwise resampling method of multiple testing. _Journal of the American Statistical Association_**90**, 370-378.
* [Tseng and BrownTseng and Brown1997] Tseng, Y. and Brown, L. D. (1997). Good exact confidence sets and minimax estimators for the mean vector of a multivariate normal distribution. _Annals of Statistics_**25**, 2228-2258.
* [TukeyTukey1949a] Tukey, J. W. (1949a). One degree of freedom for non-additivity. _Biometrics_**5**, 232-242.
* [TukeyTukey1949b] Tukey, J. W. (1949b). Standard confidence points. Unpublished Report 16, Statist, Res. Group, Princeton Univ. (To be published in Tukey's _Collected Works_, Wadsworth, Belmont, Calif.)
* [TukeyTukey1953] Tukey, J. W. (1953). The problem of multiple comparisons. Published in _The Collected Works of John W. Tukey: Multiple Comparisons, Volume VIII_. (1994). Edited by H. Braun, CRC Press, Boca Raton, Florida. [This MS, unpublished until 1994, was widely distributed and exerted a strong influence on the development and acceptance of multiple comparison procedures. It pioneered many of the basic ideas, including the \(T\)-method and a first version of Lemma 9.5.1.]
* [TukeyTukey1958a] Tukey, J. W. (1958a). Bias and confidence in not quite large samples (abstract). _Annals of Mathematical Statistics_**29**, 614.
* [TukeyTukey1958b] Tukey, J. W. (1958b). A smooth invertibility theorem. _Annals of Mathematical Statistics_**29**, 581-584.
* [TukeyTukey1960] Tukey, J. W. (1960). A survey of sampling from contaminated distributions. In _Contributions to Probability and Statistics_ (Olkin, ed.), Stanford University Press.
* [TukeyTukey1991] Tukey, J. W. (1991). The philosophy of multiple comparisons. _Statistical Science_**6**, 100-116.
* [Tukey and McLaughlinTukey and McLaughlin1963] Tukey, J. W. and McLaughlin, D. H. (1963). Less vulnerable confidence and significance procedures for location based on a single sample: Trimming/Winsorization 1. _Sankhya_**25**, 331-352.
* [TurnbullTurnbull1952] Turnbull, H. (1952). _Theory of Equations_, 5th ed., Oliver and Boyd, Edinburgh.
* [TweedieTweedie1957] Tweedie, M. C. K. (1957). Statistical properties of inverse Gaussian distributions I, II. _Annals of Mathematical Statistics_**28**, 362-377, 696-705.
* [UnniUnni1978] Unni, K. (1978). _The Theory of Estimation in Algebraic and Analytic Exponential Families with Applications to Variance Components Models_, unpublished Ph.D. Thesis, Indian Statistical Institute.
* [UthoffUthoff1970] Uthoff, V. A. (1970). An optimum test property of two well-known statistics. _Journal of the American Statistical Association_**65**, 1597-1600.
* [UthoffUthoff1973] Uthoff, V. A. (1973). The most powerful scale and location invariant test of normal versus double exponential. _Annals of Statistics_**1**, 170-174.
* [VadivelooVadiveloo1983] Vadiveloo, J. (1983). On the theory of modified randomization tests for nonparametric hypotheses. _Communications in Statistics_**A12**, 1581-1596.

* Vaeth (1985) Vaeth, M. (1985). On the use of Wald's test in exponential families. _International Statistical Review_**53**, 199-214.
* van Beek (1972) van Beek, P. (1972). An application of Fourier methods to the problem of sharpening the Berry-Esseen inequality. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_**23**, 187-196.
* van de Geer et al. (2014) van de Geer, S., Buhlmann, P., Ritov, Y. and Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. _Annals of Statistics_**42**, 1166-1202.
* van der Laan et al. (2004) van der Laan, M., Dudoit, S. and Pollard, K. (2004). Multiple testing. Part II. Step-down procedures for control of the familywise error rate. _Statistical Applications in Genetics and Molecular Biology_ **3**, Article 14.
* van der Vaart (1988) van der Vaart, A. (1988). _Statistical Estimation in Large Parameter Spaces_. C.W.I. Tract 44, Amsterdam.
* van der Vaart (1998) van der Vaart, A. (1998). _Asymptotic Statistics_. Cambridge University Press.
* van der Vaart and Wellner (1996) van der Vaart, A. and Wellner, J. (1996). _Weak Convergence and Empirical Processes._ Springer, New York.
* van Zwet (2011) van Zwet, W. (2011). Remembering Erich Lehmann. _The Annals of Statistics_**39**, 2266-2279.
* Venable and Bhapkar (1978) Venable, T. C. and Bhapkar, V. P. (1978). Gart's test of interaction in a 2 x 2 x 2 contingency table for small samples. _Biometrika_**65**, 669-672.
* von Mises (1931) von Mises, R. (1931). _Wahrscheinlichkeitsrechnung_. Franz Deuticke, Leipzig, Germany.
* Vu and Zhou (1997) Vu, H. and Zhou, S. (1997). Generalization of likelihood ratio tests under nonstandard conditions. _Annals of Statistics_**25**, 897-916.
* Wacholder and Weinberg (1982) Wacholder, S. and Weinberg, C. R. (1982). Paired versus two-sample design for a clinical trial of treatments with dichotomous outcome: Power considerations. _Biometrics_**38**, 801-812.
* Wainwright (2019) Wainwright, M. (2019). _High-Dimensional Statistics_, Cambrdige University Press, Cambridge, United Kingdom.
* Wald (1939) Wald, A. (1939). Contributions to the theory of statistical estimation and testing hypotheses. _Annals of Mathematical Statistics_**10**, 299-326. [A general formulation of statistical problems containing estimation and testing problems as special cases. Discussion of Bayes and minimax procedures.]
* Wald (1941a) Wald, A. (1941a). Asymptotically most powerful tests of statistical hypotheses. _Annals of Mathematical Statistics_**12**, 1-19.
* Wald (1941b) Wald, A. (1941b). Some examples of asymptotically most powerful tests. _Annals of Mathematical Statistics_**12**, 396-408.
* Wald (1942) Wald, A. (1942). On the power function of the analysis of variance test. _Annals of Mathematical Statistics_**13**, 434-439. [Problem 7.5. This problem is also treated by Hsu, "On the power function of the \(E^{2}\)-test and the \(T^{2}\)-test", _Annals of Mathematical Statistics_**16** (1945), 278-286.]
* Wald (1943) Wald, A. (1943). Tests of statistical hypotheses concerning several parameters when the number of observations is large. _Trans. Amer. Math. Soc._**54**, 426-482. [General asymptotic distribution and optimum theory of likelihood ratio (and asymptotically equivalent) tests.]
* Wald (1949) Wald, A. (1949). Note on the consistency of the maximum likelihood estimate. _Annals of Mathematical Statistics_**20**, 595-601.
* Wald (1950) Wald, A. (1950). _Statistical Decision Functions_. John Wiley, New York. [Definition of most stringent tests.]* Wald (1958) Wald, A. (1958). _Selected Papers in Statistics and Probability by Abraham Wald_. Stanford Univ. Press. [Defines and characterizes complete classes of decision procedures for general decision problems. The ideas of this and the preceding paper were developed further in a series of papers culminating in Wald's book (1950).]
* Wallace (1958) Wallace, D. (1958). Asymptotic approximations to distributions. _Annals of Mathematical Statistics_**29**, 635-654.
* Wallace (1980) Wallace, D. (1980). The Behrens-Fisher and Fieller-Creasy problems. In _R. A. Fisher: An Appreciation_ (Fienberg and Hinkley. eds.) Springer. New York, pp. 119-147.
* Wang (1999) Wang, H. (1999). Brown's paradox in the estimated confidence approach. _Annals of Statistics_**27**, 610-626.
* Wang (1971) Wang, Y. Y. (1971). Probabilities of the type I errors of the Welch tests for the Behrens-Fisher problem. _Journal of the American Statistical Association_**66**, 605-608.
* Weisberg (1985) Weisberg, S. (1985). _Applied Linear Regression_, 2nd edition. John Wiley, New York.
* Wasserstein and Lazar (2016) Wasserstein, R. and Lazar, N. (2016). The ASA's statement on p-values: context, process and purpose. _The American Statistician_**70**, 129-133.
* Welch (1939) Welch, B. L. (1939). On confidence limits and sufficiency with particular reference to parameters of location. _Annals of Mathematical Statistics_**10**, 58-69.
* Welch (1951) Welch, B. L. (1951). On the comparison of several mean values: An alternative approach. _Biometrika_**38**, 330-336.
* Welch (1990) Welch, W. (1990). Construction of permutation tests. _Journal of the American Statistical Association_**85**, 693-698.
* Wellek (2003) Wellek, S. (2003). _Testing Statistical Hypotheses of Equivalence_. Chapman & Hall/CRC.
* Wells et al. (1993) Wells, M., Jammalamadaka, S. and Tiwari, R. (1993). Large sample theory of spacings statistics for tests of fit for the composite hypothesis. _Journal of the Royal Statistical Society Series B_**55**, 189-203.
* Westfall (1989) Westfall, P. H. (1989). Power comparisons for invariant variance ratio tests in mixed ANOVA models. _Annals of Statistics_**17**, 318-326.
* Westfall (1997) Westfall, P. H. (1997). Multiple testing of general contrasts using logical constraints and correlations. _Journal of the American Statistical Association_**92**, 299-306.
* Westfall and Young (1993) Westfall, P. H. and Young, S. (1993). _Resampling-Based Multiple Testing: Examples and Methods for \(P\)-Value Adjustment_. John Wiley, New York.
* a need to rethink. _Biometrics_**37**, 589-594.
* Wijsman (1979) Wijsman, R. (1979). Constructing all smallest simultaneous confidence sets in a given class, with applications to manova. _Annals of Statistics_**7**, 1003-1018.
* Wijsman (1980) Wijsman, R. (1980). Smallest simultaneous confidence sets with applications in multivariate analysis. _Journal of Multivariate Analysis_**V**, 483-498.
* Wijsman (1990) Wijsman, R. (1990). _Invariant Measures on Groups and Their Use in Statistics_. IMS Lecture Notes. Institute of Mathematical Statistics, Hayward, CA.
* Wilk and Kempthorne (1955) Wilk, M. B. and Kempthorne, O. (1955). Fixed, mixed, and random models. _Journal of the American Statistical Association_**50**, 1144-1167.

* Wilks (1938) Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. _Annals of Mathematical Statistics_**9**, 60-62. [Derives the asymptotic distribution of the likelihood ratio when the hypothesis is true.]
* Williams (1991) Williams, D. (1991). _Probability With Martingales_. Cambridge University Press, Cambridge, England.
* Wilson (1927) Wilson, E. B. (1927). Probable inference, the law of succession, and statistical inference. _Journal of the American Statistical Association_**22**, 209-212.
* Wolfowitz (1949) Wolfowitz, J. (1949). The power of the classical tests associated with the normal distribution. _Annals of Mathematical Statistics_**20**, 540-551. [Proves Lemma 6.5.1 for a number of special cases. Proves that the standard tests of the univariate linear hypothesis and for testing the absence of multiple correlation are most stringent among all similar tests and possess certain related optimum properties.]
* Wolfowitz (1950) Wolfowitz, J. (1950). Minimax estimates of the mean of a normal distribution with known variance. _Annals of Mathematical Statistics_**21**, 218-230.
* Wolkonoski and Rozanov (1959) Wolkonoski, V. and Rozanov, Y. (1959). Some limit theorems for random functions, Part I. _Theory of Probability and its Applications_**4**, 178-197.
* Working and Hotelling (1929) Working, H. and Hotelling, H. (1929). Application of the theory of error to the interpretation of trends. _Journal of the American Statistical Association_**24**, Mar. Suppl., 73-85.
* Wu (1990) Wu, C. F. (1990). On the asymptotic properties of the jackknife histogram. _Annals of Statistics_**18**, 1438-1452.
* Wu and Hamada (2000) Wu, C. F. and Hamada, M. (2000). _Experiments: Planning, Analysis and Parameter Design_. John Wiley, New York.
* Wynn (1984) Wynn, H. P. (1984). An exact confidence band for one-dimensional polynomial regression. _Biometrika_**71**, 375-379.
* Wynn and Bloomfield (1971) Wynn, H. P. and Bloomfield, P. (1971). Simultaneous confidence bands in regression analysis (with discussion). _Journal of the Royal Statistical Society Series B_**33**, 202-217.
* Xue and Yao (2020) Xue, K. and Yao, F. (2020). Distribution and correlation-free two-sample test of high-dimensional means. _Annals of Statistics_**48**, 1304-1328.
* Yamada and Morimoto (1992) Yamada, S. and Morimoto, H. (1992). Sufficiency. In _Current Issues in Statistical Inference: Essays in Honor of D. Basu._ Gosh and Pathak (eds.), IMS Lecture Notes **17**, Hayward, CA.
* Yanagimoto (1990) Yanagimoto, T. (1990). Dependence ordering in statistical models and other notions. In _Topics in Statistical Dependence_, Block, Sampson and Savits (eds.) (1990), IMS Lecture Notes **16**, Hayward, CA.
* Yuen (1974) Yuen, K. K. (1974). The two-sample trimmed \(t\) for unequal population variances. _Biometrika_**61**, 165-170.
* Zabell (1992) Zabell, S. (1992). R. A. Fisher and the fiducial argument. _Statistical Science_**7**, 369-387.
* Zhang (2002) Zhang, J. (2002). Powerful goodness-of-fit tests based on the likelihood ratio. _Journal of the Royal Statistical Society Series B_**64**, 281-294.
* Zhang and Boos (1992) Zhang, J. and Boos, D. (1992). Bootstrap critical values for testing homogeneity of covariance matrices. _Journal of the American Statistical Association_**87**, 425-429.

[MISSING_PAGE_EMPTY:2590]

Approximate hypotheses

extended Neyman Pearson lemma for, 372, 375

Arcsine transformation for binomial variables, 556

Association, 150

spurious, 150

Yule's measure of, 146. _See also_ Dependence; positive

Asymptotically linear statistic, 672

Asymptotically maximin tests:

for multi-sided hypotheses, 752-754

for nonparametric mean, 755-758

for Chi-squared test, 784, 786

Asymptotically most powerful test sequence, 725

Asymptotically normal experiments, 734-738

Asymptotically perfect test, 537

Asymptotically uniformly most powerful (AUMP) tests

in univariate models, 723-734

in multiparameter models, 738-754

in nonparametric models, 755-763

Asymptotic equivalence of test sequences, 767

Asymptotic pivot, 866

Asymptotic relative efficiency, 717-723, 771

and goodness of fit, 819

of randomization tests, 840

Asymptotic normality

of functions of asymptotically normal variables, 451

of sample mean, 528

or sample median, 532. _See also_ Central limit theorem

Asymptotic optimality, vii, 709

Asymptotically uniformly integrable, 550,

560

Autocovariance, 583, 588

Automatic percentile method, 913

Autoregressive process, 584, 613

Average power, maximum, 110, 350, 826

**B**

Bahadur efficiency, 723

Bahadur Savage theorem, 633, 634

Banach space, 924-925

Bartlett correction, 694

relationship to bootstrap, 892, 918

Bartlett's test for variances, 648

Basu's theorem, 173, 239

Bayesian confidence sets. _See_ Credible regions

Bayesian inference, 15, 16, 196, 197, 200, 346

Bayes risk, 15

Bayes solution, 15, 24

and complete class of decision procedures, 18

restricted, 16

to maximize minimum power, 364

to prove admissibility, 269. _See also_ Credible region; Prior distribution

Bayes sufficiency, 22

Bayes test, 108, 301, 351

Behrens-Fisher distribution, 230

Behrens-Fisher problem, 182, 263, 510, 517,

522

bootstrap solution, 892, 893

LAUMP tests for, 744, 745

many sample, 610

nonparametric, 280

permutation test for, 846

under nonnormality, 610. _See also_ Welch-Aspin test

Benjamini-Hochberg procedure, 435

Benjamini-Yekutieli procedure, 435, 438,

491, 856

Bernoulli trials, 241, 531, 551, 557, 561, 587,

913

Berry-Esseen theorem, 531

multivariate, 798

Beta distribution, 181, 319

as distribution of order statistics, 303

noncentral, 319, 349

relation to _F_-distribution, 182

relation to gamma distribution, 224

Bimeasurable transformation, 243

Binomial distribution _b_(_p_, _n_), 4

in comparing two Poisson distributions, 142, 174, 500

as loglinear model, 152

completeness of, 132

in sign test, 97

variance stabilizing transformation for, 556. _See also_ Contingency tables; Multinomial distribution; Negative binomial distribution; Two by two table Binomial probabilities

comparison of two, 121, 143, 163, 168, 500

confidence bounds for, 83

confidence intervals for, 189, 539, 539credible region for, 196 one-sided test for, 74 two-sided test for, 128. _See also_ Contingency tables; Independence, test for; Matched pairs; Median; Paired comparisons; Sample inspection; Sign test Binomial trials, 8, 19, 151 minimal sufficient statistic for, 27. _See also_ Inverse sampling Bioassay, 166 Bivariate distribution (general) one-parametric family for, 218 testing for independence in, 219, 309. _See also_ Dependence Bivariate normal correlation coefficient asymptotic test for, 686 confidence intervals for, 229 test for, 229, 263, 297, 498 confidence bounds for, 310 Bivariate normal distribution, 217, 236 ancillary statistics in, 498 joint distribution of second moments in, 237 test for independence in, 217 Bonferroni procedure, 408, 482 Bootstrap, 869 consistency of, 871 higher order properties, 884 hypothesis testing, 889 in multiple testing, 895 Bootstrap calibration, 887 Bootstrap-_t_ consistency of, 876 higher order properties, 884-888 Bounded-Lipschitz metric, 553 Borel set, 30 Bounded completeness, 134, 260 example of, without completeness, 160. _See also_ Completeness of family of distributions Brownian Bridge process, 775, 779

Appendix C Calibration, 887 Cauchy distribution, 79, 113, 370, 395 Cauchy location model AUMP and LAUMP tests for, 731, 732 q.m.d. property, 657 Causal influence, 150 CDF. _See_ Cumulative distribution function Center of symmetry confidence intervals for, 231, 235. _See also_ Symmetric distribution Central limit theorems for dependent variables, 611, 612 for linear combinations, 615 for sample median, 532 Lindeberg, 529 Lyapounov, 530 multivariate, 529 uniform, 628, 632 Characteristic function, 528 Chebyshev inequality, 553 Chi-squared distribution, 49 for testing linear hypothesis with known variance, 353 in testing normal variance, 129, 177 limit for likelihood ratio, 692, 692 non-central, 349, 350, 354 relation to exponential distribution, 58 relation to _F_-distribution, 181 relation to _t_-distribution, 178. _See also_ Gamma distribution; Normal one-sample problem, variance; Wishart distribution Chi-squared test as a Neyman smooth test, 794 asymptotically maximin property, 785, 786 for simple hypotheses, 522, 691, 692, 781-790 for composite hypotheses, 790-792 in contingency tables, 824 for testing uniformity, 786-790 Closure method, 423, 424, 426, 427, 428, 429, 430, 431, 483, 484, 485, 856 Cluster sampling, 612 Cochran-Mantel-Haenszel test, 152 Coefficient of variation asymptotic confidence interval for, 683 confidence bounds for, 310 tests for, 179, 253, 262 Coherence, 427, 428, 491 Complement of a set \(E\), denoted \(E^{c}\), 29 Completeness of a class of decision procedures, 17, 18, 123 for one-parameter exponential family, 159 of classes of one-sided tests, 77 of class of two-sided tests, 158 relation to sufficiency, 22. _See also_ Admissibility Completeness of family of distributions, 130, 131 of binomial distributions, 132 of exponential families, 133of nonparametric family, 134 of normal distributions, 132 of order statistics, 134, 161 relations to bounded completeness, 134, 159 of uniform distributions, 132 Completion of measure, 30 Complexity: of multiple comparison procedure, 468 Components of variance, 344. _See also_ Random effects model Composite hypothesis, 64 vs. simple alternative, 95 Conditional distribution, 43 example of nonexistence, 42 Conditional expectation, 39, 44 properties of, 41 Conditional independence: test for, 151 Conditional inference, 494, 495, 510 optimal, 502 Conditional power, 140, 156, 215, 500, 502 Conditional probability, 42 Conditional test, 125, 127, 129 Confidence bands for cumulative distribution function, 291, 313, 547, 880 for linear models, 470 for regression line, 481, 491. _See also_ Simultaneous confidence intervals Confidence bounds, 79 equivariant, 310 impossible, 340, 511 in presence of nuisance parameters, 185 most accurate, 80 relation to median unbiased estimates, 185 relation to one-sided tests, 186 standard, 87 with minimum risk, 116 Confidence coefficient, 79, 185 conditional, 510 Confidence intervals, 6, 85, 185 after rejection of a hypothesis, 158, 510 distribution-free, 216, 231, 286 empty, 341 expected length of, 194 history of, 123, 240 in randomization models, 214 interpretation of, 185 logarithmically shortest, 287 loss functions for, 87 of bounded length, 225, 225 randomized, 190 relation to two-sided tests, 186 uniformly most accurate unbiased, 188. _See also_ Simultaneous confidence intervals Confidence level, 79 Confidence sets, 80 admissibility of, 273, 382 average smallest, 286 based on multiple tests, 491 derived from a pivotal quantity, 290 equivariant, 283, 383 example of inadmissible, 384 minimax, 383, 383 of smallest expected Lebesgue measure, 228 relation to tests, 195 unbiased, 188 which are not intervals, 257. _See also_ Credible region; Equivariant confidence sets; Relevant and semirelevant subsets; Simultaneous confidence sets Consonance, 429, 431 Conjugate distribution, 197 Conservative test, 144 Consumer preferences, 153 Contiguity, 662-665 and limiting distribution of a statistic, 671, 672 characterizations of, 667, 669 examples of, 669-676 Contingency tables: loglinear models for, 152 \(r\times c\) tables, 144 three factor, 150 \(2\times 2\times K\), 156, 167 \(2\times 2\times 2\), 153 \(2\times 2\times 2\times L\), 167. _See also_ Two by two tables Continuity correction, 144 Continuity point, 527 Continuity theorem, 529 Continuous Mapping theorem, 534, 535 Consistent estimator, 536 Contrasts, 479 Convergence in distribution (or in law), 527 Convergence in probability, 535 Convergence of moments, 549 Convergence theorem for densities, 923 dominated, 34 monotone, 33. _See also_ Central limit theorem; Continuity theorem; Continuous mapping theorem; Cramer-Wold theorem; Delta method; Glivenko-Cantelli theorem; Prohorov's theorem Cornish-Fisher expansion, 625, 882 Correlation coefficient in bivariate normal distribution, 217, 733, 744 confidence bounds for, 310 intraclass, 356 multiple tests of, 898 nonparametric bootstrap test of, 892 testing value of, 217, 263, 297. _See also_ Bivariate distribution; Dependence, positive; Multiple correlation coefficient; Rank correlation coefficient; Sample correlation coefficient Countable additivity, 29 Countable generators of \(\sigma\)-field, 926 Counting measure, 30 Covariance matrix, 102, 347 Coverage error, 881-888 Cramer's condition, 624 Cramer-von Mises statistic, 624 limiting distribution, 812 as a weighted quadratic statistic, 806, 808 Cramer-Wold device, 529 Credible region, 196, 197 highest probability density, 197, 199, 230 Critical function, 63 Critical region, 61 Cross product ratio. _See_ Odds ratio Cumulative distribution function (cdf), 31, 56, 527 confidence bands for, 291, 313 empirical, 279, 291 inverse of, 303. _See also_ Kolmogorov test for goodness of fit; Probability integral transformation

**D**

_d_-admissibility, 266, 301. _See also_ Admissibility Data Snooping, 474 Decision problem specification of, 4 Decision space, 4, 5 Decision theory, 28, 29 and inference, 6 Deficiency, 179 Delta method, 540-544 Density point, 211 Dependence measure of, 146 models for, 611-614 positive, 164 regression, 218, 274. _See also_ Correlation coefficient; Independence Design of experiments, 8, 9, 147, 232, 333. _See also_ Random assignment; Sample size Directional error, 158, 468, _See also_ Type 3 error Direct product (of two sets), 35 Dirichlet distribution, 231 Dissonant, 429, 431 Distribution. _See the following families of distributions_: Beta, Binomial, Bivariate normal, Cauchy, Chi-squared, Dirichlet, Double exponential, Exponential, \(F\), Gamma, Hypergeometric, Inverse Gaussian, Logistic, Lognormal, Multinomial, Multivariate normal, Negative binomial, Noncentral, Normal, Pareto, Poisson, Polya, Power series, \(t\), Hotelling's \(T2\), Triangular, Uniform, Weibull, Wishart. _See also_ Exponential family; Monotone likelihood ratio; Total positivity; Variation diminishing Dominated convergence theorem, 34 Dominated family of distributions, 47, 925, 926 Domination of one procedure over another, 17. _See also_ Admissibility; Inadmissibility Double exponential distribution, 296, 369, 398 AUMP and LAUMP property, 731, 731 locally most powerful test in, 398 q.m.d. property, 657 UMP conditional test in, 503 Duncan multiple comparison procedure, 463 Duncan's method, 415, 416, 422, 483 Dunnett's multiple comparison method, 490 Dunnett's range distribution, 416 Dvoretzky, Kiefer, Wolfowitz inequality, 547

**E**

EDF. _See_ Empirical distribution function Edgeworth expansions, 623-627, 649, 881 Efficacy, 719 Efficient likelihood estimation, 676Elliptically symmetric distribution, 358 Empirical cumulative distribution function, 279, 291, 546 statistics, 780 Empirical likelihood, 894, 917, 918 Empirical measure, 558, 780 Empirical process, 775, 779, 880 Envelope power function, 299, 385. _See also_ Most stringent test Equi-tailed confidence interval, 870 Equivalence of family of distributions or measures, 47 of statistics, 28 of two measures, 54 Equivalence classes, 77 Equivalence hypotheses, 92, 103-105 LAUMP tests for, 745-751 Equivalence relation, 919 Equivariance, 13, 497. _See also_ Invariance Equivariant confidence bands, 291, 471, 481, 490 Equivariant confidence bounds, 309 Equivariant confidence sets, 283, 286, 287, 309, 310, 313 and pivotal quantities, 311. _See also_ Uniformly most accurate confidence sets Error control strong, 407 weak, 407 Error of first and second kind, 61, 72 of type, 3, 158 familywise error rate, 407 directional, 468 Essentially complete class of decision procedures, 18, 58, 76, 109. _See also_ Completeness of a class of decision procedures Estimation. _See_ Confidence bands; Confidence bounds; Confidence intervals; Confidence sets; Equivariance; Maximum likelihood; Median: Point estimation; Unbiasedness Euclidean sample space, 43 Exchangeable, 446 Expectation (of a random available), 35, 42 conditional, 39, 42, 44 Expected order statistics, 277 Experimental design. _See_ Design of experiments Exponential distribution, 23, 75, 82 confidence bounds and intervals in, 82 order statistics from, 58 relation to Pareto distribution, 107 relation to Poisson process, 59, 75 sufficient statistics for, 28 testing against gamma distribution, 228 testing against normal or uniform distribution, 297 tests for parameters of, 106, 223 two-sample problem for, 295. _See also_ Chi-squared distribution; Gamma distribution; Life testing Exponential family, 49, 59 admissibility of tests in, 266 completeness of, 132 differentiability of, 52 equivalent forms of, 140 expansion of loglikelihood, 652, 653 median unbiased estimators in, 185 moments of sufficient statistics, 60 monotone likelihood ratio of, 73 natural parameter space of, 51, 59, 136 q.m.d. property, 658 regression models for, 239 testing in multiparameter, 135, 137, 139, 266 total positivity of, 199. _See also_ One-parameter exponential family Exponential waiting times, 22, 58, 82. _See also_ Exponential distribution Extreme order statistic, 905, 906

**F**

Factorization criterion for sufficient statistics, 19, 47, 49 False discovery proportion (FDP), 432, 433, 434, 442, 443, 444, 445, 469, 485 False discovery rate (FDR), 432, 433, 434, 435, 437, 438, 442, 445, 469, 485, 491, 856 Family of hypotheses, 407, 469, 551 Familywise error rate (FWER), 407, 408, 447, 468, 484 control based on bootstrap, 880-898 Fatou's Lemma, 33 \(F\)-distribution, 181 for simultaneous confidence intervals, 478 in Hotelling's T\({}^{2}\)-test, 348 in tests and confidence intervals for ratio of variances, 190, 340 noncentral, 349 relation to beta distribution, 181. _See also_\(F\)-test for linear hypothesis; \(F\)-test for ratio of variancesFiducial, 124

distribution, 200

probability, 124, 200

Fieller's problem, 225

Finite decision problem, 58

First-order accurate, 886

Fisher's exact test, 144, 168. _See also_ Two

by two tables

Fisher Information, 655, 656

Fisher's least significant difference

method, 462

Fisher linkage model, 790

Fisher's sharp null hypothesis, 603

Fisher's _z_-transformation, 544

Fixed effects model, 338. _See also_ Linear

model; Model I and II

Free Group, 26

Frequentist point of view, 200

Friedman's rank test, 329

\(F\)-test for linear hypothesis, 318

admissibility of, 319

as Bayes test, 351

for nested classification, 344

has best average power, 350

in Fisher's least significant difference

method, 462

in Gabriel's simultaneous test procedure,

462

in mixed models, 426

in model II analysis of variance, 339

power of, 319

robustness of, 607, 608, 610, 648. _See also_\(F\)-distribution

\(F\)-test for ratio of variances, 122, 251, 271

admissibility of, 273

nonrobustness of, 608. _See also_\(F\)-distribution; Normal two-sample problem, ratio of variances

\(F\)-test in multiple comparison

procedures, 460

Fubini's theorem, 35

Fully informative statistics, 109

Functionals, 759

Fundamental lemma. _See_ Neyman- Pearson

fundamental lemma

**G**

Gabriel's simultaneous test procedure, 462

Gamma distribution G(_g_, _b_), 113, 224

relation to Beta distribution, 224

scale parameter of, 229

shape parameter of, 224. _See also_ Beta

distribution; Chi-squared distribution;

Exponential distribution

Gaussian curvature, 396

Gaussian sequence model, 636

Generalized error rates, 432, 433

Generalized linear models, 361

Ghosh-Pratt identity, 228

Gini's mean difference, 574

Glivenko-Cantelli theorem, 546

Goodness of fit test, vii, 292, 773

bootstrap tests of, 894

in multinomial models, 691-693. _See also_ Chi-squared tests; Kolmogorov-Smirnov; Neyman's smooth tests; Separate families; Weighted quadratic tests

Group

amenable, 382

free, 26

generated by subgroups, 247

linear, 245, 259, 381, 382

of monotone transformations, 244

orthogonal, 245, 246, 377

permutation, 245

scale, 244

transformation, 241, 242

transitive, 244, 250

translation, 244, 249, 380, 381. _See also_

Equivariance; Invariance

Group, 919, 920

family, 496, 503

Guaranteed power

achieved through sequential procedure,

141, 143, 226, 227

**H**

Haar measure, 259, 378

Hazard ordering, 115

Hellinger distance, 712-717, 771

Hierarchical classification. _See_ Nested classification

Higher criticism, 426, 484, 640, 641, 649

Higher order asymptotics, 881-889

Highest probability density (HPD)

credible region, 197, 199, 230

Hilbert space, 924-925

Hodges-Lehmann efficiency, 723

Hodges' superefficient estimator, 705

Holm procedure for multiple testing, 418, 457, 483

Hommel's method, 424, 425, 484

Homogeneity of meanstests of, 324 against ordered alternatives, 326 multiple comparisons for, 459, 460 for normal means, 324, 326 nonparametric, 325, 329, 623. _See also_ Multiple comparisons Homomorphism, 12 Hotelling's \(T^{2}\)-test, 348 admissibility of, 360 as Bayes solution, 360 minimaxity of, 383 Hot hand fallacy, 587 HPD region. _See_ Highest probability density Huber condition, 620 Hunt-Stein theorem, 378 Hypergeometric distribution, 73, 151, 152 in testing equality of two binomials, 144, 145 in testing for independence in a 2 x 2 table, 148 relation to distribution of runs, 165. _See also_ Fisher's exact test; Two by two tables Hypergeometric function, 239 Hypothesis testing, 5, 61 history of, 123 loss functions for, 64, 76, 253 without stochastic basis, 149, 150

**I**

Improper prior distribution, 197 Inadmissibility, 18 of confidence sets for vector means, 383 of likelihood ratio test, 300 of UMP invariant test, 348. _See also_ Admissibility Independence: conditional, 151 of sample mean from function of differences in normal samples, 173, 174 of statistic from a complete sufficient statistic, 173 of sum and ratio of independent \(\chi^{2}\) variables, 174 of two random variables, 35 Independence: test for in bivariate normal distribution, 218 in nonparametric models, 309 in \(r\times c\) contingency tables, 144 in two by two tables, 144-147 Indicator function of a set, 34 Indifference zone, 363 Inference, statistical. _See_ Statistical inference Information matrix, 655 Integrable function, 32 Integration, 32 Interaction, 32 Interaction, 323, 332, 354 as main effects, 354 in random effects and mixed models, 356, 357 test for absence of, 331 Intersection hypothesis, 420, 421, 423, 424, 427, 428, 429, 430, 483, 484, 485, 496 Interval estimation. _See_ Confidence intervals Intraclass correlation coefficient, 356 Invariance of decision procedure, 12, 13 of likelihood ratio, 299 of measure, 339, 697 and admissibility, 27 and ancillarity, 496, 498, 502, 503 and symmetry, 241 history of, 314 of measure, 259 of power functions, 259-261 of tests, 243, 314 principle of, 243 relation to equivariance, 13 relation to minimax principle, 26, 375, 376 relation to sufficiency, 251 relation to unbiasedness, 24, 262 warning against inappropriate use of, 325. _See also_ Almost invariance; Equivariance Invariant measure, 259, 262 over orthogonal group, 377 over translation group, 381 Inverse Gaussian distribution, 114, 224 Inverse sampling for binomial trials, 74 for Poisson variables, 75, 112. _See also_ Negative binomial distribution; Poisson process; Waiting times Inverse testing, 81, 85

**J**

Jackknife, 869, 899 Joint confidence rectangles, 879. _See also_ Simultaneous confidence sets Joint tests, 426, 427

**K**

Kendall's statistic, 309\(k\)-FWER, 432, 433, 439, 441, 442, 443, 445, 469
* Kolmogorov-Smirnov and bootstrap confidence bands, 880 asymptotic behavior of, 547, 774-780 based on a pivot, 866 extensions of, 780-781 statistic, 293 test for goodness of fit, 292. _See also_ Goodness of fit Kolmogorov-Smirnov distance, 546 Kruskal-Wallis test, 325 Kullback-Leibler information (or divergence), 536 backward, 894 Kurtosis, 624

**L**

Lagrange multiplier test, 688, 707 Large-sample theory, vii, 521 Latin squares design, 333, 354 Lattice distribution, 624 Laws of large numbers Weak, 535 Strong, 546 Uniform, 628, 630 Least favorable distribution, 18, 96, 97, 98, 365, 455 Least squares estimates, 320 Lebesgue convergence theorems, 39 Lebesgue integral, 32 Lebesgue measure, 30 Legendre polynomials, 793, 793 Level of significance. _See_ Significance level Levy distance, 533 Life testing, 58. _See also_ Exponential distribution; Poisson process Likelihood, 16 function, 676, 677. _See also_ Maximum likelihood ratio, 115, 665 censored, 373 invariance of, 299 large-sample theory of, 655, 676 monotone, 72 preference order based on, 65, 73 sufficiency of, 57. _See also_ Monotone likelihood ratio
* Likelihood ratio test, 17 example of inadmissible, 300 large-sample theory of, 689-69 using bootstrap critical values, 892, 892
* Lindley's Paradox, 108 Linear functionals, 759 LAUMP property, 760-763 Linear hypothesis, 315, 381 admissibility of test for, 319 Bayes test for, 351 canonical form for, 316, 361 \(F\)-test for, 200 inhomogeneous, 321 more efficient tests for, 326 parametric form of, 323, 351 power of test for, 318 properties of test for, 318, 350, 351, 381, 386, 396 reduction of, through invariance, 317 robustness of tests for, 614-623. _See also_ Analysis of variance; Additive linear model; Generalized linear model Linear model, 315, 361 confidence intervals in, 352 history of, 361 simultaneous confidence intervals in, 476 Locally asymptotically uniformly most powerful (LAUMP) for equivalence hypotheses, 745-751 for one-sided hypotheses in multiparameter models, 738-745 in nonparametric models, 761 in univariate models, 728-734 Locally most powerful rank test, 278, 312 Locally optimal tests, 638, 394, 395, 505, 686 Locally unbiased, 395 Local power, 538 of \(t\)-test, 631, 632, 632 Location families (or models), 77, 114, 497 are stochastically increasing, 77 comparing two, 249 conditional inference for, 516 condition for monotone likelihood ratio, 369, 503 example lacking monotone likelihood ratio, 79 LAUMP tests for, 731-733 strongly unimodal, 503 Location-scale families, 12 confidence intervals based on pivot, 866 comparing two, 294 LAUMP tests in, 733. _See also_ Normality, testing for Log convexity, 369, 514 Logistic distribution, 151, 370, 503Logistic response model, 152

Loglikelihood ratio, 651

expansion due to Le Cam, 659-662

Loglinear model, 152, 361

Lognormal family, 658

Loss function, 3, 7

in confidence interval estimation, 24, 79,

87

in hypothesis testing, 76, 159, 253

monotone, 87

_L__P_-space, 924, 925

M
Main effects, 327, 332

as interactions, 354

confidence sets for, 329

tests for, 327, 331

Mallow's metric, 876

Mantel-Haenszel test, 152

Mann-Whitney statistic, 582

Markov chain, 164

Markov property, 164

Markov's inequality, 553

Matched pairs, 156, 168, 208, 210, 232, 233,

252, 273, 370, 851

comparison with complete randomization, 168

confidence intervals for, 216

rank tests for, 276, 280

Maximal invariant, 244

ancillarity of, 496

distribution of, 248

method for determining, 246

obtained in steps, 247

Maximin multiple tests, 445, 448, 450, 453

Maximin test, 364

by Hunt-Stein theorem, 381

existence of, 386

local, 368

relation to invariance, 376. _See also_ Least

favorable distribution; Minimax principle; Most stringent test

Maximum likelihood, 16, 17, 676-682

in normal model, 677, 679

in exponential family models, 678. _See also_ Likelihood ratio test

Maximum modulus confidence

intervals, 475

Maximum test, 412, 854

Max-T test, 424

McNemar's test, 156, 168

Measurable:

function, 31

set, 30

space, 30

transformation, 31, 36

Measure, 30

_m_-dependent, 583, 584, 586, 587, 594, 603,

918

Median

confidence bounds for, 120

Median unbiasedness, 23

relation to confidence bounds, 185

Meta-analysis, 124

Metric space, 709, 921. _See also_ Hellinger;

Kolmogorov-Smirnov;

Kullback-Leibler; L'evy; Mallows;

Prohorov;

Total variation
Minimal complete class of decision procedures, 18. _See also_ Completeness

of a class of distributions;

Essentially complete class of decision procedures

Minimal sufficient statistic, 22

Minimum Chi-squared estimator, 790

Minimax principle, 15, 404

and least favorable distribution, 18

in confidence estimation, 383

relation to invariance, 26

relation to unbiasedness, 25. _See also_

Maximin test; Restricted Bayes solution

Minkowski's inequality, 924

Missing observations, 512

Mixed model, 338, 346, 357, 358

Mixtures of experiments, 493, 495, 496, 512,

517

Mixing coefficients, 583, 585, 586, 588

Mixing process, 582

MLR. _See_ Monotone likelihood ratio

Model I and II, 338. _See also_ Mixed model;

Random effects model

Model selection, 11

Moment inequalities, 390, 695, 704, 889,

895, 909

Monotone class of sets, 54

Monotone convergence theorem, 33

Monotone decision rule, 446, 448, 486

Monotone increasing set, 390, 446

Monotone likelihood ratio, 72, 77, 115, 119

mixtures of distributions with, 397, 502,

504

necessary and sufficient condition for, 111

of differences, 504of distribution of correlation coefficient,

298

of exponential family, 74

of location families, 369, 503

of noncentreal \(\chi^{2}\) and \(F\), 350

of noncentral \(t\), 255

of scale families, 370

relation to total positivity, 118

tests and confidence procedures in the presence of, 72, 77, 81. _See also_ Stochastic increasing

Monotone loss function, 87

Monotone tests, 386, 388, 389, 390, 391, 392

Monte Carlo simulation, 548, 579

for bootstrap, 870

for subsampling, 906

Mortality. _See_ Hazard ordering

Most stringent test, 314, 385

existence of, 402

Moving averages, 583, 584, 585, 612, 914

Moving average process, 612

Moving blocks bootstrap, 914

Multinomial distribution, 50, 231

as conditional distribution, 58

Dirichlet prior for, 231

for entries of 2 x 2 table, 145

Multinomial model

maximum likelihood estimation in, 391,

692

testing a composite hypothesis in, 692,

791

testing a simple hypothesis in, 691-694,

781-790

for 2 x 2 table, 145, 147

for three-factor contingency table, 150.

_See also_ Chi-squared test; Contingency

tables

Multiple comparison procedures, 333

complexity of, 468

history of, 491

interpretability of, 468

significance levels for, 462, 465, 466.

_See also_ Duncan and Dunnett multiple comparison methods; Newman-Keuls multiple comparison procedure; Simultaneous confidence intervals; Stepdown procedures; Stepup procedures; Tukey levels; Tukey's \(T\)-method

Multiple decision procedures, 5. _See also_ Multiple comparisons; Multiple testing; Three-decision problems

Multiple testing, vii, 333, 405-470

history of, 491

maximin procedures, 445

Multiplicity problem, 406

Multivariate cumulative distribution

function, 527

Multivariate linear hypothesis, 348, 361. _See also_ Linear hypothesis

Multivariate mean

nonparametric confidence regions based on bootstrap, 877, 878

multiple testing for, 898

Multivariate normal distribution, 102, 346,

528

testing linear combination of means, 102 tests for, 402, 347, 691. _See also_ Bivariate normal distribution

Multivariate normal one-sample problem, the mean

confidence intervals for, 518

tests for, 347, 382, 422. _See also_ Hotelling's \(T^{2}\)-test; Simultaneous confidence sets

Multivariate \(t\)-distribution, 312

**N**

Natural parameter

space of an exponential family, 51, 59,

135

Negative binomial distribution, 22, 75, 163

Neighborhood model, 374

Nested classification, 342, 356

Nested rejection regions, 69, 109, 121

Newman-Keuls multiple comparison

procedure, 463, 465

Newton's identities, 41

Neyman-Pearson fundamental lemma, 65,

123

approximate version of, 372

generalized, 88, 123

Neyman-Pearson statistic, 675

Neyman's smooth tests, 792-795

large sample behavior, 795-802

Neyman structure, 131, 135

Neyman's weak null hypothesis, 603

Noncentral

beta distribution, 319, 349

\(\chi^{2}\)-distribution, 349, 353

\(F\)-distribution, 349

\(t\)-distribution, 178, 184, 220, 255

Noninformative prior, 197

Nonparametric

independence problem, 218, 274, 276many-sample problem, 325 methods for linear hypotheses, 329 one-sample problem, 134 test in two-way layout, 330. _See also_ Permutation test; Rank tests; Sign test Nonparametric mean, 522, 623 and the Bahadur-Savage result, 633-634 and the bootstrap, 875, 877 and Edgeworth expansions, 623-627 and the \(t\)-test, 627-632 asymptotic maximin and LAUMP property, 755-763 confidence intervals for based on a root, 866, 868 resampling-based tests for, 893, 898. _See also_ Multivariate mean Nonparametric two-sample problem, 147, 200, 276, 843 confidence intervals in, 214, 231, 305 omnibus alternatives, 279 universally unbiased test in, 306. _See also_ Normal scores test; Wilcoxon test Nonparametric test, 98 Nonparametric variance LAUMP property, 763 Normal approximation, order of error, 882, 883 Normal distribution \(N(\xi,\sigma^{2})\), 5, 99 loglikelihood for, 652 testing against Cauchy or double exponential, 296 testing against uniform or exponential, 296. _See also_ Bivariate normal distribution; Multivariate normal distribution Normality, testing for, 296, 780. _See also_ Normal distribution Normal many-sample problem confidence sets for vector means, 287, 383, 460, 470, 473 tests for means, 324, 500. _See also_ Homogeneity of means; tests of Normal one-sample problem, the coefficient of variation confidence intervals for, 310 test for, 179, 255, 334, 344 Normal one-sample problem, the mean admissibility of test for, 268 AUMP test for, 741, 742 confidence intervals for, 187, 285, 507 credible region for, 196-198 Edgeworth expansion for \(t\)-statistic, 694 LAUMP test of equivalence with unknown variance, 750, 751 likelihood ratio test for, 99 median unbiased estimate of, 187 nonexistence of test with controlled power, 179 nonexistence of UMP test for, 101 optimum test for, 105, 177, 296, 321, 502 test for, based on random sample size, 108 two-stage confidence intervals for, of fixed length, 226 two-stage test for, with controlled power, 227 two-sided test for, 297 sequential confidence intervals for, 186, 227. _See also_ Matched pairs; \(t\)-test Normal one-sample problem, the variance admissibility of test for, 271 conditional confidence intervals for, 518 confidence intervals for, 189, 229 credible region for, 198 likelihood ratio test for, 99 optimum test for, 100, 105, 175, 250, 371 Normal response model, 152 Normal scores statistic, 306 Normal scores test, 277 optimality of, 277, 278 Normal subgroup, 294 Normal two-sample problem, difference of means comparison with matched pairs, 232 confidence intervals for, 189 credible region for, 230 optimal tests for for (with variances equal), 122, 182, 222, 257, 297, 323. _See also_ Behrens-Fisher problem; Homogeneity of means; tests of; \(t\)-distribution; \(t\)-test Normal two-sample problem, ratio of variances, 122, 180, 251, 272 confidence intervals for, 190, 289, 310 credible region for, 230 nonrobustness of test for, 608 test for, 122, 180, 296. _See also_ F-test for ratio of variances; Ratio of variances Nuisance parameters, 361, 503 Null set, 42

**O**

Odds ratio, 143, 500 most accurate unbiased confidence intervals for, 228. _See also_ Binomial probabilities; Contingency table; Two by two tables One-parameter exponential family, 74, 93, 126 complete class for, 159 most stringent test in, 386 One-sided hypotheses, 75, 141 One-way layout, 324, 422 Bayesian inference for, 346 model II for, 338 nonparametric, 325. _See also_ Homogeneity; tests of; Normal many-sample problem Onto. _See_ Transformation Optimality, 9, 10 Orbit of transformation group, 244 Ordered alternatives, 326 Order notation \(O_{P}\) (1), \(op\) (1), 537 an \(\asymp\) bn, 670 an \(\sim\) bn, 718 Order statistics, 39, 41 as maximal invariants, 244 as sufficient statistics, 57, 201 completeness of, 134, 160 distribution of, 303 equivalent to sums of powers, 41 expected values of, 277 in permutation tests, 201 Orthogonal group, 245, 246, 377 Orthogonal transformations, 221, 245 vector, 925 Orthonormal system, 925 vector, 925

Paired comparisons. _See_ Matched pairs Pairwise comparisons, 406, 410, 415, 766 Pairwise sufficiency, 57 Parameter space, 3 Parameters, unrelated. _See_ Variation independent parameters Parametric bootstrap, 872-874 in Behrens-Fisher problem, 892, 893 Pareto distribution, 107, 224 Parseval's identity, 925, 925 Partial ancillarity, 499, 500 Partial sufficiency, 122 Pearson's Chi-squared test. _See_ Chi-squared test Percentile method, 911 Per-family error rate (PFER), 432, 433 Permutation group, 245 Permutation test, 147, 202, 214 approximated by standard _t_-test, 206, 609 as randomization test, 276, 835, 842-843 complete class, 212 computational methods for, 205 confidence intervals based on, 216, 231, 235 for testing independence, 219 history of, 239, 860 most powerful, 203 robustness of, 609, 638-643 most stringent, 402. _See also_ Nonparametric; Randomization model Pillai-Bartlett trace test, 463 robustness of, 465 Pitman asymptotic relative efficiency. _See_ Asymptotic relative efficiency Pivotal method, 865-866 quantity, 289, 311 Plug-in estimate, 869 Point estimation, 5, 7 equivariant, 13 history of, 28 unbiased, 14 Pointwise asymptotically level \(alpha\), 524, 605, 607, 683, 685, 741, 745, 747, 750, 874 for confidence sets, 526 for tests, 524 Pointwise consistent in power, 525 Poisson distribution, 4, 6, 58 comparison of two, 141, 499 relation to exponential distribution, 28, 75, 112 square root transformation for, 556 sufficient statistics for, 19 sums of, 58. _See also_ Exponential distribution; Poisson parameters; Poisson process Poisson model for 2 x 2 table, 147, 150 for 2 x 2 x \(K\) table, 150, 167 Poisson parameters comparing two, 142, 500 confidence intervals for the ratio of two, 192 one-sided test for, 75, 111 one-sided test for sum of, 120 Poisson process, 4, 75, 112 and 2 x 2 tables, 147confidence bounds for scale parameter, 82 distribution of waiting times in, 22 test for scale parameter in, 75, 111. _See also_ Exponential distribution Polya's theorem, 533 Polya frequency function, 369 Population models, 150 Portmanteau theorem, 528 Positive dependence. _See_ Dependence; positive part of a function, 32 Positive regression dependence on a subset (PRDS), 435, 436, 437, 485, 491 Posterior distribution, 196 percentiles of, 200. _See also_ Bayesian inference Posterior probability, 108 Potential outcomes, 563, 568, 570, 571, 603 Power function, 62 of invariant test, 260 of one-sided test, 75 of two-sided test, 93 Power of a test, 62, 111 conditional, 141, 500 unbiased estimation of, 140 Power series distribution, 161 Preference ordering of decision procedures, 10, 14 Prepivoting, 880, 888 Prior distribution, 15, 196 improper, 197 noninformative, 197. _See also_ Bayesian inference; Least favorable distribution; Posterior distribution Probability density (with respect to \(\mu\)), 35 convergence theorem for, 923 Probability distribution of a random variable, 31. _See also_ Cumulative distribution function (cdf) Probability integral transformation, 110, 303 Probability measure, 42, 31 Product measure, 35 Prohorov's theorem, 545 Projection, 59, 246, 320, 323, 416, 579, 580, 581, 603, 615, 619, 675, 699, 702 Projection, as maximal invariant, 246, 323 Pseudometric space, 621 P-value, 62, 69, 111, 112, 124 combination of, from independent experiments, 111, 124 for randomization test, 837 for randomized tests, 69 in multiple testing, 408, 460 in stepdown procedures, 453 properties of, 71, 168 versus fixed levels, 71 **Q** Quadrant dependence, 164, 239, 466, 467. _See also_ Dependence; positive Quadratic mean derivative, 653 Quadratic mean differentiable (q.m.d.) families, 654 examples of, 656, 657 loglikelihood expansion for, 658 properties of, 485-487 Quadrinomial distribution, 150 Quality control, 97, 254 Quantiles, 533, 869 **R** Rao's score tests. _See_ Score tests Radon-Nikodym derivative, 35, 54 Radon-Nikodym theorem, 35 Random assignment, 148, 207, 282, 333 Random effects model, 338 for nested classifications, 342, 356 for one-way layout, 338 for two-way layout, 357. _See also_ Ratio of variances Randomization, 8, 333 as basis for inference, 207 possibility of dispensing with, 111 relation to permutation test, 210 tests, 831-847. _See also_ Random assignment; Randomized procedure Randomization distribution, 837 Randomization hypothesis, 833 Randomization models, 149, 214 confidence intervals in, 215 history of, 239 Randomized procedure, 8 confidence intervals, 191 in conditioning, 516 Randomized test, 63 representation as nonrandomized test, 83 Randomness, hypothesis of, 308 Random sample size, 108, 161, 239 Random variable, 31 Rank correlation coefficient, 309 Ranks, 245 as maximal invariants, 246, 275 distribution under alternative, 302, 303null distribution of, 273. _See also_ Signed ranks

Rank-sum test, 166. _See also_ Wilcoxon test

Rank tests, 275

as special case of permutation tests, 836, 836

in multivariate problems, 361

surveys of, 722. _See also_ Nonparametric; Nonparametric two-sample problem; Symmetry; Trend

Ratio of variances

confidence intervals for, 191, 289, 310, 340, 745

in model II, 341

tests for, 179, 251, 296, 339, 514. _See also_

\(F\)-test for ratio of variances; Homogeneity, tests of; Random effects model

Recognizable subsets. _See_ Relevant subsets

Rectangular distribution. _See_ Uniform distribution

Regression, 193, 361, 395

as linear model, 316, 333

comparing several lines, 335, 355

confidence band for, 481, 491

confidence intervals for coefficients, 223, 335

intercepts and ordinates of line, 194

polynomial, 316

robustness of tests for, 614-623

tests for coefficients, 193, 333

with both variables subject to error, 355.

_See also_ Trend

Regression dependence, 218, 273. _See also_ Dependence; positive

Regular (estimator sequence), 682, 707

Relative efficiency, 717. _See also_ Asymptotic relative efficiency

Relevant and semirelevant subsets, 200, 506, 508, 515

history of, 517

randomized version of, 517

relation to Bayesian inference, 518

Restricted Bayes solution, 15

Riemann integral, 32

Risk function, 3, 9, 10

Robustness, 11, 404

against dependence, 611-614, 906

against \(F\)-test of means, 607, 608, 610, 648

of efficiency, 523

of general linear models tests, 614-623

of validity, 523

lack of, for \(F\)-test of variances, 608

lack of, for Chi-squared test of a normal variance, 606

of test of independence or lack of correlation, 559

for tests in two-way layout, 620

of \(t\)-test, 606. _See also_ Adaptive test; Behrens-Fisher problem; Permutation test; Rank tests

Root, 864

Runs of Bernoulli trials, 587

Runs test, for testing independence in a Markov chain, 164, 165

Sample, 5

haphazard, 207

stratified, 201, 208, 214

**S**

Sample correlation coefficient, 217, 236

distribution of, 238

limiting distribution of, 543

monotone likelihood ratio of distribution, 298

variance stabilizing transformation for, 544. _See also_ Bivariate normal distribution; Rank correlation coefficient

Sample covariance matrix, 347, 359

distribution of, 239

Sample distribution function. _See_ Empirical cumulative distribution function

Sample inspection

by attributes, 73, 254

by variables, 98, 254

for comparing two products, 153, 256

Sample size, 8

required to achieve specified power, 62, 142, 227, 364

Sample median, 532

Sample space, 31

Sample standard deviation, 539

Sampling without replacement, 563, 599, 858

\(S\)-ancillary, 499, 500

Scale families, 370

comparing two, 262, 515

conditional inference for, 517

condition for monotone likelihood ratio, 369

Scheff'e's \(S\)-method, 470, 476, 481, 488

alternatives to, 481

Score tests, 685-689

asymptotically maximin property, 753,asymptotical relative efficiency of, 719

AUMP and LAUMP property, 729

counterexample to AUMP property, 731

Score vector (or function), 658, 685

Second-order accurate, 886

Selection procedures, 117

Separable

family of distributions, 925

space, 921

Separate families of hypotheses, 250, 258

Sequential procedures, 8, 9, 164, 179, 187

Shift, confidence intervals for

based on permutation tests, 231

based on rank tests, 286, 305. _See also_

Behrens-Fisher problem; Exponential distribution; Nonparametric two-sample problem; Normal two-sample problem, difference of means

Shift model, 151, 286, 768

\(\sigma\)-field, 30

with countable generators, 926

\(\sigma\)-finite, 30

Sidak method, 409

Signed ranks, 276

distribution under alternatives, 307

null distribution of, 281

Significance level, 62

for multiple comparisons, 462, 465

for stepdown procedures, 417, 454

nominal, 387. _See also_ P-value

Significance probability. _See_ P-value

Sign test, 85

asymptotic relative efficiency of, 720,

721

for matched pairs, 156

for testing consumer preferences, 153

for testing symmetry with respect to a given point, 155

history of, 168

in double exponential distribution, 398

limiting behavior, 673, 674

treatment of ties in, 191, 186. _See also_

Binomial probabilities; Median; Sample inspection

Simes' identity, 424, 437

Simes' inequality, 437, 438, 443, 444

Similar test, 125, 131

relation to unbiased test, 126

history of, 168

Simple

class of distributions, 64

hypothesis, 64

Simple function, 32

Simple hypothesis vs. simple alternative, 65,

517

with large samples, 676. _See also_

Neyman-Pearson lemma

Simpson's paradox, 150

Simultaneous confidence intervals, 470, 491

bootstrap, 880

for all contrasts, 479. _See also_

Confidence bands; Dunnett's multiple comparison method; Scheffe's _S_-method; Tukey's _T_-method

Simultaneous confidence sets for a family of linear functions, 470, 477

smallest, 473

taut, 473

Simultaneous testing, 406. _See also_ Multiple comparisons and multiple testing

Single step procedure for multiple testing, 417

Singly truncated normal distribution (STN),

163

Skewness, 624, 882

Slutsky's theorem, 537

Smirnov test, 279

Smooth function of means, 878

Sparse alternatives, 637, 642

Spherically symmetric distributions, 222,

358

Stagewise tests, 461

Standard confidence bounds, 87, 200

Starshaped, 116

Stationarity, 164

Stationary process, 583, 588, 601, 603

Statistic, 32, 36

and random variables, 32

equivalent representations of, 38

fully informative, 109

subfield induced by, 36

Statistical inference, 3

and decision theory, 6

history of, 28

Stein's two-stage procedure, 226

Stepdown procedures, 417, 420, 491

canonical form for, 453

large-sample bootstrap, 895-899

Stepup procedures, 418, 447

Stepwise method, 415, 417, 419

Stochastically increasing, 77, 135

Stochastically larger, 77, 115, 273, 446

Stratified sampling, 201, 208, 214

Strictly unbiased, 127

Strong mixing, 583, 585, 588, 604, 918

Strongly unimodal, 369, 503, 514, 731Studentization, 326, 607

Studentized range, 462, 490

Studentized range distribution, 411, 415

Student's _t_-distribution. _See t_-distribution

Student's _t_-test. _See t_-test

Subfield, 36

Sufficient statistic, 19, 47, 58, 60

Bayes definition of, 22

factorization criterion for, 19, 47

for exponential families, 50

in presence of nuisance parameters, 109

likelihood ratio as, 57

minimal, 22

pairwise, 57

relation to ancillarity, 498

relation to fully informative statistic, 109

relation to invariance, 251

statistics independent of, 172, 173. _See also_ Partial sufficiency

Subsampling, 899-903

comparisons with bootstrap, 903-906

for hypothesis testing, 907, 908

Superefficient estimator, 705

bootstrap of, 906

Symmetric

confidence interval, 870

distribution, 57

Symmetry, 11, 13

and invariance, 12, 241

sufficient statistics for distributions with, 57

testing for, 274, 280, 308

testing, with respect to given point, 155,

280, 282, 308

T
Tautness, 473

_t_-distribution, 178, 184, 325

approximation to permutation distribution, 206

as distribution of function of sample correlation coefficient, 236

as posterior distribution, 199

Edgeworth expansion for, 694

in two-stage sampling, 226

monotone likelihood ratio of, 255

multivariate, 312

noncentral, 178, 184, 220, 225

Test (of a hypothesis), 5, 61

almost invariant, 257, 275

conditional, 495, 502, 504

invariant, 244, 314

locally maximin, 368

locally most powerful, 394

maximin, 368

most stringent, 385

of type D and E, 396

randomized, 63, 144

strictly unbiased, 128

unbiased, 125

uniformly most powerful (UMP), 64

Testing for superiority, 386

Three-decision problems, 93, 141

Three factor contingency table, 150

Ties, 154

Tight sequence, 545

Time series models, 612, 613

Total positivity, 79, 118, 130, 350, 369

Total variation distance, 711

Transformation

into, 31

of integrals, 36

onto, 31

probability integral, 110

variance stabilizing, 544

Transformation group, 12, 241, 242. _See also_

Invariance; Group

Transitive

binary relation, 569

transformation group, 285

Treatment effects, 141, 202, 208, 210, 214,

232, 273, 280, 281, 337, 563, 567,

568, 571, 572, 851

Trend

test for absence of, 308

Triangular distribution, 296

Trimmed mean, 868, 869

_t_-test

admissibility of, 268, 271, 319

as Bayes solution, 270

as likelihood ratio test, 26, 99

comparison with Wilcoxon and sign tests, 720, 721

for matched pairs, 208, 232

for regression coefficients, 193, 334

in linear hypothesis with one constraint, 319

local power of, 631, 632

one-sample, 101, 179, 219, 307

optimality in nonparametric model, 755-763

permutation version of, 205, 835, 839,

840

power of, 178, 220

relevant subsets for, 510

[MISSING_PAGE_FAIL:1005]

for one-parameter exponential families,
74

for uniform distribution, 105

in inverse Gaussian distribution, 113

in normal one-sample problem, 99, 101

in Weibull distribution, 113

nonparametric example of, 98

UMP unbiased test, 125

admissibility of, 157

example of nonexistence of, 158

for multiparameter exponential families,
135, 137, 171

for one-parameter exponential families,
126

for strictly totally positive families, 130

relation to UMP almost invariant tests,
262

via invariance, 171, 263. _See also_ Unbiasedness

Uniformly integrable, 560

Uniformly most accurate confidence sets,
80, 81

equivariant, 284

minimize expected Lebesgue measure,
287

relation to UMP tests, 82

unbiased, 188. _See also_ Confidence bands; Confidence bounds; Confidence intervals; Confidence sets; Simultaneous confidence intervals; Simultaneous confidence intervals and sets

Unimodal, 514. _See also_ Strongly unimodel

Unrelated parameters, 499

U-statistic, 563, 573, 574, 577, 580, 581,
596, 599, 600, 601, 603, 675, 699,
905

**V**

Variance components. _See_ Components of variance

Variance stabilizing transformation, 544

Variation diminishing, 79. _See also_ Total positivity

Variation independent parameters, 499

Vector space, 924-925

Vitali's theorem, 34

**W**

Waiting times, 22, 112

Wald tests and confidence regions, 682-685,

867

efficiency of, 719

AUMP and LAUMP property, 752, 734

Weak compactness theorem, 927, 928

Weak convergence, 527, 921

Weak conditionality principle, 501

Weibull distribution, 113

Weighted quadratic test statistics, 802

examples of, 806, 808

local power calculations, 811, 812

Welch approximate _t_-test, 264, 610

Welch-Aspin test, 264, 510

Wilcoxon one-sample test, 281

Wilcoxon signed-rank statistic, 307, 664,

674, 675

Wilcoxon signed-rank test. _See_ Wilcoxon one-sample test

Wilcoxon statistic, 305, 306

expectation and variance of, 302

Wilcoxon two-sample test, 277, 280

alternative form of, 305

comparison with _T_-test, 720, 721

confidence intervals based on, 286

history of, 314

optimality of, 277, 279, 305

Wilson confidence interval for binomial,
540, 868

**Y**

Yule's measure of association, 146

## Author Index

**A**

Agresti, A., 144, 146, 150, 152, 192, 361

Aiyar, R. J., 309

Akritas, M., 362

Albers, W., 309, 613, 772, 861

Albert, A., 325

Alf, E., 782

Andersen, S. L., 240

Anderson, T. W., 102, 248, 348, 362, 634

Andersson, S., 248

Andrews, D., 392, 704

Anscombe, F., 556

Antille, A., 282

Arbuthnot, J., 123, 169

Arcones, M., 877, 905

Arias-Castro, E., 649

Armsen, P., 144

Arnold, S., 273, 311, 333, 362, 469

Aronow, P., 571

Arrow, K., 63

Arthur, K. H., 349

Arvesen, J. N., 341

Athreya, K., 877

Atkinson, A., 193, 333, 361

**B**

Babu, G., 877, 918

Bahadur, R., 58, 239, 633, 648, 764, 772

Bain, L. J., 228, 229

Bai, Y., 209, 851, 861

Baker, R., 609

Balakrishnan, N., 178, 181, 220, 224, 319, 349, 609

Banach, S., 927

Barankin, E. W., 50

Barber, R., 491

Bar-Lev, S., 135, 229

Barlow, R. E., 326

Barnard, G. A., 200, 515

Barndorff-Nielsen, O., 50, 60, 122, 499, 505, 694

Barnett, V., 6

Barron, A., 829

Bartholomew, D. J., 326

Barlett, M. S., 108, 694

Barwick, P., 392, 704

Basu, D., 122, 240, 496, 498, 499, 512-514

Basu, S., 628, 648

Bayarri, J., 124

Bayarri, M., 124, 200

Becker, B., 124

Becker, N., 498

Bednarski, T., 374

Behnen, K., 772

Bell, C. B., 134, 275

Bell, C. D., 240

Benichou, J., 734, 707

Benjing, V., 772

Benjamini, Y., 434, 435, 438, 469, 491, 607

Bennett, B., 144, 195

Bentkus, V., 798

Bera, A., 707

Beran, R., 648, 707, 722, 772, 780, 828, 864, 874, 880, 881, 888, 892, 893, 895, 905, 915, 918

Berger, A., 16, 364, 925Berger, J., 15, 18, 28, 108, 124, 197, 200, 378, 501, 516, 518, 707
* () Berger, R., 124, 326, 404, 518, 748
* () Berk, R., 11, 251, 258, 275
* () Bernardo, J., 16
* () Bernoulli, D., 123
* () Best, D., 813, 828
* () Bhapkar, V. P., 153
* () Bhattacharya, P. K., 282
* () Bhattacharya, R., 625, 649, 889
* () Bhat, U., 1,64
* () Bickel, P., 11, 28, 258, 275, 556, 649, 657, 694, 722, 760, 772, 860, 876, 904-906, 918
* () Bilias, Y., 707
* () Billingsley, P., 44, 60, 133, 166, 211, 254, 292, 527, 529, 530, 559, 561, 585, 807
* () Birch, M. W., 153
* () Birnbaum, A., 112, 143, 315, 501, 517
* () Birnbaum, Z. W., 123, 292, 547
* () Bishop, Y. M. M., 152, 705
* () Blackwell, D., 16, 22, 28, 43, 109, 134
* () Blair, R. C., 722
* () Blanchard, G., 491
* () Bloomfield, P., 473, 482
* () Blyth, C. R., 6, 85, 124, 192
* () Bohrer, R., 482
* () Boldrick, J., 124, 491
* () Bondar, J. V., 382, 511, 517
* () Bondessen, L., 13
* () Boos, D., 124, 239, 282, 609, 648
* () Boschloo, R. D., 144
* () Bose, R. C., 491
* () Boukai, B., 200
* () Bowker, A. H., 704
* () Box, G. E. P., 240, 346, 523, 556, 648
* () Box, J. F., 28, 333
* () Bradley, R. D., 604
* () Brain, C. W., 828
* () Braun, H. (Ed.), 491
* () Breiman, L., 134
* () Bremner, J. M., 326
* () Bretagnolle, J., 877, 905
* () Brockwell, P. J., 613
* () Broemeling, L. D., 346
* () Bross, I. D. J., 144
* () Brown, K. G., 271, 346
* () Brown, L. D., 18, 50, 60, 76, 79, 123, 130, 159, 179, 192, 350, 384, 404, 510, 511, 517, 518, 539, 540, 748, 868, 889
* () Brown, M. B., 610, 648
* () Brownie, C., 511, 609
* () Brunk, H. D., 326
* () Brunner, E., 362, 860
* () Buehler, R., 200, 224, 510, 511, 515-517
* () Bugni, F., 851, 861
* () Buhlmann, P., 772
* () Burkholder, D. L., 58

## Appendix C

Cabana, A., 828
* () Cabana, E., 828
* () Cai, T., 192, 539, 540
* () Canay, I., 390, 861
* () Candes, E., 491, 649
* () Carroll, R. J., 361
* () Casella, G., 5, 13, 17, 22, 60, 140, 197, 199, 229, 332, 383, 384, 496, 497, 510, 517, 518, 679, 680, 733, 905
* () Castillo, J., 163
* () Chakraborti, S., 165, 280, 286, 325, 329, 547
* () Chalmers, T. C., 63
* () Chambers, E. A., 152
* () Chang, C.-K., 438
* () Chapman, D. G., 123
* () Chatterjee, Samprit, 193
* () Chatterjee, Sourav, 604
* () Chebychev, P., 578
* () Chen, H. J., 828
* () Chen, J., 491
* () Chen, L., 593, 604, 607
* () Chernoff, H., 264, 707, 791, 828
* () Chernozhukov, V., 918
* () Chetverikov, D., 918
* () Chhikara, R. S., 114, 224, 225
* () Chmielewski, M. A., 358
* () Choi, K., 362
* () Choi, S., 772
* () Chou, Y. M., 349
* () Choy, K., 772
* () Christensen, R., 361
* () Chung, E., 838, 850, 860
* () Cima, J. A., 482
* () Clarke, D., 918
* () Clinch, J. C., 610, 648
* () Clopper, C. J., 85, 426, 641
* () Cochran, W. G., 611
* () Cohen, A., 76, 229, 239, 273, 326, 359, 361, 389, 396, 828
* () Cohen, J., 63, 319
* () Cohen, L., 109
* () Conover, W. J., 609, 648
* () Coull, B., 192Cox, D., 243

Cox, D. R., 6, 124, 151, 152, 250, 498, 517, 556

Cramer, H., 649, 680, 706, 828

Cressie, N., 607, 828

Csorgo, S., 877

Cvitanic, J., 394

Cyr, J. L., 648

D'Agostino, R., 511, 780, 812, 826, 828

Dantzig, G. B., 89, 123

Darmois, G., 50

DasGupta, A., 192, 314, 539, 540, 628, 648

Davenport, J. M., 264

David, H. A., 277

Davis, B. M., 144

Davison, A., 918

Davis, R. A., 613

Davydov, Y., 604

Dawid, A. P., 122, 514

Dayton, C., 469

Dedecker, J., 604

DeGroot, M., vii

de Leeuw, J., 11

de Moivre, A., 561

Dempster, A. P., 200

Deshpande, J. V., 828

Deuchler, G., 314

Devroye, L., 549

de Wet, T., 828

Dezeure, R., 772

Diaconis, P., 205, 308, 361, 824, 841, 870

DiCiccio, C., 860

DiCiccio, T., 85, 694, 913, 918

Dickhaus, T., 491

Ding, P., 571, 603

Dobson, A., 361

Doksum, K. A., 28, 556, 828

Donev, A., 333

Donoghue, J., 460

Donoho, D., 426, 641, 642, 649

Doukhan, P., 585, 604

Draper, D., 326

Drost, F., 828

Dubins, L. E., 43

Ducharme, G., 892

Dudley, R., 60, 527, 560, 561, 656, 759, 924

du Dudoit, S., 124, 491, 918

Dumbgen, L., 828

Duncan, D. B., 491

Durbin, J., 280, 517, 547, 812, 827, 828

Dvoretzky, A., 109

Dykstra, R., 326

Eaton, M., 240, 248, 259, 362, 378

Edelman, D., 607, 628

Edgeworth, F. Y., 123, 649

Edgington, E. S., 860

Edwards, A. W. F., 146, 200

Efron, B., 217, 222, 491, 544, 648, 824, 869, 888, 894, 918

Eisenhart, C., 165

Elfving, G., 58

Engelhardt, M., 228, 229

Engelhardt, M. E., 228

Eubank, R., 802, 813, 829

Falk, M., 613

Fan, J., 707, 802, 829

Faraway, J., 491

Farrell, R., 124, 248, 378

Fears, T., 707, 734

Feddersen, A. P., 510

Feller, W., 4, 292, 530, 624, 630, 647, 797

Fenstad, G. U., 610

Ferguson, T. S., 5, 16, 18, 28

Fienberg, S., 152, 239

Fienberg, S. E., 152, 705

Finch, P. D., 150

Finner, H., 158, 424, 468, 491

Finney, D. J., 144

Fisher, R. A., 28, 111, 124, 144, 169, 200, 239, 426, 510, 517, 706, 790, 828, 835

Folks, J. L., 114, 224

Forsythe, A., 217, 235, 610, 648

Fourier, J. B. J., 123

Franck, W. E., 296

Fraser, D., 200

Fraser, D. A. S., 122, 160, 200

Freedman, D., 149, 876, 904, 905, 918

Freeman, M. F., 556

Freiman, J. A., 63

Frisen, M., 156

Fuller, W., 613

G Gabriel, K. R., 215, 216, 423, 462, 469

Gail, M., 707, 734

Galambos, J., 639, 828Gan, L., 680

Garside, G. R., 144

Garthwaite, P., 216

Gart, J. J., 151

Gastwirth, J. L., 282, 613

Gatsonis, C., 153

Gauss, C. F., 123, 561

Gavarret, J., 123

George, E. I., 518

Ghosh, J., 49, 228, 251, 649, 694

Ghosh, M., 169, 369

Gibbons, J., 123, 165, 280, 286, 325, 329, 547

Giesbrecht, F., 333

Gine, E., 877, 881

Giraud, C., 772

Giri, N., 368, 382, 383

Girshick, M. A., 16, 28

Glaser, R. E., 228, 648

Gleser, L. J., 225, 547

Gokhale, D. V., 149

Goldstein, L., 593, 604

Good, P., 205, 240, 860

Goodman, L. A., 146

Gordon, I., 498

Gotze, F., 904

Goutis, C., 229, 510

Grandhi, A., 468

Graybill, F. A., 319

Gree, D., 571

Green, B. F., 205

Green, J. R., 828

Greenwood, P., 791, 828

Grenander, U., 123

Groeneboom, P., 722, 723, 772

Guenther, W. C., 165

Guillier, C. L., 309

Gumpertz, M., 333

Guo, W., 445, 468, 491

Gupta, A., 394

H Haberman, S. J., 146, 152

Hackney, O. P., 332

Hadi, A., 193

Hajek, J., 34, 280, 292, 325, 565, 603, 655, 657, 706, 707, 772, 779, 860

Hald, A., 561

Hall, P., 85, 585, 604, 609, 625, 649, 694, 828, 877, 884, 886, 888, 889, 906, 918

Hall, W., 124, 251, 688, 707

Hall, W. J., 216

Hallin, M., 772

Halmos, P. R., 109, 378, 926

Hamada, M., 333

Hamilton, J. D., 613

Hartigan, J. A., 217, 235, 240, 918

Hartley, H. O., 319

Harville, D. A., 346

Has'minskii, R., 679

Hastie, T., 361

Hayter, T., 415, 491

Haytner, A., 491

Hedges, L., 124

Hegazy, Y. A. S., 828

Hegemann, V., 330, 331

He, L., 445

Heritier, S., 707

Hettmansperger, T., 326

Hettmansperger, T. P., 325, 329, 362, 609, 610

Heyde, C., 585, 604

Higgins, J. J., 722

Hillier, G. H., 648

Hinkley, D., 503, 556, 918

Hipp, C., 50

Hochberg, Y., 422, 427, 434, 463, 469, 470, 482, 491

Hocking, R. R., 332, 333, 359

Hodges, J. L., Jr., 179, 526, 772

Hoeffding, W., 135, 240, 314, 603, 604, 648, 828, 838, 860

Hoel, P. G., 169

Hogg, R. V., 296

Holland, P. W., 152, 705

Holm, S., 469, 470, 491, 841, 870

Holmes, S., 205, 841, 870

Hommel, G., 425, 444

Hooper, P. M., 251, 273

Horst, C., 144

Hotelling, H., 123, 314, 362, 491, 556, 611, 648

Hsu, C. F., 215

Hsu, C. T., 236

Hsu, J., 463, 491, 748

Hsu, P., 144, 362

Huang, J. S., 369

Huber, P. J., 374, 404, 619, 648

Hughes-Oliver, J., 239

Hung, K., 164, 838, 850, 860

Hunt, G., 314, 404

Hunter, J. S., 333

Hunter, W. G., 333

Hutchinson, D. W., 192* (19) Hwang, J., 124, 179, 225, 384, 518
* (20) Ibragimov, I., 679
* (21) Ibragimov, J. A., 369, 588, 604
* (22) Imbens, G., 603
* (23) Inglot, T., 772, 802, 829
* (24) Ingster, I., 649
* (25) Ingster, Y., 641, 790, 820, 822
* (26) Isaacson, S. L., 396

Jagers, P., 317
* (27) James, A. T., 610, 648
* (28) James, G. S., 610, 648
* (29) Jammalamadaka, S., 829
* (30) Janssen, A., 772, 773, 813, 814, 818, 819, 847, 860, 918
* (31) Jensen, J., 694
* (32) Jentsch, C., 860
* (33) Jiang, J., 680
* (34) Jing, B., 649
* (35) Jin, J., 426, 641, 642, 649
* (36) Jockel, K., 548
* (37) Johansen, S., 135, 610, 648
* (38) John, R. D., 205, 215
* (39) Johnson, D. E., 330, 331
* (40) Johnson, M. E., 609, 648
* (41) Johnson, M. M., 609, 648
* (42) Johnson, N. L., 111, 113, 144, 178, 181, 220, 224, 238, 319, 349, 540
* (43) Johnson, N. S., 149
* (44) Johnstone, I., 649
* (45) Johnstone, I. M., 350, 130
* (46) Joshi, V., 273

Kabe, D. G., 106
* (47) Kakutani, S., 772
* (48) Kalbfleisch, J. D., 496
* (49) Kallenberg, W., 309, 772, 802
* (50) Kallenberg, W. C. M., 158, 394, 828
* (51) Kanoh, S., 491
* (52) Kappenman, R. F., 502
* (53) Karatzas, I., 394
* (54) Kariya, T., 358
* (55) Karlin, S., 4, 23, 76, 77, 263, 369
* (56) Kasten, E. L., 144
* (57) Kato, K., 918
* (58) Kemp, A., 129, 144
* (59) Kemperman, J., 239
* (60) Kempthorne, O., 333
* (61) Kempthorne, P., 11
* (62) Kendall, M. G., 309, 828
* (63) Kent, J., 828
* (64) Kersting, G., 282
* (65) Kesselman, H. J., 610
* (66) Khmaladze, E., 828
* (67) Khurshid, A., 144
* (68) Kiefer, J., 314, 333, 348, 361, 368, 382, 383, 511, 516
* (69) King, M. L., 648
* (70) Klaassen, C., 657, 760, 772
* (71) Klett, G. W., 189, 229, 287
* (72) Knight, K., 877
* (73) Knott, M., 812
* (74) Koehn, U., 239
* (75) Kohne, W., 613
* (76) Kolassa, J., 889
* (77) Kolmogorov, A., 21, 775, 828
* (78) Kolodziejczyk, S., 361
* (79) Koopman, B., 50
* (80) Korn, E., 469
* (81) Koschat, M., 225
* (82) Koshevnik, Y., 772
* (83) Kotz, S., 111, 113, 129, 144, 164, 178, 181, 220, 224, 238, 319, 349
* (84) Kowalski, J., 18, 603
* (85) Koziol, J. A., 282
* (86) Krafft, O., 98
* (87) Kraft, C., 364
* (88) Kruskal, W., 146
* (89) Kruskal, W. H., 72, 314
* (90) Kuebler, R. R., 63
* (91) Kuffner, T., 72
* (92) Kunsch, H. R., 914
* (93) Kusunoki, U., 491

L

Lairi, S. N., 614, 914, 918
* (94) Lambert, D., 124, 374, 609, 860
* (95) Lane, D., 149
* (96) Lang, G., 604
* (97) Laplace, P. S., 123, 561
* (98) LaRiccia, V., 802, 813
* (99) Latscha, R., 144
* (100) Laurent, A. G., 106
* (101) Lawless, J. F., 502
* (102) Layard, M. W. J., 341
* (103) Lazar, N., 72
* (104) Le Cam, L., 22, 58, 657, 660, 705, 706, 707, 716, 734, 735, 772
* (105) Ledwina, T., 309, 772, 802, 829
* (106)Lee, A., 603

Lee, D., 571

Leger, C., 874

Lehmann, E. L., 5, 13, 17, 22, 28, 60, 63, 98,

101, 116, 123, 124, 140, 150, 169,

179, 197, 199, 200, 239, 240, 271,

286, 319, 325, 329, 332, 383, 384,

404, 442, 444, 445, 453, 466, 469,

491, 496, 497, 535, 648, 679, 680,

691, 722, 733, 767, 772, 791, 828,

905

Lentner, M. M., 224

Leon, J., 604

Levit, B., 772

Levy, K. J., 290

Lexis, W., 123

Liang, K. Y., 505, 705

Lieberman, G. J., 144

Lindley, D. V., 108

Linnik, Y. V., 382

Lin, S., 829

Liseo, B., 707

Littell, R. C., 124

Liu, H., 326, 404

Liu, R. Y., 124, 885, 914

Li, X., 603

Loh, W.-Y., 250, 369, 824, 887, 888

Loh, W-Y., 648

Loomis, L. H., 378

Lorenzen, T. J., 333

Louhichi, S., 604

Louv, W. C., 124

Lou, W., 165

Low, M., 772

Lyapounov, A. M., 109

M
Maatta, J., 518

MacGibbon, K. G., 79, 130, 350

Mack, C., 144

Mack, G. A., 329

Madansky, A., 228

Maitra, A. P., 50

Mandelbaum, A., 134

Mann, H., 600, 789

Manoukian, E. B., 648

Mantel, N., 734

Marasinghe, M. C., 330

Marcus, R., 423, 469

Marden, J., 18, 124, 153, 159, 326, 361, 362,

389, 505, 506

Mardia, K. V., 181

Maritz, J. S., 231

Marshall, A. W., 350, 369

Martin, A., 144

Martin, M., 888

Mason, D., 877

Massart, P., 548

Massey, F. J., 776, 777

Mathew, M., 341

Mathiason, D., 688, 707

Mattner, L., 134, 135

McCullagh, P., 361, 782, 889

McCulloch, C., 333

McDermott, M., 404

McDonald, L. L., 144

McElroy, T., 613

McKean, J., 326, 609, 610

McLaughlin, D. H., 523

McMurry, T. L., 918

McShane, L., 469

Meeks, S. L., 510

Mee, R., 305

Meng, X., 124, 491

Michel, R., 137

Milbrodt, H., 813

Millar, W., 707, 828, 881

Miller, F. L., 813

Miller, F. R., 361

Miller, G., 164

Miller, J., 346, 359

Miller, R. G., 333, 490

Milliken, G. A., 144

Milnes, P., 382

Miwa, T., 491

Mogstad, M., 918

Mokkadem, A., 585

Montgomery, D., 333

Morgan, W. A., 236

Morgenstern, D., 239

Morimoto, H., 22, 49

Mosteller, F., 160, 523

Mottonen, J., 362, 772

Mudholkar, G. S., 544

Muller, C., 648

Munk, A., 179

Murphy, S., 707

Murray, L. W., 341

Muyot, M., 361

N

Nachbin, L., 378

Naiman, D. Q., 482

Narula, S. C., 290* () Neill, J., 361
* () Nelder, J., 361
* () Neubert, K., 860
* () Neuhaus, G., 772, 828, 860
* () Neumann, M., 604
* () Neyman, J., 28, 123, 124, 168, 239, 240, 568, 648, 790, 792, 793, 828
* () Nicolaou, A., 199
* () Niederhausen, H., 547
* () Nikitin, Y., 723, 772, 819, 828
* () Nikulin, M., 791, 828
* () Noether, G., 772
* () Nogales, A., 251
* () Nolle, G., 927
* () Nomakuchi, K., 389

**O**

Oden, A., 204
* () Oja, H., 362, 772
* () Ojeda, M., 338
* () Olkin, I., 124, 350, 369, 427
* () Olshen, R. A., 510
* () Omelka, M., 860
* () Oosterhoff, J., 717, 723, 772, 828
* () Ord, J., 28, 359, 544, 789
* () Owen, A., 547, 707, 894, 918
* () Owen, D. B., 144, 220, 349, 610
* () Oyola, J., 251

**P**

Pace, L., 250
* () Pachares, J., 130
* () Padmanabhan, A., 609
* () Patel, J. K., 238
* () Pauls, T., 819, 860, 918
* () Pauly, M., 860
* () Pawitan, Y., 707, 734
* () Pearson, E. S., 28, 85, 123, 168, 239, 319, 426, 641, 648
* () Pearson, K., 707, 781, 828
* () Pedersen, K., 50
* () Pena, E., 829
* () Pereira, B., 250
* () Perez, P., 251
* () Peritz, E., 423, 469, 475
* () Perlman, M., 179, 266, 326, 389, 404, 505, 506, 680, 748
* () Pesarin, F., 610, 860
* () Peters, D., 772
* () Pfanzagl, J., 72, 74, 185, 264, 771, 772
* () Piegorsch, W. W., 482
* () Pierce, D. A., 516, 517
* () Pitman, E. J. G., 50, 240, 314, 517
* () Plachky, D., 135, 927
* () Plackett, R. L., 152, 705
* () Plan, Y., 649
* () Pliss, V. A., 382
* () Politis, D. N., 613, 880, 899, 903, 906, 914, 918
* () Pollard, D., 527, 553, 653, 698, 775
* () Pollard, K., 918
* () Polonik, W., 828
* () Posten, H. O., 610
* () Pratt, J. W., 112, 171, 228, 280, 515, 516, 610
* () Prescott, P., 326
* () Price, B., 193
* () Prieur, C., 604
* () Puig, P., 163
* () Pukelsheim, F., 333

**Q**

Quenouille, M., 918
* () Quesenberry, C., 813
* () Quesenberry, C. P., 296, 300, 828
* () Quine, M. P., 828

**R**

Radlow, R., 782
* () Ramachandran, K. V., 190
* () Ramamoorthi, R. V., 22, 58
* () Ramsey, P. H., 610
* () Randles, R., 286, 325, 722, 772, 780, 828
* () Rao, C. R., 11, 707, 719, 828
* () Rao, P., 657
* () Rao, R., 625, 889
* () Rayner, J., 813, 828
* () Read, C. B., 238
* () Read, T., 828
* () Reid, C., 28, 123
* () Reid, N., 243
* () Reinhardt, H. E., 98
* () Reiser, B., 229
* () Riani, M., 193, 361
* () Richmond, J., 481, 482, 490
* () Rieder, H., 374, 772
* () Ripley, B., 549
* () Ritov, Y., 657, 772
* () Ritzwoller, D., 587, 861
* () Robbins, H., 604, 923
* () Robert, C., 15, 108, 124, 197, 200, 124
* () Robertson, T., 326
* () Robins, J., 124, 215
* () Robinson, G., 215, 230, 264, 510, 511, 517Robinson, J., 85, 205, 215, 828, 860
* [19] Rojo, J., 23, 116
* [20] Romano, J. P., 85, 109, 161, 392, 404, 442, 431, 444, 445, 453, 468, 469, 491, 551, 587, 614, 628, 648, 764, 772, 780, 828, 838, 847, 850, 851, 860, 861, 874, 880, 895, 899, 903, 906, 909, 913, 914, 916, 918
* [21] Ronchetti, E., 326, 707
* [22] Roquain, E., 491
* [23] Rosenblatt, M., 583, 604
* [24] Rosenstein, R. B., 349
* [25] Rosenthal, R., 63
* [26] Ross, N., 593, 594, 604
* [27] Ross, S., 4
* [28] Roters, M., 491
* [29] Rothenberg, T. J., 179, 610, 648
* [30] Roussas, G., 706, 727, 772
* [31] Royden, H. L., 927
* [32] Roy, K. K., 22, 58
* [33] Roy, S. N., 491
* [34] Rozanov, Y., 604
* [35] Rubin, D., 568, 603
* [36] Rubin, D. B., 63
* [37] Rubin, H., 76, 282, 613
* [38] Rukhin, A., 250
* [39] Runger, G., 240, 556
* [40] Ruppert, D., 361
* [41] Ruschendorf, L., 134

S
* [42] Sackrowitz, H., 124, 271, 326, 396, 828
* [43] Sahai, H., 144, 338
* [44] Sakata, T., 389
* [45] Salaevskii, O. V., 382
* [46] Salaevskii, Y., 382
* [47] Salvan, A., 250
* [48] Samuel-Cahn, E., 124
* [49] Sanathan, L., 63
* [50] Sarkar, S. K., 438, 445, 491
* [51] Savage, L. J., 16, 28, 63, 517, 633, 648, 926
* [52] Schafer, G., 108
* [53] Scheffe, H., 169, 190, 264, 333, 359, 361, 469, 610, 648, 923
* [54] Schervish, M., 28
* [55] Schick, A., 772
* [56] Scholz, F. W., 124
* [57] Schrader, R. M., 326
* [58] Schriever, B. F., 828
* [59] Schuirmann, D., 748
* [60] Schwartz, R., 314, 348, 361
* [61] Schwartz, R. E., 348
* [62] Schweder, T., 124
* [63] Seal, H. L., 361
* [64] Searle, S., 333
* [65] Seber, G. A. F., 362
* [66] Seidenfeld, T., 200
* [67] Self, S. G., 705
* [68] Sellke, T., 108, 124
* [69] Sen, P., 34, 240, 280, 292, 325, 706, 772, 779, 860
* [70] Serfling, R. H., 280, 723, 763, 772, 869
* [71] Serroukh, A., 772
* [72] Severini, T., 199
* [73] Shaffer, J. P., 141, 354, 459, 466, 468, 469, 490, 491, 705
* [74] Shaikh, A., 390, 404, 445, 469, 851, 861, 909, 918
* [75] Shao, J., 28, 869, 918
* [76] Shao, Q., 593, 604
* [77] Shapiro, S. S., 828
* [78] Sheather, S., 326
* [79] Sherfey, B., 361
* [80] Shevstova, I., 531
* [81] Shewhart, W., 648
* [82] Shorack, G., 228, 779, 781, 808, 819, 828
* [83] Shorrock, G., 229
* [84] Shuster, J., 114
* [85] Sidak, Z., 280, 325, 706
* [86] Siegel, A. F., 109, 161, 551
* [87] Siegmund, D., 9, 779
* [88] Sierpinski, W., 369
* [89] Silvapulle, M., 707
* [90] Silvapulle, P., 707
* [91] Silverman, B., 649
* [92] Silvey, S. D., 333, 707
* [93] Simes, R., 424
* [94] Simon, R., 469
* [95] Simpson, D., 772
* [96] Singh, K., 124, 610, 885, 914, 918
* [97] Singh, M., 610
* [98] Sinha, B., 341, 358
* [99] Skillings, J. H., 329
* [100] Small, C., 680
* [101] Smirnov, N. V., 292
* [102] Smith, A., 16
* [103] Smith, D. W., 341
* [104] Smith, H., 63
* [105] Sonnemann, E., 491
* [106] Sophister (G. Story), 648
* [107] Speed, F. M., 332
* [108] Speed, T., 239, 361
* [109] Spiegelhalter, D. J., 828
* [110] Spjotvoll, E., 124, 333, 341, 465, 469
* [111] Sprott, D. A., 122* (19) Spurrier, J. D., 491, 828
* (20) Srivastava, M. S., 648
* (21) Starbuck, R. R., 296, 300
* (22) Staudte, R., 124
* (23) Stein, C. M., 101, 271, 314, 319, 348, 382, 384, 404, 589, 604, 722, 758, 772
* (24) Stephens, M. A., 780, 812, 826, 828
* (25) Stern, S., 694
* (26) Stigler, S. M., 123, 561
* (27) Still, H. A., 192
* (28) Stone, C. J., 11, 722
* (29) Stone, M., 382
* (30) Strassburger, K., 424, 491
* (31) Strassen, V., 374
* (32) Strasser, H., 28, 301, 813
* (33) Strawderman, W. E., 76, 273
* (34) Stuart, A., 28, 359, 789, 828
* (35) Student (W. S. Gosset), 611
* (36) Sugiura, N., 279
* (37) Sun, J., 491
* (38) Suslina, I., 820, 822
* (39) Sutton, C., 607
* (40) Swed, F. S., 165

Takeuchi, K., 106
* (41) Tallis, G. M., 828
* (42) Tamhane, A., 422, 427, 463, 469, 470, 491
* (43) Tang, D., 326
* (44) Taniguchi, M., 772
* (45) Tanur, J., 239
* (46) Tan, W. Y., 607
* (47) Tapia, J., 144
* (48) Tate, R. F., 189, 229, 287
* (49) Taylor, H., 4, 23
* (50) Thomas, D. L., 239
* (51) Thombs, L. A., 614
* (52) Tiao, G. C., 346, 523
* (53) Tibshirani, R., 361, 544, 888, 913, 918
* (54) Tienari, J., 772
* (55) Tiku, M. L., 319, 349, 609, 610
* (56) Tirlea, M. A., 861
* (57) Tiwari, R., 829
* (58) Tong, Y. L., 164
* (59) Tritchler, D., 216
* (60) Troendle, J., 469, 491
* (61) Tseng, Y., 384
* (62) Tsybakov, A., 649
* (63) Tu, D., 918
* (64) Tukey, J. W., 21, 87, 331, 426, 470, 491, 523, 556, 641, 648, 918
* (65) Tu, X., 603
* (66) Turnbull, H., 41
* (67) Tweedie, M. C. K., 225

**U**

Unni, K., 499
* (68) Uthoff, V. A., 296

**V**

Vadiveloo, J., 216
* (69) Vaeth, M., 734
* (70) van Beek, P., 531
* (71) van de Geer, S., 772
* (72) van der Laan, M., 918
* (73) van der Vaart, A., 103, 697, 707, 762, 772, 775, 808, 828, 881, 913
* (74) van Zwet, W., 717, 772, 860
* (75) van Zwet, W. R., 772, 860, 904, 906
* (76) Venable, T. C., 153
* (77) Ventura, V., 124
* (78) Vermeire, L., 394
* (79) von Mises, R., 828
* (80) von Randow, R., 382
* (81) Vu, H., 707

**W**

Wacholder, S., 168
* (82) Wainwright, M., 772
* (83) Wald, A., 18, 89, 109, 123, 404, 680, 727, 772, 789
* (84) Walker, S., 72
* (85) Wallace, D., 264, 517
* (86) Wand, M. P., 361
* (87) Wang, H., 517
* (88) Wang, J., 491, 680
* (89) Wang, Q., 164
* (90) Wang, Y., 200, 404
* (91) Wang, Y. Y., 264, 610
* (92) Wasserstein, R., 72
* (93) Webster, J. T., 264
* (94) Wedel, H., 204
* (95) Weinberg, C. R., 168
* (96) Weisberg, S., 193, 361
* (97) Welch, B. L., 516, 610, 648
* (98) Welch, W., 240
* (99) Wellek, S., 772
* (100) Wellner, J., 657, 760, 772, 775, 779, 781, 808, 819, 828, 881, 913
* (101) Wells, M., 124, 829
* (102) Welsh, A. H., 828
* (103) Westfall, P. H., 124, 333, 341, 460, 470, 484, 491, 918* [19] Westlake, W., 748
* [20] Wijsman, R., 248, 251, 378, 473, 491
* [21] Wilhelm, D., 918
* [22] Wilk, M. B., 333, 828
* [23] Wilks, S. S., 707
* [24] Williams, D., 60
* [25] Wilson, E. B., 123, 540
* [26] Winters, F., 648
* [27] Witting, H., 98
* [28] Wolf, M., 392, 404, 431, 491, 635, 705, 772, 880, 899, 903, 906, 917, 918
* [29] Wolfe, D. A., 286, 325, 722
* [30] Wolfowitz, J., 87, 109
* [31] Wolkonoski, V., 604
* [32] Wolpert, R., 124, 501, 516, 518, 707
* [33] Working, H., 123, 491
* [34] Wright, A. L., 282
* [35] Wright, F., 326
* [36] Wu, C. F., 333, 905, 918
* [37] Wu, L., 179, 266, 326, 389
* [38] Wu, Y., 11
* [39] Wynn, H. P., 473, 482
* [40] **X**
* [41] Xue, K., 918

**Y**
* [42] Yamada, S., 22, 49
* [43] Yanagimoto, T., 164
* [44] Yandell, B. S., 828
* [45] Yang, G., 657, 706, 716, 772
* [46] Yang, Z., 680
* [47] Yao, F., 918
* [48] Yeh, H. C., 610
* [49] Yekutieli, D., 435, 437, 438, 491
* [50] Young, S., 124, 333, 470, 484, 491, 918
* [51] Yuen, K. K., 610

**Z**
* [52] Zabell, S., 200
* [53] Zemroch, P. J., 181
* [54] Zhang, C., 707
* [55] Zhang, J., 648, 707, 829
* [56] Zhou, S., 707
* [57] Zinn, J., 877
* [58] Zucchini, W., 282