\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }

\title{3. 微调 }

\author{}
\date{}


%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}
\maketitle
假设第 $i$ 个位置是掩码位置, $\boldsymbol{h}_{i}^{(L)}$ 是在第 $L$ 层第 $i$ 个位置的表示, $x_{i}$ 是预测的单词, $\boldsymbol{W}_{x}$ 是单词的权重矩阵。

下句预测计算条件概率:


\begin{equation*}
P_{\boldsymbol{\theta}}\left(s \mid \tilde{x}_{0}, \tilde{x}_{1}, \cdots, \tilde{x}_{m+n}\right)=\sigma\left(\boldsymbol{w}_{s}^{\mathrm{T}} \cdot \boldsymbol{h}_{\mathrm{cls}}^{(L)}\right)=\frac{\exp \left(\boldsymbol{w}_{s}^{\mathrm{T}} \cdot \boldsymbol{h}_{\mathrm{cls}}^{(L)}\right)}{1+\exp \left(\boldsymbol{w}_{s}^{\mathrm{T}} \cdot \boldsymbol{h}_{\mathrm{cls}}^{(L)}\right)} \tag{27.26}
\end{equation*}


其中, $\boldsymbol{h}_{\mathrm{cls}}^{(L)}$ 是在第 $L$ 层的类别特殊字符 $<\mathrm{cls}>$ 的表示向量; $\boldsymbol{w}_{s}$ 是下句预测的权重向量; $s$取值为 1 或 0 , 表示两个单词序列是否来自同一篇文章。

预训练的损失函数为


\begin{equation*}
L_{\mathrm{PT}}=L_{\mathrm{MLM}}+\lambda \cdot L_{\mathrm{NSP}} \tag{27.27}
\end{equation*}


其中, $L_{\mathrm{MLM}}$ 是掩码语言模型化损失, $L_{\mathrm{NSP}}$ 是下句预测损失, $\lambda$ 是系数。


\begin{equation*}
L_{\mathrm{MLM}}=-\sum_{i=0}^{m+n} \delta_{i} \log P_{\theta}\left(x_{i} \mid \tilde{x}_{0}, \tilde{x}_{1}, \cdots, \tilde{x}_{m+n}\right) \tag{27.28}
\end{equation*}


其中, $\delta_{i}$ 取值为 1 或 0 , 表示第 $i$ 个位置是否被掩码; $\boldsymbol{\theta}$ 是模型的参数。


\begin{equation*}
L_{\mathrm{NSP}}=-\log P_{\boldsymbol{\theta}}\left(s \mid \tilde{x}_{0}, \tilde{x}_{1}, \cdots, \tilde{x}_{m+n}\right) \tag{27.29}
\end{equation*}


预训练得到的模型参数 $\theta$ 作为下游任务模型的初始值。

掩码语言模型化是预训练的主要部分, 下句预测的目标是让 BERT 既能用于以一个单词序列为输入的任务, 如文本分类, 也能用于以两个单词序列为输入的任务, 如文本匹配。后续的研究发现, 下句预测未必一定需要。当数据量足够大时, 可以只通过掩码语言模型化进行预训练。也就是说,

$$
L_{\mathrm{PT}}=L_{\mathrm{MLM}}
$$

改进版 RoBERTa 模型就采用这个方法。

微调时, 进一步调节参数, 使模型对下游任务有准确的预测。假设下游任务是文本分类,输入单词序列是 $\boldsymbol{x}^{\prime}=x_{0}, x_{1}, \cdots, x_{l}$, 输出是类别 $y$, 计算条件概率 $P\left(y \mid x_{0}, x_{1}, \cdots, x_{l}\right)$ :


\begin{equation*}
P_{\theta, \boldsymbol{\phi}}\left(y \mid x_{0}, x_{1}, \cdots, x_{l}\right)=\operatorname{softmax}\left(\boldsymbol{W}_{y}^{\mathrm{T}} \boldsymbol{h}_{\mathrm{cls}}^{(L)}\right)=\frac{\exp \boldsymbol{w}_{y}^{\mathrm{T}} \cdot \boldsymbol{h}_{\mathrm{cls}}^{(L)}}{\sum_{y^{\prime}} \exp \boldsymbol{w}_{y^{\prime}}^{\mathrm{T}} \cdot \boldsymbol{h}_{\mathrm{cls}}^{(L)}} \tag{27.30}
\end{equation*}


其中, $\boldsymbol{h}_{\mathrm{cls}}^{(L)}$ 是第 $L$ 层的类别特殊字符 $<\mathrm{cls}>$ 的表示向量, $\boldsymbol{W}_{y}$ 是类别的权重矩阵, $\phi$ 表示分类的参数。这时单词序列 $x_{0}, x_{1}, \cdots, x_{l}$ 是一个句子或一段文章, 以特殊字符 $\langle\mathrm{cls}>$ 开始, 以特殊字符 $<$ sep $>$ 结束。

微调的损失函数为


\begin{equation*}
L_{\mathrm{FT}}=-\log P_{\boldsymbol{\theta}, \boldsymbol{\phi}}\left(y \mid x^{\prime}\right) \tag{27.31}
\end{equation*}


微调中, 预训练模型的参数 $\theta$ 作为初始值, 在这个过程中进一步得到学习, 以帮助更好地分类; 同时分类的参数 $\phi$ 也得到学习。\\
如果下游任务是文本匹配, 如判断两句话是否形成一问一答。输入单词序列是 $x_{0}, x_{1}, \cdots, x_{l}$, 输出是类别 $y$, 仍然计算条件概率 $P\left(y \mid x_{0}, x_{1}, \cdots, x_{l}\right)$ 。类别有两类, 表示匹配或不匹配。这时单词序列 $x_{0}, x_{1}, \cdots, x_{l}$ 是两个单词序列合并的序列, 如一个问句和一个答句合并而成。以特殊字符 $<\mathrm{cls}>$ 开始, 中间以特殊字符 $<\mathrm{sep}>$ 间隔, 最后以特殊字符 $<$ sep > 结束。

\subsection*{27.2.3 模型特点}
BERT 通过其多层多头注意力机制能够有效地表示语言的词汇、语法、语义信息 (Transformer 和 GPT 也有类似的特点)。通过自注意力, 每一层的每一个位置的单词表示与其他位置的单词表示组合成新的表示, 传递到上一层的同一位置。自注意力是多头的, 一个头代表一个侧面, 因此每一个位置的单词表示由多个不同侧面的表示组合而成。单词表示的内容可以通过自注意力的权重推测。图 27.7 和图 27.8 给出显示 BERT 的权重分布的例子。权重的大小代表了单词表示的组合过程中各个单词表示的作用的大小。\\
\includegraphics[max width=\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-02}

图 27.7 BERT 模型的注意力权重分布的例子存在于不同层不同头

注意力权重的分布有几种类型。如图 27.7 所示, 注意力可能是发散的, 可能集中到前一个位置的单词或者后一个位置的单词, 可能集中到特殊字符 , 也可能集中到标点符号。这里说的注意力集中是指自注意力计算中只有一个位置的权重很大而其他位置的权重很小的情况。研究发现, 有些注意力是冗余的, 屏蔽掉它们 (权重置为 0 ), 模型预测的结果并没有大的改变, 但模型整体的多层多头自注意力机制对语言刻画是有必要的。

BERT 的各层有不同的特点。底层主要表示词汇信息, 中层主要表示语法信息, 上层主要表示语义信息。从图 27.8 中的例子可以看出, 对给定的自然语言输入, 不同层不同头可以表示其中的动词-宾语关系、冠词-名词关系、介词-名词关系、代词指代关系等。

对 BERT 和 GPT 的直观解释是: 机器基于大量的语料, 做了大量的词语填空 (BERT)或词语接龙 (GPT) 练习, 捕捉到了由单词组成句子、再由句子组成文章的各种规律, 并且把它们表示并记忆在模型之中 (注意: 文章不是由单词和句子随机组成的, 而是遵循词汇、语法、语义规则组合而成)。也就是说, BERT 通过无监督学习获取了大量的词汇、语法、语

Head 8-10

\begin{itemize}
  \item Direct objects attend to their verbs
  \item $86.8 \%$ accuracy at the dobj relation
\end{itemize}

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-03(3)}
\end{center}

\section*{Head 8-11}
\begin{itemize}
  \item Noun modifiers (e.g., determiners) attend to their noun
  \item $94.3 \%$ accuracy at the det relation\\
\includegraphics[max width=\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-03}
\end{itemize}

\section*{Head 5-4}
\begin{itemize}
  \item Coreferent mentions attend to their antecedents
  \item $65.1 \%$ accuracy at linking the head of a coreferent mention to the head of an antecedent
  \item $76.3 \%$ accuracy at the pobj relation\\
\includegraphics[max width=\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-03(2)}
\end{itemize}

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-03(1)}
\end{center}

图 27.8 BERT 模型中的注意力权重分布的例子 (见文前彩图)

可以表示词汇、语法、语义关系

义知识。当用于一个下游任务时, 只需要很少的标注数据就可以学习到完成该任务所需的知识。

\section*{本章概要}
\begin{enumerate}
  \item 预训练语言模型是基于具有强大表示能力的神经网络的语言模型。首先在预训练中,使用大规模的语料通过无监督学习的方式学习模型的参数。之后在微调中, 将模型用于一个具体任务, 使用少量的标注数据通过监督学习的方式进一步调节模型的参数。预训练语言模型通常可以有效地表示语言的词汇、句法和语义特征，用于下游任务。

  \item 有代表性的预训练语言模型有 GPT 和 BERT, 分别由 Transformer 的解码器和编码器实现。GPT 是单向语言模型, 适用于语言生成, 也可以用于语言理解。BERT 是双向语言模型，只能用于语言理解。

\end{enumerate}

GPT 的单向语言模型由以下单词的生成条件概率组成:

$$
P\left(x_{i} \mid x_{1}, \cdots, x_{i-1}\right), \quad i=1,2, \cdots, n
$$

每一个位置的单词依赖于之前位置的单词。GPT 的预训练通过语言模型化进行, 基于序列概率估计原理。

BERT 的双向语言模型由以下单词生成的条件概率组成:

$$
P\left(x_{i} \mid x_{1}, \cdots, x_{i-1}, x_{i+1}, \cdots, x_{n}\right), \quad i=1,2, \cdots, n
$$

每一个位置的单词依赖于之前位置和之后位置的单词。BERT 的预训练主要通过掩码语言模型化进行, 基于去噪自动编码器原理。

\begin{enumerate}
  \setcounter{enumi}{2}
  \item GPT 模型的输入是单词序列, 可以是一个句子或一段文章。首先经过输入层, 产生初始的单词表示向量的序列。之后经过 $L$ 个 Transformer 解码层, 得到单词表示向量的序列, GPT 模型的输出是在单词序列各个位置上的条件概率。
\end{enumerate}

GPT 预训练时, 通过极大似然估计学习模型的参数。

$$
L_{\text {train }}=-\sum_{i=1}^{n} \log P_{\boldsymbol{\theta}}\left(x_{i} \mid x_{1}, x_{2}, \cdots, x_{i-1}\right)
$$

GPT 微调时, 通过优化下游任务的目标函数, 进一步调节模型的参数。

\begin{enumerate}
  \setcounter{enumi}{3}
  \item 自动编码器是用于数据表示的无监督学习的一种神经网络。自动编码器由编码器网络和解码器网络组成。学习时编码器将输入向量转换为中间表示向量, 解码器再将中间表示向量转换为输出向量。编码器和解码器可以是
\end{enumerate}

$$
\begin{aligned}
& \boldsymbol{z}=F(\boldsymbol{x})=a\left(\boldsymbol{W}_{\mathrm{E}} \boldsymbol{x}+\boldsymbol{b}_{\mathrm{E}}\right) \\
& \boldsymbol{y}=G(\boldsymbol{z})=a\left(\boldsymbol{W}_{\mathrm{D}} \boldsymbol{z}+\boldsymbol{b}_{\mathrm{D}}\right)
\end{aligned}
$$

学习的目标是尽量使输出向量和输入向量保持一致, 或者说重建输入向量。认为学到的中间表示向量就是数据的表示。

$$
L=\frac{1}{N} \sum_{i=1}^{N} L\left(\boldsymbol{x}_{i}, G\left(F\left(\boldsymbol{x}_{i}\right)\right)\right)
$$

学习的算法一般是梯度下降。自动编码器学习实际进行的是对数据的压缩。

\begin{enumerate}
  \setcounter{enumi}{4}
  \item 去噪自动编码器是自动编码器的一种扩展, 去噪自动编码器不仅可以用于数据表示学习, 而且可以用于数据去噪。学习时首先根据对输入向量进行的随机变换, 得到有噪声的输入向量。编码器将有噪声的输入向量转换为中间表示向量, 解码器再将中间表示向量转换为输出向量。编码器、解码器、目标函数分别是
\end{enumerate}

$$
\begin{aligned}
\boldsymbol{z} & =F(\boldsymbol{x})=a\left(\boldsymbol{W}_{\mathrm{E}} \tilde{\boldsymbol{x}}+\boldsymbol{b}_{\mathrm{E}}\right) \\
\boldsymbol{y} & =G(\boldsymbol{z})=a\left(\boldsymbol{W}_{\mathrm{D}} \boldsymbol{z}+\boldsymbol{b}_{\mathrm{D}}\right) \\
L & =\frac{1}{N} \sum_{i=1}^{N} L\left(\boldsymbol{x}_{i}, G\left(F\left(\tilde{\boldsymbol{x}}_{i}\right)\right)\right)
\end{aligned}
$$

学习的目标是尽量使输出向量和原始输入向量保持一致, 或者说重建原始输入向量。因为学习的目标是排除噪声的干扰重建数据, 去噪自动编码器能更有效地学到数据的主要特征。

\begin{enumerate}
  \setcounter{enumi}{5}
  \item BERT 模型的输入是两个合并的单词序列。首先经过输入层, 产生初始的单词表示向量的序列。之后经过 $L$ 个 Transformer 编码层, 得到单词的表示向量的序列。BERT 模型的输出是在单词序列的各个位置上的条件概率。
\end{enumerate}

BERT 模型的预训练由掩码语言模型化和下句预测组成。掩码语言模型化的目标是复原输入单词序列中被掩码的单词。下句预测的目标是判断输入单词序列是否来自同一篇文章。预训练以掩码语言模型化为主, 其损失函数是

$$
L_{1}=-\sum_{i=0}^{m+n} \delta_{i} \log P_{\boldsymbol{\theta}}\left(x_{i} \mid \tilde{x}_{0}, \tilde{x}_{1}, \cdots \tilde{x}_{m+n}\right)
$$

BERT 微调时, 通过优化下游任务的目标函数, 进一步调节模型的参数。

\section*{继续阅 读}
BERT 的原始论文是文献 [1], GPT, GPT-2, GPT-3 的原始论文是文献 [2] 文献 [4]。 BERT 的改进工作有 RoBERTa ${ }^{[5]}$, XLNet ${ }^{[6]}$ 等。本章介绍的 BERT 的分析结果见文献 [7]。 DAE 的原始论文可见文献 [8]。BERT 和 GPT 之前的预训练语言模型有 ELMo ${ }^{[9]}$ 。

\section*{习题}
27.1 设计基于双向 LSTM 的预训练语言模型, 假设下游任务是文本分类。

27.2 假设 GPT 微调的下游任务是两个文本的匹配, 写出学习的目标函数。

27.3 设计一个 2 层卷积神经网络编码器和 2 层卷积神经网络解码器组成的自动编码器 (使用第 28 章介绍的转置卷积)。

27.4 证明当编码器和解码器都是线性函数时, 主成分分析可以作为自动编码器学习的方法。

27.5 解释为什么 BERT 预训练中的掩码语言模型化是基于去噪自动编码器原理的。

27.6 比较 BERT 与 Transformer 编码器在模型上的异同。

\section*{参考文献}
[1] DEVLIN J, CHANG M W, LEE K, et al. BERT: pre-training of deep bidirectional transformers for language understanding $[\mathrm{C}] / /$ Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2019: 4171-4186.

[2] RADFORD A, NARASIMHAN K, SALIMANS T, et al. Improving language understanding by generative pre-training $[\mathrm{J}] .2018$.

[3] RADFORD A, WU J, CHILD R, et al. Language models are unsupervised multitask learners[J]. OpenAI Blog, 2019, 1(8).

[4] BROWN T B, MANN B, RYDER N, et al. Language models are few-shot learners[Z/OL]. arXiv preprint arXiv:2005.14165, 2020.

[5] LIU Y, OTT M, GOYAL N, et al. Roberta: A robustly optimized bert pretraining approach[Z/OL]. arXiv preprint arXiv:1907.11692, 2019.

[6] YANG Z, DAI Z, YANG Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[J]. Advances in Neural Information Processing Systems, 2019: 5754-5764.

[7] CLARK K, KHANDELWAL U, LEVY O, et al. What does BERT look at? An analysis of BERT's attention[C]//Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 2019: 276-286.

[8] VINCENT P, LAROCHELLE H, BENGIO Y, et al. Extracting and composing robust features with denoising autoencoders[C]//Proceedings of the 25th International Conference on Machine Learning. 2018: 1096-1103.

[9] PETERS ME, NEUMANN M, IYYER M, et al. Deep contextualized word representations[C]// Proceedings of NAACL-HLT. 2018: 2227-2237.

\section*{第 28 章 生成对抗网络}
生成对抗网络 (generative adversarial networks, GAN) 是一种基于博弈的生成模型, 在图像生成等领域被广泛使用。GAN 于 2014 年由 Goodfellow 等提出, 之后有诸多的模型被开发, 包括 DCGAN 和 W-GAN。其中 DCGAN 是 Radford 等于 2015 年开发的用于图像生成的模型。

GAN 由生成网络和判别网络组成, 生成网络自动生成数据, 判别网络判断数据是已给的 (真的) 还是生成的 (假的)。学习的目标是构建生成网络, 能自动生成同已给训练数据同分布的数据。学习的过程就是博亦的过程, 生成网络和判别网络不断通过优化自己网络的参数进行博亦。当达到均衡状态时, 学习结束, 生成网络可以生成以假乱真的数据, 判别网络难以判断数据的真假。GAN 在没有使用标注数据的意义下属于无监督学习方法。

本章 28.1 节讲述 GAN 基本模型, 28.2 节介绍用于图像生成的 DCGAN 模型。

\subsection*{28.1 GAN 基本模型}
本节首先介绍 GAN 基本模型的定义, 然后给出其学习算法, 最后给出相关理论分析结果。

\subsection*{28.1.1 模型}
目标是从已给训练数据中学习生成数据的模型, 用模型自动生成新的数据, 包括图像、语音数据。一个直接的方法是假设已给数据是由一个概率分布产生的数据, 通过极大似然估计学习这个概率分布, 即概率密度函数。当数据分布非常复杂时, 很难给出适当的概率密度函数的定义, 以及有效地学习概率密度函数。生成对抗网络 GAN 不直接定义和学习数据生成的概率分布, 而是通过导入评价生成数据 “真假” 的机制来解决这个问题。

GAN 由一个生成网络 (generator) 和一个判别网络 (discriminator) 组成, 相互进行博峦 (对抗), 生成网络生成数据 (假数据), 判别网络判别数据是已给数据 (真数据) 还是生成数据 (假数据)。学习的过程就是博恋的过程。生成网络和判别网络不断提高自己的能力, 当最终达到纳什均衡 (Nash equilibrium) 时, 生成网络可以以假乱真地生成数据, 判别网络不能判断数据的真假。

这里假设生成网络和判别网络是深度神经网络, 都有足够强的学习能力。训练数据并没\\
有直接用于生成网络的学习, 而是用于判别网络的学习。判别网络能力提高之后用于生成网络能力的提高, 生成网络能力提高之后再用于判别网络能力的提高, 不断循环。

图 28.1 显示 GAN 的框架。假设已给训练数据 $\mathcal{D}$ 遵循分布 $P_{\mathrm{data}}(\boldsymbol{x})$, 其中 $\boldsymbol{x}$ 是样本。生成网络用 $\boldsymbol{x}=G(\boldsymbol{z} ; \boldsymbol{\theta})$ 表示, 其中 $\boldsymbol{z}$ 是输入向量 (种子), $\boldsymbol{x}$ 是输出向量 (生成数据), $\boldsymbol{\theta}$ 是网络参数。判别网络是一个二类分类器, 用 $P(1 \mid \boldsymbol{x})=D(\boldsymbol{x} ; \boldsymbol{\varphi})$ 表示, 其中 $\boldsymbol{x}$ 是输入向量, $P(1 \mid \boldsymbol{x})$和 $1-P(1 \mid \boldsymbol{x})$ 是输出概率, 分别表示输入 $\boldsymbol{x}$ 来自训练数据和生成数据的概率, $\boldsymbol{\varphi}$ 是网络参数。种子 $\boldsymbol{z}$ 遵循分布 $P_{\text {seed }}(\boldsymbol{z})$, 如标准正态分布或均匀分布。生成网络生成的数据分布表示为 $P_{\text {gen }}(\boldsymbol{x})$, 由 $P_{\text {seed }}(\boldsymbol{z})$ 和 $\boldsymbol{x}=G(\boldsymbol{z} ; \boldsymbol{\theta})$ 决定。

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-08}
\end{center}

图 28.1 GAN 的框架

如果生成网络参数 $\theta$ 固定, 可以通过最大化以下目标函数学习判别网络参数 $\varphi$, 使其具备判别真假数据的能力。


\begin{equation*}
\max _{\boldsymbol{\varphi}}\left\{E_{\boldsymbol{x} \sim P_{\text {data }(\boldsymbol{x})}}[\log D(\boldsymbol{x} ; \boldsymbol{\varphi})]+E_{\boldsymbol{z} \sim P_{\mathrm{seed}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \overline{\boldsymbol{\varphi}})]\}\right. \tag{28.1}
\end{equation*}


如果判别网络参数 $\varphi$ 固定, 那么可以通过最小化以下目标函数学习生成网络参数 $\boldsymbol{\theta}$, 使其具备以假乱真地生成数据的能力。


\begin{equation*}
\min _{\boldsymbol{\theta}}\left\{E_{\boldsymbol{z} \sim P_{\text {seed }}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \overline{\boldsymbol{\varphi}})]\}\right. \tag{28.2}
\end{equation*}


判别网络和生成网络形成博弈关系, 可以定义以下的极小极大问题, 也就是 GAN 的学习目标函数。


\begin{equation*}
\min _{\boldsymbol{\theta}} \max _{\boldsymbol{\varphi}}\left\{E_{\boldsymbol{x} \sim P_{\text {data }(\boldsymbol{x})}}[\log D(\boldsymbol{x} ; \boldsymbol{\varphi})]+E_{\boldsymbol{z} \sim P_{\text {seed }}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \boldsymbol{\varphi})]\}\right. \tag{28.3}
\end{equation*}


后述定理证明这个极小极大问题的解 $\varphi^{*}$ 和 $\boldsymbol{\theta}^{*}$ 存在, 也就是纳什均衡存在。GAN 的学习算法就是求极小极大问题的最优解的方法。

可以对 GAN 做这样一个比喻。生成网络是仿造者, 判别网络是鉴别者。仿造者制作噟品; 鉴别者既得到真品又得到赝品, 判断作品的真伪。仿造者与鉴别者之间展开博亪, 各自不断提高自己的能力, 最终仿造者制作出的噟品真假难辨, 鉴别者无法判断作品的真伪。注意在这个过程中鉴别者间接地把自己的判别方法告诉了仿造者, 所以两者之间既有对抗关系,又有 “合作” 关系。

\subsection*{28.1.2 学习算法}
对 GAN 的目标函数 (式 (28.3)) 进行优化, 迭代地学习判别网络和生成网络的参数, 就是 GAN 的学习算法。

\section*{算法 28.1 (GAN 学习算法)}
输入: 训练数据集 $\mathcal{D}$ 。

输出: 生成网络 $G(\boldsymbol{z} ; \boldsymbol{\theta})$ 。

超参数: 训练数据集, 对抗训练次数 $T$, 判别网络训练次数 $S$, 小批量样本数量 $M$, 学习率 $\eta$ 。

\begin{enumerate}
  \item 随机初始化参数 $\boldsymbol{\theta}, \boldsymbol{\varphi}$

  \item for $(t=1,2, \cdots, T)\{$

\end{enumerate}

\# 训练判别网络 $D(\boldsymbol{x} ; \boldsymbol{\varphi})$

for $(s=1,2, \cdots, S)\{$

从训练数据中随机采样 $M$ 个样本 $\left\{\boldsymbol{x}^{(m)}\right\}, 1 \leqslant m \leqslant M$

根据分布 $P_{\text {seed }}(\boldsymbol{z})$ 随机采样 $M$ 个样本 $\left\{\boldsymbol{z}^{(m)}\right\}, 1 \leqslant m \leqslant M$

计算以下梯度, 使用梯度上升法更新参数 $\varphi$

$$
\begin{gathered}
\nabla_{\boldsymbol{\varphi}}\left[\frac{1}{M} \sum_{m=1}^{M} \log D\left(\boldsymbol{x}^{(m)} ; \boldsymbol{\varphi}\right)+\log \left(1-D\left(G\left(\boldsymbol{z}^{(m)} ; \boldsymbol{\theta}\right) ; \boldsymbol{\varphi}\right)\right)\right] \\
\boldsymbol{\varphi}+\eta \nabla_{\boldsymbol{\varphi}}
\end{gathered}
$$

\}

\# 训练生成网络 $G(\boldsymbol{z} ; \boldsymbol{\theta})$

根据分布 $P_{\text {seed }}(\boldsymbol{z})$ 随机采样 $M$ 个样本 $\left\{\boldsymbol{z}^{(m)}\right\}, 1 \leqslant m \leqslant M$

计算以下梯度, 使用梯度上升法更新参数 $\theta$

$$
\begin{gathered}
\nabla_{\boldsymbol{\theta}}\left[\frac{1}{M} \sum_{m=1}^{M} \log \left(D\left(G\left(\boldsymbol{z}^{(m)} ; \boldsymbol{\theta}\right) ; \boldsymbol{\varphi}\right)\right)\right] \\
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\eta \nabla_{\boldsymbol{\theta}}
\end{gathered}
$$

\}

\begin{enumerate}
  \setcounter{enumi}{2}
  \item 输出生成网络 $G(\boldsymbol{z} ; \boldsymbol{\theta})$ 。
\end{enumerate}

这里不进行 $\log (1-D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \boldsymbol{\varphi}))$ 的最小化, 而是进行 $\log (D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \boldsymbol{\varphi})$ 的最大化。这是因为在学习的初始阶段, 生成网络较弱, 判别网络很容易区分训练数据和生成数据, 最小化 $\log (1-D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \boldsymbol{\phi}))$ 会使学习很难进行下去。因此, 判别网络和生成网络的学习都使用梯度上升法。

判别网络训练时从训练数据和生成数据中同采样 $M$ 个样本, 也就是各以 0.5 的概率选取训练数据和生成数据。判别网络学习迭代 $S$ 次后, 生成网络学习迭代 1 次。这样可以保证训练判别网络有足够能力时再训练生成网络。 $M$ 和 $S$ 是超参数, 要在具体应用中调节。

\subsection*{28.1.3 理论分析}
不考虑网络参数, 将 GAN 学习的极小极大问题写成


\begin{equation*}
\min _{G} \max _{D} L(G, D)=\min _{G} \max _{D}\left\{E_{\boldsymbol{x} \sim P_{\text {data }(\boldsymbol{x})}}[\log D(\boldsymbol{x})]+E_{\boldsymbol{z} \sim P_{\text {seed }}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z}))]\}\right. \tag{28.4}
\end{equation*}


定理 28.1 当生成网络固定为 $\bar{G}$ 时, 问题 (28.4) 变成以下最大化问题:

$$
\max _{D} L(\bar{G}, D)=\max _{D}\left\{E_{\boldsymbol{x} \sim P_{\text {data }(\boldsymbol{x})}}[\log D(\boldsymbol{x})]+E_{\boldsymbol{z} \sim P_{\text {seed }}(\boldsymbol{z})}[\log (1-D(\bar{G}(\boldsymbol{z}))]\}\right.
$$

该最大化问题的解一判别网络 $D_{G}^{*}$ 满足以下关系:


\begin{equation*}
D_{G}^{*}(\boldsymbol{x})=\frac{P_{\text {data }}(\boldsymbol{x})}{P_{\text {data }}(\boldsymbol{x})+P_{\text {gen }}(\boldsymbol{x})} \tag{28.5}
\end{equation*}


证明


\begin{align*}
L(\bar{G}, D) & =\int_{\boldsymbol{x}} P_{\text {data }}(\boldsymbol{x}) \log D(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}+\int_{\boldsymbol{z}} P_{\text {seed }}(\boldsymbol{z}) \log (1-D(\bar{G}(\boldsymbol{z}))) \mathrm{d} \boldsymbol{z} \\
& =\int_{\boldsymbol{x}} P_{\text {data }}(\boldsymbol{x}) \log D(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}+\int_{\boldsymbol{x}} P_{\text {gen }}(\boldsymbol{x}) \log (1-D(\boldsymbol{x})) \mathrm{d} \boldsymbol{x} \tag{28.6}
\end{align*}


式 (28.6) 达到最大值的判别网络表示为 $D_{G}^{*}$, 则有式 (28.5) 成立。这是因为, 针对任意的 $(a, b) \in \mathcal{R}^{2} \backslash(0,0)$, 函数 $f(x)=a \log x+b \log (1-x), x \in(0,1)$, 当 $x=\frac{a}{a+b}$ 时取最大值。函数 $D(\boldsymbol{x})$ 在 $\operatorname{supp}\left(P_{\text {data }}(\boldsymbol{x})\right) \cup \operatorname{supp}\left(P_{\text {gen }}(\boldsymbol{x})\right)$ 之外无须定义。

定理 28.2 当判别网络固定为 $D_{G}^{*}$ 时, 问题 (28.4) 变成以下最小化问题:

$$
\min _{G} L\left(G, D_{G}^{*}\right)=\min _{G}\left\{E_{\boldsymbol{x} \sim P_{\text {data }(\boldsymbol{x})}}\left[\log D_{G}^{*}(\boldsymbol{x})\right]+E_{\boldsymbol{z} \sim P_{\text {seed }}(\boldsymbol{z})}\left[\log \left(1-D_{G}^{*}(G(\boldsymbol{z}))\right]\right\}\right.
$$

该最小化问题的解——生成网络 $G^{*}$ 满足以下关系:


\begin{equation*}
P_{\text {gen }}^{*}(\boldsymbol{x})=P_{\text {data }}(\boldsymbol{x}) \tag{28.7}
\end{equation*}


最小值是 $-2 \log 2$ 。

\section*{证明}

\begin{align*}
L\left(G, D_{G}^{*}\right) & =\int_{\boldsymbol{x}} P_{\text {data }}(\boldsymbol{x}) \log D_{G}^{*}(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}+\int_{\boldsymbol{z}} P_{\text {seed }}(\boldsymbol{z}) \log \left(1-D_{G}^{*}(G(\boldsymbol{z}))\right) \mathrm{d} \boldsymbol{z} \\
& =\int_{\boldsymbol{x}} P_{\text {data }}(\boldsymbol{x}) \log D_{G}^{*}(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}+\int_{\boldsymbol{x}} P_{\text {gen }}(\boldsymbol{x}) \log \left(1-D_{G}^{*}(\boldsymbol{x})\right) \mathrm{d} \boldsymbol{x} \\
& =\int_{\boldsymbol{x}} P_{\text {data }}(\boldsymbol{x}) \log \frac{P_{\text {data }}(\boldsymbol{x})}{P_{\text {data }}(\boldsymbol{x})+P_{\text {gen }}(\boldsymbol{x})} \mathrm{d} \boldsymbol{x}+\int_{\boldsymbol{x}} P_{\text {gen }}(\boldsymbol{x}) \log \left(\frac{P_{\text {gen }}(\boldsymbol{x})}{P_{\text {data }}(\boldsymbol{x})+P_{\text {gen }}(\boldsymbol{x})}\right) \mathrm{d} \boldsymbol{x} \\
& =\mathrm{KL}\left(P_{\text {data }}(\boldsymbol{x}) \| \frac{P_{\text {data }}(\boldsymbol{x})+P_{\text {gen }}(\boldsymbol{x})}{2}\right)+\mathrm{KL}\left(P_{\text {gen }}(\boldsymbol{x}) \| \frac{P_{\text {data }}(\boldsymbol{x})+P_{\text {gen }}(\boldsymbol{x})}{2}\right)-2 \log 2 \\
& =\log \mathrm{JS}\left(P_{\text {data }}(\boldsymbol{x}) \| P_{\text {gen }}(\boldsymbol{x})\right)-2 \log 2 \tag{28.8}
\end{align*}


$\mathrm{JS}(P \| Q)$ 是两个概率分布 $P$ 和 $Q$ 之间的 Jessen-Shannon 散度, 当且仅当两个概率分布相同时, 取最小值 0 。所以, 式 (28.8) 当且仅当 $P_{\mathrm{gen}}(\boldsymbol{x})=P_{\mathrm{data}}(\boldsymbol{x})$ 时达到最小值, 且最小值为 $-2 \log 2$ 。达到最小值的生成分布表示为 $P_{\mathrm{gen}}^{*}(\boldsymbol{x})$, 即有式 (28.7) 成立。

理论上的最优解（即纳什均衡状态）满足：


\begin{gather*}
P_{\text {gen }}^{*}(\boldsymbol{x})=P_{\text {data }}(\boldsymbol{x})  \tag{28.9}\\
D^{*}(\boldsymbol{x})=\frac{1}{2} \tag{28.10}
\end{gather*}


也就是生成网络可以以与训练数据相同的分布生成数据, 判别网络无法辨别数据是来自训练数据还是生成的数据。以上定理只是表示理论上最优解存在。实际上, 生成网络和判别网络需要用参数 $\boldsymbol{\theta}$ 和 $\boldsymbol{\varphi}$ 表示, 算法 28.1 不能保证求得最优解。

图 28.2 示意 GAN 的学习过程。图中下面横线表示生成网络输入 $\boldsymbol{z}$ 的分布, 这里假设是均匀分布。中间横线表示生成网络输出 $\boldsymbol{x}$ 的分布。两条横线之间的有向实线表示生成网络的映射 $\boldsymbol{x}=G(\boldsymbol{z} ; \boldsymbol{\theta})$ 。上面黑色点线表示真实数据分布 $P_{\mathrm{data}}(\boldsymbol{x})$, 绿色实线表示生成数据分布 $P_{\mathrm{gen}}(\boldsymbol{x})$, 蓝色点线表示判别网络判别分布 $D(\boldsymbol{x})$ 。训练初始, 生成数据分布和真实数据分布相差较远, 判别网络的判别概率也不准确 (图 28.2(a))。生成网络固定判别网络训练后, 其判别概率趋于 $D_{G}^{*}(\boldsymbol{x})=\frac{P_{\mathrm{data}}(\boldsymbol{x})}{P_{\mathrm{data}}(\boldsymbol{x})+P_{\mathrm{gen}}(\boldsymbol{x})}($ 图 28.2(b))。判别网络固定生成网络训练后, 其生成数据分布和真实数据分布趋于接近 (图 28.2(c))。训练收玫后, 生成网络达到最优 $P_{\text {gen }}^{*}(\boldsymbol{x})=P_{\text {data }}(\boldsymbol{x})$, 判别网络也达到最优 $D^{*}(\boldsymbol{x})=\frac{1}{2}($ 图 28.2(d))。

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-11}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-11(1)}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-11(3)}
\end{center}

(c)

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-11(2)}
\end{center}

(d)

图 28.2 GAN 的学习过程 (见文前彩图)

GAN 的模型训练并不容易, 需要一定的技巧。有很多改进的模型被提出, 包括 W-GAN (Wasserstein GAN)。

\section*{28.2 图像生成中的应用}
可以使用 GAN 技术从图像数据中学习生成网络, 用于图像数据的自动生成。比如, 训练数据是人脸图片, 可以学习 GAN, 自动生成 “人脸” 的图片。本节介绍常用的用于图像生成的 GAN 模型 DCGAN, 先讲解 DCGAN 使用的转置卷积。

\subsection*{28.2.1 转置卷积}
\section*{1. 转置卷积的定义}
转置卷积 (transposed convolution) 也称为微步卷积 (fractionally strided convolution)或反卷积 (deconvolution) ${ }^{1}$, 在图像生成网络、图像自动编码器等模型中广泛使用。卷积可以用于图像数据尺寸的缩小, 而转置卷积可以用于图像数据尺寸的放大, 又分别称为下采样和上采样 (参见第 24 章)。

卷积运算可以表示为线形变换。假设有核矩阵为以下矩阵 $\boldsymbol{W}$ 、填充为 0 、步幅为 1 的卷积运算。

\[
\boldsymbol{W}=\left[\begin{array}{lll}
w_{11} & w_{12} & w_{13}  \tag{28.11}\\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33}
\end{array}\right]
\]

图 28.3 显示以上卷积运算的过程, 蓝色格点表示输入矩阵, 绿色格点表示输出矩阵, 深色部分表示具体的卷积计算。输入矩阵的大小是 $4 \times 4$, 输出矩阵的大小是 $2 \times 2$, 这个卷积进行的是下采样。\\
\includegraphics[max width=\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-12}

图 28.3 卷积例 (见文前彩图)

构建矩阵 $\boldsymbol{C}$ :

$\left[\begin{array}{cccccccccccccccc}w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0 & 0 & 0 & 0 & 0 \\ 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0 \\ 0 & 0 & 0 & 0 & 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33}\end{array}\right]$

考虑基于矩阵 $\boldsymbol{C}$ 的线性变换, 其输入是输入矩阵展开的向量, 输出是输出矩阵展开的向量。这个线性变换对应神经网络前一层到后一层的信号传递 (正向传播), 而以上卷积运算表示在这个线性变换中。

另一方面, 考虑基于转置矩阵 $C^{\mathrm{T}}$ 的线性变换。这个线性变换对应神经网络后一层到前一层的信号传递 (反向传播)。事实上, 存在另一个卷积运算, 表示在基于转置矩阵 $\boldsymbol{C}^{\mathrm{T}}$ 的线性变换中, 其核矩阵为以下矩阵:
\footnotetext{(1) 反卷积是容易引起误解的名称, 因为它不是卷积的逆运算。
}

\[
\operatorname{rot} 180(\boldsymbol{W})=\left[\begin{array}{lll}
w_{33} & w_{32} & w_{31}  \tag{28.12}\\
w_{23} & w_{22} & w_{21} \\
w_{13} & w_{12} & w_{11}
\end{array}\right]
\]

称这个卷积为转置卷积。这个转置卷积是核矩阵为 $\operatorname{rot} 180(\boldsymbol{W})$ 、填充为 2 、步幅为 1 的卷积运算。这里 rot180 表示矩阵 180 度旋转, 卷积计算时对输入矩阵进行全填充。

图 28.4 显示以上转置卷积运算的过程, 蓝色格点表示输入矩阵, 绿色格点表示输出矩阵, 虚线部分表示填充, 深色部分表示具体的卷积计算。输入矩阵的大小是 $2 \times 2$, 输出矩阵的大小是 $4 \times 4$, 转置卷积进行的是上采样。\\
\includegraphics[max width=\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-13}

图 28.4 转置卷积例 (见文前彩图)

原始卷积和转置卷积是相互对应、互为反向的运算, 注意不是逆运算。这个关系的直观解释是在卷积神经网络的两层之间，正向和反向的传播（不考虑基于激活函数的非线性变换）都是卷积运算，相互对应，方向相反。

给定任意一个以 $\boldsymbol{W}$ 为核矩阵的卷积, 可以构建一个以 $\operatorname{rot} 180(\boldsymbol{W})$ 为核矩阵的转置卷积。卷积核和转置卷积核之间有 $\operatorname{rot} 180(\operatorname{rot} 180(\boldsymbol{W}))=\boldsymbol{W}$ 成立, 相应地, 矩阵和转置矩阵之间有 $\left(\boldsymbol{C}^{\mathrm{T}}\right)^{\mathrm{T}}=\boldsymbol{C}$ 成立。

\section*{2. 转置卷积的大小}
首先, 计算原始卷积的大小。这里考虑简单的情况。假设输入矩阵是方阵, 卷积核矩阵也是方阵。设 $I$ 是输入矩阵的尺寸, $K$ 是卷积核的尺寸, $P$ 是填充的尺寸, $S$ 是步幅。输出矩阵的尺寸 $O$ 满足


\begin{equation*}
O=\frac{I+2 P-K}{S}+1 \tag{28.13}
\end{equation*}


这里考虑可以整除的情况。式 (28.13) 可以改为对应的形式:

$$
I=\frac{[O+(O-1)(S-1)]+2(K-P-1)-K}{1}+1
$$

接着, 计算转置卷积的大小。设 $I^{\prime}$ 是输入矩阵的尺寸, $K^{\prime}$ 是卷积核的尺寸, $P^{\prime}$ 是填充的尺寸, $S^{\prime}$ 是步幅。输出矩阵的尺寸 $O^{\prime}$ 满足


\begin{equation*}
O^{\prime}=\frac{I^{\prime}+2 P^{\prime}-K^{\prime}}{S^{\prime}}+1 \tag{28.14}
\end{equation*}


这里也考虑可以整除的情况。转置卷积的输出矩阵尺寸 $O^{\prime}$ 与原始卷积的输入矩阵尺寸 $I$ 相\\
同。因此, 可以推算, 当 $S=1, P=0$ 时, 转置卷积的大小和原始卷积的大小之间有以下关系成立:

$$
\begin{gathered}
I^{\prime}=O, P^{\prime}=K-1, K^{\prime}=K, S^{\prime}=1 \\
O^{\prime}=O+K-1
\end{gathered}
$$

图 28.3 的卷积有 $I=4, K=3, S=1, P=0, O=2$ 。图 28.4 的转置卷积有 $I^{\prime}=2, K^{\prime}=$ $3, S^{\prime}=1, P^{\prime}=2, O^{\prime}=4$ 。

\section*{3. 转置卷积的上采样}
可以通过增大卷积的步幅 $S>1$ 实现下采样, 即将大尺寸的输入矩阵降低为小尺寸的输出矩阵。相反, 也可以通过减小转置卷积的步幅 $S^{\prime}<1$ 实现上采样, 即将小尺寸的输入矩阵提高为大尺寸的输出矩阵。采用 $S^{\prime}<1$ 的步幅, 实际是在输入矩阵的相邻两行之间插入适当数量的 0 行向量, 相邻的两列之间插入适当数量的 0 列向量。转置卷积中经常使用这样的处理, 这是被称为微步卷积的原因。

图 28.5 给出一个转置卷积的例子。原始卷积输入矩阵尺寸为 5 , 卷积核尺寸为 3 , 步幅为 2 , 填充尺寸为 0 , 输出矩阵尺寸为 4 , 即 $I=5, K=3, S=2, P=0, O=2$ 。转置卷积实际是在输入矩阵的相邻的两行之间插入一行 0 向量, 相邻的两列之间插入一列 0 向量。转置卷积实际的输入矩阵 (插入 0 向量后) 尺寸为 3 , 卷积核尺寸为 3 , 实际的步幅为 1 , 填充为 2 , 输出矩阵尺寸为 5 , 即 $\hat{I}^{\prime}=3, K^{\prime}=3, \hat{S}^{\prime}=1, P^{\prime}=2, O^{\prime}=5$ 。\\
\includegraphics[max width=\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-14}

图 28.5 转置卷积例 (见文前彩图)

当 $S=2, P=0$ 时，转置卷积的大小和原始卷积的大小之间有以下关系成立:

$$
\begin{gathered}
\hat{I}^{\prime}=O+(O-1), P^{\prime}=K-1, K^{\prime}=K, \hat{S}^{\prime}=1 \\
O^{\prime}=2(O-1)+K
\end{gathered}
$$

\subsection*{28.2.2 DCGAN}
深度卷积生成对抗网络 (deep convolutional generative adversarial networks, DCGAN)是 GAN 用于图像生成的代表性模型。DCGAN 和其他 GAN 模型一样由生成网络和判别网络组成。图 28.6 给出 DCGAN 的架构, 用特征图表示各层的卷积运算。DCGAN 的学习算法和 GAN 的算法完全一样 (算法 28.1), 但包含一些实现上的技巧。

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-15}
\end{center}

图 28.6 DCGAN 整体的架构 (用特征图表示)

DCGAN 的生成网络和判别网络有以下特点:

\begin{itemize}
  \item 生成网络使用转置卷积进行上采样, 判别网络使用卷积进行下采样。
  \item 生成网络和判别网络都没有汇聚层 (pooling layer)。
  \item 生成网络和判别网络都没有全连接的隐层。
  \item 生成网络的激活函数除输出层使用双曲正切以外, 其他层均使用 ReLU。
  \item 判别网络的激活函数除输出层使用 $\mathrm{S}$ 型函数以外, 其他层均使用渗漏整流线性函数 (Leaky ReLu)。
  \item 生成网络和判别网络的学习都采用批量归一化。
\end{itemize}

渗漏整流线性函数 $a(z)$ 的定义如下:

\[
a(z)= \begin{cases}z, & z \geqslant 0  \tag{28.15}\\ \alpha \cdot z, & z<0\end{cases}
\]

其中, $\alpha>0$ 是参数, 比如取 $\alpha=0.01$ 。

生成网络的输入是 100 维的向量, 按照均匀分布采样得到, 输出是 $64 \times 64 \times 3$ 的张量。第一层是线性变换层, 将 100 维的向量转换为 $4 \times 4 \times 1024$ 的张量, 接着连续通过 4 个由转置卷积组成的卷积层, 对张量连续进行卷积变换。判别网络的输入是 $64 \times 64 \times 3$ 的张量, 连续通过 4 个由 (原始) 卷积组成的卷积层, 对张量连续进行卷积变换, 得到 $4 \times 4 \times 512$ 的张量, 最后一层是 $\mathrm{S}$ 型函数层, 输出是 $1 / 0$ 标量。

生成网络的所有卷积层的转置卷积核尺寸都是 5 , 步幅都是 2 , 进行的是上采样。判别网络的所有卷积层的卷积核尺寸都是 5 , 步幅都是 2 , 进行的是下采样。\\
图 28.7 是 MNIST 手写数字数据的例子, 包括训练数据、GAN 生成的数据、DCGAN 生成的数据。可以看出 DCGAN 生成的数据更接近真实的手写数字数据。

\includegraphics[max width=\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-16}\\
(a) MNIST训练数据\\
(b) GAN生成的数据\\
(c) DCGAN生成的数据

图 28.7 手写数字数据生成例

\section*{本章概 要}
\begin{enumerate}
  \item 对抗生成网络 GAN 由一个生成网络和一个判别网络组成, 生成网络生成数据, 判别网络判别数据是真实数据还是生成数据。两者进行博亦, 不断提高自己的能力, 最终达到纳什均衡。生成网络可以以假乱真地生成数据, 判别网络不能判断数据的真假。

  \item 判别网络和生成网络的博弈关系可以定义为以下的极小极大问题, 也就是 GAN 的学习目标函数。

\end{enumerate}

$$
\min _{\boldsymbol{\theta}} \max _{\boldsymbol{\varphi}}\left\{E_{\boldsymbol{x} \sim P_{\mathrm{data}(\boldsymbol{x})}}[\log D(\boldsymbol{x} ; \boldsymbol{\varphi})]+E_{\boldsymbol{z} \sim P_{\text {seed }}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \boldsymbol{\varphi})]\}\right.
$$

这里生成网络由 $\boldsymbol{x}=G(\boldsymbol{z} ; \boldsymbol{\theta})$ 表示, $\boldsymbol{\theta}$ 是网络参数。判别网络由 $D(\boldsymbol{x} ; \boldsymbol{\varphi})$ 表示, 是一个二类分类器, $\varphi$ 是网络参数。 $P_{\mathrm{data}}(\boldsymbol{x})$ 是训练数据 $\boldsymbol{x}$ 的分布, $P_{\mathrm{seed}}(\boldsymbol{z})$ 是输入 $\boldsymbol{z}$ 的分布。

\begin{enumerate}
  \setcounter{enumi}{2}
  \item GAN 的学习算法如下。
\end{enumerate}

$$
\begin{aligned}
& \text { for }(t=1,2, \cdots, T)\{ \\
& \# \text { 训练判别网络 } D(\boldsymbol{x} ; \boldsymbol{\varphi}) \\
& \text { for }(s=1,2, \cdots, S)\{
\end{aligned}
$$

从训练数据中随机采样 $M$ 个样本 $\left\{\boldsymbol{x}^{(m)}\right\}$

随机采样 $M$ 个样本 $\left\{\boldsymbol{z}^{(m)}\right\}$

计算以下梯度, 使用梯度上升法更新参数 $\varphi$

$$
\varphi \leftarrow \varphi+\eta \nabla_{\varphi}
$$

\}

\# 训练生成网络 $G(\boldsymbol{z} ; \boldsymbol{\theta})$

随机采样 $M$ 个样本 $\left\{\boldsymbol{z}^{(m)}\right\}$\\
计算以下梯度, 使用梯度上升法更新参数 $\theta$

$$
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\eta \nabla_{\boldsymbol{\theta}}
$$

\}

\begin{enumerate}
  \setcounter{enumi}{3}
  \item GAN 学习的最优解存在, 这时生成网络和判别网络满足:
\end{enumerate}

$$
\begin{gathered}
P_{\text {gen }}^{*}(\boldsymbol{x})=P_{\text {data }}(\boldsymbol{x}) \\
D^{*}(\boldsymbol{x})=\frac{1}{2}
\end{gathered}
$$

也就是说, 生成网络与训练数据有相同的分布, 判别网络不能对训练数据和生成数据进行区分。

\begin{enumerate}
  \setcounter{enumi}{4}
  \item 对任意一个卷积运算, 存在对应的线性变换的矩阵 $\boldsymbol{C}$ 。针对转置矩阵 $\boldsymbol{C}^{\mathrm{T}}$, 引入新的卷积运算, 称为转置卷积。原始卷积和转置卷积是相互对应、互为反向的运算。原始卷积的卷积核是 $\boldsymbol{W}$ 时, 转置卷积的卷积核是 $\operatorname{rot} 180(\boldsymbol{W})$ 。卷积核和转置卷积核之间有 $\operatorname{rot} 180(\operatorname{rot} 180(\boldsymbol{W}))=\boldsymbol{W}$ 成立。

  \item 深度卷积生成对抗网络 DCGAN 是 GAN 用于图像生成的代表性模型。DCGAN 由生成网络和判别网络组成。生成网络和判别网络都只使用卷积运算, 不使用汇聚运算和隐藏的全连接。生成网络利用转置卷积进行上采样, 判别网络利用卷积进行下采样。

\end{enumerate}

\section*{继续阅 读}
GAN 的第一个工作发表在文献 [1], 在文献 [2] 和文献 [3] 中也有介绍。DCGAN 的最初论文是文献 [4], W-GAN 的最初论文是文献 [5]。

\section*{习 题}
28.1 GAN 的生成网络的学习也可以定义为以下的最小化问题:

$$
\min _{\boldsymbol{\theta}}\left\{E_{\boldsymbol{z} \sim P_{\text {seed }}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \overline{\boldsymbol{\varphi}})-\log (D(G(\boldsymbol{z} ; \boldsymbol{\theta}) ; \overline{\boldsymbol{\varphi}})]\}\right.
$$

比较与式 (28.2) 的不同, 并考虑其作用。

28.2 两个人进行零和博恋, 参与人 $\mathrm{X}$ 和 $\mathrm{Y}$ 可选择的策略分别是 $\mathcal{X}=\{1,2\}$ 和 $\mathcal{Y}=\{1,2\}$ 。在博弈中, 若参与人 $\mathrm{X}$ 和 $\mathrm{Y}$ 分别选择 $i \in \mathcal{X}$ 和 $j \in \mathcal{Y}$, 则 $\mathrm{X}$ 的损失或 $\mathrm{Y}$ 的收益是 $a_{i j}$ 。整体由矩阵 $\boldsymbol{A}=\left(a_{i j}\right)$ 表示, 矩阵 $\boldsymbol{A}$ 定义为

$$
\boldsymbol{A}=\left[\begin{array}{rr}
-1 & 2 \\
4 & 1
\end{array}\right]
$$

针对这个博弈求 $\min _{i} \max _{j} a_{i j}$ 和 $\max _{j} \min _{i} a_{i j}$, 并验证这时 $\max _{j} \min _{i} a_{i j} \leqslant \min _{i} \max _{j} a_{i j}$成立。

28.3 计算以下两个概率分布的 Jessen-Shannon 散度。设 $0 \log 0=0$ 。

\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
0.1 & 0.7 & 0.1 & 0.1 & 0 \\
\hline
0.2 & 0 & 0 & 0.8 & 0 \\
\hline
\end{tabular}
\end{center}

28.4 证明两个概率分布 $P$ 和 $Q$ 之间的 Jessen-Shannon 散度满足以下关系, 当且仅当 $P$ 和 $Q$ 相同时取最小值 0 , 设对数是自然对数。

$$
0 \leqslant \mathrm{JS}(P \| Q) \leqslant \ln 2
$$

28.5 考虑一维卷积运算, 其输入是 5 维的向量 $\boldsymbol{x}$, 输出是 3 维的向量 $\boldsymbol{z}$ 。卷积核是 $\boldsymbol{w}=\left(w_{1}, w_{2}, w_{3}\right)$, 步幅为 1 , 填充为 0 。写出该卷积运算的矩阵表示, 给出对应的转置卷积,并且验证原始卷积核 $\boldsymbol{w}$ 和转置卷积核 $\boldsymbol{w}^{\prime}$ 之间有 $\boldsymbol{w}=\operatorname{rot} 180\left(\boldsymbol{w}^{\prime}\right)$ 成立。

28.6 写出图 28.8 中转置卷积的大小和原始卷积的大小之间的关系, 转置卷积有输入矩阵尺寸 $\hat{I}^{\prime}$ 、卷积核尺寸 $K^{\prime}$ 、步幅 $S^{\prime}$ 、填充尺寸 $P^{\prime}$ 、输出矩阵尺寸 $O^{\prime}$ 。\\
\includegraphics[max width=\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-18}

图 28.8

\section*{参考文献}
[1] GOODFEllow I, POUGET-ABADIE J, MIRZA M, et al. Generative adversarial nets[J]. Advances in neural information processing systems, 2014: 2672-2680.

[2] GOOdfEllow i, BEngIO Y, COURVILLE A, et al. Deep Learning[M]. MIT Press, 2016.

[3] 邱锡鹏. 神经网络与深度学习 $[\mathrm{M}]$. 北京: 机械工业出版社, 2020.

[4] RADFORD A, METZ L, CHINTALA S. Unsupervised representation learning with deep convolutional generative adversarial networks[Z/OL]. arXiv preprint arXiv:1511.06434. 2015.

[5] ARJOVSKY M, CHINTALA S, BOTTOU L. Wasserstein Generative Adversarial Networks[C]// InInternational Conference on Machine Learning. 2017: 214-223.

\section*{第 29 章 深度学习方法总结}
\section*{29.1 深度学习的模型}
\section*{1. 基本神经网络}
深度学习是指以复杂神经网络为模型的机器学习。神经网络是含有参数的非线性函数的复合函数, 其参数通过学习得到, 可以用于监督学习、无监督学习、强化学习。用于监督学习的基本神经网络有前馈神经网络、卷积神经网络、循环神经网络、图神经网络、Transformer 等。

前馈神经网络是最基本的神经网络。前馈神经网络 (FNN) 以实数向量为输入, 对实数向量进行分类或回归。卷积神经网络 (CNN) 和循环神经网络 (RNN) 以一维格点数据或二维格点数据为输入, 其中每一个格点由实数向量表示, 模型对各个格点进行分类或回归, 或者对格点数据整体进行分类或回归。语言数据可以表示为一维格点数据, 图像数据可以表示为二维格点数据。本书介绍了卷积神经网络用于图像数据和语言数据处理的情况, 以及循环神经网络用于语言数据处理的情况。其实循环神经网络也可以用于图像数据处理 ${ }^{[1]}$ 。卷积神经网络和循环神经网络拥有各自的局部重复的结构, 卷积神经网络在各个格点上进行卷积运算, 循环神经网络在各个格点上进行基本单元运算。

神经网络也可以定义在图数据上, 称为图神经网络 (graph neural network), 如图卷积神经网络 (graph convolutional neural network), 参见文献 [2] 和文献 [3]。图神经网络对图的每一个结点进行分类或回归，或者对图整体进行分类或回归。

Transformer 是序列到序列学习模型, 由编码器和解码器组成, 编码器将输入序列转换为中间表示序列, 解码器将中间表示序列转换为输出序列。编码器和解码器有类似的结构,一般是多层, 使用多头注意力、非线性变换、残差连接、层归一化、位置嵌入。编码器和解码器之间通过注意力进行信息传递。比如, Transformer 将自然语言的一个单词序列转换为单词的上下文表示序列, 再将单词的上下文表示序列转换为另一个单词序列。编码器进行非自回归的预测, 解码器进行自回归的预测。Transformer 是定义在输入序列和已生成输出序列上的模型。

表 29.1 总结了监督学习的基本神经网络的作用和特点。

用于无监督学习的基本神经网络有自动编码器、去噪自动编码器、变分自动编码器等。自动编码器由编码器网络和解码器网络组成。学习时编码器将输入向量转换为中间表示向量 (编码), 解码器再将中间表示向量转换为输出向量 (解码), 实际是对数据进行压缩, 得到的中间表示能有效地刻画数据的主要特征。预测时通常用编码器将新的输入向量转换为中间表示向量。去噪自动编码器也由编码器网络和解码器网络组成, 不同点在于学习时对输入向\\
表 29.1 监督学习的基本神经网络的作用和特点

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
 & 模型输入 & 模型输出 & 模型作用 & 模型特点 \\
\hline
前馈神经网络 & 实数向量 & \begin{tabular}{l}
分类或回归 \\
结果 \\
\end{tabular} & \begin{tabular}{l}
分类或回归: 对实数向 \\
量的分类或回归 \\
\end{tabular} & \begin{tabular}{l}
在每层进行非线性变 \\
换, 一般是多层 \\
\end{tabular} \\
\hline
卷积神经网络 & \begin{tabular}{l}
一维或二维 \\
格点数据 \\
\end{tabular} & \begin{tabular}{l}
分类或回归 \\
结果 \\
\end{tabular} & \begin{tabular}{l}
分类或回归: 对各个格 \\
点的分类或回归, 或者 \\
对整体的分类或回归 \\
\end{tabular} & \begin{tabular}{l}
在每一个格点上进行卷 \\
积运算, 一般是多层 \\
\end{tabular} \\
\hline
循环神经网络 & \begin{tabular}{l}
一维或二维 \\
格点数据 \\
\end{tabular} & \begin{tabular}{l}
分类或回归 \\
结果 \\
\end{tabular} & \begin{tabular}{l}
分类或回归: 对各个格 \\
点的分类或回归 (序列 \\
标注), 或者对整体的分 \\
类或回归 \\
\end{tabular} & \begin{tabular}{l}
在每一个格点上进行基 \\
本单元运算, 可以是多 \\
层 \\
\end{tabular} \\
\hline
图神经网络 & 图数据 & \begin{tabular}{l}
分类或回归 \\
结果 \\
\end{tabular} & \begin{tabular}{l}
分类或回归: 对各个结 \\
点的分类或回归, 或者 \\
对整体的分类或回归 \\
\end{tabular} & \begin{tabular}{l}
在每一个结点上进行卷 \\
积等运算, 可以是多层 \\
\end{tabular} \\
\hline
Transformer & 输入序列 & 输出序列 & \begin{tabular}{l}
序列到序列: 将输入序 \\
列转换为中间表示序列, \\
再将中间表示序列转换 \\
为输出序列 \\
\end{tabular} & \begin{tabular}{l}
使用多头注意力、非线 \\
性变换、残差连接、层 \\
归一化、位置嵌入, 一 \\
般是多层 \\
\end{tabular} \\
\hline
\end{tabular}
\end{center}

量加入随机噪声, 对有噪声的输入向量进行编码。去噪自动编码器能更有效地学习到数据的主要特征。变分自动编码器 (variational autoencoder) [4-5] 也由编码器和解码器组成, 但与自动编码器不同, 本质上是数据生成模型 (也就是其解码器)。编码器表示基于输入向量生成参数向量的条件概率分布, 解码器表示基于参数向量生成输出向量的条件概率分布。假设参数向量的先验分布是高斯分布。学习的目标是使输出向量与输入向量尽量一致, 也就是使学到的模型能生成给定的数据 (输入向量)。得到的解码器用于数据的随机生成。

表 29.2 总结了无监督学习的基本神经网络的作用和特点。

表 29.2 无监督学习的基本神经网络的作用和特点

\begin{center}
\begin{tabular}{l|l|l|l|l}
\hline
 & 模型输入 & 模型输出 & 模型作用和特点 & \multicolumn{1}{|c}{模型特点} \\
\hline
自动编码器 & \begin{tabular}{l}
原始实数 \\
向量 \\
\end{tabular} & \begin{tabular}{l}
还原的实 \\
数向量 \\
\end{tabular} & \begin{tabular}{l}
压缩或表示学习: 首先进行数据压 \\
缩, 然后进行数据还原 \\
\end{tabular} & \begin{tabular}{l}
编码器和解码器由 \\
前馈神经网络实现 \\
\end{tabular} \\
\hline
\begin{tabular}{l}
去噪自动编 \\
码器 \\
\end{tabular} & \begin{tabular}{l}
带噪声的 \\
实数向量 \\
\end{tabular} & \begin{tabular}{l}
原始实数 \\
向量 \\
\end{tabular} & \begin{tabular}{l}
压缩、去噪或表示学习: 首先对带 \\
噪声的数据进行压缩, 然后进行原 \\
始数据还原 \\
\end{tabular} & \begin{tabular}{l}
编码器和解码器 \\
由前馈神经网络实 \\
现, 学习时在输入 \\
中加入随机噪声 \\
\end{tabular} \\
\hline
\begin{tabular}{l}
变分自动编 \\
码器 \\
\end{tabular} & \begin{tabular}{l}
原始实数 \\
向量 \\
\end{tabular} & \begin{tabular}{l}
生成的实 \\
数向量 \\
\end{tabular} & \begin{tabular}{l}
数据生成: 编码器表示基于输入向 \\
量生成参数向量的条件概分布, \\
解码器表示基于参数向量生成输 \\
出向量的条件概率分布 \\
\end{tabular} & \begin{tabular}{l}
级, 器和解码器 \\
神经向网络实 \\
\end{tabular} \\
\hline
\end{tabular}
\end{center}

\section*{2. 深度学习与表示学习}
相比传统机器学习, 深度学习的最大特点是系统可以进行端到端 (end-to-end) 的模型训练; 系统可以自动地学习模型的特征, 而不需要人工定义。所以深度学习与表示学习 (representation learning) 密切相关。\\
输入的特征 (如实例的特征、格点数据中的格点特征、图数据中的结点特征)、模型中的特征都用实数向量表示, 都是分布式表示。

\section*{3. 深度学习与计算}
可以把深度学习中的各种建模工具看作是计算机编程工具的扩展。前馈神经网络可以近似地表示 AND, OR, XOR, NAND 等逻辑门电路。深度学习中的函数、指针、门控、残差连接、注意力可以分别看作是计算机编程工具中的函数、指针、分支、递归、键-值查询的扩展 ${ }^{[6]}$ 。计算机编程工具一般是定义在符号或数值上的, 深度学习工具定义在向量、矩阵或张量上。计算机编程工具实施的是 “硬的” (离散的) 操作, 深度学习实施的是 “软的” (连续的)的操作。深度学习中的函数是指前馈神经网络等模型。指针在指针网络 (pointer network) 中使用 ${ }^{[7]}$, 门控在 GRU 模型中使用, 残差连接在 ResNet 和 Transformer 中使用, 注意力在 Transformer 中使用。

表 29.3 深度学习工具与计算机编程工具的比较

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 & 计算机编程工具 & 深度学习工具 \\
\hline
函数 & 输入: $x$, 输出: $y=f(x)$ & 输入: $\boldsymbol{x}$, 输出: $\boldsymbol{y}=f(\boldsymbol{x})$ \\
\hline
指针 & 输入: $x$, 输出: $\# y=f(x)$ & 输入: $\boldsymbol{x}$, 输出: $\# \boldsymbol{y}=f(\boldsymbol{x})$ \\
\hline
分支、门控 & \begin{tabular}{l}
输入: $x$, 输出: \\
IF $\delta(x)=1$, THEN $y=f(x)$, ELSE \\
$y=g(x)$ \\
\end{tabular} & \begin{tabular}{l}
输入: $\boldsymbol{x}$, 输出: \\
$\boldsymbol{y}=\delta(\boldsymbol{x}) \odot f(\boldsymbol{x})+(\mathbf{1}-\delta(\boldsymbol{x})) \odot g(\boldsymbol{x})$ \\
\end{tabular} \\
\hline
\begin{tabular}{l}
递归、残差 \\
连接 \\
\end{tabular} & \begin{tabular}{l}
输入: $x_{1}$, 输出: $x_{n+1}$ \\
For $(l=1,2, \cdots, n)\left\{x_{l+1}=x_{l}+\right.$ \\
$\left.f_{l}\left(x_{l}\right)\right\}$ \\
\end{tabular} & \begin{tabular}{l}
输入: $\boldsymbol{x}_{1}$, 输出: $\boldsymbol{x}_{n+1}$ \\
For $(l=1,2, \cdots, n)\left\{\boldsymbol{x}_{l+1}=\boldsymbol{x}_{l}+f_{l}\left(\boldsymbol{x}_{l}\right)\right\}$ \\
\end{tabular} \\
\hline
\begin{tabular}{l}
键-值查询、 \\
注意力 \\
\end{tabular} & \begin{tabular}{l}
输入: $q,\left(k_{1}, v_{1}\right),\left(k_{2}, v_{2}\right), \cdots,\left(k_{n}, v_{n}\right)$, \\
输出: IF $\delta\left(q, k_{i}\right)=1$, THEN $v_{i}$ \\
\end{tabular} & \begin{tabular}{l}
输入: $\boldsymbol{q},\left(\boldsymbol{k}_{1}, \boldsymbol{v}_{1}\right),\left(\boldsymbol{k}_{2}, \boldsymbol{v}_{2}\right), \cdots,\left(\boldsymbol{k}_{n}, \boldsymbol{v}_{n}\right)$, \\
输出: $\sum_{i=1}^{\boldsymbol{n}} \alpha\left(\boldsymbol{q}, \boldsymbol{k}_{i}\right) \cdot \boldsymbol{v}_{i}$ \\
\end{tabular} \\
\hline
\end{tabular}
\end{center}

\section*{29.2 深度学习的方法}
深度学习的算法主要是梯度下降, 具体地是反向传播, 可以用于监督学习和无监督学习。预训练语言模型和生成对抗网络使用大量无标注数据学习, 可以认为是无监督学习方法。

\section*{1. 学习算法和技巧}
深度学习无论是监督学习还是无监督学习, 学习的目标一般都是最大化似然函数或者最小化交叉熵, 也就是进行极大似然估计。神经网络是复杂的非线性模型, 比起传统机器学习模型有更多的参数, 但无论模型如何复杂, 只要目标函数对参数可导, 主要是神经网络函数对参数可导, 就可以进行学习。

学习的算法通常使用随机梯度下降法。因为神经网络的参数很多, 更适合于使用一阶优化算法, 如随机梯度下降, 而不是二阶优化算法, 如拟牛顿法。\\
反向传播算法提供了一个高效的随机梯度下降法的实现。只需要依照网络结构进行一次正向传播和一次反向传播, 就可以完成梯度下降的一次迭代。正向传播使用当前的所有参数重新计算神经网络所有变量, 从前往后进行计算。反向传播使用当前的所有变量重新计算网络的所有参数, 过程中基于当前模型的预测值与真实值之间的误差, 从后往前进行梯度计算以及参数更新计算。

反向传播算法也可以在计算图上实现, 每一个结点表示一个函数或变量。正向传播从起点的输入开始, 顺着有向边, 依次对结点的函数进行计算, 直到得到终点的输出为止, 都可以看作是张量的流动。反向传播从终点的梯度 (整体函数的梯度) 开始, 逆着有向边, 依次对结点的梯度进行运算, 直到得到起点的梯度为止, 也都可以看作是张量的流动。

深度学习中常常不做正则化也不产生过拟合。常用的防止过拟合的方法有早停法和暂退法 (dropout)。暂退法在训练过程中每一步随机选取一些神经元, 让它们不参与训练, 学习结束后, 对权重进行调整, 然后用整体网络进行预测。

深度学习训练中有时会遇到稳定性问题, 包括梯度消失和梯度爆炸、内部协变量偏移。梯度消失和梯度爆炸是指在学习过程中, 目标函数对参数的梯度有时会接近 0 (梯度消失) 或接近无穷 (梯度爆炸), 导致无法有效地学习的问题。本质原因是反向传播过程中要进行矩阵连乘计算, 使得结果矩阵的一些元素趋近于零或趋近于无穷。为防止这个问题, 可以进行更恰当的初始化或使用更合适的激活函数, 如整流线性函数 ReLu。更重要的是使用更合理的网络架构, 比如 LSTM 和 ResNet。

在深度神经网络的学习过程中, 各个层的参数会发生变化, 各个层的输出也会随之发生变化。对于其中任意一层, 其输入也会不断改变, 其结果是这一层及其后面层的学习会产生振荡, 学习速度会变缓。也就是说会发生内部协变量偏移现象。防止这个问题的方法有批量归一化和层归一化。这些归一化方法也有防止梯度消失和梯度爆炸的作用。

\section*{2. 预训练语言模型}
实际应用中深度学习主要用于监督学习, 主要挑战是缺少标注数据。自然语言处理中的预训练语言模型成功地解决了这个问题。

预训练语言模型的基本想法是基于 Transformer 的编码器或解码器实现语言模型, 在其基础上定义监督学习模型。在预训练中, 使用大规模的语料通过无监督学习的方式学习模型的参数, 在微调中, 将模型用于一个具体任务, 使用少量的标注数据通过监督学习的方式进一步调节模型的参数。常用的预训练语言模型有 BERT 和 GPT, 前者用于语言理解任务, 后者用于语言生成任务。

\section*{3. 生成对抗网络}
深度学习也可以用于生成模型的学习, 也就是针对给定的数据学习生成这些数据的分布。生成对抗网络 GAN 是一个有效的方法, 特别是对图像数据的生成。

GAN 由一个生成网络和一个判别网络组成, 两者的学习是一个博亦的过程。在学习的过程中生成网络尝试学习生成接近真实的数据, 判别网络尝试判别数据是已给的真实数据还是对手生成的数据。两者不断提高自己的能力, 最终达到均衡状态时, 生成网络可以以假乱真地生成数据。

\section*{29.3 深度学习的优化算法}
\section*{1. 随机梯度下降法}
深度学习的优化算法一般是随机梯度下降法 (SGD)。随机梯度下降法在第 $t$ 步对参数进行如下更新 (通常用小批量随机梯度下降):


\begin{equation*}
\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{t-1}-\eta \boldsymbol{g}_{t} \tag{29.1}
\end{equation*}


其中, $\boldsymbol{\theta}_{t}$ 和 $\boldsymbol{\theta}_{t-1}$ 分别是第 $t$ 步和第 $t-1$ 步的参数, $\boldsymbol{g}_{t}$ 是第 $t$ 步的梯度, $\eta$ 是学习率。

基本的随机梯度下降法计算效率并不高, 因为每一步的梯度更新（包括方向和大小）都是基于当前所在位置, 整体收玫速度未必很快。有多个改进的算法, 其中 Adam 是最常用的算法 ${ }^{[8]}$, 是动量算法和 RMSProp 算法的组合。其核心想法是在每一步利用之前的所有步的梯度对当前的梯度进行调整。这里做一简单介绍。

\section*{2. 动量算法}
动量 (momentum) 算法在第 $t$ 步对参数进行如下梯度更新:


\begin{gather*}
\boldsymbol{v}_{t}=\beta_{1} \boldsymbol{v}_{t-1}+\left(1-\beta_{1}\right) \boldsymbol{g}_{t}  \tag{29.2}\\
\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{t-1}-\eta \boldsymbol{v}_{t} \tag{29.3}
\end{gather*}


其中, $\boldsymbol{\theta}_{t}$ 和 $\boldsymbol{\theta}_{t-1}$ 分别是第 $t$ 步和第 $t-1$ 步的参数, $\boldsymbol{v}_{t}$ 和 $\boldsymbol{v}_{t-1}$ 分别是第 $t$ 步和第 $t-1$ 步的中间变量, $\boldsymbol{g}_{t}$ 是第 $t$ 步的梯度, $\beta_{1}$ 是系数 $\left(0 \leqslant \beta_{1}<1\right), \eta$ 是学习率。

设 $v_{0}=O$, 则在第 $t$ 步有


\begin{equation*}
\boldsymbol{v}_{t}=\left(1-\beta_{1}\right) \sum_{\tau=1}^{t} \beta_{1}^{t-\tau} \boldsymbol{g}_{\tau} \tag{29.4}
\end{equation*}


动量算法在第 $t$ 步计算迄今为止所有步的梯度的加权平均 $v_{t}$, 并用 $v_{t}$ 进行参数的更新,其中权重从后往前指数性递减, 称为指数加权移动平均 (exponentially weighted moving average）。当 $\beta_{1}=0$ 时, 动量算法回退到 SGD。

下面给出动量算法的直观解释, 图 29.1 是一个随机梯度下降 (SGD) 的例子。图中显示的是目标函数的等高线以及 SGD 的迭代轨迹。目标函数是一个凸的椭球面。SGD 每步都从最陡的方向下降, 形成的迭代轨迹在纵向上不断振荡, 在横向上移动缓慢, 并不能以最快的

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-23}
\end{center}

图 29.1 随机梯度下降的例子\\
速度达到最小点。在动量算法的每一步, 如果迄今为止梯度在某一个方向 (图中纵向) 上的值有正有负, 其加权平均在这个方向上就会取一个小的值, 那么就在这个方向上进行小幅度的更新; 如果迄今为止梯度在某一个方向 (图中横向) 上的值同正或同负, 其加权平均在这个方向上就会取一个大的值, 那么就在这个方向上进行大幅度的更新。这样就可以减轻 SGD 收玫不快的问题。

动量算法有物理解释——梯度加权平均 $v_{t}$ 表示速度, 这里不予介绍。

\section*{3. RMSProp 算法}
RMSProp (root mean square propagation) 算法是 AdaGrad 算法的改进。在第 $t$ 步对参数进行如下更新:


\begin{gather*}
s_{t}=\beta_{2} s_{t-1}+\left(1-\beta_{2}\right) \boldsymbol{g}_{t} \odot \boldsymbol{g}_{t}  \tag{29.5}\\
\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{t-1}-\eta \frac{1}{\sqrt{s_{t}+\varepsilon}} \boldsymbol{g}_{t} \tag{29.6}
\end{gather*}


其中, $\boldsymbol{\theta}_{t}$ 和 $\boldsymbol{\theta}_{t-1}$ 分别是第 $t$ 步和第 $t-1$ 步的参数; $s_{t}$ 和 $s_{t-1}$ 分别是第 $t$ 步和第 $t-1$ 步的中间变量; $\boldsymbol{g}_{t}$ 是第 $t$ 步的梯度; $\odot$ 表示向量的逐元素积; $\beta_{2}$ 是系数 $\left(0 \leqslant \beta_{2}<1\right)$; $\varepsilon$ 是每一个元素都为正数 $\varepsilon$ 的向量, 防止分母为 $0 ; \eta$ 是学习率。

设 $s_{0}=O$, 则在第 $t$ 步有


\begin{equation*}
s_{t}=\left(1-\beta_{2}\right) \sum_{\tau=1}^{t} \beta_{2}^{t-\tau} \boldsymbol{g}_{\tau} \odot \boldsymbol{g}_{\tau} \tag{29.7}
\end{equation*}


RMSProp 算法在第 $t$ 步计算迄今为止梯度的元素平方的加权平均 $s_{t}$, 并用 $s_{t}$ 对当前梯度的元素进行归一化, 然后进行参数的更新, 其中加权平均是指数加权移动平均。当 $\beta_{2}=0$时, RMSProp 算法是 SGD 的近似, 每一步对梯度的元素进行归一化。

RMSProp 算法也可以减轻 SGD 收玫不快的问题。在每一步, 如果迄今为止梯度在某一个方向 (图 29.1 中纵向) 上的值都很大, 梯度元素平方的加权平均就会很大, 归一化的梯度在这个方向上就会取一个小的值, 在这个方向上进行小幅度的更新; 如果梯度在某一个方向 (图 29.1 中横向) 上的值都很小, 梯度元素平方的加权平均就会很小, 归一化的梯度在这个方向上就会取一个大的值, 在这个方向上进行大幅度的更新。RMSProp 算法实际上是在每一步对梯度的每一个元素做自适应的调整。

\section*{4. Adam 算法}
Adam 算法 (adaptive moment estimation algorithm) 将动量算法和 RMSProp 算法的技巧结合, 以提高迭代的收玫速度。在第 $t$ 步对参数依次进行如下更新:


\begin{gather*}
\boldsymbol{v}_{t}=\beta_{1} \boldsymbol{v}_{t-1}+\left(1-\beta_{1}\right) \boldsymbol{g}_{t}  \tag{29.8}\\
s_{t}=\beta_{2} s_{t-1}+\left(1-\beta_{2}\right) \boldsymbol{g}_{t} \odot \boldsymbol{g}_{t}  \tag{29.9}\\
\boldsymbol{v}_{t}=\frac{\boldsymbol{v}_{t}}{1-\beta_{1}^{t}}  \tag{29.10}\\
s_{t}=\frac{\boldsymbol{v}_{t}}{1-\beta_{2}^{t}} \tag{29.11}
\end{gather*}



\begin{equation*}
\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{t-1}-\eta \frac{\boldsymbol{v}_{t}}{\sqrt{\boldsymbol{s}_{t}+\varepsilon}} \tag{29.12}
\end{equation*}


在式 (29.10) 和式 (29.11) 对 $\boldsymbol{v}_{t}$ 和 $s_{t}$ 进行矫正计算。超参数通常取 $\beta_{1}=0.9, \beta_{2}=0.999$, $\varepsilon=10^{-8}$ 。

实验证明 Adam 算法对不同的神经网络都有很好的学习收玫速度。

\section*{29.4 深度学习的优缺点}
\section*{1. 优势}
深度学习的优点主要体现在三个方面:

(1) 神经网络拥有强大的函数近似能力。通用函数近似定理指出, 二层神经网络就可以以任意精度近似任意一个连续函数。假设实现某一功能的 “理想” 的函数存在, 则存在一个神经网络是这个函数的充分近似。

(2) 深的神经网络比浅的神经网络拥有更精简的表达能力, 更高的样本效率。存在这样的情况, 深而窄的神经网络与浅而宽的神经网络是等价的。但前者的参数比后者更少, 只需要较少的样本就可以学到。在极端情况下, 浅而宽的神经网络的宽度是指数级的, 现实中并不可取。

(3) 深度学习有很强的泛化能力, 也就是从训练集上学到的预测误差小的模型在测试集上也同样有小的预测误差。深度学习中常常不做正则化也不产生过拟合。通常是在大规模训练数据、过参数化神经网络以及随机梯度下降训练的条件下发生的, 这里过参数化是指网络的参数量大于训练数据量。已有机器学习理论尚不能很好地解释这种现象, 是当前重要的研究课题。

\section*{2. 不足}
深度学习也有缺点, 缺乏稳健性 (robustness) 是一个突出的问题, 也就是数据中很小的扰动就会导致预测错误。这也是深度学习的强大学习能力所致。稳健的学习可以定义为极小极大 $(\min \max )$ 的优化问题。一般的机器学习的目标是在平均情况下预测误差最小, 而稳健的学习的目标是在最坏情况下预测误差最小, 具体地, 数据在某个范围内发生对自己最不利的扰动时也能保证预测误差最小。最近的理论研究证明, 在一些条件下, 稳健的学习比一般的学习需要更多的样本, 结论对深度学习和传统机器学习都适用。这意味着深度学习需要更多的样本才能变得稳健。稳健的学习可以定义为以下极小极大优化问题: [9]

$$
\min _{\theta} E_{x}\left[\max _{\left\|x-x^{\prime}\right\|_{\infty} \leqslant \varepsilon} L\left(\theta, x^{\prime}\right)\right]
$$

其中, $L$ 是损失函数, $x$ 和 $x^{\prime}$ 是样本, $\theta$ 是模型参数。

深度学习的另一个缺点是恰当性 (adequacy) 的问题。由于训练数据的偏差和机器学习的特点 (预测误差最小化导向、训练中的随机性) 等原因, 深度学习常常 “学到不恰当的知识”。比如, 图像识别中认为有把手的就是杯子, 有轮胎的就是汽车。传统机器学习也存在这个问题, 但深度学习的问题更突出。恰当性是站在人的角度看到的问题, 并不一定能算是缺陷。

\section*{3. 可解释性}
神经网络不具备可解释性, 但这并不一定是缺点。可解释性依赖于应用, 比如在金融、医疗等领域的预测需要可解释性, 但是在其他领域的预测未必如此。人也不能解释自己是如何进行感知和认知处理的, 未必需要深度神经网络能够解释自己的判断过程。

\section*{参考文献}
[1] GOODFELlow i, BENGIO Y, COURVILLE A. Deep learning[M]. MIT Press, 2016.

[2] SCARSELLI F, GORI M, TSOI A C, et al. The graph neural network model[J]. IEEE Transactions on Neural Networks, 2008, 20(1): 61-80.

[3] KIPF T N, WELLING M. Semi-supervised classification with graph convolutional networks[J]. ICLR 2017.

[4] KINGMA D P, WELLING M. Auto-encoding variational bayes[Z/OL]. arXiv preprint arXiv:1312.6114, 2013.

[5] KINGMA D P, WELLING M. An introduction to variational autoencoders[Z/OL]. arXiv preprint arXiv:1906.02691, 2019.

[6] MCALLESTER D. Universality in deep learning and models of computation[C]//The 2nd International Workshop on Symbolic Neural Learning, 2018.

[7] VINYALS O, FORTUNATO M, JAITLY N. Pointer networks[J]. Advances in Neural Information Processing Systems, 2015, 28: 2692-2700.

[8] KINGMA D P, BA J. Adam: A method for stochastic optimization[Z/OL]. arXiv preprint arXiv:1412.6980, 2014.

[9] SCHMID L, SANTURKAR S, TSIPRAS D, et al. Adversarially robust generalization requires more data[J]. Advances in Neural Information Processing Systems, 2018, 31: 5014-5026.

\section*{附录 A 梯度下降法}
梯度下降法 (gradient descent) 或最速下降法 (steepest descent) 是求解无约束最优化问题的一种最常用的方法, 具有实现简单的优点。梯度下降法是迭代算法, 每一步需要求解目标函数的梯度向量。

假设 $f(x)$ 是 $\boldsymbol{R}^{n}$ 上具有一阶连续偏导数的函数, 要求解的无约束最优化问题是


\begin{equation*}
\min _{x \in \boldsymbol{R}^{n}} f(x) \tag{A.1}
\end{equation*}


$x^{*}$ 表示目标函数 $f(x)$ 的极小点。

梯度下降法是一种迭代算法。选取适当的初值 $x^{(0)}$, 不断迭代, 更新 $x$ 的值, 进行目标函数的极小化, 直到收玫。由于负梯度方向是使函数值下降最快的方向, 在迭代的每一步, 以负梯度方向更新 $x$ 的值, 从而达到减少函数值的目的。

由于 $f(x)$ 具有一阶连续偏导数, 若第 $k$ 次迭代值为 $x^{(k)}$, 则可将 $f(x)$ 在 $x^{(k)}$ 附近进行一阶泰勒展开:


\begin{equation*}
f(x)=f\left(x^{(k)}\right)+g_{k}^{\mathrm{T}}\left(x-x^{(k)}\right) \tag{A.2}
\end{equation*}


这里, $g_{k}=g\left(x^{(k)}\right)=\nabla f\left(x^{(k)}\right)$ 为 $f(x)$ 在 $x^{(k)}$ 的梯度。

求出第 $k+1$ 次迭代值 $x^{(k+1)}$ :


\begin{equation*}
x^{(k+1)} \leftarrow x^{(k)}+\lambda_{k} p_{k} \tag{A.3}
\end{equation*}


其中, $p_{k}$ 是搜索方向, 取负梯度方向 $p_{k}=-\nabla f\left(x^{(k)}\right) ; \lambda_{k}$ 是步长, 由一维搜索确定, 即 $\lambda_{k}$使得


\begin{equation*}
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left(x^{(k)}+\lambda p_{k}\right) \tag{A.4}
\end{equation*}


梯度下降法算法如下:

\section*{算法 A. 1 (梯度下降法)}
输入: 目标函数 $f(x)$, 梯度函数 $g(x)=\nabla f(x)$, 计算精度 $\varepsilon$ 。

输出: $f(x)$ 的极小点 $x^{*}$ 。

(1) 取初始值 $x^{(0)} \in \boldsymbol{R}^{n}$, 置 $k=0$ 。

(2) 计算 $f\left(x^{(k)}\right)$ 。

(3) 计算梯度 $g_{k}=g\left(x^{(k)}\right)$, 当 $\left\|g_{k}\right\|<\varepsilon$ 时, 停止迭代, 令 $x^{*}=x^{(k)}$; 否则, 令 $p_{k}=$ $-g\left(x^{(k)}\right)$, 求 $\lambda_{k}$, 使

$$
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left(x^{(k)}+\lambda p_{k}\right)
$$

(4) 置 $x^{(k+1)}=x^{(k)}+\lambda_{k} p_{k}$, 计算 $f\left(x^{(k+1)}\right)$ 。当 $\left\|f\left(x^{(k+1)}\right)-f\left(x^{(k)}\right)\right\|<\varepsilon$ 或 $\| x^{(k+1)}-$ $x^{(k)} \|<\varepsilon$ 时, 停止迭代, 令 $x^{*}=x^{(k+1)}$ 。

(5) 否则, 置 $k=k+1$, 转步骤 (3)。

当目标函数是凸函数时, 梯度下降法的解是全局最优解。一般情况下, 其解不保证是全局最优解。梯度下降法的收敛速度也未必是很快的。

\section*{附录 B 牛顿法和拟牛顿法}
牛顿法 (Newton method) 和拟牛顿法 (quasi-Newton method) 也是求解无约束最优化问题的常用方法, 有收玫速度快的优点。牛顿法是迭代算法, 每一步需要求解目标函数的黑塞矩阵的逆矩阵, 计算比较复杂。拟牛顿法通过正定矩阵近似黑塞矩阵的逆矩阵或黑塞矩阵,简化了这一计算过程。

\section*{1. 牛顿法}
考虑无约束最优化问题


\begin{equation*}
\min _{x \in \boldsymbol{R}^{n}} f(x) \tag{B.1}
\end{equation*}


其中, $x^{*}$ 为目标函数的极小点。

假设 $f(x)$ 具有二阶连续偏导数, 若第 $k$ 次迭代值为 $x^{(k)}$, 则可将 $f(x)$ 在 $x^{(k)}$ 附近进行二阶泰勒展开:


\begin{equation*}
f(x)=f\left(x^{(k)}\right)+g_{k}^{\mathrm{T}}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{\mathrm{T}} H\left(x^{(k)}\right)\left(x-x^{(k)}\right) \tag{B.2}
\end{equation*}


这里, $g_{k}=g\left(x^{(k)}\right)=\nabla f\left(x^{(k)}\right)$ 是 $f(x)$ 的梯度向量在点 $x^{(k)}$ 的值, $H\left(x^{(k)}\right)$ 是 $f(x)$ 的黑塞矩阵 (Hessian matrix)


\begin{equation*}
H(x)=\left(\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}\right)_{n \times n} \tag{B.3}
\end{equation*}


在点 $x^{(k)}$ 的值。函数 $f(x)$ 有极值的必要条件是在极值点处一阶导数为 0 , 即梯度向量为 0 。特别是当 $H\left(x^{(k)}\right)$ 是正定矩阵时, 函数 $f(x)$ 的极值为极小值。

牛顿法利用极小点的必要条件


\begin{equation*}
\nabla f(x)=0 \tag{B.4}
\end{equation*}


每次迭代中从点 $x^{(k)}$ 开始, 求目标函数的极小点, 作为第 $k+1$ 次迭代值 $x^{(k+1)}$ 。具体地, 假设 $x^{(k+1)}$ 满足:


\begin{equation*}
\nabla f\left(x^{(k+1)}\right)=0 \tag{B.5}
\end{equation*}


由式 (B.2) 有


\begin{equation*}
\nabla f(x)=g_{k}+H_{k}\left(x-x^{(k)}\right) \tag{B.6}
\end{equation*}


其中, $H_{k}=H\left(x^{(k)}\right)$ 。这样, 式 (B.5) 成为


\begin{equation*}
g_{k}+H_{k}\left(x^{(k+1)}-x^{(k)}\right)=0 \tag{B.7}
\end{equation*}


因此,


\begin{equation*}
x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k} \tag{B.8}
\end{equation*}


或者


\begin{equation*}
x^{(k+1)}=x^{(k)}+p_{k} \tag{B.9}
\end{equation*}


其中,


\begin{equation*}
H_{k} p_{k}=-g_{k} \tag{B.10}
\end{equation*}


用式 (B.8) 作为迭代公式的算法就是牛顿法。

\section*{算法 B. 1 (牛顿法)}
输入: 目标函数 $f(x)$, 梯度 $g(x)=\nabla f(x)$, 黑塞矩阵 $H(x)$, 精度要求 $\varepsilon$ 。

输出: $f(x)$ 的极小点 $x^{*}$ 。

(1) 取初始点 $x^{(0)}$, 置 $k=0$ 。

(2) 计算 $g_{k}=g\left(x^{(k)}\right)$ 。

(3) 若 $\left\|g_{k}\right\|<\varepsilon$, 则停止计算, 得近似解 $x^{*}=x^{(k)}$ 。

(4) 计算 $H_{k}=H\left(x^{(k)}\right)$, 并求 $p_{k}$ :

$$
H_{k} p_{k}=-g_{k}
$$

(5) 置 $x^{(k+1)}=x^{(k)}+p_{k}$ 。

(6) 置 $k=k+1$, 转步骤 (2)。

步骤 (4) 求 $p_{k}, p_{k}=-H_{k}^{-1} g_{k}$, 要求 $H_{k}^{-1}$, 计算比较复杂, 所以有其他改进的方法。

\section*{2. 拟牛顿法的思路}
在牛顿法的迭代中, 需要计算黑塞矩阵的逆矩阵 $H^{-1}$, 这一计算比较复杂, 考虑用一个 $n$ 阶矩阵 $G_{k}=G\left(x^{(k)}\right)$ 来近似代替 $H_{k}^{-1}=H^{-1}\left(x^{(k)}\right)$ 。这就是拟牛顿法的基本想法。

先看牛顿法迭代中黑塞矩阵 $H_{k}$ 满足的条件。首先, $H_{k}$ 满足以下关系。在式 (B.6) 中取 $x=x^{(k+1)}$, 即得:


\begin{equation*}
g_{k+1}-g_{k}=H_{k}\left(x^{(k+1)}-x^{(k)}\right) \tag{B.11}
\end{equation*}


记 $y_{k}=g_{k+1}-g_{k}, \delta_{k}=x^{(k+1)}-x^{(k)}$, 则


\begin{equation*}
y_{k}=H_{k} \delta_{k} \tag{B.12}
\end{equation*}


或


\begin{equation*}
H_{k}^{-1} y_{k}=\delta_{k} \tag{B.13}
\end{equation*}


式 (B.12) 或式 (B.13) 称为拟牛顿条件。

如果 $H_{k}$ 是正定的 ( $H_{k}^{-1}$ 也是正定的), 那么可以保证牛顿法搜索方向 $p_{k}$ 是下降方向。这是因为搜索方向是 $p_{k}=-H_{k}^{-1} g_{k}$, 由式 (B.8) 有


\begin{equation*}
x=x^{(k)}+\lambda p_{k}=x^{(k)}-\lambda H_{k}^{-1} g_{k} \tag{B.14}
\end{equation*}


所以 $f(x)$ 在 $x^{(k)}$ 的泰勒展开式 (B.2) 可以近似写成


\begin{equation*}
f(x)=f\left(x^{(k)}\right)-\lambda g_{k}^{\mathrm{T}} H_{k}^{-1} g_{k} \tag{B.15}
\end{equation*}


因 $H_{k}^{-1}$ 正定, 故有 $g_{k}^{\mathrm{T}} H_{k}^{-1} g_{k}>0$ 。当 $\lambda$ 为一个充分小的正数时, 总有 $f(x)<f\left(x^{(k)}\right)$, 也就是说 $p_{k}$ 是下降方向。

拟牛顿法将 $G_{k}$ 作为 $H_{k}^{-1}$ 的近似, 要求矩阵 $G_{k}$ 满足同样的条件。首先, 每次迭代矩阵 $G_{k}$ 是正定的。同时, $G_{k}$ 满足下面的拟牛顿条件:


\begin{equation*}
G_{k+1} y_{k}=\delta_{k} \tag{B.16}
\end{equation*}


按照拟牛顿条件选择 $G_{k}$ 作为 $H_{k}^{-1}$ 的近似或选择 $B_{k}$ 作为 $H_{k}$ 的近似的算法称为拟牛顿法。

按照拟牛顿条件, 在每次迭代中可以选择更新矩阵 $G_{k+1}$ :


\begin{equation*}
G_{k+1}=G_{k}+\Delta G_{k} \tag{B.17}
\end{equation*}


这种选择有一定的灵活性, 因此有多种具体实现方法。下面介绍 Broyden 类拟牛顿法。

\section*{3. DFP (Davidon-Fletcher-Powell) 算法 (DFP algorithm)}
DFP 算法选择 $G_{k+1}$ 的方法是假设每一步迭代中矩阵 $G_{k+1}$ 是由 $G_{k}$ 加上两个附加项构成的, 即


\begin{equation*}
G_{k+1}=G_{k}+P_{k}+Q_{k} \tag{B.18}
\end{equation*}


其中, $P_{k}, Q_{k}$ 是待定矩阵。这时,


\begin{equation*}
G_{k+1} y_{k}=G_{k} y_{k}+P_{k} y_{k}+Q_{k} y_{k} \tag{B.19}
\end{equation*}


为使 $G_{k+1}$ 满足拟牛顿条件, 可使 $P_{k}$ 和 $Q_{k}$ 满足:


\begin{gather*}
P_{k} y_{k}=\delta_{k}  \tag{B.20}\\
Q_{k} y_{k}=-G_{k} y_{k} \tag{B.21}
\end{gather*}


事实上, 不难找出这样的 $P_{k}$ 和 $Q_{k}$, 例如, 取


\begin{gather*}
P_{k}=\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}  \tag{B.22}\\
Q_{k}=-\frac{G_{k} y_{k} y_{k}^{\mathrm{T}} G_{k}}{y_{k}^{\mathrm{T}} G_{k} y_{k}} \tag{B.23}
\end{gather*}


这样就可得到矩阵 $G_{k+1}$ 的迭代公式:


\begin{equation*}
G_{k+1}=G_{k}+\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}-\frac{G_{k} y_{k} y_{k}^{\mathrm{T}} G_{k}}{y_{k}^{\mathrm{T}} G_{k} y_{k}} \tag{B.24}
\end{equation*}


称为 DFP 算法。

可以证明, 如果初始矩阵 $G_{0}$ 是正定的, 则迭代过程中的每个矩阵 $G_{k}$ 都是正定的。

DFP 算法如下。

\section*{算法 B. 2 (DFP 算法)}
输入: 目标函数 $f(x)$, 梯度 $g(x)=\nabla f(x)$, 精度要求 $\varepsilon$ 。

输出: $f(x)$ 的极小点 $x^{*}$ 。

(1) 选定初始点 $x^{(0)}$, 取 $G_{0}$ 为正定对称矩阵, 置 $k=0$ 。

(2) 计算 $g_{k}=g\left(x^{(k)}\right)$ 。若 $\left\|g_{k}\right\|<\varepsilon$, 则停止计算, 得近似解 $x^{*}=x^{(k)}$; 否则转步骤 (3)。

(3) 置 $p_{k}=-G_{k} g_{k}$ 。

(4) 一维搜索: 求 $\lambda_{k}$ 使得

$$
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left(x^{(k)}+\lambda p_{k}\right)
$$

(5) 置 $x^{(k+1)}=x^{(k)}+\lambda_{k} p_{k}$ 。

(6) 计算 $g_{k+1}=g\left(x^{(k+1)}\right)$, 若 $\left\|g_{k+1}\right\|<\varepsilon$, 则停止计算, 得近似解 $x^{*}=x^{(k+1)}$; 否则,按式 (B.24) 算出 $G_{k+1}$ 。

(7) 置 $k=k+1$, 转步骤 (3)。

\section*{4. BFGS (Broyden-Fletcher-Goldfarb-Shanno) 算法 (BFGS algorithm)}
BFGS 算法是最流行的拟牛顿算法。

可以考虑用 $G_{k}$ 逼近黑塞矩阵的逆矩阵 $H^{-1}$, 也可以考虑用 $B_{k}$ 逼近黑塞矩阵 $H$ 。这时,相应的拟牛顿条件是


\begin{equation*}
B_{k+1} \delta_{k}=y_{k} \tag{B.25}
\end{equation*}


可以用同样的方法得到另一迭代公式。首先令


\begin{gather*}
B_{k+1}=B_{k}+P_{k}+Q_{k}  \tag{B.26}\\
B_{k+1} \delta_{k}=B_{k} \delta_{k}+P_{k} \delta_{k}+Q_{k} \delta_{k} \tag{B.27}
\end{gather*}


考虑使 $P_{k}$ 和 $Q_{k}$ 满足:


\begin{gather*}
P_{k} \delta_{k}=y_{k}  \tag{B.28}\\
Q_{k} \delta_{k}=-B_{k} \delta_{k} \tag{B.29}
\end{gather*}


找出适合条件的 $P_{k}$ 和 $Q_{k}$, 得到 BFGS 算法矩阵 $B_{k+1}$ 的迭代公式:


\begin{equation*}
B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\mathrm{T}}}{y_{k}^{\mathrm{T}} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\mathrm{T}} B_{k} \delta_{k}} \tag{B.30}
\end{equation*}


可以证明, 如果初始矩阵 $B_{0}$ 是正定的, 则迭代过程中的每个矩阵 $B_{k}$ 都是正定的。

下面写出 BFGS 拟牛顿算法。

\section*{算法 B. 3 (BFGS 算法)}
输入: 目标函数 $f(x), g(x)=\nabla f(x)$, 精度要求 $\varepsilon$ 。

输出: $f(x)$ 的极小点 $x^{*}$ 。

(1) 选定初始点 $x^{(0)}$, 取 $B_{0}$ 为正定对称矩阵, 置 $k=0$ 。

(2) 计算 $g_{k}=g\left(x^{(k)}\right)$ 。若 $\left\|g_{k}\right\|<\varepsilon$, 则停止计算, 得近似解 $x^{*}=x^{(k)}$; 否则转步骤 (3)。

(3) 由 $B_{k} p_{k}=-g_{k}$ 求出 $p_{k}$ 。

(4) 一维搜索: 求 $\lambda_{k}$ 使得

$$
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left(x^{(k)}+\lambda p_{k}\right)
$$

(5) 置 $x^{(k+1)}=x^{(k)}+\lambda_{k} p_{k}$ 。

(6) 计算 $g_{k+1}=g\left(x^{(k+1)}\right)$, 若 $\left\|g_{k+1}\right\|<\varepsilon$, 则停止计算, 得近似解 $x^{*}=x^{(k+1)}$; 否则,按式 (B.30) 算出 $B_{k+1}$ 。

(7) 置 $k=k+1$, 转步骤 (3)。

\section*{5. Broyden 类算法 (Broyden’s algorithm)}
我们可以从 BFGS 算法矩阵 $B_{k}$ 的迭代公式 (B.30) 得到 BFGS 算法关于 $G_{k}$ 的迭代公式。事实上, 若记 $G_{k}=B_{k}^{-1}, G_{k+1}=B_{k+1}^{-1}$, 那么对式 (B.30) 两次应用 Sherman-Morrison 公式 ${ }^{1}$ 即得:


\begin{equation*}
G_{k+1}=\left(I-\frac{\delta_{k} y_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}\right) G_{k}\left(I-\frac{\delta_{k} y_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}\right)^{\mathrm{T}}+\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}} \tag{B.31}
\end{equation*}


称为 BFGS 算法关于 $G_{k}$ 的迭代公式。

由 DFP 算法 $G_{k}$ 的迭代公式 (B.23) 得到的 $G_{k+1}$ 记作 $G^{\mathrm{DFP}}$, 由 BFGS 算法 $G_{k}$ 的迭代公式 (B.31) 得到的 $G_{k+1}$ 记作 $G^{\mathrm{BFGS}}$, 它们都满足方程拟牛顿条件式, 所以它们的线性组合


\begin{equation*}
G_{k+1}=\alpha G^{\mathrm{DFP}}+(1-\alpha) G^{\mathrm{BFGS}} \tag{B.32}
\end{equation*}


也满足拟牛顿条件式, 而且是正定的。其中 $0 \leqslant \alpha \leqslant 1$ 。这样就得到了一类拟牛顿法, 称为 Broyden 类算法。

(1) Sherman-Morrison 公式: 假设 $A$ 是 $n$ 阶可逆矩阵, $u, v$ 是 $n$ 维向量, 且 $A+u v^{\mathrm{T}}$ 也是可逆矩阵, 则

$$
\left(A+u v^{\mathrm{T}}\right)^{-1}=A^{-1}-\frac{A^{-1} u v^{\mathrm{T}} A^{-1}}{1+v^{\mathrm{T}} A^{-1} u}
$$

\section*{附录 C 拉格朗日对偶性}
在约束最优化问题中, 常常利用拉格朗日对偶性 (Lagrange duality) 将原始问题转换为对偶问题, 通过解对偶问题得到原始问题的解。该方法应用在许多统计学习方法中, 例如, 最大熵模型与支持向量机。这里简要叙述拉格朗日对偶性的主要概念和结果。

\section*{1. 原始问题}
假设 $f(x), c_{i}(x), h_{j}(x)$ 是定义在 $\boldsymbol{R}^{n}$ 上的连续可微函数。考虑约束最优化问题:

\[
\begin{array}{ll}
\min _{x \in \boldsymbol{R}^{n}} & f(x) \\
\text { s.t. } & c_{i}(x) \leqslant 0, \quad i=1,2, \cdots, k \\
& h_{j}(x)=0, \quad j=1,2, \cdots, l \tag{C.3}
\end{array}
\]

称此约束最优化问题为原始最优化问题或原始问题。

首先, 引入广义拉格朗日函数 (generalized Lagrange function):


\begin{equation*}
L(x, \alpha, \beta)=f(x)+\sum_{i=1}^{k} \alpha_{i} c_{i}(x)+\sum_{j=1}^{l} \beta_{j} h_{j}(x) \tag{C.4}
\end{equation*}


这里, $x=\left(x^{(1)}, x^{(2)}, \cdots, x^{(n)}\right)^{\mathrm{T}} \in \boldsymbol{R}^{n}, \alpha_{i}, \beta_{j}$ 是拉格朗日乘子, $\alpha_{i} \geqslant 0$ 。考虑 $x$ 的函数:


\begin{equation*}
\theta_{P}(x)=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha, \beta) \tag{C.5}
\end{equation*}


这里, 下标 $P$ 表示原始问题。

假设给定某个 $x$ 。如果 $x$ 违反原始问题的约束条件, 即存在某个 $i$ 使得 $c_{i}(x)>0$ 或者存在某个 $j$ 使得 $h_{j}(x) \neq 0$, 那么就有


\begin{equation*}
\theta_{P}(x)=\max _{\alpha, \beta: \alpha_{i} \geqslant 0}\left[f(x)+\sum_{i=1}^{k} \alpha_{i} c_{i}(x)+\sum_{j=1}^{l} \beta_{j} h_{j}(x)\right]=+\infty \tag{C.6}
\end{equation*}


因为若某个 $i$ 使约束 $c_{i}(x)>0$, 则可令 $\alpha_{i} \rightarrow+\infty$; 若某个 $j$ 使 $h_{j}(x) \neq 0$, 则可令 $\beta_{j}$ 使 $\beta_{j} h_{j}(x) \rightarrow+\infty$, 而将其余各 $\alpha_{i}, \beta_{j}$ 均取为 0 。

相反地, 如果 $x$ 满足约束条件式 (C.2) 和式 (C.3), 则由式 (C.5) 和式 (C.4) 可知, $\theta_{P}(x)=$ $f(x)$ 。 因此,

\[
\theta_{P}(x)= \begin{cases}f(x), & x \text { 满足原始问题约束 }  \tag{C.7}\\ +\infty, & \text { 其他 }\end{cases}
\]

所以如果考虑极小化问题:


\begin{equation*}
\min _{x} \theta_{P}(x)=\min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha, \beta) \tag{C.8}
\end{equation*}


它是与原始最优化问题 (C.1) (C.3) 等价的, 即它们有相同的解。问题 $\min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha, \beta)$称为广义拉格朗日函数的极小极大问题。这样一来, 就把原始最优化问题表示为广义拉格朗日函数的极小极大问题。为了方便, 定义原始问题的最优值:


\begin{equation*}
p^{*}=\min _{x} \theta_{P}(x) \tag{C.9}
\end{equation*}


称为原始问题的值。

\section*{2. 对偶问题}
定义


\begin{equation*}
\theta_{\mathrm{D}}(\alpha, \beta)=\min _{x} L(x, \alpha, \beta) \tag{C.10}
\end{equation*}


再考虑极大化 $\theta_{\mathrm{D}}(\alpha, \beta)=\min _{x} L(x, \alpha, \beta)$, 即


\begin{equation*}
\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \theta_{\mathrm{D}}(\alpha, \beta)=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L(x, \alpha, \beta) \tag{C.11}
\end{equation*}


问题 $\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L(x, \alpha, \beta)$ 称为广义拉格朗日函数的极大极小问题。

可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题:


\begin{gather*}
\max _{\alpha, \beta} \theta_{\mathrm{D}}(\alpha, \beta)=\max _{\alpha, \beta} \min _{x} L(x, \alpha, \beta)  \tag{C.12}\\
\text { s.t. } \quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, k \tag{C.13}
\end{gather*}


称为原始问题的对偶问题。定义对偶问题的最优值:


\begin{equation*}
d^{*}=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \theta_{\mathrm{D}}(\alpha, \beta) \tag{C.14}
\end{equation*}


称为对偶问题的值。

\section*{3. 原始问题和对偶问题的关系}
下面讨论原始问题和对偶问题的关系。

定理 C. 1 若原始问题和对偶问题都有最优值, 则


\begin{equation*}
d^{*}=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L(x, \alpha, \beta) \leqslant \min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha, \beta)=p^{*} \tag{C.15}
\end{equation*}


证明 由式 (C.12) 和式 (C.5) 知, 对任意的 $\alpha, \beta$ 和 $x$, 有


\begin{equation*}
\theta_{\mathrm{D}}(\alpha, \beta)=\min _{x} L(x, \alpha, \beta) \leqslant L(x, \alpha, \beta) \leqslant \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha, \beta)=\theta_{P}(x) \tag{C.16}
\end{equation*}


即


\begin{equation*}
\theta_{\mathrm{D}}(\alpha, \beta) \leqslant \theta_{P}(x) \tag{C.17}
\end{equation*}


由于原始问题和对偶问题均有最优值，所以，


\begin{equation*}
\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \theta_{\mathrm{D}}(\alpha, \beta) \leqslant \min _{x} \theta_{P}(x) \tag{C.18}
\end{equation*}


即


\begin{equation*}
d^{*}=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L(x, \alpha, \beta) \leqslant \min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha, \beta)=p^{*} \tag{C.19}
\end{equation*}


推论 C. 1 设 $x^{*}$ 和 $\alpha^{*}, \beta^{*}$ 分别是原始问题 (C.1) (C.3) 和对偶问题 (C.12) (C.13) 的可行解, 并且 $d^{*}=p^{*}$, 则 $x^{*}$ 和 $\alpha^{*}, \beta^{*}$ 分别是原始问题和对偶问题的最优解。

在某些条件下, 原始问题和对偶问题的最优值相等, $d^{*}=p^{*}$ 。这时可以用解对偶问题替代解原始问题。下面以定理的形式叙述有关的重要结论而不予证明。

定理 C. 2 考虑原始问题 (C.1) (C.3) 和对偶问题 (C.12) (C.13)。假设函数 $f(x)$ 和 $c_{i}(x)$ 是凸函数, $h_{j}(x)$ 是仿射函数; 并且假设不等式约束 $c_{i}(x)$ 是严格可行的, 即存在 $x$, 对所有 $i$ 有 $c_{i}(x)<0$, 则存在 $x^{*}, \alpha^{*}, \beta^{*}$, 使 $x^{*}$ 是原始问题的解, $\alpha^{*}, \beta^{*}$ 是对偶问题的解, 并且


\begin{equation*}
p^{*}=d^{*}=L\left(x^{*}, \alpha^{*}, \beta^{*}\right) \tag{C.20}
\end{equation*}


定理 C. 3 对原始问题 (C.1) (C.3) 和对偶问题 (C.12) (C.13), 假设函数 $f(x)$ 和 $c_{i}(x)$是凸函数, $h_{j}(x)$ 是仿射函数, 并且不等式约束 $c_{i}(x)$ 是严格可行的, 则 $x^{*}$ 和 $\alpha^{*}, \beta^{*}$ 分别是原始问题和对偶问题的解的充分必要条件是 $x^{*}, \alpha^{*}, \beta^{*}$ 满足下面的 Karush-Kuhn-Tucker (KKT) 条件:


\begin{gather*}
\nabla_{x} L\left(x^{*}, \alpha^{*}, \beta^{*}\right)=0  \tag{C.21}\\
\alpha_{i}^{*} c_{i}\left(x^{*}\right)=0, \quad i=1,2, \cdots, k  \tag{C.22}\\
c_{i}\left(x^{*}\right) \leqslant 0, \quad i=1,2, \cdots, k  \tag{C.23}\\
\alpha_{i}^{*} \geqslant 0, \quad i=1,2, \cdots, k  \tag{C.24}\\
h_{j}\left(x^{*}\right)=0, \quad j=1,2, \cdots, l \tag{C.25}
\end{gather*}


特别指出, 式 (C.22) 称为 KKT 的对偶互补条件。由此条件可知: 若 $\alpha_{i}^{*}>0$, 则 $c_{i}\left(x^{*}\right)=0$ 。

\section*{附录 D 矩阵的基本子空间}
简要介绍本书用到的矩阵的基本子空间相关的定义和定理。

\section*{1. 向量空间的子空间}
若 $S$ 是向量空间 $V$ 的非空子集，且 $S$ 满足以下条件:

(1) 对任意实数 $a$, 若 $x \in S$, 则 $a x \in S$;

(2) 若 $x \in S$ 且 $y \in S$, 则 $x+y \in S$ 。

则 $S$ 称为 $V$ 的子空间。

设 $v_{1}, v_{2}, \cdots, v_{n}$ 为向量空间 $V$ 中的向量, 则其线性组合

$$
a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}
$$

构成 $V$ 的子空间, 称为 $v_{1}, v_{2}, \cdots, v_{n}$ 张成 (span） 的子空间, 或 $v_{1}, v_{2}, \cdots, v_{n}$ 的张成, 记作

$$
\operatorname{span}\left(v_{1}, v_{2}, \cdots, v_{n}\right) \text { 。 }
$$

如果 $\operatorname{span}\left\{v_{1}, v_{2}, \cdots, v_{n}\right\}=V$, 就说 $v_{1}, v_{2}, \cdots, v_{n}$ 张成 $V$ 。

\section*{2. 向量空间的基和维数}
向量空间 $V$ 中的向量 $v_{1}, v_{2}, \cdots, v_{n}$ 称为空间 $V$ 的基, 如果满足条件

(1) $v_{1}, v_{2}, \cdots, v_{n}$ 线性无关;

(2) $v_{1}, v_{2}, \cdots, v_{n}$ 张成 $V$ 。

反之亦然, 则向量空间的基的个数即向量空间的维数。

\section*{3. 矩阵的行空间和列空间}
设 $A$ 为 $m \times n$ 矩阵。 $A$ 的每一行可以看作是 $\boldsymbol{R}^{n}$ 中的一个向量, 称为 $A$ 的行向量。类似地, $A$ 的每一列可以看作是 $\boldsymbol{R}^{m}$ 中的一个向量, 称为 $A$ 的列向量。

设 $A$ 为 $m \times n$ 矩阵, 则由 $A$ 的行向量张成的 $\boldsymbol{R}^{n}$ 的子空间称为 $A$ 的行空间; 由 $A$ 的列向量张成的 $\boldsymbol{R}^{m}$ 的子空间称为 $A$ 的列空间。

矩阵 $A$ 的行空间的维数等于列空间的维数。

一个矩阵的行空间的维数 (等价地列空间的维数) 称为矩阵的秩。

\section*{4. 矩阵的零空间}
设 $A$ 为 $m \times n$ 矩阵, 令 $N(A)$ 为齐次方程组 $A x=0$ 的所有解的集合, 则 $N(A)$ 为 $\boldsymbol{R}^{n}$\\
的一个子空间, 称为 $A$ 的零空间 (null space）, 即


\begin{equation*}
N(A)=\left\{x \in \boldsymbol{R}^{n} \mid A x=0\right\} \tag{D.1}
\end{equation*}


一个矩阵的零空间的维数称为矩阵的零度。

秩-零度定理。设 $A$ 为 $m \times n$ 矩阵, 则 $A$ 的秩与 $A$ 的零度之和为 $n$ 。事实上, 若 $A$ 的秩为 $r$, 则方程组 $A x=0$ 的独立变量的个数为 $r$, 自由变量的个数为 $(n-r) \circ N(A)$ 的维数等于自由变量的个数。所以定理成立。

\section*{5. 子空间的正交补}
设 $X$ 和 $Y$ 为 $\boldsymbol{R}^{n}$ 的子空间, 若对每一 $x \in X$ 和 $y \in Y$ 都满足 $x^{\mathrm{T}} y=0$, 则称 $X$ 和 $Y$是正交的, 记作 $X \perp Y$ 。

令 $Y$ 为 $\boldsymbol{R}^{n}$ 的子空间, $\boldsymbol{R}^{n}$ 中与 $Y$ 中的每一向量正交的向量集合记作 $Y^{\perp}$, 即


\begin{equation*}
Y^{\perp}=\left\{x \in \boldsymbol{R}^{n} \mid x^{\mathrm{T}} y=0, \forall y \in Y\right\} \tag{D.2}
\end{equation*}


集合 $Y^{\perp}$ 称为 $Y$ 的正交补。

可以证明, 若 $Y$ 是 $\boldsymbol{R}^{n}$ 的子空间, 则 $Y^{\perp}$ 也是 $\boldsymbol{R}^{n}$ 的子空间。

\section*{6. 矩阵的基本子空间}
设 $A$ 为 $m \times n$ 矩阵, 可以将 $A$ 看成是将 $\boldsymbol{R}^{n}$ 映射到 $\boldsymbol{R}^{m}$ 的线性变换。一个向量 $z \in \boldsymbol{R}^{m}$在 $A$ 的列空间的充要条件是存在 $x \in \boldsymbol{R}^{n}$, 使得 $z=A x$ 。这样 $A$ 的列空间和 $A$ 的值域是相同的。记 $A$ 的值域为 $R(A)$, 则


\begin{align*}
R(A) & =\left\{z \in \boldsymbol{R}^{m} \mid \exists x \in \boldsymbol{R}^{n}, z=A x\right\} \\
& =A \text { 的列空间 } \tag{D.3}
\end{align*}


类似地, 一个向量 $y \in \boldsymbol{R}^{n}, y^{\mathrm{T}}$ 在 $A$ 的行空间的充要条件是存在 $x \in \boldsymbol{R}^{m}$, 使得 $y=A^{\mathrm{T}} x$ 。这样 $A$ 的行空间和 $A^{\mathrm{T}}$ 的值域 $R\left(A^{\mathrm{T}}\right)$ 是相同的。


\begin{align*}
R\left(A^{\mathrm{T}}\right) & =\left\{y \in \boldsymbol{R}^{n} \mid \exists x \in \boldsymbol{R}^{m}, y=A^{\mathrm{T}} x\right\} \\
& =A \text { 的行空间 } \tag{D.4}
\end{align*}


矩阵 $A$ 有四个基本子空间: 列空间、行空间、零空间、 $A$ 的转置零空间（左零空间）。有下面的定理成立。

定理 D. 1 若 $A$ 为 $m \times n$ 矩阵, 则 $N(A)=R\left(A^{\mathrm{T}}\right)^{\perp}$, 且 $N\left(A^{\mathrm{T}}\right)=R(A)^{\perp}$ 。

证明 容易验证 $R\left(A^{\mathrm{T}}\right) \perp N(A)$ 。由于 $R\left(A^{\mathrm{T}}\right) \perp N(A)$, 故得 $N(A) \subset R\left(A^{\mathrm{T}}\right)^{\perp}$ 。另一方面,若 $x$ 为 $R\left(A^{\mathrm{T}}\right)^{\perp}$ 中的任何向量, 则 $x$ 和 $A^{\mathrm{T}}$ 的每一个列向量正交。因此, 可得 $A x=0$ 。于是 $x$ 必为 $N(A)$ 的元素, 由此得到:


\begin{equation*}
N(A)=R\left(A^{\mathrm{T}}\right)^{\perp} \tag{D.5}
\end{equation*}


类似可得


\begin{equation*}
N\left(A^{\mathrm{T}}\right)=R(A)^{\perp} \tag{D.6}
\end{equation*}


图 D. 1 示意了矩阵的基本子空间之间的关系。

\begin{center}
\includegraphics[max width=\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-39}
\end{center}

图 D. 1 矩阵的基本子空间之间的关系

\section*{附录 E KL 散度的定义和狄利克雷分布的性质}
\section*{1. KL 散度的定义}
首先给出 KL 散度 (KL divergence, Kullback-Leibler divergence) 的定义。KL 散度是描述两个概率分布 $Q(x)$ 和 $P(x)$ 相似度的一种度量, 记作 $D(Q \| P)$ 。对离散随机变量, KL 散度定义为


\begin{equation*}
D(Q \| P)=\sum_{i} Q(i) \log \frac{Q(i)}{P(i)} \tag{E.1}
\end{equation*}


对连续随机变量, KL 散度定义为


\begin{equation*}
D(Q \| P)=\int Q(x) \log \frac{Q(x)}{P(x)} \mathrm{d} x \tag{E.2}
\end{equation*}


容易证明 KL 散度具有性质: $D(Q \| P) \geqslant 0$ 。当且仅当 $Q=P$ 时, $D(Q \| P)=0$ 。事实上,利用 Jensen 不等式即得:


\begin{align*}
-D(Q \| P) & =\int Q(x) \log \frac{P(x)}{Q(x)} \mathrm{d} x \\
& \leqslant \log \int Q(x) \frac{P(x)}{Q(x)} \mathrm{d} x \\
& =\log \int P(x) \mathrm{d} x=0 \tag{E.3}
\end{align*}


KL 散度是非对称的, 也不满足三角不等式, 不是严格意义上的距离度量。

\section*{2. 狄利克雷分布的性质}
设随机变量 $\theta$ 服从狄利克雷分布 $\theta \sim \operatorname{Dir}(\theta \mid \alpha)$, 利用指数分布族性质, 求函数 $\log \theta$ 的关于狄利克雷分布的数学期望 $E[\log \theta]$ 。

指数分布族是指概率分布密度可以写成如下形式的概率分布集合:


\begin{equation*}
p(x \mid \eta)=h(x) \exp \left(\eta^{\mathrm{T}} T(x)-A(\eta)\right) \tag{E.4}
\end{equation*}


其中, $\eta$ 是自然参数, $T(x)$ 是充分统计量, $h(x)$ 是潜在测度, $A(\eta)$ 是对数规范化因子 $A(\eta)=$ $\log \int h(x) \exp \left(\eta^{\mathrm{T}} T(x)\right) \mathrm{d} x$ 。


\end{document}